<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine-learning on LESS IS MORE</title>
    <link>http://r9y9.github.io/categories/machine-learning/</link>
    <description>Recent content in machine-learning on LESS IS MORE</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja-jp</language>
    <lastBuildDate>Sun, 19 Oct 2014 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="http://r9y9.github.io/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Naive Bayesの復習（導出編）</title>
      <link>http://r9y9.github.io/blog/2013/07/28/naive-bayes-formulation/</link>
      <pubDate>Sun, 28 Jul 2013 00:00:00 +0000</pubDate>
      
      <guid>http://r9y9.github.io/blog/2013/07/28/naive-bayes-formulation/</guid>
      <description>すぐ忘れるのでメモ。ナイーブベイズの学習アルゴリズムの導出とか、そもそもナイーブベイズが定番過ぎて意外とやったことなかった気もするので、復習がてらやってみた。
ちょっと修正 2013/07/30
 ナイーブベイズについて整理 学習アルゴリズムの導出  Naive bayes （ナイーブベイズ） スパムフィルタで使われたことで有名な確率モデルで、シンプルだけどそこそこ実用的なのが良い所。Naive bayesという名前は、特徴ベクトル間に条件付き独立性を仮定してることにある（実際は相関あることが多いけど、まぁ簡単のためって感じ）。具体的に例を挙げて言うと、例えば文書分類タスクの場合、各単語は独立に生起するという仮定を置くことに相当する。
まずはモデルを書き下す。入力データを$\mathbf{x}$（D次元）、ラベルを$y$（離散値）とすると、ナイーブベイズでは以下のように同時確率をモデル化する。
 \begin{align} p(\mathbf{x}, y) &amp;= p(y)p(\mathbf{x}|y)\\ &amp;= p(y)p(x_{1}, x_{2}, \dots, x_{D}|y)\\ &amp;= p(y)\prod_{d=1}^{D} p(x_{d}|y) \end{align}  カンタン。基本的にdは次元に対するインデックス、nはデータに対するインデックスとして書く。
ポイントは特徴ベクトル間に条件付き独立性の仮定を置いていること（二度目）で、それによってパラメータの数が少なくて済む。
分類 一番確率の高いラベルを選べばいい。数式で書くと以下のようになる。
 \begin{align} \hat{y} &amp;= \argmax_{y} [p(y|\mathbf{x})]\\ &amp;= \argmax_{y} [p(\mathbf{x}, y)]\\ &amp;= \argmax_{y} \Bigl[ p(y)\prod_{d=1}^{D} p(x_{d}|y)\Bigr] \end{align}  argmaxを取る上では、$y$に依存しない項は無視していいので、事後確率の最大化は、同時確率の最大化に等しくなる。
学習アルゴリズムの導出 ここからが本番。学習データを$X = {\mathbf{x}_{n}}_{n=1}^{N}$、対応する正解ラベルを$Y = {y_n}_{n=1}^{N} $として、最尤推定により学習アルゴリズムを導出する。実際はMAP推定をすることが多いけど、今回は省略。拡張は簡単。
尤度関数 各サンプルが独立に生起したと仮定すると、尤度関数は以下のように書ける。
 \begin{align} L(X,Y; \mathbf{\theta}) &amp;= \prod_{n=1}^{N}p(y_{n})p(\mathbf{x_{n}}|y_{n})\\ &amp;= \prod_{n=1}^{N} \Bigl[ p(y_{n})\prod_{d=1}^{D}p(x_{nd}|y_{n})\Bigr] \end{align}  対数を取って、</description>
    </item>
    
  </channel>
</rss>