<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Hugo 0.55.6" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">
<link rel="stylesheet" href="/css/normalize.css">
<link rel="stylesheet" href="/css/skeleton.css">
<link rel="stylesheet" href="/css/custom.css">
<link rel="alternate" href="/index.xml" type="application/rss+xml" title="LESS IS MORE">
<link rel="shortcut icon" href="/favicon.png" type="image/x-icon" />
<title>統計的声質変換クッソムズすぎワロタ（チュートリアル編） - LESS IS MORE</title>
</head>
<body>

<div class="container">

	<header role="banner">
		<div class="header-logo">
			<a href="/"><img src="/images/r9y9.jpg" width="70" height="70"></a>
		</div>
		
	</header>


	<main role="main">
		<article itemscope itemtype="https://schema.org/BlogPosting">
			<h1 class="entry-title" itemprop="headline">統計的声質変換クッソムズすぎワロタ（チュートリアル編）</h1>
			<span class="entry-meta"><time itemprop="datePublished" datetime="2014-11-12">November 12, 2014</time></span>
			<section itemprop="entry-text">
				

<h2 id="はじめに">はじめに</h2>

<p>こんばんは。統計的声質変換（以降、簡単に声質変換と書きます）って面白いなーと思っているのですが、興味を持つ人が増えたらいいなと思い、今回は簡単なチュートリアルを書いてみます。間違っている箇所があれば、指摘してもらえると助かります。よろしくどうぞ。</p>

<p>前回の記事（<a href="http://r9y9.github.io/blog/2014/07/13/statistical-voice-conversion-wakaran/">統計的声質変換クッソムズすぎワロタ（実装の話） - LESS IS MORE</a>）では変換部分のコードのみを貼りましたが、今回はすべてのコードを公開します。なので、記事内で示す声質変換の結果を、この記事を読んでいる方が再現することも可能です。対象読者は、特に初学者の方で、声質変換を始めたいけれど論文からコードに落とすにはハードルが高いし、コードを動かしながら仕組みを理解していきたい、という方を想定しています。役に立てば幸いです。</p>

<h2 id="コード">コード</h2>

<p><a href="https://github.com/r9y9/VoiceConversion.jl">https://github.com/r9y9/VoiceConversion.jl</a></p>

<p><a href="http://julialang.org">Julia</a> という言語で書かれています。Juliaがどんな言語かをさっと知るのには、以下のスライドがお勧めです。人それぞれ好きな言語で書けばいいと思いますが、個人的にJuliaで書くことになった経緯は、最後の方に簡単にまとめました。</p>

<div align="center"><iframe src="//www.slideshare.net/slideshow/embed_code/39141184" width="425" height="355" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/kentaroiizuka/julia-39141184" title="プログラミング言語 Julia の紹介" target="_blank">プログラミング言語 Julia の紹介</a> </strong> from <strong><a href="//www.slideshare.net/kentaroiizuka" target="_blank">Kentaro Iizuka</a></strong> </div></div>

<h2 id="サードパーティライブラリ">サードパーティライブラリ</h2>

<p>声質変換は多くのコンポーネントによって成り立っていますが、すべてを自分で書くのは現実的ではありません。僕は、主に以下のライブラリを活用しています。</p>

<ul>
<li><a href="http://ml.cs.yamanashi.ac.jp/world/">WORLD</a> - 音声分析合成のフレームワークとして、あるいは単にスペクトル包絡を抽出するツールとして使っています。<a href="https://github.com/r9y9/WORLD.jl">Juliaラッパー</a>を書きました。</li>
<li><a href="sp-tk.sourceforge.net">SPTK</a> - メル対数スペクトル近似（Mel-Log Spectrum Approximation; MLSA）フィルタを変換処理に使っています。これも<a href="https://github.com/r9y9/SPTK.jl">Juliaラッパー</a>を書きました。</li>
<li><a href="http://scikit-learn.org/stable/">sklearn</a> - sklearn.mixture をGMMの学習に使っています。pythonのライブラリは、juliaから簡単に呼べます。</li>
</ul>

<p>音声分析合成に関しては、アカデミック界隈ではよく使われている<a href="http://www.wakayama-u.ac.jp/~kawahara/STRAIGHTadv/index_j.html">STRAIGHT</a>がありますが、WORLDの方がライセンスもゆるくソースも公開されていて、かつ性能も劣らない（正確な話は、森勢先生の論文を参照してください）ので、おすすめです。</p>

<h2 id="voiceconversion-jl-https-github-com-r9y9-voiceconversion-jl-でできること"><a href="https://github.com/r9y9/VoiceConversion.jl">VoiceConversion.jl</a> でできること</h2>

<h3 id="追記-2015-01-07">追記 2015/01/07</h3>

<p>この記事を書いた段階のv0.0.1は、依存ライブラリの変更のため、現在は動きません。すみません。何のためのタグだ、という気がしてきますが、、最低限masterは動作するようにしますので、そちらをお試しください（基本的には、新しいコードの方が改善されています）。それでも動かないときは、issueを投げてください。</p>

<p>2014/11/10現在（v0.0.1のタグを付けました）、できることは以下の通りです（外部ライブラリを叩いているものを含む）。</p>

<ul>
<li>音声波形からのメルケプストラムの抽出</li>
<li>DPマッチングによるパラレルデータの作成</li>
<li>GMMの学習</li>
<li>GMMベースのframe-by-frame特徴量変換</li>
<li>GMMベースのtrajectory特徴量変換</li>
<li>GMMベースのtrajectory特徴量変換（GV考慮版）</li>
<li>音声分析合成系WORLDを使った声質変換</li>
<li>MLSAフィルタを使った差分スペクトルに基づく声質変換</li>
</ul>

<p>これらのうち、trajectory変換以外を紹介します。</p>

<h2 id="チュートリアル-cmu-arcticを使ったgmmベースの声質変換-特徴抽出からパラレルデータの作成-gmmの学習-変換-合成処理まで">チュートリアル：CMU_ARCTICを使ったGMMベースの声質変換（特徴抽出からパラレルデータの作成、GMMの学習、変換・合成処理まで）</h2>

<p>データセットに<a href="http://festvox.org/cmu_arctic/">CMU_ARCTIC</a>を使って、GMMベースの声質変換（clb -&gt; slt）を行う方法を説明します。なお、VoiceConversion.jl のv0.0.1を使います。ubuntuで主に動作確認をしていますが、macでも動くと思います。</p>

<h2 id="0-前準備">0. 前準備</h2>

<h3 id="0-1-データセットのダウンロード">0.1. データセットのダウンロード</h3>

<p><a href="http://festvox.org/cmu_arctic/">Festvox: CMU_ARCTIC Databases</a> を使います。コマンド一発ですべてダウンロードする<a href="https://gist.github.com/r9y9/ff67c05aeb87410eae2e">スクリプト</a>を書いたので、ご自由にどうぞ。</p>

<h3 id="0-2-juliaのインストール">0.2. juliaのインストール</h3>

<p><a href="http://julialang.org/">公式サイト</a>からバイナリをダウンロードするか、<a href="https://github.com/JuliaLang/julia">githubのリポジトリ</a>をクローンしてビルドしてください。バージョンは、現在の最新安定版のv0.3.2を使います。</p>

<p>記事内では、juliaの基本的な使い方については解説しないので、前もってある程度調べておいてもらえると、スムーズに読み進められるかと思います。</p>

<h3 id="0-3-voiceconversion-jl-のインストール">0.3. VoiceConversion.jl のインストール</h3>

<p>juliaを起動して、以下のコマンドを実行してください。</p>

<pre><code class="language-julia">julia&gt; Pkg.clone(&quot;https://github.com/r9y9/VoiceConversion.jl&quot;)
julia&gt; Pkg.build(&quot;VoiceConversion&quot;)
</code></pre>

<p>サードパーティライブラリは、sklearnを除いてすべて自動でインストールされます。sklearnは、例えば以下のようにしてインストールしておいてください。</p>

<pre><code class="language-bash">sudo pip install sklearn
</code></pre>

<p>これで準備は完了です！</p>

<h2 id="1-音声波形からのメルケプストラムの抽出">1. 音声波形からのメルケプストラムの抽出</h2>

<p>まずは、音声から声質変換に用いる特徴量を抽出します。特徴量としては、声質変換や音声合成の分野で広く使われているメルケプストラムを使います。メルケプストラムの抽出は、<code>scripts/mcep.jl</code> を使うことでできます。</p>

<h3 id="2014-11-15-追記">2014/11/15 追記</h3>

<p>実行前に、<code>julia&gt; Pkg.add(&quot;WAV&quot;)</code> として、WAVパッケージをインストールしておいてください。(2014/11/15時点のmasterでは自動でインストールされますが、v0.0.1ではインストールされません、すいません）。また、メルケプストラムの出力先ディレクトリは事前に作成しておいてください（最新のスクリプトでは自動で作成されます）。</p>

<p>以下のようにして、2話者分の特徴量を抽出しましょう。以下のスクリプトでは、 <code>~/data/cmu_arctic/</code> にデータがあることを前提としています。</p>

<pre><code class="language-bash"># clb
julia mcep.jl ~/data/cmu_arctic/cmu_us_clb_arctic/wav/ ~/data/cmu_arctic_jld/speakers/clb/
# slt
julia mcep.jl ~/data/cmu_arctic/cmu_us_slt_arctic/wav/ ~/data/cmu_arctic_jld/speakers/slt/
</code></pre>

<p>基本的な使い方は、<code>mcep.jl &lt;wavファイルがあるディレクトリ&gt; &lt;メルケプストラムが出力されるディレクトリ&gt;</code> になっています。オプションについては、 <code>mcep.jl -h</code> としてヘルプを見るか、コードを直接見てください。</p>

<p>抽出されたメルケプストラムは、HDF5フォーマットで保存されます。メルケプストラムの中身を見てみると、以下のような感じです。可視化には、PyPlotパッケージが必要です。Juliaを開いて、<code>julia&gt; Pkg.add(&quot;PyPlot&quot;)</code> とすればOKです。IJuliaを使いたい場合（僕は使っています）は、<code>julia&gt; Pkg.add(&quot;IJulia&quot;)</code> としてIJuliaもインストールしておきましょう。</p>

<pre><code class="language-julia"># メルケプストラムの可視化

using HDF5, JLD, PyPlot

x = load(&quot;clb/arctic_a0028.jld&quot;)

figure(figsize=(16, 6), dpi=80, facecolor=&quot;w&quot;, edgecolor=&quot;k&quot;)
imshow(x[&quot;feature_matrix&quot;], origin=&quot;lower&quot;, aspect=&quot;auto&quot;)
colorbar()
</code></pre>

<div align="center"><img src="/images/clb_a0028_melcepstrum.png "Mel-cepstrum of clb_a0028."" class="image"></div>

<p>0次成分だけ取り出してみると、以下のようになります。</p>

<pre><code class="language-julia"># メルケプストラムの0次成分のみを可視化

figure(figsize=(16, 6), dpi=80, facecolor=&quot;w&quot;, edgecolor=&quot;k&quot;)
plot(vec(x[&quot;feature_matrix&quot;][1,:]), linewidth=2.0, label=&quot;0th order mel-cesptrum of clb_a0028&quot;)
xlim(0, size(x[&quot;feature_matrix&quot;], 2)-10) # 末尾がsilenceだった都合上…（決め打ち）
xlabel(&quot;Frame&quot;)
legend(loc=&quot;upper right&quot;)
ylim(-10, -2) # 見やすいように適当に決めました
</code></pre>

<div align="center"><img src="/images/clb_a0028_melcepstrum_0th.png "Mel-cepstrum of clb_a0028 0th."" class="image"></div>

<p>こんな感じです。話者clbの<code>clb_a0028.wav</code>を聞きながら、特徴量見てみてください。0次の成分からは、音量の大小が読み取れると思います。</p>

<h2 id="2-dpマッチングによるパラレルデータの作成">2. DPマッチングによるパラレルデータの作成</h2>

<p>次に、2話者分の特徴量を時間同期して連結します。基本的に声質変換では、音韻の違いによらない特徴量（非言語情報）の対応関係を学習するために、同一発話内容の特徴量を時間同期し（音韻の違いによる変動を可能な限りなくすため）、学習データとして用います。このデータのことを、パラレルデータと呼びます。</p>

<p>パラレルデータの作成には、DPマッチングを使うのが一般的です。<code>scripts/align.jl</code> を使うとできます。</p>

<pre><code class="language-bash">julia align.jl ~/data/cmu_arctic_jld/speakers/clb ~/data/cmu_arctic_jld/speakers/slt ~/data/cmu_arctic_jld/parallel/clb_and_slt/
</code></pre>

<p>使い方は、<code>align.jl &lt;話者1（clb）の特徴量のパス&gt; &lt;話者2（slt）の特徴量のパス&gt; &lt;パラレルデータの出力先&gt;</code> になっています。</p>

<p>きちんと時間同期されているかどうか、0次成分を見て確認してみましょう。</p>

<p>時間同期を取る前のメルケプストラムを以下に示します。</p>

<pre><code class="language-julia"># 時間同期前のメルケプストラム（0次）を可視化

x = load(&quot;clb/arctic_a0028.jld&quot;)
y = load(&quot;slt/arctic_a0028.jld&quot;)

figure(figsize=(16, 6), dpi=80, facecolor=&quot;w&quot;, edgecolor=&quot;k&quot;)
plot(vec(x[&quot;feature_matrix&quot;][1,:]), linewidth=2.0, label=&quot;0th order mel-cesptrum of clb_a0028&quot;)
plot(vec(y[&quot;feature_matrix&quot;][1,:]), linewidth=2.0, label=&quot;0th order mel-cesptrum of slt_a0028&quot;)
xlim(0, min(size(x[&quot;feature_matrix&quot;], 2), size(y[&quot;feature_matrix&quot;], 2))-10) # 決め打ち
xlabel(&quot;Frame&quot;)
legend(loc=&quot;upper right&quot;)
ylim(-10, -2) # 決め打ち
</code></pre>

<div align="center"><img src="/images/clb_and_slt_a0028_melcepstrum_0th.png "0th order mel-cepstrum (not aligned)"" class="image"></div>

<p>ちょっとずれてますね</p>

<p>次に、時間同期後のメルケプストラムを示します。</p>

<pre><code class="language-julia"># 時間同期後のメルケプストラム（0次）を可視化

parallel = load(&quot;arctic_a0028_parallel.jld&quot;)

figure(figsize=(16, 6), dpi=80, facecolor=&quot;w&quot;, edgecolor=&quot;k&quot;)
plot(vec(parallel[&quot;src&quot;][&quot;feature_matrix&quot;][1,:]), linewidth=2.0, &quot;b&quot;, label=&quot;0th order mel-cesptrum of clb_a0028&quot;)
plot(vec(parallel[&quot;tgt&quot;][&quot;feature_matrix&quot;][1,:]), linewidth=2.0, &quot;g&quot;, label=&quot;0th order mel-cesptrum of slt_a0028&quot;)
xlim(0, size(parallel[&quot;tgt&quot;][&quot;feature_matrix&quot;], 2))
xlabel(&quot;Frame&quot;)
legend()
</code></pre>

<div align="center"><img src="/images/clb_and_slt_a0028_melcepstrum_0th_aligned.png "0th order mel-cepstrum (aligned)"" class="image"></div>

<p>ずれが修正されているのがわかりますね。注意として、<code>align.jl</code> の中身を追えばわかるのですが、無音区間をしきい値判定で検出して、パラレルデータから除外しています。</p>

<p>結果、時間同期されたパラレルデータは以下のようになります。</p>

<pre><code class="language-julia"># パラレルデータの可視化

figure(figsize=(16, 6), dpi=80, facecolor=&quot;w&quot;, edgecolor=&quot;k&quot;)
imshow(vcat(parallel[&quot;src&quot;][&quot;feature_matrix&quot;], parallel[&quot;tgt&quot;][&quot;feature_matrix&quot;]), origin=&quot;lower&quot;, aspect=&quot;auto&quot;)
colorbar()
</code></pre>

<div align="center"><img src="/images/clb_and_slt_a0028_parallel.png "example of parallel data"" class="image"></div>

<p>このパラレルデータを（複数の発話分さらに結合して）使って、特徴量の対応関係を学習していきます。モデルには、GMMを使います。</p>

<h2 id="3-gmmの学習">3. GMMの学習</h2>

<p>GMMの学習には、<code>sklearn.mixture.GMM</code> を使います。GMMは古典的な生成モデルで、実装は探せばたくさん見つかるので、既存の有用なライブラリを使えば十分です。（余談ですが、pythonのライブラリを簡単に呼べるのはjuliaの良いところの一つですね）</p>

<p><code>scripts/train_gmm.jl</code> を使うと、モデルのダンプ、julia &lt;-&gt; python間のデータフォーマットの変換等、もろもろやってくれます。</p>

<pre><code class="language-bash">julia train_gmm.jl ~/data/cmu_arctic_jld/parallel/clb_and_slt/ clb_and_slt_gmm32_order40.jld --max 200 --n_components 32 --n_iter=100 --n_init=1
</code></pre>

<p>使い方は、<code>train_gmm.jl &lt;パラレルデータのパス&gt; &lt;出力するモデルデータのパス&gt;</code> になっています。上の例では、学習に用いる発話数、GMMの混合数、反復回数等を指定しています。オプションの詳細はスクリプトをご覧ください。</p>

<p>僕の環境では、上記のコマンドを叩くと2時間くらいかかりました。学習が終わったところで、学習済みのモデルのパラメータを可視化してみましょう。</p>

<p>まずは平均を見てみます。</p>

<pre><code class="language-julia"># GMMの平均ベクトルを（いくつか）可視化
gmm = load(&quot;clb_and_slt_gmm32_order40.jld&quot;)

figure(figsize=(16, 6), dpi=80, facecolor=&quot;w&quot;, edgecolor=&quot;k&quot;)
for k=1:3
    plot(gmm[&quot;means&quot;][:,k], linewidth=2.0, label=&quot;mean of mixture $k&quot;)
end
legend()
</code></pre>

<div align="center"><img src="/images/clb_and_slt_gmm32_order40_mean.png "means of trained GMM"" class="image"></div>

<p>共分散の一部可視化してみると、以下のようになります。</p>

<pre><code class="language-julia"># GMMの共分散行列を一部可視化

figure(figsize=(16, 6), dpi=80, facecolor=&quot;w&quot;, edgecolor=&quot;k&quot;)
imshow(gmm[&quot;covars&quot;][:,:,2])
colorbar()
clim(0.0, 0.16)
</code></pre>

<div align="center"><img src="/images/clb_and_slt_gmm32_order40_covar.png "covariance of trained GMM"" class="image"></div>

<p>まぁこんなもんですね。</p>

<h2 id="4-音声分析合成worldを用いたgmmベースのframe-by-frame声質変換">4. 音声分析合成WORLDを用いたGMMベースのframe-by-frame声質変換</h2>

<p>さて、ようやく声質変換の準備が整いました。学習したモデルを使って、GMMベースのframe-by-frame声質変換（clb -&gt; slt ）をやってみましょう。具体的な変換アルゴリズムは、論文（例えば<a href="http://isw3.naist.jp/~tomoki/Tomoki/Journals/IEEE-Nov-2007_MLVC.pdf">戸田先生のこれ</a>）をチェックしてみてください。音声分析合成系にはWORLDを使います。</p>

<p>一般的な声質変換では、まず音声を以下の三つの成分に分解します。</p>

<ul>
<li>基本周波数</li>
<li>スペクトル包絡（今回いじりたい部分）</li>
<li>非周期性成分</li>
</ul>

<p>その後、スペクトル包絡に対して変換を行い、変換後のパラメータを使って音声波形を合成するといったプロセスを取ります。これらは、<code>scripts/vc.jl</code> を使うと簡単にできるようになっています。本当にWORLDさまさまです。</p>

<pre><code class="language-bash">julia vc.jl ~/data/cmu_arctic/cmu_us_clb_arctic/wav/arctic_a0028.wav clb_and_slt_gmm32_order40.jld clb_to_slt_a0028.wav --order 40
</code></pre>

<p>使い方は、<code>vc.jl &lt;変換対象の音声ファイル&gt; &lt;変換モデル&gt; &lt;出力wavファイル名&gt;</code> となっています。</p>

<p>上記のコマンドを実行すると、GMMベースのframe-by-frame声質変換の結果が音声ファイルに出力されます。以下に結果を貼っておくので、聞いてみてください。</p>

<h3 id="変換元となる音声-clb-a0028">変換元となる音声 clb_a0028</h3>

<iframe width="100%" height="166" scrolling="no" frameborder="no" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/176093202&amp;color=ff5500&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false"></iframe>

<h3 id="変換目標となる話者-slt-a0028">変換目標となる話者 slt_a0028</h3>

<iframe width="100%" height="166" scrolling="no" frameborder="no" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/176093240&amp;color=ff5500&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false"></iframe>

<h3 id="変換結果-clb-to-slt-a0028">変換結果 clb_to_slt_a0028</h3>

<iframe width="100%" height="166" scrolling="no" frameborder="no" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/176093403&amp;color=ff5500&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false"></iframe>

<p>話者性はなんとなく目標話者に近づいている気がしますが、音質が若干残念な感じですね。。</p>

<h2 id="5-差分スペクトル補正に基づく声質変換">5. 差分スペクトル補正に基づく声質変換</h2>

<p>最後に、より高品質な声質変換を達成可能な差分スペクトル補正に基づく声質変換を紹介します。差分スペクトル補正に基づく声質変換では、基本周波数や非周期性成分をいじれない代わりに音質はかなり改善します。以前書いた記事（<a href="http://r9y9.github.io/blog/2014/07/05/statistical-voice-conversion-muzui/">統計的声質変換クッソムズすぎワロタ - LESS IS MORE</a>）から、着想に関連する部分を引用します。</p>

<blockquote>
<p>これまでは、音声を基本周波数、非周期性成分、スペクトル包絡に分解して、スペクトル包絡を表す特徴量を変換し、変換後の特徴量を元に波形を再合成していました。ただ、よくよく考えると、そもそも基本周波数、非周期性成分をいじる必要がない場合であれば、わざわざ分解して再合成する必要なくね？声質の部分のみ変換するようなフィルタかけてやればよくね？という考えが生まれます。実は、そういったアイデアに基づく素晴らしい手法があります。それが、差分スペクトル補正に基づく声質変換です。</p>
</blockquote>

<p>差分スペクトル補正に基づく声質変換の詳細ついては、最近inter speechに論文が出たようなので、そちらをご覧ください。</p>

<ul>
<li><a href="http://isw3.naist.jp/~kazuhiro-k/resource/kobayashi14IS.pdf">[Kobayashi 2014] Kobayashi, Kazuhiro, et al. &ldquo;Statistical Singing Voice Conversion with Direct Waveform Modification based on the Spectrum Differential.&rdquo; Fifteenth Annual Conference of the International Speech Communication Association. 2014.</a></li>
</ul>

<p>こばくん、論文を宣伝しておきますね＾＾</p>

<h3 id="5-1-差分特徴量の学習">5.1 差分特徴量の学習</h3>

<p>さて、差分スペクトル補正に基づく声質変換行うには、変換元話者$X$と目標話者$Y$の特徴量の同時分布$P(X,Y)$を学習するのではなく、$P(X, Y-X)$ （日本語で書くとややこしいのですが、変換元話者の特徴量$X$と、変換元話者と目標話者の差分特徴量$Y-X$の同時分布）を学習します。これは、 <code>train_gmm.jl</code> を使ってGMMを学習する際に、<code>--diff</code> とオプションをつけるだけでできます。</p>

<pre><code class="language-bash">julia train_gmm.jl ~/data/cmu_arctic_jld/parallel/clb_and_slt/ clb_to_slt_gmm32_order40_diff.jld --max 200 --n_components 32 --n_iter=100 --n_init=1 --diff
</code></pre>

<p>可視化してみます。</p>

<p>平均</p>

<div align="center"><img src="/images/clb_to_slt_gmm32_order40_mean.png "means of trained DIFFGMM"" class="image"></div>

<p>共分散</p>

<div align="center"><img src="/images/clb_to_slt_gmm32_order40_covar.png "covar of trained DIFFGMM"" class="image"></div>

<p>さっき学習したGMMとは、共分散はかなり形が違いますね。高次元成分でも、分散が比較的大きな値をとっているように見えます。形が異っているのは見てすぐにわかりますが、では具体的には何が異っているのか、それはなぜなのか、きちんと考えると面白そうですね。</p>

<h3 id="5-2-mlsaフィルタによる声質変換">5.2 MLSAフィルタによる声質変換</h3>

<p>差分スペクトル補正に基づく声質変換では、WORLDを使って音声の分析合成を行うのではなく、生の音声波形を入力として、MLSAフィルタをかけるのみです。これは、 <code>scripts/diffvc.jl</code> を使うと簡単にできます。</p>

<pre><code class="language-bash">julia diffvc.jl ~/data/cmu_arctic/cmu_us_clb_arctic/wav/arctic_a0028.wav clb_to_slt_gmm32_order40_diff.jld clb_to_slt_a0028_diff.wav --order 40
</code></pre>

<p>さて、結果を聞いてみましょう。</p>

<h3 id="5-3-差分スペクトル補正に基づく声質変換結果">5.3 差分スペクトル補正に基づく声質変換結果</h3>

<iframe width="100%" height="166" scrolling="no" frameborder="no" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/176093513&amp;color=ff5500&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false"></iframe>

<p>アイデアはシンプル、結果は良好、最高の手法ですね（べた褒め</p>

<h2 id="おわりに">おわりに</h2>

<p>以上、長くなりましたが、統計的声質変換についてのチュートリアルはこれで終わります。誰の役に立つのか知らないけれど、役に立てば嬉しいです。トラジェクトリ変換やGVを考慮したバージョンなど、今回紹介していないものも実装しているので、詳しくは<a href="https://github.com/r9y9/VoiceConversion.jl">Githubのリポジトリ</a>をチェックしてください。バグをレポートしてくれたりすると、僕は喜びます。</p>

<h2 id="参考">参考</h2>

<h3 id="以前書いた声質変換に関する記事">以前書いた声質変換に関する記事</h3>

<ul>
<li><a href="http://r9y9.github.io/blog/2014/07/05/statistical-voice-conversion-muzui/">統計的声質変換クッソムズすぎワロタ - LESS IS MORE</a></li>
<li><a href="http://r9y9.github.io/blog/2014/07/13/statistical-voice-conversion-wakaran/">統計的声質変換クッソムズすぎワロタ（実装の話） - LESS IS MORE</a></li>
</ul>

<h3 id="論文">論文</h3>

<ul>
<li><a href="http://isw3.naist.jp/~tomoki/Tomoki/Journals/IEEE-Nov-2007_MLVC.pdf">[Toda 2007] T. Toda, A. W. Black, and K. Tokuda, “Voice conversion based on maximum likelihood estimation of spectral parameter trajectory,” IEEE
Trans. Audio, Speech, Lang. Process, vol. 15, no. 8, pp. 2222–2235,
Nov. 2007.</a></li>
<li><a href="http://isw3.naist.jp/~kazuhiro-k/resource/kobayashi14IS.pdf">[Kobayashi 2014] Kobayashi, Kazuhiro, et al. &ldquo;Statistical Singing Voice Conversion with Direct Waveform Modification based on the Spectrum Differential.&rdquo; Fifteenth Annual Conference of the International Speech Communication Association. 2014.</a></li>
</ul>

<h2 id="faq">FAQ</h2>

<h3 id="前はpythonで書いてなかった">前はpythonで書いてなかった？</h3>

<p>はい、<a href="https://gist.github.com/r9y9/88bda659c97f46f42525">https://gist.github.com/r9y9/88bda659c97f46f42525</a> ですね。正確には、GMMの学習・変換処理はpythonで書いて、特徴抽出、パラレルデータの作成、波形合成はGo言語で書いていました。が、Goとpythonでデータのやりとり、Goとpythonをいったり来たりするのが面倒になってしまって、一つの言語に統一したいと思うようになりました。Goで機械学習は厳しいと感じていたので、pythonで書くかなぁと最初は思ったのですが、WORLDやSPTKなど、Cのライブラリをpythonから使うのが思いの他面倒だったので（<a href="https://github.com/r9y9/SPTK">SPTKのpythonラッパー</a>は書きましたが）、Cやpythonとの連携がしやすく、スクリプト言語でありながらCに速度面で引けをとらないjuliaに興味があったので、juliaですべて完結するようにしました。かなり実験的な試みでしたが、今はかなり満足しています。juliaさいこー</p>

<h3 id="新規性は">新規性は？</h3>

<p>ありません</p>

				
<div class="social">
    <div>
        <a href="https://twitter.com/share" class="twitter-share-button" data-via="r9y9" data-text="統計的声質変換クッソムズすぎワロタ（チュートリアル編）" data-related="r9y9">Tweet</a>
        <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
    </div>
</div>


			</section>
		</article>
	</main>


	<footer role="contentinfo">
		<div class="hr"></div>
		<address>
			<div class="avatar-bottom">
				<a href="/">
					
					<img src="/images/r9y9.jpg">
					
				</a>
			</div>

		<div class="copyright">Copyright &copy;
			<a href="/about">Ryuichi YAMAMOTO</a> All rights reserved.

			<a href="https://github.com/r9y9">
				<span class="github">r9y9@Github</span>
			</a>
		</div>
		</address>
	</footer>

</div>

<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-44433856-1', 'auto');
	ga('send', 'pageview');
</script>

<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
     MathJax.Hub.Config({
         HTML: ["input/TeX","output/HTML-CSS"],
         TeX: {
                Macros: {
                         bm: ["\\boldsymbol{#1}", 1],
                         argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                         argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
                extensions: ["AMSmath.js","AMSsymbols.js"],
                equationNumbers: { autoNumber: "AMS" } },
         extensions: ["tex2jax.js"],
         jax: ["input/TeX","output/HTML-CSS"],
         tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true },
         "HTML-CSS": { availableFonts: ["TeX"],
                       linebreaks: { automatic: true } }
     });
 </script>

 <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       tex2jax: {
         skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
       }
     });
 </script>

 <script type="text/javascript" async
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
 </script>




</body>
</html>

