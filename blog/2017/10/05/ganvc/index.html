<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Hugo 0.20.5" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="http://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">
<link rel="stylesheet" href="/css/normalize.css">
<link rel="stylesheet" href="/css/skeleton.css">
<link rel="stylesheet" href="/css/custom.css">
<link rel="alternate" href="/index.xml" type="application/rss+xml" title="LESS IS MORE">
<link rel="shortcut icon" href="/favicon.png" type="image/x-icon" />
<title>【声質変換編】Statistical Parametric Speech Synthesis Incorporating Generative Adversarial Networks [arXiv:1709.08041] - LESS IS MORE</title>
</head>
<body>

<div class="container">

	<header role="banner">
		<div class="header-logo">
			<a href="/"><img src="/images/r9y9.jpg" width="70" height="70"></a>
		</div>
		
	</header>


	<main role="main">
		<article itemscope itemtype="http://schema.org/BlogPosting">
			<h1 class="entry-title" itemprop="headline">【声質変換編】Statistical Parametric Speech Synthesis Incorporating Generative Adversarial Networks [arXiv:1709.08041]</h1>
			<span class="entry-meta"><time itemprop="datePublished" datetime="2017-10-05">October 05, 2017</time></span>
			<section itemprop="entry-text">
				

<p><strong><sup>10</sup>&frasl;<sub>11</sub> 追記</strong>: IEEE TASLPのペーパー (Open access) が公開されたようなので、リンクを貼っておきます: <a href="http://ieeexplore.ieee.org/document/8063435/">http://ieeexplore.ieee.org/document/8063435/</a></p>

<p>arXiv論文リンク: <a href="https://arxiv.org/abs/1709.08041">arXiv:1709.08041</a></p>

<p>2017年9月末に、表題の <a href="https://arxiv.org/abs/1709.08041">論文</a> が公開されたのと、<a href="https://github.com/r9y9/nnmnkwii">nnmnkwii</a> という designed for easy and fast prototyping を目指すライブラリを作ったのもあるので、実装してみました。僕が実験した限りでは、声質変換 (Voice conversion; VC) では安定して良くなりました（音声合成ではまだ実験中です）。この記事では、声質変換について僕が実験した結果をまとめようと思います。音声合成については、また後日まとめます</p>

<ul>
<li>コードはこちら: <a href="https:github.com/r9y9/gantts">r9y9/gantts | PyTorch implementation of GAN-based text-to-speech and voice conversion (VC) </a> (TTSのコードも一緒です)</li>
<li>音声サンプルを聴きたい方はこちら: <a href="http://nbviewer.jupyter.org/github/r9y9/gantts/blob/master/notebooks/Test%20VC.ipynb">The effects of adversarial training in voice conversion | nbviewer</a> (※解説はまったくありませんのであしからず)</li>
</ul>

<p>なお、厳密に同じ結果を再現しようとは思っていません。同様のアイデアを試す、といったことに主眼を置いています。コードに関しては、ここに貼った結果を再現できるように気をつけました。</p>

<h2 id="概要">概要</h2>

<p>一言でいえば、音響モデルの学習に Generative Adversarial Net (<strong>GAN</strong>) を導入する、といったものです。少し具体的には、</p>

<ol>
<li>音響モデル（生成モデル）が生成した音響特徴量を偽物か本物かを見分けようとする識別モデルと、</li>
<li>生成誤差を小さくしつつ (Minimum Generation Error loss; <strong>MGE loss</strong> の最小化) 、生成した特徴量を識別モデルに本物だと誤認識させようとする (Adversarial loss; <strong>ADV loss</strong> の最小化) 生成モデル</li>
</ol>

<p>を交互に学習することで、自然音声の特徴量と生成した特徴量の分布を近づけるような、より良い音響モデルを獲得する、といった方法です。</p>

<h2 id="ベースライン">ベースライン</h2>

<p>ベースラインとしては、 <strong>MGE training</strong> が挙げられています。DNN音声合成でよくあるロス関数として、音響特徴量 (静的特徴量 + 動的特徴量) に対する Mean Squared Error (<strong>MSE loss</strong>) というものがあります。これは、特徴量の各次元毎に誤差に正規分布を考えて、その対数尤度を最大化することを意味します。
しかし、</p>

<ol>
<li>静的特徴量と動的特徴量の間には本来 deterministic な関係があることが無視されていること</li>
<li>ロスがフレーム単位で計算されるので、 (動的特徴量が含まれているとはいえ) 時間構造が無視されてしまっていること</li>
</ol>

<p>から、それらの問題を解決するために、系列単位で、かつパラメータ生成後の静的特徴量の領域でロスを計算する方法、MGE training が提案されています。<sup class="footnote-ref" id="fnref:1"><a rel="footnote" href="#fn:1">1</a></sup></p>

<h2 id="実験">実験</h2>

<h3 id="実験条件">実験条件</h3>

<p><a href="http://festvox.org/cmu_arctic/">CMU ARCTIC</a> から、話者 <code>clb</code> と <code>slt</code> のwavデータそれぞれ500発話を用います。439を学習用、56を評価用、残り5をテスト用にします。音響特徴量には、WORLDを使って59次のメルケプストラムを抽出し、0次を除く59次元のベクトルを各フレーム毎の特徴量とします。F0、非周期性指標に関しては、元話者のものをそのまま使い、差分スペクトル法を用いて波形合成を行いました。F0の変換はしていません。音響モデルには、</p>

<ul>
<li><a href="https://www.jstage.jst.go.jp/article/transinf/E100.D/8/E100.D_2017EDL8034/_article">Yuki Saito, Shinnosuke Takamichi, and Hiroshi Saruwatari, &ldquo;Voice conversion using input-to-output highway networks,&rdquo; IEICE Transactions on Information and Systems, Vol.E100-D, No.8, pp.1925&ndash;1928, Aug. 2017</a></li>
</ul>

<p>で述べられている highway network を用います。ただし、活性化関数をReLUからLeakyReLUにしたり、Dropoutを入れたり、アーキテクチャは微妙に変えています。前者は、調べたら勾配が消えにくくて学習の不安定なGANに良いと書いてある記事があったので（ちゃんと理解しておらず安直ですが、実験したところ悪影響はなさそうでしたので様子見）、後者は、GANの学習の安定化につながった気がします（少なくともTTSでは）。Discriminatorには、Dropout付きの多層ニューラルネットを使いました。MGE loss と ADV loss をバランスする重み <code>w_d</code> は、 1.0 にしました。層の数、ニューロンの数等、その他詳細が知りたい方は、コードを参照してください。実験に使用したコードの正確なバージョンは  <a href="https://github.com/r9y9/gantts/tree/ccbb51b51634b272f0a71f29ad4c28edd8ce3429">ccbb51b</a> です。ハイパーパラメータは <a href="https://github.com/r9y9/gantts/blob/ccbb51b51634b272f0a71f29ad4c28edd8ce3429/hparams.py">こちら</a> です。</p>

<p>ここで示す結果を再現したい場合は、</p>

<ul>
<li>コードをチェックアウト</li>
<li>パッケージと依存関係をインストール</li>
<li><code>clb</code> と <code>slt</code> のデータをダウンロード（僕の場合は、 <code>~/data/cmu_arctic</code> にあります</li>
</ul>

<p>そして、以下のスクリプトを実行すればOKです。</p>

<pre><code>./vc_demo.sh ~/data/cmu_arctic
</code></pre>

<p>なお実行には、GPUメモリが4GBくらいは必要です（バッチサイズ32の場合）。GTX 1080Ti + i7-7700K の計算環境で、約1時間半くらいで終わります。スクリプト実行が完了すれば、<code>generated</code> ディレクトリに、ベースライン/GAN それぞれで変換した音声が出力されます。以下に順に示す図については、<a href="http://nbviewer.jupyter.org/github/r9y9/gantts/blob/master/notebooks/Test%20VC.ipynb">デモノートブック</a> を実行すると作ることができます。</p>

<h3 id="変換音声の比較">変換音声の比較</h3>

<p>テストセットの5つのデータに対しての変換音声、およびその元音声、ターゲット音声を比較できるように貼っておきます。下記の順番です。</p>

<ol>
<li>元話者の音声</li>
<li>ターゲット話者の音声</li>
<li><strong>MGE Loss</strong> を最小化して得られたモデルによる変換音声</li>
<li><strong>MGE loss + ADV loss</strong> を最小化して得られたモデルによる変換音声</li>
</ol>

<p>比較しやすいように、音量はsoxで正規化しました。</p>

<p><strong>arctic_a0496</strong></p>

<p><audio controls="controls" >
<source src="/audio/ganvc/src/arctic_a0496.wav" autoplay/>
Your browser does not support the audio element.
</audio></p>

<p><audio controls="controls" >
<source src="/audio/ganvc/tgt/arctic_a0496.wav" autoplay/>
Your browser does not support the audio element.
</audio></p>

<p><audio controls="controls" >
<source src="/audio/ganvc/baseline/test/arctic_a0496.wav" autoplay/>
Your browser does not support the audio element.
</audio></p>

<p><audio controls="controls" >
<source src="/audio/ganvc/gan/test/arctic_a0496.wav" autoplay/>
Your browser does not support the audio element.
</audio></p>

<p><strong>arctic_a0497</strong></p>

<p><audio controls="controls" >
<source src="/audio/ganvc/src/arctic_a0497.wav" autoplay/>
Your browser does not support the audio element.
</audio></p>

<p><audio controls="controls" >
<source src="/audio/ganvc/tgt/arctic_a0497.wav" autoplay/>
Your browser does not support the audio element.
</audio></p>

<p><audio controls="controls" >
<source src="/audio/ganvc/baseline/test/arctic_a0497.wav" autoplay/>
Your browser does not support the audio element.
</audio></p>

<p><audio controls="controls" >
<source src="/audio/ganvc/gan/test/arctic_a0497.wav" autoplay/>
Your browser does not support the audio element.
</audio></p>

<p><strong>arctic_a0498</strong></p>

<p><audio controls="controls" >
<source src="/audio/ganvc/src/arctic_a0498.wav" autoplay/>
Your browser does not support the audio element.
</audio></p>

<p><audio controls="controls" >
<source src="/audio/ganvc/tgt/arctic_a0498.wav" autoplay/>
Your browser does not support the audio element.
</audio></p>

<p><audio controls="controls" >
<source src="/audio/ganvc/baseline/test/arctic_a0498.wav" autoplay/>
Your browser does not support the audio element.
</audio></p>

<p><audio controls="controls" >
<source src="/audio/ganvc/gan/test/arctic_a0498.wav" autoplay/>
Your browser does not support the audio element.
</audio></p>

<p><strong>arctic_a0499</strong></p>

<p><audio controls="controls" >
<source src="/audio/ganvc/src/arctic_a0499.wav" autoplay/>
Your browser does not support the audio element.
</audio></p>

<p><audio controls="controls" >
<source src="/audio/ganvc/tgt/arctic_a0499.wav" autoplay/>
Your browser does not support the audio element.
</audio></p>

<p><audio controls="controls" >
<source src="/audio/ganvc/baseline/test/arctic_a0499.wav" autoplay/>
Your browser does not support the audio element.
</audio></p>

<p><audio controls="controls" >
<source src="/audio/ganvc/gan/test/arctic_a0499.wav" autoplay/>
Your browser does not support the audio element.
</audio></p>

<p><strong>arctic_a0500</strong></p>

<p><audio controls="controls" >
<source src="/audio/ganvc/src/arctic_a0500.wav" autoplay/>
Your browser does not support the audio element.
</audio></p>

<p><audio controls="controls" >
<source src="/audio/ganvc/tgt/arctic_a0500.wav" autoplay/>
Your browser does not support the audio element.
</audio></p>

<p><audio controls="controls" >
<source src="/audio/ganvc/baseline/test/arctic_a0500.wav" autoplay/>
Your browser does not support the audio element.
</audio></p>

<p><audio controls="controls" >
<source src="/audio/ganvc/gan/test/arctic_a0500.wav" autoplay/>
Your browser does not support the audio element.
</audio></p>

<p><code>clb</code>, <code>slt</code> は違いがわかりにくいと以前誰かから指摘されたのですが、これに慣れてしまいました。わかりづらかったらすいません。僕の耳では、明瞭性が上がって、良くなっているように思います。</p>

<h3 id="global-variance-は補償されているのか">Global variance は補償されているのか？</h3>

<p>統計ベースの手法では、変換音声の <strong>Global variance (GV)</strong> が落ちてしまい、品質が劣化してしまう問題がよく知られています。GANベースの手法によって、この問題に対処できているのかどうか、実際に確認しました。以下に、データセット中の一サンプルを適当にピックアップして、GVを計算したものを示します。縦軸は対数、横軸はメルケプストラムの次元です。</p>

<div align="center"><img src="/images/ganvc/gv.png" /></div>

<p>おおおまか、論文で示されているのと同等の結果を得ることができました。</p>

<h3 id="modulation-spectrum-変調スペクトル-は補償されているのか">Modulation spectrum (変調スペクトル) は補償されているのか？</h3>

<p>GVをより一般化ものとして、変調スペクトルという概念があります。端的に言えば、パラメータ系列の時間方向に対する離散フーリエ変換の二乗（の対数※定義によるかもですが、ここでは対数をとったもの）です。統計処理によって劣化した変換音声は、変調スペクトルが自然音声と比べて小さくなっていることが知られています。というわけで、GANベースの方法によって、変調スペクトルは補償されているのか？ということを調べてみました。これは、論文には書いていません（が、きっとされていると思います）。以下に、評価用の音声56発話それぞれで変調スペクトルを計算し、それらの平均を取り、適当な特徴量の次元をピックアップしたものを示します。横軸は変調周波数です。一番右端が50Hzです。</p>

<div align="center"><img src="/images/ganvc/ms.png" /></div>

<p>特に高次元の変調スペクトルに対して、ベースラインは大きく落ちている一方で、GANベースでは比較的自然音声と近いことがわかります。しかし、高次元になるほど、自然音声とGANベースでも違いが出ているのがわかります。改善の余地はありそうですね。</p>

<h3 id="特徴量の分布">特徴量の分布</h3>

<p>論文で示されているscatter plotですが、同じことをやってみました。</p>

<div align="center"><img src="/images/ganvc/scatter.png" /></div>

<p>概ね、論文通りの結果となっています。</p>

<h3 id="詐称率について">詐称率について</h3>

<p><code>w_d</code> を変化させて、詐称率がどうなるかは実験していないのですが、<code>w_d = 1.0</code> の場合に、だいたい0.7 ~ 0.9 くらいに収まることを確認しました。TTSでは0.99くらいの、論文と同様の結果が出ました。くらい、というのは、どのくらい Discriminator を学習させるか、初期化としてのMGE学習（例えば25epochくらい）のあと生成された特徴量に対して学習させるのか、それとも初期化とは別でベースライン用のモデル（100epochとか）を使って生成された特徴量に対して学習させるのか、によって変わってくるのと、その辺りが論文からではあまりわからなかったのと、学習率や最適化アルゴリズムやデータによっても変わってくるのと、詐称率の計算は品質にはまったく関係ないのもあって、あまり真面目にやっていません。すいません</p>

<h2 id="感想">感想</h2>

<ul>
<li>効果は劇的、明らかに良くなりました。素晴らしいですね</li>
<li>論文で書かれている反復回数 (25epochとか)よりも、100, 200と多く学習させる方がよかったです（知覚的な差は微妙ですが）ロスは下がり続けていました。</li>
<li>実装はそんなに大変ではなかったですが、GANの学習が難しい感じがしました（VCではあまり失敗しないが、TTSではよく失敗する。落とし所を探し中</li>
<li>Adam は学習は速いが、過学習ししやすい。GANも不安定になりがちな気がしました</li>
<li>Adagrad は収束は遅いが、安定</li>
<li>MGE loss と ADV loss の重みの計算は、適当にclipするようにしました。しなくてもだいたい収束しますが、バグがあると簡単に発散しますね〜haha</li>
<li>gradient clipping をいれました。TTSでは少なくとも良くなった気がします。VCはなしでも安定しているようです。</li>
</ul>

<h2 id="まとめ">まとめ</h2>

<p>とても良くなりました。素晴らしいです。今回もWORLDにお世話になりました。続いて、TTSでも実験を進めていきます。</p>

<h2 id="参考">参考</h2>

<p>Arxivにあるペーパーだけでなく、その他いろいろ参考にしました。ありがとうございます。</p>

<ul>
<li><a href="https://arxiv.org/abs/1709.08041">Yuki Saito, Shinnosuke Takamichi, Hiroshi Saruwatari, &ldquo;Statistical Parametric Speech Synthesis Incorporating Generative Adversarial Networks&rdquo;, arXiv:1709.08041 [cs.SD], Sep. 2017</a></li>
<li><a href="http://sython.org/papers/SIG-SLP/saito201702slp.pdf">Yuki Saito, Shinnosuke Takamichi, and Hiroshi Saruwatari, &ldquo;Training algorithm to deceive anti-spoofing verification for DNN-based text-to-speech synthesis,&rdquo; IPSJ SIG Technical Report, 2017-SLP-115, no. 1, pp. 1-6, Feb., 2017. (in Japanese)</a></li>
<li><a href="https://www.jstage.jst.go.jp/article/transinf/E100.D/8/E100.D_2017EDL8034/_article">Yuki Saito, Shinnosuke Takamichi, and Hiroshi Saruwatari, &ldquo;Voice conversion using input-to-output highway networks,&rdquo; IEICE Transactions on Information and Systems, Vol.E100-D, No.8, pp.1925&ndash;1928, Aug. 2017</a></li>
<li><a href="https://www.slideshare.net/ShinnosukeTakamichi/dnnantispoofing">https://www.slideshare.net/ShinnosukeTakamichi/dnnantispoofing</a></li>
<li><a href="https://www.slideshare.net/YukiSaito8/Saito2017icassp">https://www.slideshare.net/YukiSaito8/Saito2017icassp</a></li>
<li><a href="https://github.com/SythonUK/whisperVC">https://github.com/SythonUK/whisperVC</a></li>
<li>Kobayashi, Kazuhiro, et al. &ldquo;Statistical Singing Voice Conversion with Direct Waveform Modification based on the Spectrum Differential.&rdquo; Fifteenth Annual Conference of the International Speech Communication Association. 2014.</li>
</ul>
<div class="footnotes">

<hr />

<ol>
<li id="fn:1">論文では有効性が示されていますが、僕が試した範囲内で、かつ僕の耳にによれば、あまり大きな改善は確認できていません。客観的な評価は、そのうちする予定です。
 <a class="footnote-return" href="#fnref:1"><sup>[return]</sup></a></li>
</ol>
</div>

				
<div class="social">
    <div>
        <a href="https://twitter.com/share" class="twitter-share-button" data-via="r9y9" data-text="【声質変換編】Statistical Parametric Speech Synthesis Incorporating Generative Adversarial Networks [arXiv:1709.08041]" data-related="r9y9">Tweet</a>
        <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
    </div>
</div>


			</section>
		</article>
	</main>


	<footer role="contentinfo">
		<div class="hr"></div>
		<address>
			<div class="avatar-bottom">
				<a href="/">
					
					<img src="/images/r9y9.jpg">
					
				</a>
			</div>

		<div class="copyright">Copyright &copy;
			<a href="/about">Ryuichi YAMAMOTO</a> All rights reserved.

			<a href="https://github.com/r9y9">
				<span class="github">r9y9@Github</span>
			</a>
		</div>
		</address>
	</footer>

</div>

<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-44433856-1', 'auto');
	ga('send', 'pageview');
</script>

<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
     MathJax.Hub.Config({
         HTML: ["input/TeX","output/HTML-CSS"],
         TeX: {
                Macros: {
                         bm: ["\\boldsymbol{#1}", 1],
                         argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                         argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
                extensions: ["AMSmath.js","AMSsymbols.js"],
                equationNumbers: { autoNumber: "AMS" } },
         extensions: ["tex2jax.js"],
         jax: ["input/TeX","output/HTML-CSS"],
         tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true },
         "HTML-CSS": { availableFonts: ["TeX"],
                       linebreaks: { automatic: true } }
     });
 </script>

 <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       tex2jax: {
         skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
       }
     });
 </script>

<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>




</body>
</html>

