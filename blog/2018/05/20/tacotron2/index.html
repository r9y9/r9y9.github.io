<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Hugo 0.20.5" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">
<link rel="stylesheet" href="/css/normalize.css">
<link rel="stylesheet" href="/css/skeleton.css">
<link rel="stylesheet" href="/css/custom.css">
<link rel="alternate" href="/index.xml" type="application/rss+xml" title="LESS IS MORE">
<link rel="shortcut icon" href="/favicon.png" type="image/x-icon" />
<title> WN-based TTSやりました / Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions [arXiv:1712.05884] - LESS IS MORE</title>
</head>
<body>

<div class="container">

	<header role="banner">
		<div class="header-logo">
			<a href="/"><img src="/images/r9y9.jpg" width="70" height="70"></a>
		</div>
		
	</header>


	<main role="main">
		<article itemscope itemtype="https://schema.org/BlogPosting">
			<h1 class="entry-title" itemprop="headline"> WN-based TTSやりました / Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions [arXiv:1712.05884]</h1>
			<span class="entry-meta"><time itemprop="datePublished" datetime="2018-05-20">May 20, 2018</time></span>
			<section itemprop="entry-text">
				

<p>Thank you for coming to see my blog post about WaveNet text-to-speech.</p>

<p><audio controls="controls" >
<source src="/audio/tacotron2/intro.wav" autoplay/>
Your browser does not support the audio element.
</audio></p>

<ul>
<li>論文リンク: <a href="https://arxiv.org/abs/1712.05884">https://arxiv.org/abs/1712.05884</a></li>
<li>オンラインデモ: <a href="https://colab.research.google.com/github/r9y9/Colaboratory/blob/master/Tacotron2_and_WaveNet_text_to_speech_demo.ipynb">Tacotron2: WaveNet-based text-to-speech demo</a></li>
<li>コード <a href="https://github.com/r9y9/wavenet_vocoder">r9y9/wavenet_vocoder</a>, <a href="https://github.com/Rayhane-mamah/Tacotron-2">Rayhane-mamah/Tacotron-2</a></li>
<li>音声サンプル: <a href="https://r9y9.github.io/wavenet_vocoder/">https://r9y9.github.io/wavenet_vocoder/</a></li>
</ul>

<h2 id="三行まとめ">三行まとめ</h2>

<ul>
<li>自作WaveNet (<strong>WN</strong>) と既存実装Tacotron 2 (WNを除く) を組み合わせて、英語TTSを作りました</li>
<li>LJSpeechを学習データとした場合、自分史上 <strong>最高品質</strong> のTTSができたと思います</li>
<li>Tacotron 2と Deep Voice 3 のabstractを読ませた音声サンプルを貼っておきますので、興味のある方はどうぞ</li>
</ul>

<p>なお、Tacotron 2 の解説はしません。申し訳ありません（なぜなら僕がまだ十分に読み込んでいないため）</p>

<h2 id="背景">背景</h2>

<p>過去に、WaveNetを実装しました（参考: <a href="/blog/2018/01/28/wavenet_vocoder/">WaveNet vocoder をやってみましたので、その記録です / WaveNet: A Generative Model for Raw Audio [arXiv:1609.03499]</a>）。過去事   から引用します。</p>

<blockquote>
<p>Tacotron2 は、あとはやればほぼできる感じですが、直近では僕の中で優先度が低めのため、しばらく実験をする予定はありません。興味のある方はやってみてください。</p>
</blockquote>

<p>やりたいことの一つとしてあったとはいえ、当初の予定通り、スクラッチでTacotron 2を実装する時間は取れなかったのですが、既存実装を使ってみたところ十分に上手く動いているように思えたので、ありがたく使わせていただき、WaveNet TTSを実現することができました。というわけで、結果をここにカジュアルに残しておこうという趣旨の記事になります。</p>

<p>オープンなデータセット、コードを使って、実際どの程度の品質が得られるのか？学習/推論にどのくらい時間がかかるのか？いうのが気になる方には、参考になるかもしれませんので、よろしければ続きをどうぞ。</p>

<h2 id="実験条件">実験条件</h2>

<p>細かい内容はコードに譲るとして、重要な点だけリストアップします</p>

<h3 id="pre-trained-models-hyper-parameters-へのリンク">Pre-trained models、hyper parameters へのリンク</h3>

<ul>
<li>Tacotron2 (mel-spectrogram prediction part): trained 189k steps on LJSpeech dataset (<a href="https://www.dropbox.com/s/vx7y4qqs732sqgg/pretrained.tar.gz?dl=0">Pre-trained model</a>, <a href="https://github.com/r9y9/Tacotron-2/blob/9ce1a0e65b9217cdc19599c192c5cd68b4cece5b/hparams.py">Hyper params</a>).</li>
<li>WaveNet: trained over 1000k steps on LJSpeech dataset (<a href="https://www.dropbox.com/s/zdbfprugbagfp2w/20180510_mixture_lj_checkpoint_step000320000_ema.pth?dl=0">Pre-trained model</a>, <a href="https://www.dropbox.com/s/0vsd7973w20eskz/20180510_mixture_lj_checkpoint_step000320000_ema.json?dl=0">Hyper params</a>)</li>
</ul>

<h3 id="wavenet">WaveNet</h3>

<ul>
<li>1000k step以上訓練されたモデル (2018/1/27に作ったもの、10日くらい<sup class="footnote-ref" id="fnref:1"><a rel="footnote" href="#fn:1">1</a></sup>学習した）をベースに、さらに 320k step学習（約3日）しました。再学習したのは、以前のコードには <a href="https://github.com/r9y9/wavenet_vocoder/issues/33">wavenet_vocoder/issues/33</a> こんなバグがあったからです。</li>
<li>評価には、exponential moving averagingされたパラメータを使いました。decay パラメータはTaco2論文と同じ 0.9999</li>
<li>学習には、Mel-spectrogram prediction networkにより出力される Ground-truth-aligned (GTA) なメルスペクトログラムではなく、生音声から計算されるメルスペクトログラムを使いました。時間の都合上そうしましたが、GTAを使うとより品質が向上すると考えられます</li>
</ul>

<h3 id="tacotron-2-mel-spectrogram-prediction">Tacotron 2 (mel-spectrogram prediction)</h3>

<ul>
<li><a href="https://github.com/Rayhane-mamah/Tacotron-2">https://github.com/Rayhane-mamah/Tacotron-2</a> にはWaveNet実装も含まれていますが、mel-spectrogram prediction の部分だけ使用しました<sup class="footnote-ref" id="fnref:2"><a rel="footnote" href="#fn:2">2</a></sup></li>
<li><a href="https://github.com/Rayhane-mamah/Tacotron-2/issues/30#issue-317360759">https://github.com/Rayhane-mamah/Tacotron-2/issues/30#issue-317360759</a> で公開されている 182k step学習されたモデルを、さらに7k stepほど（数時間くらい）学習させました。再学習させた理由は、自分の実装とRayhane氏の実装で想定するメルスペクトログラムのレンジが異なっていたためです（僕: <code>[0, 1]</code>, Rayhane: <code>[-4, 4]</code>）。そういう経緯から、<code>[-4, 4]</code> のレンジであったところ，<code>[0, 4]</code> にして学習しなおしました。直接 <code>[0, 1]</code> にして学習しなかったのは（それでも動く、と僕は思っているのですが）、mel-spectrogram のレンジを大きく取った方が良い、という報告がいくつかあったからです（例えば <a href="https://github.com/Rayhane-mamah/Tacotron-2/issues/4#issuecomment-377728945">https://github.com/Rayhane-mamah/Tacotron-2/issues/4#issuecomment-377728945</a> )。Attention seq2seq は経験上学習が難しいので、僕の直感よりも先人の知恵を優先することにした次第です。WNに入力するときには、 Taco2が出力するメルスペクトログラムを <code>c = np.interp(c, (0, 4), (0, 1))</code> とレンジを変換して与えました</li>
</ul>

<h2 id="デモ音声">デモ音声</h2>

<p><a href="https://r9y9.github.io/wavenet_vocoder/">https://r9y9.github.io/wavenet_vocoder/</a> にサンプルはたくさんあります。が、ここでは違うサンプルをと思い、Tacotron 2 と Deep Voice 3の abstract を読ませてみました。
学習データに若干残響が乗っているので（ノイズっぽい）それが反映されてしまっているのですが、個人的にはまぁまぁよい結果が得られたと思っています。興味がある方は、DeepVoice3など僕の過去記事で触れているTTS結果と比べてみてください。</p>

<p>なお、推論の計算速度は,、僕のローカル環境（GTX 1080Ti, i7-7700K）でざっと 170 timesteps / second といった感じでした。これは、Parallel WaveNet の論文で触れられている数字とおおまかに一致します。</p>

<p>This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text.</p>

<p><audio controls="controls" >
<source src="/audio/tacotron2/20180510_mixture_lj_checkpoint_step000320000_ema_speech-mel-00001.wav" autoplay/>
Your browser does not support the audio element.
</audio></p>

<p>The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize timedomain waveforms from those spectrograms.</p>

<p><audio controls="controls" >
<source src="/audio/tacotron2/20180510_mixture_lj_checkpoint_step000320000_ema_speech-mel-00002.wav" autoplay/>
Your browser does not support the audio element.
</audio></p>

<p>Our model achieves a mean opinion score of 4.53 comparable to a MOS of 4.58 for professionally recorded speech.</p>

<p><audio controls="controls" >
<source src="/audio/tacotron2/20180510_mixture_lj_checkpoint_step000320000_ema_speech-mel-00003.wav" autoplay/>
Your browser does not support the audio element.
</audio></p>

<p>To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the input to WaveNet instead of linguistic, duration, and F0 features.</p>

<p><audio controls="controls" >
<source src="/audio/tacotron2/20180510_mixture_lj_checkpoint_step000320000_ema_speech-mel-00004.wav" autoplay/>
Your browser does not support the audio element.
</audio></p>

<p>We further demonstrate that using a compact acoustic intermediate representation enables significant simplification of the WaveNet architecture.</p>

<p><audio controls="controls" >
<source src="/audio/tacotron2/20180510_mixture_lj_checkpoint_step000320000_ema_speech-mel-00005.wav" autoplay/>
Your browser does not support the audio element.
</audio></p>

<p>We present Deep Voice 3, a fully-convolutional attention-based neural text-to-speech system.</p>

<p><audio controls="controls" >
<source src="/audio/tacotron2/20180510_mixture_lj_checkpoint_step000320000_ema_speech-mel-00006.wav" autoplay/>
Your browser does not support the audio element.
</audio></p>

<p>Deep Voice 3 matches state-of-the-art neural speech synthesis systems in naturalness while training ten times faster.</p>

<p><audio controls="controls" >
<source src="/audio/tacotron2/20180510_mixture_lj_checkpoint_step000320000_ema_speech-mel-00007.wav" autoplay/>
Your browser does not support the audio element.
</audio></p>

<p>We scale Deep Voice 3 to data set sizes unprecedented for TTS, training on more than eight hundred hours of audio from over two thousand speakers.</p>

<p><audio controls="controls" >
<source src="/audio/tacotron2/20180510_mixture_lj_checkpoint_step000320000_ema_speech-mel-00008.wav" autoplay/>
Your browser does not support the audio element.
</audio></p>

<p>In addition, we identify common error modes of attention-based speech synthesis networks, demonstrate how to mitigate them, and compare several different waveform synthesis methods.</p>

<p><audio controls="controls" >
<source src="/audio/tacotron2/20180510_mixture_lj_checkpoint_step000320000_ema_speech-mel-00009.wav" autoplay/>
Your browser does not support the audio element.
</audio></p>

<p>We also describe how to scale inference to ten million queries per day on one single-GPU server.</p>

<p><audio controls="controls" >
<source src="/audio/tacotron2/20180510_mixture_lj_checkpoint_step000320000_ema_speech-mel-00010.wav" autoplay/>
Your browser does not support the audio element.
</audio></p>

<h2 id="オンラインデモ">オンラインデモ</h2>

<p><a href="https://colab.research.google.com/github/r9y9/Colaboratory/blob/master/Tacotron2_and_WaveNet_text_to_speech_demo.ipynb">Tacotron2: WaveNet-based text-to-speech demo</a></p>

<p>Google Colabで動かせるようにデモノートブックを作りました。環境構築が不要なので、手軽にお試しできるかと思います。</p>

<h2 id="雑記">雑記</h2>

<ul>
<li>WaveNetを学習するときに、Mel-spectrogram precition networkのGTAな出力でなく、生メルスペクトログラムをそのまま使っても品質の良い音声合成ができるのは個人的に驚きでした。これはつまり、Taco2が　(non teacher-forcingな条件で) 十分良いメルスペクトログラムを予測できている、ということなのだと思います。</li>
<li>収束性を向上させるために、出力を127.5 倍するとよい、という件ですが、僕はやっていません。なぜなら、僕がまだこの方法の妥当性を理解できていないからです。<a href="https://twitter.com/__dhgrs__/status/995962302896599040">@__dhgrs__さんの報告</a> によると、やはり有効に働くようですね…</li>
<li>これまた <a href="http://www.monthly-hack.com/entry/2018/02/23/203208">@__dhgrs__さんのブログ記事</a> にも書かれていますが、Mixture of Logistic distributions (MoLとします) を使った場合は、categoricalを考えてsoftmaxを使う場合に比べると十分な品質を得るのに大幅に計算時間が必要になりますね、、体験的には10倍程度です。計算にあまりに時間がかかるので、スクラッチで何度も学習するのは厳しく、学習済みモデルを何度も繰り返しfine turningしていくという、秘伝のタレ方式で学習を行いました（再現性なしです、懺悔）</li>
<li><a href="https://github.com/Rayhane-mamah/Tacotron-2">https://github.com/Rayhane-mamah/Tacotron-2</a> 今回使わせてもらったTaco2実装は、僕の実装も一部使われているようでした。これとは別の NVIDIA から出た <a href="https://github.com/NVIDIA/tacotron2">https://github.com/NVIDIA/tacotron2</a> の謝辞には僕の名前を入れていただいていたり、他にもそういうケースがそれなりにあって、端的にいって光栄であり、うれしいお思いです。</li>
<li>非公開のデータセットを使って学習/生成したWaveNet TTS のサンプルもあります。公開できないのでここにはあげていませんが、とても高品質な音声合成（主観ですが）ができることを確認しています</li>
<li>このプロジェクトをはじめたことで、なんと光栄にも<a href="http://www.nict.go.jp/">NICT</a>でのトークの機会をもらうことができました。オープソースについて是非はあると思いますが、個人的には良いことがとても多いなと思います。プレゼン資料は、<a href="https://github.com/r9y9/wavenet_vocoder/issues/57">https://github.com/r9y9/wavenet_vocoder/issues/57</a> に置いてあります（が、スライドだけで読み物として成立するものではないと思います、すみません）</li>
</ul>

<h2 id="おわりに">おわりに</h2>

<p>WaveNet TTSをようやく作ることができました。Sample-levelでautoregressive modelを考えるというアプローチが本当に動かくのか疑問だったのですが、実際に作ってみて、上手く行くということを体感することができました。めでたし。</p>

<p>Googleの研究者さま、素晴らしい研究をありがとうございます。WaveNetは本当にすごかった</p>

<h2 id="参考">参考</h2>

<ul>
<li><a href="https://arxiv.org/abs/1609.03499">Aaron van den Oord, Sander Dieleman, Heiga Zen, et al, &ldquo;WaveNet: A Generative Model for Raw Audio&rdquo;,  arXiv:1609.03499, Sep 2016.</a></li>
<li><a href="https://arxiv.org/abs/1711.10433">Aaron van den Oord, Yazhe Li, Igor Babuschkin, et al, &ldquo;Parallel WaveNet: Fast High-Fidelity Speech Synthesis&rdquo;,   arXiv:1711.10433, Nov 2017.</a></li>
<li><a href="http://www.isca-speech.org/archive/Interspeech_2017/pdfs/0314.PDF">Tamamori, Akira, et al. &ldquo;Speaker-dependent WaveNet vocoder.&rdquo; Proceedings of Interspeech. 2017.</a></li>
<li><a href="https://arxiv.org/abs/1712.05884">Jonathan Shen, Ruoming Pang, Ron J. Weiss, et al, &ldquo;Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions&rdquo;, arXiv:1712.05884, Dec 2017.</a></li>
<li><a href="https://arxiv.org/abs/1710.07654">Wei Ping, Kainan Peng, Andrew Gibiansky, et al, &ldquo;Deep Voice 3: 2000-Speaker Neural Text-to-Speech&rdquo;, arXiv:1710.07654, Oct. 2017.</a></li>
<li><a href="https://arxiv.org/abs/1611.09482">Tom Le Paine, Pooya Khorrami, Shiyu Chang, et al, &ldquo;Fast Wavenet Generation Algorithm&rdquo;, arXiv:1611.09482, Nov. 2016</a></li>
<li><a href="http://www.monthly-hack.com/entry/2018/02/23/203208">VQ-VAEの追試で得たWaveNetのノウハウをまとめてみた。 - Monthly Hacker&rsquo;s Blog</a></li>
</ul>
<div class="footnotes">

<hr />

<ol>
<li id="fn:1">曖昧な表現で申し訳ございません
 <a class="footnote-return" href="#fnref:1"><sup>[return]</sup></a></li>
<li id="fn:2">僕が使った当時は、WNの部分は十分にテストされていなかったのと、WNのコードは僕のコードをtfにtranslateした感じな（著者がそういってます）ので、WNは自分の実装を使った次第です
 <a class="footnote-return" href="#fnref:2"><sup>[return]</sup></a></li>
</ol>
</div>

				
<div class="social">
    <div>
        <a href="https://twitter.com/share" class="twitter-share-button" data-via="r9y9" data-text=" WN-based TTSやりました / Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions [arXiv:1712.05884]" data-related="r9y9">Tweet</a>
        <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
    </div>
</div>


			</section>
		</article>
	</main>


	<footer role="contentinfo">
		<div class="hr"></div>
		<address>
			<div class="avatar-bottom">
				<a href="/">
					
					<img src="/images/r9y9.jpg">
					
				</a>
			</div>

		<div class="copyright">Copyright &copy;
			<a href="/about">Ryuichi YAMAMOTO</a> All rights reserved.

			<a href="https://github.com/r9y9">
				<span class="github">r9y9@Github</span>
			</a>
		</div>
		</address>
	</footer>

</div>

<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-44433856-1', 'auto');
	ga('send', 'pageview');
</script>

<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
     MathJax.Hub.Config({
         HTML: ["input/TeX","output/HTML-CSS"],
         TeX: {
                Macros: {
                         bm: ["\\boldsymbol{#1}", 1],
                         argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                         argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
                extensions: ["AMSmath.js","AMSsymbols.js"],
                equationNumbers: { autoNumber: "AMS" } },
         extensions: ["tex2jax.js"],
         jax: ["input/TeX","output/HTML-CSS"],
         tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true },
         "HTML-CSS": { availableFonts: ["TeX"],
                       linebreaks: { automatic: true } }
     });
 </script>

 <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       tex2jax: {
         skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
       }
     });
 </script>

<script type="text/javascript"
   src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>




</body>
</html>

