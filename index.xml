<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LESS IS MORE</title>
    <link>https://r9y9.github.io/</link>
      <atom:link href="https://r9y9.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>LESS IS MORE</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Copyright © Ryuichi YAMAMOTO All rights reserved.</copyright><lastBuildDate>Tue, 18 Jan 2022 20:59:28 +0900</lastBuildDate>
    <image>
      <url>https://r9y9.github.io/media/icon_hu71488a41e9448d472219f1cc71ecc0ad_259818_512x512_fill_lanczos_center_3.png</url>
      <title>LESS IS MORE</title>
      <link>https://r9y9.github.io/</link>
    </image>
    
    <item>
      <title>Hugo Academic を使ってウェブサイトをアップデートしました</title>
      <link>https://r9y9.github.io/blog/2022/01/18/hugo-academic/</link>
      <pubDate>Tue, 18 Jan 2022 20:59:28 +0900</pubDate>
      <guid>https://r9y9.github.io/blog/2022/01/18/hugo-academic/</guid>
      <description>&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;新: 
  &lt;i class=&#34;fab fa-github  pr-1 fa-fw&#34;&gt;&lt;/i&gt; &lt;a href=&#34;https://github.com/r9y9/website&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/website&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/&#34;&gt;トップページ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/post/&#34;&gt;ブログ一覧&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/project/&#34;&gt;デモページ一覧&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;旧: 
  &lt;i class=&#34;fab fa-github  pr-1 fa-fw&#34;&gt;&lt;/i&gt; &lt;a href=&#34;https://github.com/r9y9/blog&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/blog&lt;/a&gt; + 
  &lt;i class=&#34;fab fa-github  pr-1 fa-fw&#34;&gt;&lt;/i&gt; &lt;a href=&#34;https://github.com/r9y9/demos-src&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/demos-src&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;これまで、ブログとデモページ&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;をそれぞれ個別に &lt;a href=&#34;https://gohugo.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugo&lt;/a&gt; で管理していましたが、それらを統合して一つの Hugo site として管理するように変更しました。
内部的に色々変わっていますが、URL はほぼ変わらないようにしています。&lt;/p&gt;
&lt;h2 id=&#34;はじめに&#34;&gt;はじめに&lt;/h2&gt;
&lt;p&gt;2年振りくらいにブログを書いています。大したことを書くわけではないのですが、ウェブサイト（ブログ含む）を大幅にアップデートしたので、その記録を残しておきます。&lt;/p&gt;
&lt;h2 id=&#34;why&#34;&gt;Why&lt;/h2&gt;
&lt;p&gt;なぜ大幅にアップデートをしたのか、理由は以下のとおりです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;経歴をまとめたプロフィールページなどコンテンツを追加したかった。ただし、これまでは、ブログとデモページをそれらに特化した個別の Hugo site として管理しており、それ以外の情報を追加することが容易ではなかった。プロフィール用に別の Hugo site を作ることも検討したが、管理のし易さの観点から一つの Hugo site にまとめたかった。&lt;/li&gt;
&lt;li&gt;Hugo の theme を修正してレイアウトを調整するのが大変だったので、いい感じレスポンシブにしてくれる、スマホで見てもレイアウトが崩れない Hugo theme を使いたくなった。
&lt;ul&gt;
&lt;li&gt;これまでは、minimal な theme をベースに自分で修正して使っていたが、限界がきた。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;後者のレイアウトの問題は、theme を変えれば済む話とも言えますが、前者の問題を解決するためには、theme 変更に加えて複数のsiteを統合する必要がありました。&lt;/p&gt;
&lt;h2 id=&#34;アップデートに関してやったこと&#34;&gt;アップデートに関してやったこと&lt;/h2&gt;
&lt;h3 id=&#34;基本方針&#34;&gt;基本方針&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Static site generator として Hugo を使う。速い。&lt;/li&gt;
&lt;li&gt;Hugo theme には &lt;a href=&#34;https://github.com/wowchemy/starter-hugo-academic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/wowchemy/starter-hugo-academic&lt;/a&gt; を使う。デザインがシンプルで好み、かつカスタマイズが柔軟にできそうだったので。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;具体的な手順&#34;&gt;具体的な手順&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/wowchemy/starter-hugo-academic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;starter-hugo-academic&lt;/a&gt; をベースに、全体のレイアウトを作る。僕の場合、Home (トップページ) に &lt;code&gt;Profile&lt;/code&gt;, &lt;code&gt;Posts&lt;/code&gt;, &lt;code&gt;Projects&lt;/code&gt;, &lt;code&gt;Talks&lt;/code&gt; の4つのコンテンツを配置しました。このうち、&lt;code&gt;Profile&lt;/code&gt; には、CV&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;のリンクを貼るようにしました。
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Publications&lt;/code&gt; コンテンツを作ることも考えましたが、Google Scholarで良くない？と思って作っていません。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/blog&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ブログ&lt;/a&gt; のコンテンツを &lt;code&gt;Posts&lt;/code&gt; に移行する。一覧は&lt;a href=&#34;https://r9y9.github.io/post/&#34;&gt;こちら&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://wowchemy.com/docs/content/front-matter/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://wowchemy.com/docs/content/front-matter/&lt;/a&gt; にも書かれている通り、Hugo academic ならではの front matter がありますが、基本的には markdown + 必要なstatic filesをコピーするだけで移行はできました。&lt;/li&gt;
&lt;li&gt;Hugo academic に &lt;code&gt;summary&lt;/code&gt; を表示する機能があったので、すべてのブログ記事に &lt;code&gt;summary&lt;/code&gt; を設定しました。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-themes/issues/1404&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tableのレイアウトが崩れる問題&lt;/a&gt;があったので、微妙にcssを修正しました。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/demos-src&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;デモページ&lt;/a&gt; のコンテンツを &lt;code&gt;Projects&lt;/code&gt; に移行する。一覧は&lt;a href=&#34;https://r9y9.github.io/project/&#34;&gt;こちら&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;blogを移行するのとまったく同じように移行できました&lt;/li&gt;
&lt;li&gt;論文に記載したデモページのURLが404にならないように、URLが変わらないようにする、あるいは リダイレクトを設定しました。Hugoだと、&lt;a href=&#34;https://gohugo.io/content-management/urls/#aliases&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;aliases&lt;/code&gt;&lt;/a&gt; というパラメータを front matter に書くことでリダイレクトを実現できます。&lt;/li&gt;
&lt;li&gt;これまでの取り組みを整理するいい機会だと思って、共著論文のデモページもコンテンツに追加しました。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Talks&lt;/code&gt; のコンテンツをおまけで新しく追加しました。&lt;a href=&#34;https://r9y9.github.io/talk/linedevday2020pwg/&#34;&gt;LINE DEV DAYでの過去の発表&lt;/a&gt;とか&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;雑感&#34;&gt;雑感&lt;/h2&gt;
&lt;h3 id=&#34;良かったこと&#34;&gt;良かったこと&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ウェブサイトはいい感じになった。プロフィールの追加ができたことはもちろん、モバイル端末で見てもレイアウトが崩れなくなったのは地味に嬉しい&lt;/li&gt;
&lt;li&gt;ブログとデモサイトを一つの Hugo site として管理することで、相互のコンテンツを行き来しやすくなった。具体的には、記事の終わりに、&lt;code&gt;Related&lt;/code&gt; というセクションに関連するブログ記事やデモページのリンクが表示されるようになった。また、ページのヘッダーから、相互のコンテンツを行き来することもできる。&lt;/li&gt;
&lt;li&gt;管理するリポジトリの数が4から2に減った。
&lt;ul&gt;
&lt;li&gt;旧: 
  &lt;i class=&#34;fab fa-github  pr-1 fa-fw&#34;&gt;&lt;/i&gt;&lt;a href=&#34;https://github.com/r9y9/blog&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog&lt;/a&gt;, 
  &lt;i class=&#34;fab fa-github  pr-1 fa-fw&#34;&gt;&lt;/i&gt;&lt;a href=&#34;https://github.com/r9y9/r9y9.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9.github.io&lt;/a&gt;, 
  &lt;i class=&#34;fab fa-github  pr-1 fa-fw&#34;&gt;&lt;/i&gt;&lt;a href=&#34;https://github.com/r9y9/demos-src&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;demos-src&lt;/a&gt;, 
  &lt;i class=&#34;fab fa-github  pr-1 fa-fw&#34;&gt;&lt;/i&gt;&lt;a href=&#34;https://github.com/r9y9/demos&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;demos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;新: 
  &lt;i class=&#34;fab fa-github  pr-1 fa-fw&#34;&gt;&lt;/i&gt;&lt;a href=&#34;https://github.com/r9y9/website&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt;, 
  &lt;i class=&#34;fab fa-github  pr-1 fa-fw&#34;&gt;&lt;/i&gt;&lt;a href=&#34;https://github.com/r9y9/r9y9.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9.github.io&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;悪かったこと&#34;&gt;悪かったこと&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/wowchemy/starter-hugo-academic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/wowchemy/starter-hugo-academic&lt;/a&gt; はカスタマイズの自由度は高い一方で、適切に設定するのが難しかった。ドキュメントは豊富だが量が多く、全部読んで使いこなすのは大変に感じた。&lt;/li&gt;
&lt;li&gt;大変だった・・・もうしばらくウェブサイトを大きく更新したくないなと思いました。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;おわりです。ただの備忘録ですが、ここまで読んで頂きありがとうございました。&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;音声合成の論文を書くときに、読者や査読者が音声サンプルを聴取できるようにデモページを作ることがあります。例えば&lt;a href=&#34;https://r9y9.github.io/projects/vuvd-pwg/&#34;&gt;こちら&lt;/a&gt;です。&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;CVの提出を求められることがたまにあるので、これまでの実績を整理するいい機会かなと思って作りました &lt;a href=&#34;https://github.com/r9y9/cv&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/cv&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>国際会議Interspeech2021参加報告 / Report on Participation in Interspeech2021 @SLP研究会</title>
      <link>https://r9y9.github.io/talk/sp-interspeech2021report/</link>
      <pubDate>Fri, 03 Dec 2021 13:00:00 +0900</pubDate>
      <guid>https://r9y9.github.io/talk/sp-interspeech2021report/</guid>
      <description>&lt;h3 id=&#34;abstract-ja&#34;&gt;Abstract (ja)&lt;/h3&gt;
&lt;p&gt;2021 年 8 月 30 日から 9 月 3 日にかけてチェコ・ブルノおよびオンラインのハイブリッド形式で Interspeech2021 が開催された．ここでは，会議概要や最新の技術動向，注目の発表について報告する．&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ESPnet2-TTS: Extending the Edge of TTS Research</title>
      <link>https://r9y9.github.io/projects/espnet2-tts/</link>
      <pubDate>Wed, 06 Oct 2021 20:42:37 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/espnet2-tts/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ttslearn: Library for Pythonで学ぶ音声合成 (Text-to-speech with Python)</title>
      <link>https://r9y9.github.io/projects/ttslearn/</link>
      <pubDate>Wed, 11 Aug 2021 16:27:22 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/ttslearn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Voicing-Aware Parallel WaveGAN for High-Quality Speech Synthesis</title>
      <link>https://r9y9.github.io/projects/va-pwg/</link>
      <pubDate>Fri, 30 Jul 2021 12:11:03 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/va-pwg/</guid>
      <description>&lt;p&gt;Submitted to &lt;a href=&#34;https://signalprocessingsociety.org/publications-resources/ieee-signal-processing-letters&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IEEE signal processing letters&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#authors&#34;&gt;Authors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#abstract&#34;&gt;Abstract&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tts-samples&#34;&gt;TTS samples&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#m1-male&#34;&gt;M1 (male)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#m2-male&#34;&gt;M2 (male)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#f1-female&#34;&gt;F1 (female)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#f2-female&#34;&gt;F2 (female)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#acknowledgements&#34;&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;authors&#34;&gt;Authors&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Ryuichi Yamamoto (LINE Corp.)&lt;/li&gt;
&lt;li&gt;Min-Jae Hwang (Search Solutions Inc.)&lt;/li&gt;
&lt;li&gt;Eunwoo Song (NAVER Corp.)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;This letter proposes a voicing-aware Parallel Wave- GAN (VA-PWG) vocoder for a neural text-to-speech (TTS) system. To generate a high-quality speech waveform, it is important to reflect the distinct characteristics of voiced and unvoiced speech signals well. However, it is difficult for the conventional PWG model to accurately represent this condition, since the single unified architectures of the generator and discriminator are insufficient to capture those characteristics. In the proposed method, both the generator and discriminator are divided into their subnetworks to individually model the voicing state-dependent characteristics of a speech signal. In particular, a VA-generator consisting of two sub-WaveNets generates the harmonic and noise components of a speech signal by inputting pitch-dependent sine wave and Gaussian noise sources, respectively. Likewise, a VA-discriminator consisting of two sub-discriminators learns the distinct characteristics of harmonic and noise components by feeding the voiced and unvoiced waveforms, respectively. Subjective evaluation results verified the effectiveness of the proposed VA-PWG vocoder by achieving a 4.25 mean opinion score from a speaker-independent training scenario that was 11% higher than that of a conventional PWG vocoder.&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/va-pwg.png&#34; width=&#34;80%&#34; /&gt;&lt;/div&gt;
&lt;h2 id=&#34;tts-samples&#34;&gt;TTS samples&lt;/h2&gt;
&lt;h3 id=&#34;m1-male&#34;&gt;M1 (male)&lt;/h3&gt;
&lt;p&gt;Sample 1: “鹿児島県で最大震度三を観測しています。”&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test01]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test01]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test01]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;VA-PWG-G&lt;/th&gt;&lt;th&gt;VA-PWG-D&lt;/th&gt;&lt;th&gt;VA-PWG-GD (proposed)&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test01]-S3-VA-PWG-G.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test01]-S4-VA-PWG-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test01]-S5-VA-PWG-GD (proposed).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2: “葉加瀬太郎の情熱大陸です。”&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test02]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test02]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test02]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;VA-PWG-G&lt;/th&gt;&lt;th&gt;VA-PWG-D&lt;/th&gt;&lt;th&gt;VA-PWG-GD (proposed)&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test02]-S3-VA-PWG-G.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test02]-S4-VA-PWG-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test02]-S5-VA-PWG-GD (proposed).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3: “それでうちの部は半分に減らされる。”&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test03]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test03]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test03]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;VA-PWG-G&lt;/th&gt;&lt;th&gt;VA-PWG-D&lt;/th&gt;&lt;th&gt;VA-PWG-GD (proposed)&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test03]-S3-VA-PWG-G.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test03]-S4-VA-PWG-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test03]-S5-VA-PWG-GD (proposed).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&#34;m2-male&#34;&gt;M2 (male)&lt;/h3&gt;
&lt;p&gt;Sample 1: “ヨメの、レオンティーンさんですね。”&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test01]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test01]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test01]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;VA-PWG-G&lt;/th&gt;&lt;th&gt;VA-PWG-D&lt;/th&gt;&lt;th&gt;VA-PWG-GD (proposed)&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test01]-S3-VA-PWG-G.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test01]-S4-VA-PWG-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test01]-S5-VA-PWG-GD (proposed).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2: “御予約は、二泊三日ですね。”&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test02]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test02]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test02]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;VA-PWG-G&lt;/th&gt;&lt;th&gt;VA-PWG-D&lt;/th&gt;&lt;th&gt;VA-PWG-GD (proposed)&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test02]-S3-VA-PWG-G.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test02]-S4-VA-PWG-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test02]-S5-VA-PWG-GD (proposed).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3: “わたさちの、ローリーさんですね。”&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test03]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test03]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test03]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;VA-PWG-G&lt;/th&gt;&lt;th&gt;VA-PWG-D&lt;/th&gt;&lt;th&gt;VA-PWG-GD (proposed)&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test03]-S3-VA-PWG-G.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test03]-S4-VA-PWG-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test03]-S5-VA-PWG-GD (proposed).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&#34;f1-female&#34;&gt;F1 (female)&lt;/h3&gt;
&lt;p&gt;Sample 1: “かわいそうに、助けてやらなくてはと、家に連れて帰りましたとさ。”&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test01]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test01]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test01]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;VA-PWG-G&lt;/th&gt;&lt;th&gt;VA-PWG-D&lt;/th&gt;&lt;th&gt;VA-PWG-GD (proposed)&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test01]-S3-VA-PWG-G.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test01]-S4-VA-PWG-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test01]-S5-VA-PWG-GD (proposed).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2: “失礼のないよう、笑顔で挨拶して。”&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test02]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test02]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test02]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;VA-PWG-G&lt;/th&gt;&lt;th&gt;VA-PWG-D&lt;/th&gt;&lt;th&gt;VA-PWG-GD (proposed)&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test02]-S3-VA-PWG-G.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test02]-S4-VA-PWG-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test02]-S5-VA-PWG-GD (proposed).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3: “照れていたので、ちょっと意外な気がしましたー。”&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test03]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test03]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test03]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;VA-PWG-G&lt;/th&gt;&lt;th&gt;VA-PWG-D&lt;/th&gt;&lt;th&gt;VA-PWG-GD (proposed)&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test03]-S3-VA-PWG-G.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test03]-S4-VA-PWG-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test03]-S5-VA-PWG-GD (proposed).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&#34;f2-female&#34;&gt;F2 (female)&lt;/h3&gt;
&lt;p&gt;Sample 1: “そして目に留まったのは、お気に入りの居酒屋の前にあるゴミの山。”&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test01]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test01]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test01]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;VA-PWG-G&lt;/th&gt;&lt;th&gt;VA-PWG-D&lt;/th&gt;&lt;th&gt;VA-PWG-GD (proposed)&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test01]-S3-VA-PWG-G.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test01]-S4-VA-PWG-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test01]-S5-VA-PWG-GD (proposed).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2: “今ひとつ、時間が足りず。”&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test02]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test02]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test02]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;VA-PWG-G&lt;/th&gt;&lt;th&gt;VA-PWG-D&lt;/th&gt;&lt;th&gt;VA-PWG-GD (proposed)&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test02]-S3-VA-PWG-G.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test02]-S4-VA-PWG-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test02]-S5-VA-PWG-GD (proposed).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3: “実はこの道の先に、高い山があってね。”&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test03]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test03]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test03]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;VA-PWG-G&lt;/th&gt;&lt;th&gt;VA-PWG-D&lt;/th&gt;&lt;th&gt;VA-PWG-GD (proposed)&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test03]-S3-VA-PWG-G.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test03]-S4-VA-PWG-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test03]-S5-VA-PWG-GD (proposed).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;Work performed with nVoice, Clova Voice, Naver Corp.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Unified accent estimation method based on multi-task learning for Japanese text-to-speech</title>
      <link>https://r9y9.github.io/projects/mtl_accent/</link>
      <pubDate>Sat, 10 Jul 2021 12:11:03 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/mtl_accent/</guid>
      <description></description>
    </item>
    
    <item>
      <title>High-fidelity Parallel WaveGAN with Multi-band Harmonic-plus-Noise Model</title>
      <link>https://r9y9.github.io/projects/mbhnpwg/</link>
      <pubDate>Fri, 02 Apr 2021 20:34:36 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/mbhnpwg/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Phrase break prediction with bidirectional encoder representations in Japanese text-to-speech synthesis</title>
      <link>https://r9y9.github.io/projects/pbp_bert/</link>
      <pubDate>Fri, 02 Apr 2021 20:34:36 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/pbp_bert/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ここまで来た音声技術・今後の展望 / Current progress on speech technologies and its future prospects @ LINE DEV DAY 2020</title>
      <link>https://r9y9.github.io/talk/linedevday2020panel/</link>
      <pubDate>Wed, 25 Nov 2020 16:40:00 +0900</pubDate>
      <guid>https://r9y9.github.io/talk/linedevday2020panel/</guid>
      <description>&lt;h3 id=&#34;abstract-ja&#34;&gt;Abstract (ja)&lt;/h3&gt;
&lt;p&gt;人の音声をテキストに変換する音声認識技術、テキストから人の音声を生成する音声合成技術をはじめとした音声処理技術が目覚ましい速度で進歩を続けている。さらに、音声に限らないドアの開け閉めの音など一般の音を識別する音響シーン・イベント検出技術などの新しい技術分野が拓けつつある。本セッションでは、LINEから2名のエンジニア（木田祐介・山本龍一）がパネリストとして登壇し、音声認識・音声合成の現状を語る。さらに、同志社大学の井本桂右准教授に登壇いただき、今年国際会議(DCASE)を日本に誘致するなど、日本の研究者の活躍が目覚ましい音響シーン・イベント検出技術の分野の現状を語っていただく。これらの技術分野の進歩には深層学習の進歩が強い影響を与えているが、音声処理特有の要素がどのようにして深層学習と絡み合い技術進化につながっているか掘り下げていきたい。また、様々な音声処理分野で、分野間で共通要素として進展が進む技術要素と特有の要素の分析を通し、各技術分野の特性を明らかにしていきたい。そして、今後どのような方向性で技術が進化していくか、将来の展望について議論していきたい。&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/iSPBCot6n7g&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Parallel WaveGAN: GPUを利用した高速かつ高品質な音声合成 / Parallel WaveGAN: Fast and High-Quality GPU Text-to-Speech @ LINE DEV DAY 2020</title>
      <link>https://r9y9.github.io/talk/linedevday2020pwg/</link>
      <pubDate>Wed, 25 Nov 2020 14:20:00 +0900</pubDate>
      <guid>https://r9y9.github.io/talk/linedevday2020pwg/</guid>
      <description>&lt;h3 id=&#34;abstract-ja&#34;&gt;Abstract (ja)&lt;/h3&gt;
&lt;p&gt;コンピュータによってテキストから人間の声を合成する技術は、テキスト音声合成と呼ばれます。LINE CLOVAのスマートスピーカーを初めとするユーザとのリアルタイムのインタラクションが必要なサービスでは、音声合成システムには合成品質が高いことだけでなく、高速に音声を生成できることが求められます。本セッションでは、高速かつ高品質な音声合成を実現するために、NAVERとLINEで共同で開発したGPUベースの音声合成の研究成果について発表します。従来の方法では、品質が良くても合成速度が遅い、合成速度は速い一方でモデルの学習に多大な時間がかかるなどの問題がありました。我々はそのような問題に対してどのようにアプローチしたのか、音声信号処理のトップカンファレンスICASSP 2020に採択された論文の内容を元に、近年の関連分野の発展を交えて紹介します。&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/BZxqf-Wkhig&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;/br&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/knzT7M6qsl0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Improved Parallel WaveGAN with perceptually weighted spectrogram loss</title>
      <link>https://r9y9.github.io/projects/pwg-pwsl/</link>
      <pubDate>Fri, 06 Nov 2020 16:43:44 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/pwg-pwsl/</guid>
      <description></description>
    </item>
    
    <item>
      <title>TTS-by-TTS: TTS-driven Data Augmentation for Fast and High-Quality Speech Synthesis</title>
      <link>https://r9y9.github.io/projects/tts-by-tts/</link>
      <pubDate>Mon, 26 Oct 2020 16:37:12 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/tts-by-tts/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Parallel waveform synthesis based on generative adversarial networks with voicing-aware conditional discriminators</title>
      <link>https://r9y9.github.io/projects/vuvd-pwg/</link>
      <pubDate>Wed, 21 Oct 2020 23:38:48 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/vuvd-pwg/</guid>
      <description>&lt;p&gt;Preprint: &lt;a href=&#34;https://arxiv.org/abs/2010.14151&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:2010.14151&lt;/a&gt; (accepted to &lt;a href=&#34;https://2021.ieeeicassp.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ICASSP 2021&lt;/a&gt;)&lt;/p&gt;
&lt;h2 id=&#34;table-of-contents&#34;&gt;Table of contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#analysis-synthesis&#34;&gt;Analysis/synthesis samples (Japanese)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#text-to-speech&#34;&gt;Text-to-speech samples (Japanese)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bonus-analysis-synthesis-english&#34;&gt;Bonus: analysis/synthesis samples for CMU ARCTIC (English)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;authors&#34;&gt;Authors&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Ryuichi Yamamoto (LINE Corp.)&lt;/li&gt;
&lt;li&gt;Eunwoo Song (NAVER Corp.)&lt;/li&gt;
&lt;li&gt;Min-Jae Hwang (Search Solutions Inc.)&lt;/li&gt;
&lt;li&gt;Jae-Min Kim (NAVER Corp.)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;This paper proposes voicing-aware conditional discriminators for Parallel WaveGAN-based waveform synthesis systems. In this framework, we adopt a projection-based conditioning method that can significantly improve the discriminator’s performance. Furthermore, the conventional discriminator is separated into two waveform discriminators for modeling voiced and unvoiced speech. As each discriminator learns the distinctive characteristics of the harmonic and noise components, respectively, the adversarial training process becomes more efficient, allowing the generator to produce more realistic speech waveforms. Subjective test results demonstrate the superiority of the proposed method over the conventional Parallel WaveGAN and WaveNet systems. In particular, our speaker-independently trained model within a FastSpeech 2 based text-to-speech framework achieves the mean opinion scores of 4.20, 4.18, 4.21, and 4.31 for four Japanese speakers, respectively.&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/icassp2021_fig.png&#34; width=&#34;50%&#34; /&gt;&lt;/div&gt;
&lt;h2 id=&#34;systems-for-comparision&#34;&gt;Systems for comparision&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;System&lt;/th&gt;
&lt;th&gt;Voiced segments&lt;/th&gt;
&lt;th&gt;Unvoiced segments&lt;/th&gt;
&lt;th&gt;Discriminator conditioning&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;S1-WaveNet &lt;a href=&#34;https://arxiv.org/abs/1807.07281&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[1]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;S2-PWG &lt;a href=&#34;https://arxiv.org/abs/1910.11480&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;S3-PWG-cGAN-D&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;S4-PWG-V/UV-D&lt;/td&gt;
&lt;td&gt;$D^{\mathrm{{v}}}$&lt;/td&gt;
&lt;td&gt;$D^{\mathrm{{v}}}$&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;S5-PWG-V/UV-D&lt;/td&gt;
&lt;td&gt;$D^{\mathrm{{uv}}}$&lt;/td&gt;
&lt;td&gt;$D^{\mathrm{{v}}}$&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;S6-PWG-V/UV-D&lt;/td&gt;
&lt;td&gt;$D^{\mathrm{{uv}}}$&lt;/td&gt;
&lt;td&gt;$D^{\mathrm{{uv}}}$&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;S7-PWG-V/UV-D (proposed)&lt;/td&gt;
&lt;td&gt;$D^{\mathrm{{v}}}$&lt;/td&gt;
&lt;td&gt;$D^{\mathrm{{uv}}}$&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Recordings&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;$D^{\mathrm{{v}}}$: 1-D dilated CNN discrimiantor with the reseptive field size of 127.&lt;/li&gt;
&lt;li&gt;$D^{\mathrm{{uv}}}$: 1-D CNN discrimiantor with the reseptive field size of 13.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;PWG denotes Parallel WaveGAN for short. Systems S2-PWG and S3-PWG-cGAN-D used $D^{\mathrm{{v}}}$  as the primary discriminator. &lt;strong&gt;Note that all the Parallel WaveGAN systems used the same generator architecture and training configurations; they only differed in the discriminator settings.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;analysissynthesis&#34;&gt;Analysis/synthesis&lt;/h2&gt;
&lt;h3 id=&#34;f1-female&#34;&gt;F1 (female)&lt;/h3&gt;
&lt;p&gt;Sample 1&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;S1-WaveNet&lt;/th&gt;&lt;th&gt;S2-PWG&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;S7-PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/AnaSyn/[Test01]-R1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/AnaSyn/[Test01]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/AnaSyn/[Test01]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/AnaSyn/[Test01]-S7-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;S3-PWG-cGAN-D&lt;/th&gt;&lt;th&gt;S4-PWG-V/UV-D&lt;/th&gt;&lt;th&gt;S5-PWG-V/UV-D&lt;/th&gt;&lt;th&gt;S6-PWG-V/UV-D&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/AnaSyn/[Test01]-S3-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/AnaSyn/[Test01]-S4-PWG-VUV-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/AnaSyn/[Test01]-S5-PWG-VUV-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/AnaSyn/[Test01]-S6-PWG-VUV-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&#34;f2-female&#34;&gt;F2 (female)&lt;/h3&gt;
&lt;p&gt;Sample 1&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;S1-WaveNet&lt;/th&gt;&lt;th&gt;S2-PWG&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;S7-PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/AnaSyn/[Test01]-R1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/AnaSyn/[Test01]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/AnaSyn/[Test01]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/AnaSyn/[Test01]-S7-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;S3-PWG-cGAN-D&lt;/th&gt;&lt;th&gt;S4-PWG-V/UV-D&lt;/th&gt;&lt;th&gt;S5-PWG-V/UV-D&lt;/th&gt;&lt;th&gt;S6-PWG-V/UV-D&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/AnaSyn/[Test01]-S3-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/AnaSyn/[Test01]-S4-PWG-VUV-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/AnaSyn/[Test01]-S5-PWG-VUV-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/AnaSyn/[Test01]-S6-PWG-VUV-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&#34;m1-male&#34;&gt;M1 (male)&lt;/h3&gt;
&lt;p&gt;Sample 1&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;S1-WaveNet&lt;/th&gt;&lt;th&gt;S2-PWG&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;S7-PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/AnaSyn/[Test01]-R1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/AnaSyn/[Test01]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/AnaSyn/[Test01]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/AnaSyn/[Test01]-S7-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;S3-PWG-cGAN-D&lt;/th&gt;&lt;th&gt;S4-PWG-V/UV-D&lt;/th&gt;&lt;th&gt;S5-PWG-V/UV-D&lt;/th&gt;&lt;th&gt;S6-PWG-V/UV-D&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/AnaSyn/[Test01]-S3-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/AnaSyn/[Test01]-S4-PWG-VUV-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/AnaSyn/[Test01]-S5-PWG-VUV-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/AnaSyn/[Test01]-S6-PWG-VUV-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&#34;m2-male&#34;&gt;M2 (male)&lt;/h3&gt;
&lt;p&gt;Sample 1&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;S1-WaveNet&lt;/th&gt;&lt;th&gt;S2-PWG&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;S7-PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/AnaSyn/[Test01]-R1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/AnaSyn/[Test01]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/AnaSyn/[Test01]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/AnaSyn/[Test01]-S7-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;S3-PWG-cGAN-D&lt;/th&gt;&lt;th&gt;S4-PWG-V/UV-D&lt;/th&gt;&lt;th&gt;S5-PWG-V/UV-D&lt;/th&gt;&lt;th&gt;S6-PWG-V/UV-D&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/AnaSyn/[Test01]-S3-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/AnaSyn/[Test01]-S4-PWG-VUV-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/AnaSyn/[Test01]-S5-PWG-VUV-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/AnaSyn/[Test01]-S6-PWG-VUV-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h2 id=&#34;text-to-speech&#34;&gt;Text-to-speech&lt;/h2&gt;
&lt;p&gt;FastSpeech 2 (&lt;a href=&#34;https://arxiv.org/abs/2006.04558&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[3]&lt;/a&gt;) based acoustic models were used for text-to-speech experiments.&lt;/p&gt;
&lt;h3 id=&#34;f1-female-1&#34;&gt;F1 (female)&lt;/h3&gt;
&lt;p&gt;Sample 1&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;FastSpeech 2 + WaveNet&lt;/th&gt;&lt;th&gt;FastSpeech 2 + PWG&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;FastSpeech 2 + PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/TTS/[Test01]-R1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/TTS/[Test01]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/TTS/[Test01]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/TTS/[Test01]-S7-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;FastSpeech 2 + PWG-cGAN-D&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/TTS/[Test01]-S3-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;FastSpeech 2 + WaveNet&lt;/th&gt;&lt;th&gt;FastSpeech 2 + PWG&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;FastSpeech 2 + PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/TTS/[Test02]-R1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/TTS/[Test02]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/TTS/[Test02]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/TTS/[Test02]-S7-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;FastSpeech 2 + PWG-cGAN-D&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/TTS/[Test02]-S3-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;FastSpeech 2 + WaveNet&lt;/th&gt;&lt;th&gt;FastSpeech 2 + PWG&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;FastSpeech 2 + PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/TTS/[Test03]-R1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/TTS/[Test03]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/TTS/[Test03]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/TTS/[Test03]-S7-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;FastSpeech 2 + PWG-cGAN-D&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/TTS/[Test03]-S3-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&#34;f2-female-1&#34;&gt;F2 (female)&lt;/h3&gt;
&lt;p&gt;Sample 1&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;FastSpeech 2 + WaveNet&lt;/th&gt;&lt;th&gt;FastSpeech 2 + PWG&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;FastSpeech 2 + PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/TTS/[Test01]-R1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/TTS/[Test01]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/TTS/[Test01]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/TTS/[Test01]-S7-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;FastSpeech 2 + PWG-cGAN-D&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/TTS/[Test01]-S3-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;FastSpeech 2 + WaveNet&lt;/th&gt;&lt;th&gt;FastSpeech 2 + PWG&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;FastSpeech 2 + PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/TTS/[Test02]-R1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/TTS/[Test02]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/TTS/[Test02]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/TTS/[Test02]-S7-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;FastSpeech 2 + PWG-cGAN-D&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/TTS/[Test02]-S3-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;FastSpeech 2 + WaveNet&lt;/th&gt;&lt;th&gt;FastSpeech 2 + PWG&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;FastSpeech 2 + PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/TTS/[Test03]-R1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/TTS/[Test03]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/TTS/[Test03]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/TTS/[Test03]-S7-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;FastSpeech 2 + PWG-cGAN-D&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/TTS/[Test03]-S3-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&#34;m1-male-1&#34;&gt;M1 (male)&lt;/h3&gt;
&lt;p&gt;Sample 1&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;FastSpeech 2 + WaveNet&lt;/th&gt;&lt;th&gt;FastSpeech 2 + PWG&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;FastSpeech 2 + PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/TTS/[Test01]-R1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/TTS/[Test01]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/TTS/[Test01]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/TTS/[Test01]-S7-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;FastSpeech 2 + PWG-cGAN-D&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/TTS/[Test01]-S3-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;FastSpeech 2 + WaveNet&lt;/th&gt;&lt;th&gt;FastSpeech 2 + PWG&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;FastSpeech 2 + PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/TTS/[Test02]-R1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/TTS/[Test02]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/TTS/[Test02]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/TTS/[Test02]-S7-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;FastSpeech 2 + PWG-cGAN-D&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/TTS/[Test02]-S3-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;FastSpeech 2 + WaveNet&lt;/th&gt;&lt;th&gt;FastSpeech 2 + PWG&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;FastSpeech 2 + PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/TTS/[Test03]-R1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/TTS/[Test03]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/TTS/[Test03]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/TTS/[Test03]-S7-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;FastSpeech 2 + PWG-cGAN-D&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/TTS/[Test03]-S3-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&#34;m2-male-1&#34;&gt;M2 (male)&lt;/h3&gt;
&lt;p&gt;Sample 1&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;FastSpeech 2 + WaveNet&lt;/th&gt;&lt;th&gt;FastSpeech 2 + PWG&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;FastSpeech 2 + PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/TTS/[Test01]-R1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/TTS/[Test01]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/TTS/[Test01]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/TTS/[Test01]-S7-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;FastSpeech 2 + PWG-cGAN-D&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/TTS/[Test01]-S3-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;FastSpeech 2 + WaveNet&lt;/th&gt;&lt;th&gt;FastSpeech 2 + PWG&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;FastSpeech 2 + PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/TTS/[Test02]-R1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/TTS/[Test02]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/TTS/[Test02]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/TTS/[Test02]-S7-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;FastSpeech 2 + PWG-cGAN-D&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/TTS/[Test02]-S3-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;FastSpeech 2 + WaveNet&lt;/th&gt;&lt;th&gt;FastSpeech 2 + PWG&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;FastSpeech 2 + PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/TTS/[Test03]-R1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/TTS/[Test03]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/TTS/[Test03]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/TTS/[Test03]-S7-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;FastSpeech 2 + PWG-cGAN-D&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/TTS/[Test03]-S3-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h2 id=&#34;bonus-analysissynthesis-english&#34;&gt;Bonus: Analysis/synthesis (English)&lt;/h2&gt;
&lt;p&gt;Samples for &lt;a href=&#34;http://www.festvox.org/cmu_arctic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CMU ARCTIC database&lt;/a&gt; are provided as follows. The models were trained using total six speakers (clb, slt, bdl, rms, jmk, and ksp) in a speaker-independent way.
The models were similary configured as the above experiments.&lt;/p&gt;
&lt;h3 id=&#34;clb-female&#34;&gt;clb (female)&lt;/h3&gt;
&lt;p&gt;Sample 1&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/clb/AnaSyn/[Test01]-clb_arctic_b0490_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/clb/AnaSyn/[Test01]-clb_arctic_b0490_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/clb/AnaSyn/[Test01]-clb_arctic_b0490_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/clb/AnaSyn/[Test01]-clb_arctic_b0490_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/clb/AnaSyn/[Test02]-clb_arctic_b0491_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/clb/AnaSyn/[Test02]-clb_arctic_b0491_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/clb/AnaSyn/[Test02]-clb_arctic_b0491_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/clb/AnaSyn/[Test02]-clb_arctic_b0491_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/clb/AnaSyn/[Test03]-clb_arctic_b0492_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/clb/AnaSyn/[Test03]-clb_arctic_b0492_gen-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/clb/AnaSyn/[Test03]-clb_arctic_b0492_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/clb/AnaSyn/[Test03]-clb_arctic_b0492_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&#34;slt-female&#34;&gt;slt (female)&lt;/h3&gt;
&lt;p&gt;Sample 1&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/slt/AnaSyn/[Test01]-slt_arctic_b0490_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/slt/AnaSyn/[Test01]-slt_arctic_b0490_gen-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/slt/AnaSyn/[Test01]-slt_arctic_b0490_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/slt/AnaSyn/[Test01]-slt_arctic_b0490_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/slt/AnaSyn/[Test02]-slt_arctic_b0491_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/slt/AnaSyn/[Test02]-slt_arctic_b0491_gen-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/slt/AnaSyn/[Test02]-slt_arctic_b0491_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/slt/AnaSyn/[Test02]-slt_arctic_b0491_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/slt/AnaSyn/[Test03]-slt_arctic_b0492_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/slt/AnaSyn/[Test03]-slt_arctic_b0492_gen-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/slt/AnaSyn/[Test03]-slt_arctic_b0492_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/slt/AnaSyn/[Test03]-slt_arctic_b0492_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&#34;bdl-male&#34;&gt;bdl (male)&lt;/h3&gt;
&lt;p&gt;Sample 1&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/bdl/AnaSyn/[Test01]-bdl_arctic_b0490_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/bdl/AnaSyn/[Test01]-bdl_arctic_b0490_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/bdl/AnaSyn/[Test01]-bdl_arctic_b0490_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/bdl/AnaSyn/[Test01]-bdl_arctic_b0490_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/bdl/AnaSyn/[Test02]-bdl_arctic_b0491_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/bdl/AnaSyn/[Test02]-bdl_arctic_b0491_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/bdl/AnaSyn/[Test02]-bdl_arctic_b0491_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/bdl/AnaSyn/[Test02]-bdl_arctic_b0491_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/bdl/AnaSyn/[Test03]-bdl_arctic_b0492_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/bdl/AnaSyn/[Test03]-bdl_arctic_b0492_gen-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/bdl/AnaSyn/[Test03]-bdl_arctic_b0492_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/bdl/AnaSyn/[Test03]-bdl_arctic_b0492_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&#34;rms-male&#34;&gt;rms (male)&lt;/h3&gt;
&lt;p&gt;Sample 1&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/rms/AnaSyn/[Test01]-rms_arctic_b0490_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/rms/AnaSyn/[Test01]-rms_arctic_b0490_gen-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/rms/AnaSyn/[Test01]-rms_arctic_b0490_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/rms/AnaSyn/[Test01]-rms_arctic_b0490_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/rms/AnaSyn/[Test02]-rms_arctic_b0491_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/rms/AnaSyn/[Test02]-rms_arctic_b0491_gen-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/rms/AnaSyn/[Test02]-rms_arctic_b0491_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/rms/AnaSyn/[Test02]-rms_arctic_b0491_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/rms/AnaSyn/[Test03]-rms_arctic_b0492_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/rms/AnaSyn/[Test03]-rms_arctic_b0492_gen-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/rms/AnaSyn/[Test03]-rms_arctic_b0492_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/rms/AnaSyn/[Test03]-rms_arctic_b0492_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&#34;jmk-male&#34;&gt;jmk (male)&lt;/h3&gt;
&lt;p&gt;Sample 1&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/jmk/AnaSyn/[Test01]-jmk_arctic_b0490_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/jmk/AnaSyn/[Test01]-jmk_arctic_b0490_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/jmk/AnaSyn/[Test01]-jmk_arctic_b0490_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/jmk/AnaSyn/[Test01]-jmk_arctic_b0490_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/jmk/AnaSyn/[Test02]-jmk_arctic_b0491_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/jmk/AnaSyn/[Test02]-jmk_arctic_b0491_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/jmk/AnaSyn/[Test02]-jmk_arctic_b0491_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/jmk/AnaSyn/[Test02]-jmk_arctic_b0491_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/jmk/AnaSyn/[Test03]-jmk_arctic_b0492_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/jmk/AnaSyn/[Test03]-jmk_arctic_b0492_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/jmk/AnaSyn/[Test03]-jmk_arctic_b0492_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/jmk/AnaSyn/[Test03]-jmk_arctic_b0492_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&#34;ksp-male&#34;&gt;ksp (male)&lt;/h3&gt;
&lt;p&gt;Sample 1&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/ksp/AnaSyn/[Test01]-ksp_arctic_b0490_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/ksp/AnaSyn/[Test01]-ksp_arctic_b0490_gen-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/ksp/AnaSyn/[Test01]-ksp_arctic_b0490_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/ksp/AnaSyn/[Test01]-ksp_arctic_b0490_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/ksp/AnaSyn/[Test02]-ksp_arctic_b0491_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/ksp/AnaSyn/[Test02]-ksp_arctic_b0491_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/ksp/AnaSyn/[Test02]-ksp_arctic_b0491_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/ksp/AnaSyn/[Test02]-ksp_arctic_b0491_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/ksp/AnaSyn/[Test03]-ksp_arctic_b0492_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/ksp/AnaSyn/[Test03]-ksp_arctic_b0492_gen-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/ksp/AnaSyn/[Test03]-ksp_arctic_b0492_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/ksp/AnaSyn/[Test03]-ksp_arctic_b0492_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;[1] W. Ping, K. Peng, and J. Chen, “ClariNet: Parallel wave generation in end-to-end text-to-speech,” in Proc. ICLR, 2019 &lt;a href=&#34;https://arxiv.org/abs/1807.07281&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;[2] R Yamamoto, E Song, and J.-M Kim, “Parallel WaveGAN:A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram,”  in Proc. ICASSP, 2020, pp. 6199–6203. &lt;a href=&#34;https://arxiv.org/abs/1910.11480&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;[3] Y Ren, C Hu, T Qin, S Zhao, Z Zhao, and T.-Y Liu, “Fast-speech 2: Fast and high-quality end-to-end text-to-speech,”arXiv preprint arXiv:2006.04558, 2020. &lt;a href=&#34;https://arxiv.org/abs/2006.04558&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;Work performed with nVoice, Clova Voice, Naver Corp.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;@inproceedings{yamamoto2020parallel,
  title={Parallel waveform synthesis based on generative adversarial networks with voicing-aware conditional discriminators},
  author={Ryuichi Yamamoto and Eunwoo Song and Min-Jae Hwang and Jae-Min Kim},
  booktitle=&amp;quot;Proc. of ICASSP (in press)&amp;quot;,
  year={2021},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>NNSVS: Pytorchベースの研究用歌声合成ライブラリ</title>
      <link>https://r9y9.github.io/blog/2020/05/10/nnsvs/</link>
      <pubDate>Sun, 10 May 2020 14:42:25 +0900</pubDate>
      <guid>https://r9y9.github.io/blog/2020/05/10/nnsvs/</guid>
      <description>&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;コード: &lt;a href=&#34;https://github.com/r9y9/nnsvs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/nnsvs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Discussion: &lt;a href=&#34;https://github.com/r9y9/nnsvs/issues/1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/nnsvs/issues/1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/r9y9/Colaboratory/blob/master/Neural_network_based_singing_voice_synthesis_demo_using_kiritan_singing_database_%28Japanese%29.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Demo on Google colab&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;春が来た　春が来た　どこに来た。　山に来た　里に来た、野にも来た。花がさく　花がさく　どこにさく。山にさく　里にさく、野にもさく。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;audio controls=&#34;controls&#34; &gt;&lt;source src=&#34;https://r9y9.github.io/audio/nnsvs/20200510_haru.wav&#34; autoplay/&gt;Your browser does not support the audio element.&lt;/audio&gt;&lt;/p&gt;
&lt;h2 id=&#34;nnsvs-はなに&#34;&gt;NNSVS はなに？&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Neural network-based singing voice synthesis library for research&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;研究用途を目的とした、歌声合成エンジンを作るためのオープンソースのライブラリを作ることを目指したプロジェクトです。このプロジェクトについて、考えていることをまとめておこうと思います。&lt;/p&gt;
&lt;h3 id=&#34;なぜやるか&#34;&gt;なぜやるか？&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://n3utrino.work/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NEUTRINO&lt;/a&gt; レベルの品質の歌声合成エンジンが作れるのかやってみたかった&lt;/li&gt;
&lt;li&gt;オープンソースのツールがほぼない分野なので、ツールを作ると誰かの役にも立っていいかなと思った。研究分野が盛り上がると良いですね&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;というのが理由です。前者の割合が大きく、後者は建前の要素が強いです。要は、できるかどうかがどうしても気になって、気がづいたら熱中していた、という感じです。&lt;/p&gt;
&lt;h3 id=&#34;研究用途&#34;&gt;研究用途&lt;/h3&gt;
&lt;p&gt;機械学習や信号処理にある程度明るい人を想定しています。歌声合成技術を使って創作したい人ではなく、どのようにすればより良い歌声合成を作ることができるのか？といった興味を持つ人が主な対象です。&lt;/p&gt;
&lt;p&gt;創作活動のために歌声合成の技術を使う場合には、すでに優れたツールがあると思いますので、そちらを使っていただくのがよいと思います&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;。&lt;a href=&#34;https://n3utrino.work/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NEUTRINO&lt;/a&gt;、&lt;a href=&#34;https://synthesizerv.com/jp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Synthesizer V&lt;/a&gt;、&lt;a href=&#34;http://cevio.jp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CeVIO&lt;/a&gt; など&lt;/p&gt;
&lt;h3 id=&#34;オープンソース&#34;&gt;オープンソース&lt;/h3&gt;
&lt;p&gt;オープンソースであることを重視します。歌声合成ソフトウェアは多くありますが、オープンソースのものは多くありません&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;。このプロジェクトは僕が趣味として始めたもので、ビジネスにする気はまったくないので&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;、誰でも自由に使えるようにしたいと思っています。オープンなソフトウェアが、研究分野の一助になることを期待しています。&lt;/p&gt;
&lt;h3 id=&#34;pytorchベース&#34;&gt;Pytorchベース&lt;/h3&gt;
&lt;p&gt;過去に &lt;a href=&#34;https://github.com/r9y9/nnmnkwii&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nnmnkwii&lt;/a&gt;という音声合成のためのライブラリを作りました。その際には、任意の数値微分ライブラリと使えるようにと考えて設計しましたが、nnsvsはあえてpytorchに依存した形で作ります。&lt;/p&gt;
&lt;p&gt;Pytorchと切り離して設計すると汎用的にしやすい一方で、&lt;a href=&#34;https://github.com/kaldi-asr/kaldi&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kaldi&lt;/a&gt; や&lt;a href=&#34;https://github.com/espnet/espnet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ESPnet&lt;/a&gt; のようなプロジェクトで成功している&lt;strong&gt;レシピ&lt;/strong&gt;というものが作りずらいです。ESPnetに多少関わって、再現性の担保の重要性を身にしみて感じつつあるので、Pytorchベースの学習、推論など、歌声合成のモデルを構築するために必要なすべてをひっくるめたソフトウェアを目指したいと思います。&lt;/p&gt;
&lt;h3 id=&#34;レシピの提供&#34;&gt;レシピの提供&lt;/h3&gt;
&lt;p&gt;再現性を重視します。そのために、KaldiやESPnetの成功に習って、レシピという実験を再現するのに必要なすべてのステップが含まれたスクリプトを提供します。レシピは、データの前処理、特徴量抽出、モデル学習、推論、波形の合成などを含みます。&lt;/p&gt;
&lt;p&gt;例えば、このブログのトップに貼った音声サンプルを合成するのに使われたモデルは、公開されているレシピで再現することが可能です。歌声合成エンジンを作るためのありとあらゆるものを透明な形で提供します。&lt;/p&gt;
&lt;h2 id=&#34;プロジェクトの進め方について&#34;&gt;プロジェクトの進め方について&lt;/h2&gt;
&lt;p&gt;完全に完成してから公開する、というアプローチとは正反対で、構想のみで実態はまったくできていない状態から始めて、進捗を含めてすべてオープンで確認できるような状態で進めます。進捗は &lt;a href=&#34;https://github.com/r9y9/nnsvs/issues/1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/nnsvs/issues/1&lt;/a&gt; から確認できます。&lt;/p&gt;
&lt;p&gt;過去に&lt;a href=&#34;https://github.com/r9y9/wavenet_vocoder&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;wavenet vocoder&lt;/a&gt;をつくったときにも同じような方法ではじめました。突然知らない人がコメントをくれたりするのがオープンソースの面白いところの一つだと思っているので、この方式で進めます。&lt;/p&gt;
&lt;h2 id=&#34;現時点の状況&#34;&gt;現時点の状況&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://zunko.jp/kiridev/login.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;きりたんデータベース&lt;/a&gt;を使って、parametric SVS（Sinsyの中身に近いもの）が一通り作れるところまでできました。MusicXMLを入力として、音声波形を出力します。作った歌声合成システムは、time-lagモデル、音素継続長モデル、音響モデルの3つのtrainableなモデルで成り立っています。音楽/言語的特徴量は&lt;a href=&#34;https://github.com/r9y9/sinsy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sinsy&lt;/a&gt;で抽出して、音声分析合成には&lt;a href=&#34;https://github.com/mmorise/World&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WORLD&lt;/a&gt;を使います。仕組みは、以下の論文の内容に近いです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Y. Hono et al, &amp;ldquo;Recent Development of the DNN-based Singing Voice Synthesis System — Sinsy,&amp;rdquo; Proc. of APSIPA, 2017. (&lt;a href=&#34;http://www.apsipa.org/proceedings/2018/pdfs/0001003.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Mixture density networkは使っていない、ビブラートパラメータを推定していない等、違いはたくさんあります。現時点では劣化sinsyといったところですね T.T&lt;/p&gt;
&lt;h2 id=&#34;開発履歴&#34;&gt;開発履歴&lt;/h2&gt;
&lt;h3 id=&#34;20200408-初期版&#34;&gt;2020/04/08 (初期版)&lt;/h3&gt;
&lt;p&gt;一番最初につくったものです。見事な音痴歌声合成になりました。TTSの仕組みを使うだけでは当然だめでした、というオチです。音響モデルでは対数lf0を予測するようにしました。このころはtime-lagモデルを作っていなくて、phonetic timeingはアノテーションされたデータのものを使っています。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;iframe width=&#34;90%&#34; height=&#34;200&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; allow=&#34;autoplay&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/792271372&amp;color=%23ff5500&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false&amp;show_teaser=true&amp;visual=true&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;h3 id=&#34;20200426-本ブログ執筆時点での最新版&#34;&gt;2020/04/26 (本ブログ執筆時点での最新版)&lt;/h3&gt;
&lt;p&gt;Time-lag, duration, acoustic modelのすべてを一旦実装し終わったバージョンです。lf0の絶対値を予測するのではなく、relativeなlf0を予測するように変えました。phonetic timing はすべて予測されたものを使っています。ひととおりできたにはいいですが、完成度はいまいちというのが正直なところですね&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;iframe width=&#34;90%&#34; height=&#34;200&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; allow=&#34;autoplay&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/806654083&amp;color=%23ff5500&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false&amp;show_teaser=true&amp;visual=true&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;h2 id=&#34;今後の予定&#34;&gt;今後の予定&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/nnsvs/issues/1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/nnsvs/issues/1&lt;/a&gt; を随時更新しますが、重要なものをいくつかピップアップします。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;音響モデルの強化&lt;/strong&gt;：特にF0のモデル化が難しい印象で、改善を考えています。いまは本当に適当なCNNをつかっていますが、autoreggresive modelに変えたいと思っています。いくつか選択肢がありますが、WaveNetのようなモデルにする予定です。https://mtg.github.io/singing-synthesis-demos/ 彼らの論文を大いに参考にする予定です。NIIのWangさんのshallow ARモデルを使うもよし。最重要課題で、目下やることリストに入っています&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;離散F0モデリング&lt;/strong&gt;: NIIのWangさんの論文が大変参考になりました。音声合成では広く連続F0が使われている印象ですが、離散F0モデリングを試したいと思っています。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transformerなどの強力なモデル&lt;/strong&gt;: 今年の &lt;a href=&#34;https://2020.ieeeicassp.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ICASSP 2020&lt;/a&gt; で &lt;a href=&#34;https://mtg.github.io/singing-synthesis-demos/transformer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Feed-forward Transformerを使った歌声合成の研究発表&lt;/a&gt;がありましたが、近年のnon-autoregressiveモデルの発展はすごいので、同様のアプローチを試してみたいと思っています。製品化は考えないし、どんなにデカくて遅いモデルを使ってもよし&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ニューラルボコーダ&lt;/strong&gt;: 音響モデルの改善がある程度できれば、ニューラルボコーダを入れて高品質にできるといいですね。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;音楽/言語特徴量の簡略化&lt;/strong&gt;: 今は450次元くらいの特徴量を使っていますが、https://mtg.github.io/singing-synthesis-demos/ 彼らのグループの研究を見ると、もっとシンプルにできそうに思えてきています。音楽/言語特徴量の抽出は今はsinsyに頼りっきりですが、どこかのタイミングでシンプルにしたいと思っています。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Time-lag/duration modelの改善&lt;/strong&gt;: 現時点ではめっちゃ雑なつくりなので、https://mtg.github.io/singing-synthesis-demos/ 彼らの研究を見習って細部まで詰めたい&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;音素アライメントツール&lt;/strong&gt;: きりたんDBの音素アライメントが微妙に不正確なのがあったりします。今のところある程度手修正していますが、自動でやったほうがいいのではと思えてきました。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;その他データセット&lt;/strong&gt;: JVSなど。きりたんDBである程度できてからですかね&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;これまで歌声合成をやってみての所感&#34;&gt;これまで歌声合成をやってみての所感&lt;/h2&gt;
&lt;p&gt;歌声合成クッソムズすぎワロタ&lt;/p&gt;
&lt;p&gt;新しいことにチャレンジするのはとても楽しいですが、やっぱり難しいですね。離散化F0、autoregressive modelの導入でそれなりの品質に持っていけるという淡い期待をしていますが、さてどうなることやら。地道に頑張って改善していきます。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;きりたんデータベース: &lt;a href=&#34;https://zunko.jp/kiridev/login.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://zunko.jp/kiridev/login.php&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;NEUTRINO: &lt;a href=&#34;https://n3utrino.work/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://n3utrino.work/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;NNSVS: &lt;a href=&#34;https://github.com/r9y9/nnsvs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/nnsvs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;NNSVS 進捗: &lt;a href=&#34;https://github.com/r9y9/nnsvs/issues/1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/nnsvs/issues/1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;sinsy: &lt;a href=&#34;http://sinsy.sourceforge.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://sinsy.sourceforge.net/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;My fork of sinsy: &lt;a href=&#34;https://github.com/r9y9/sinsy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/sinsy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;nnmnkwii: &lt;a href=&#34;https://github.com/r9y9/nnmnkwii&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/nnmnkwii&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;WORLD: &lt;a href=&#34;https://github.com/mmorise/World&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/mmorise/World&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Y. Hono et al, &amp;ldquo;Recent Development of the DNN-based Singing Voice Synthesis System — Sinsy,&amp;rdquo; Proc. of APSIPA, 2017. (&lt;a href=&#34;http://www.apsipa.org/proceedings/2018/pdfs/0001003.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;NEUTRINO並の品質の歌声合成エンジンが作れたらいいなとは思っていますが、まだまだ道のりは長そうです。&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://sinsy.sourceforge.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://sinsy.sourceforge.net/&lt;/a&gt; 有名なものにsinsyがありますが、DNNモデルの学習など、すべてがオープンソースなわけではありません&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;万が一の場合は、察してください…&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Neural text-to-speech with a modeling-by-generation excitation vocoder</title>
      <link>https://r9y9.github.io/projects/mbg_excitnet/</link>
      <pubDate>Wed, 22 Apr 2020 16:51:09 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/mbg_excitnet/</guid>
      <description></description>
    </item>
    
    <item>
      <title>End-to-End 音声合成の研究を加速させるツールキット ESPnet-TTS / ESPnet-TTS: A toolkit to accelerate research on end-to-end speech synthesis @ ASJ 2020s</title>
      <link>https://r9y9.github.io/talk/asj-espnet2-tutorial/</link>
      <pubDate>Mon, 16 Mar 2020 13:00:00 +0900</pubDate>
      <guid>https://r9y9.github.io/talk/asj-espnet2-tutorial/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ESPnet-TTS: Unified, Reproducible, and Integratable Open Source End-to-End Text-to-Speech Toolkit</title>
      <link>https://r9y9.github.io/projects/espnet-tts/</link>
      <pubDate>Thu, 24 Oct 2019 16:21:27 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/espnet-tts/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Parallel WaveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram</title>
      <link>https://r9y9.github.io/projects/pwg/</link>
      <pubDate>Mon, 21 Oct 2019 20:18:27 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/pwg/</guid>
      <description>&lt;p&gt;Preprint: &lt;a href=&#34;https://arxiv.org/abs/1910.11480&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1910.11480&lt;/a&gt; (accepted to &lt;a href=&#34;https://2020.ieeeicassp.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ICASSP 2020&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#audio-samples-japanese&#34;&gt;Audio samples (Japanese)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#audio-samples-english&#34;&gt;Audio samples (English)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Japanese samples were used in the subjective evaluations reported in our paper.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;authors&#34;&gt;Authors&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Ryuichi Yamamoto (LINE Corp.)&lt;/li&gt;
&lt;li&gt;Eunwoo Song (NAVER Corp.)&lt;/li&gt;
&lt;li&gt;Jae-Min Kim (NAVER Corp.)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;We propose Parallel WaveGAN&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, a distillation-free, fast, and small-footprint waveform generation method using a generative adversarial network. In the proposed method, a non-autoregressive WaveNet is trained by jointly optimizing multi-resolution spectrogram and adversarial loss functions, which can effectively capture the time-frequency distribution of the realistic speech waveform. As our method does not require density distillation used in the conventional teacher-student framework, the entire model can be easily trained even with a small number of parameters. In particular, the proposed Parallel WaveGAN has only 1.44 M parameters and can generate 24 kHz speech waveform 28.68 times faster than real-time on a single GPU environment. Perceptual listening test results verify that our proposed method achieves 4.16 mean opinion score within a Transformer-based text-to-speech framework, which is comparative to the best distillation-based Parallel WaveNet system.&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/icassp2020_fig.png&#34; width=&#34;60%&#34; /&gt;&lt;/div&gt;
&lt;h3 id=&#34;systems-used-for-comparision&#34;&gt;Systems used for comparision&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Ground truth&lt;/strong&gt;: Recorded speech.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;WaveNet&lt;/strong&gt;: Gaussian WaveNet &lt;a href=&#34;https://arxiv.org/abs/1807.07281&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[1]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ClariNet-$L^{(1)}$&lt;/strong&gt;: ClariNet &lt;a href=&#34;https://arxiv.org/abs/1807.07281&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[1]&lt;/a&gt; with the single STFT auxiliary loss&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ClariNet-$L^{(1,2,3)}$&lt;/strong&gt;: ClariNet with the multi-resolution STFT loss&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ClariNet-GAN-$L^{(1,2,3)}$&lt;/strong&gt;: ClariNet with the multi-resolution STFT and adversarial losses &lt;a href=&#34;https://arxiv.org/abs/1807.07281&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Parallel WaveGAN-$L^{(1)}$&lt;/strong&gt;: Parallel WaveGAN with th single STFT loss&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Parallel WaveGAN-$L^{(1,2,3)}$&lt;/strong&gt;: Parallel WaveGAN with the multi-resolution STFT loss&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;audio-samples-japanese&#34;&gt;Audio samples (Japanese)&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Analysis/synthesis&lt;/li&gt;
&lt;li&gt;Text-to-speech (Transformer TTS + vocoder models)&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;analysissynthesis&#34;&gt;Analysis/synthesis&lt;/h3&gt;
&lt;p&gt;Sample 1&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;ClariNet-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample01]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample01]-2-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample01]-3-ClariNet (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;ClariNet-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;ClariNet-GAN-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample01]-4-ClariNet (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample01]-5-ClariNet-GAN (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample01]-6-Parallel WaveGAN (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1,2,3)}$ &lt;font color=&#34;#0000cd&#34;&gt;(ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample01]-7-Parallel WaveGAN (3spec) (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;ClariNet-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample02]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample02]-2-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample02]-3-ClariNet (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;ClariNet-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;ClariNet-GAN-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample02]-4-ClariNet (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample02]-5-ClariNet-GAN (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample02]-6-Parallel WaveGAN (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1,2,3)}$ &lt;font color=&#34;#0000cd&#34;&gt;(ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample02]-7-Parallel WaveGAN (3spec) (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;ClariNet-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample03]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample03]-2-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample03]-3-ClariNet (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;ClariNet-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;ClariNet-GAN-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample03]-4-ClariNet (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample03]-5-ClariNet-GAN (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample03]-6-Parallel WaveGAN (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1,2,3)}$ &lt;font color=&#34;#0000cd&#34;&gt;(ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample03]-7-Parallel WaveGAN (3spec) (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 4&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;ClariNet-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample04]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample04]-2-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample04]-3-ClariNet (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;ClariNet-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;ClariNet-GAN-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample04]-4-ClariNet (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample04]-5-ClariNet-GAN (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample04]-6-Parallel WaveGAN (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1,2,3)}$ &lt;font color=&#34;#0000cd&#34;&gt;(ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample04]-7-Parallel WaveGAN (3spec) (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 5&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;ClariNet-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample05]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample05]-2-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample05]-3-ClariNet (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;ClariNet-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;ClariNet-GAN-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample05]-4-ClariNet (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample05]-5-ClariNet-GAN (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample05]-6-Parallel WaveGAN (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1,2,3)}$ &lt;font color=&#34;#0000cd&#34;&gt;(ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample05]-7-Parallel WaveGAN (3spec) (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&#34;text-to-speech&#34;&gt;Text-to-speech&lt;/h3&gt;
&lt;p&gt;Sample 1&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;Transformer + WaveNet&lt;/th&gt;&lt;th&gt;Transformer + ClariNet-$L^{(1,2,3)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample01]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample01]-2-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample01]-4-ClariNet (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Transformer + ClariNet-GAN-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;Transformer + Parallel WaveGAN--$L^{(1,2,3)}$ &lt;font color=&#34;#0000cd&#34;&gt;(ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample01]-5-ClariNet-GAN (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample01]-7-Parallel WaveGAN (3spec) (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;Transformer + WaveNet&lt;/th&gt;&lt;th&gt;Transformer + ClariNet-$L^{(1,2,3)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample02]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample02]-2-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample02]-4-ClariNet (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Transformer + ClariNet-GAN-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;Transformer + Parallel WaveGAN--$L^{(1,2,3)}$ &lt;font color=&#34;#0000cd&#34;&gt;(ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample02]-5-ClariNet-GAN (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample02]-7-Parallel WaveGAN (3spec) (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;Transformer + WaveNet&lt;/th&gt;&lt;th&gt;Transformer + ClariNet-$L^{(1,2,3)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample03]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample03]-2-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample03]-4-ClariNet (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Transformer + ClariNet-GAN-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;Transformer + Parallel WaveGAN--$L^{(1,2,3)}$ &lt;font color=&#34;#0000cd&#34;&gt;(ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample03]-5-ClariNet-GAN (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample03]-7-Parallel WaveGAN (3spec) (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 4&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;Transformer + WaveNet&lt;/th&gt;&lt;th&gt;Transformer + ClariNet-$L^{(1,2,3)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample04]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample04]-2-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample04]-4-ClariNet (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Transformer + ClariNet-GAN-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;Transformer + Parallel WaveGAN--$L^{(1,2,3)}$ &lt;font color=&#34;#0000cd&#34;&gt;(ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample04]-5-ClariNet-GAN (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample04]-7-Parallel WaveGAN (3spec) (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 5&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;Transformer + WaveNet&lt;/th&gt;&lt;th&gt;Transformer + ClariNet-$L^{(1,2,3)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample05]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample05]-2-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample05]-4-ClariNet (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Transformer + ClariNet-GAN-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;Transformer + Parallel WaveGAN--$L^{(1,2,3)}$ &lt;font color=&#34;#0000cd&#34;&gt;(ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample05]-5-ClariNet-GAN (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample05]-7-Parallel WaveGAN (3spec) (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h2 id=&#34;audio-samples-english&#34;&gt;Audio samples (English)&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Analysis/synthesis&lt;/li&gt;
&lt;li&gt;Text-to-speech (&lt;a href=&#34;https://arxiv.org/abs/1910.10909&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ESPnet-TTS&lt;/a&gt; + Our Parallel WaveGAN)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;https://keithito.com/LJ-Speech-Dataset/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LJSpeech dataset&lt;/a&gt; is used for the test. Mel-spectrograms (with the range of 70 - 7600 Hz&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;) were used for local conditioning.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Please note that the English samples were not used in the subjective evaluations reported in our paper.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;analysissynthesis-1&#34;&gt;Analysis/synthesis&lt;/h3&gt;
&lt;p&gt;That is reflected in definite and comprehensive operating procedures.&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;ClariNet-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample01]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample01]-2-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample01]-3-ClariNet (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;ClariNet-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;ClariNet-GAN-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample01]-4-ClariNet (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample01]-5-ClariNet-GAN (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample01]-6-Parallel WaveGAN (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1,2,3)}$ &lt;font color=&#34;#0000cd&#34;&gt;(ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample01]-7-Parallel WaveGAN (3spec) (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;The commission also recommends.&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;ClariNet-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample02]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample02]-2-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample02]-3-ClariNet (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;ClariNet-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;ClariNet-GAN-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample02]-4-ClariNet (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample02]-5-ClariNet-GAN (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample02]-6-Parallel WaveGAN (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1,2,3)}$ &lt;font color=&#34;#0000cd&#34;&gt;(ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample02]-7-Parallel WaveGAN (3spec) (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;That the secret service consciously set about the task of inculcating and maintaining the highest standard of excellence and esprit, for all of its personnel.&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;ClariNet-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample03]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample03]-2-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample03]-3-ClariNet (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;ClariNet-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;ClariNet-GAN-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample03]-4-ClariNet (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample03]-5-ClariNet-GAN (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample03]-6-Parallel WaveGAN (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1,2,3)}$ &lt;font color=&#34;#0000cd&#34;&gt;(ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample03]-7-Parallel WaveGAN (3spec) (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;This involves tight and unswerving discipline as well as the promotion of an outstanding degree of dedication and loyalty to duty.&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;ClariNet-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample04]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample04]-2-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample04]-3-ClariNet (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;ClariNet-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;ClariNet-GAN-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample04]-4-ClariNet (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample04]-5-ClariNet-GAN (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample04]-6-Parallel WaveGAN (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1,2,3)}$ &lt;font color=&#34;#0000cd&#34;&gt;(ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample04]-7-Parallel WaveGAN (3spec) (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;The commission emphasizes that it finds no causal connection between the assassination.&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;ClariNet-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample05]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample05]-2-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample05]-3-ClariNet (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;ClariNet-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;ClariNet-GAN-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample05]-4-ClariNet (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample05]-5-ClariNet-GAN (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample05]-6-Parallel WaveGAN (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1,2,3)}$ &lt;font color=&#34;#0000cd&#34;&gt;(ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample05]-7-Parallel WaveGAN (3spec) (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&#34;text-to-speech-1&#34;&gt;Text-to-speech&lt;/h3&gt;
&lt;p&gt;We combined our Parallel WaveGAN with &lt;a href=&#34;https://arxiv.org/abs/1910.10909&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ESPnet-TTS&lt;/a&gt;. The systems used here are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Transformer.v3&lt;/strong&gt;: Transformer.v3 presented in &lt;a href=&#34;https://arxiv.org/abs/1910.10909&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ESPnet-TTS&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MoL WaveNet&lt;/strong&gt;: WaveNet with mixture of logistics output distribution (shipped with ESPnet). Pre-emphasis/de-emphasis were applied to reduce perceptual noise (similar to &lt;a href=&#34;https://arxiv.org/abs/1810.11846&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LPCNet&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Parallel WaveGAN&lt;/strong&gt;: Our Parallel WaveGAN with multi-resolution spectrogram loss.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that &lt;strong&gt;Transformer.v3 + MoL WaveNet&lt;/strong&gt; is the same as used in &lt;a href=&#34;https://espnet.github.io/icassp2020-tts/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://espnet.github.io/icassp2020-tts/&lt;/a&gt;.
Mel-spectrograms (with the range of 70 - 11025 Hz) were used for local conditioning.&lt;/p&gt;
&lt;p&gt;That is reflected in definite and comprehensive operating procedures.&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;Transformer.v3 + MoL WaveNet&lt;/th&gt;&lt;th&gt;Transformer.v3 + Parallel WaveGAN&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample01]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/TTS/[LJ-Sample01]-1-MoL-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/TTS/[LJ-Sample01]-2-Parallel WaveGAN.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;The commission also recommends.&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;Transformer.v3 + MoL WaveNet&lt;/th&gt;&lt;th&gt;Transformer.v3 + Parallel WaveGAN&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample02]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/TTS/[LJ-Sample02]-1-MoL-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/TTS/[LJ-Sample02]-2-Parallel WaveGAN.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;That the secret service consciously set about the task of inculcating and maintaining the highest standard of excellence and esprit, for all of its personnel.&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;Transformer.v3 + MoL WaveNet&lt;/th&gt;&lt;th&gt;Transformer.v3 + Parallel WaveGAN&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample03]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/TTS/[LJ-Sample03]-1-MoL-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/TTS/[LJ-Sample03]-2-Parallel WaveGAN.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;This involves tight and unswerving discipline as well as the promotion of an outstanding degree of dedication and loyalty to duty.&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;Transformer.v3 + MoL WaveNet&lt;/th&gt;&lt;th&gt;Transformer.v3 + Parallel WaveGAN&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample04]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/TTS/[LJ-Sample04]-1-MoL-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/TTS/[LJ-Sample04]-2-Parallel WaveGAN.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;The commission emphasizes that it finds no causal connection between the assassination.&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;Transformer.v3 + MoL WaveNet&lt;/th&gt;&lt;th&gt;Transformer.v3 + Parallel WaveGAN&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample05]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/TTS/[LJ-Sample05]-1-MoL-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/TTS/[LJ-Sample05]-2-Parallel WaveGAN.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;W. Ping, K. Peng, and J. Chen, “ClariNet: Parallel wave generation in end-to-end text-to-speech,” in Proc. ICLR, 2019 (&lt;a href=&#34;https://arxiv.org/abs/1807.07281&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;R. Yamamoto, E. Song, and J.-M. Kim, “Probability density distillation with generative adversarial networks for high-quality parallel waveform generation,” in Proc. INTERSPEECH, 2019, pp. 699–703. (&lt;a href=&#34;https://www.isca-speech.org/archive/Interspeech_2019/abstracts/1965.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ISCA archive&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;Work performed with nVoice, Clova Voice, Naver Corp.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;@inproceedings{yamamoto2020parallel,
  title={Parallel {WaveGAN}: {A} fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram},
  author={Yamamoto, Ryuichi and Song, Eunwoo and Kim, Jae-Min},
  booktitle	= &amp;quot;Proc. of ICASSP&amp;quot;,
  pages={6199--6203},
  year={2020}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Note that our work is not closely related to an unsupervised waveform synthesis model, &lt;a href=&#34;https://arxiv.org/abs/1802.04208&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WaveGAN&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Audio quaility can be improved by using the full-band frequency range, but it may suffer from the over-smoothing problem in TTS.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Probability Density Distillation with Generative Adversarial Networks for High-Quality Parallel Waveform Generation</title>
      <link>https://r9y9.github.io/projects/gan-pwn/</link>
      <pubDate>Tue, 25 Jun 2019 17:20:29 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/gan-pwn/</guid>
      <description>&lt;p&gt;Preprint: &lt;a href=&#34;https://arxiv.org/abs/1904.04472&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1904.04472&lt;/a&gt;, Published version: &lt;a href=&#34;https://www.isca-speech.org/archive_v0/Interspeech_2019/abstracts/1965.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ISCA Archive Interspeech 2019&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;authors&#34;&gt;Authors&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Ryuichi Yamamoto (LINE Corp.)&lt;/li&gt;
&lt;li&gt;Eunwoo Song (NAVER Corp.)&lt;/li&gt;
&lt;li&gt;Jae-Min Kim (NAVER Corp.)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;This paper proposes an effective probability density distillation (PDD) algorithm for WaveNet-based parallel waveform generation (PWG) systems. Recently proposed teacher-student frameworks in the PWG system have successfully achieved a real-time generation of speech signals. However, the difficulties optimizing the PDD criteria without auxiliary losses result in quality degradation of synthesized speech. To generate more natural speech signals within the teacher-student framework, we propose a novel optimization criterion based on generative adversarial networks (GANs). In the proposed method, the inverse autoregressive flow-based student model is incorporated as a generator in the GAN framework, and jointly optimized by the PDD mechanism with the proposed adversarial learning method. As this process encourages the student to model the distribution of realistic speech waveform, the perceptual quality of the synthesized speech becomes much more natural. Our experimental results verify that the PWG systems with the proposed method outperform both those using conventional approaches, and also autoregressive generation systems with a well-trained teacher WaveNet.&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/interspeech2019_fig.png&#34; width=&#34;90%&#34; /&gt;&lt;/div&gt;
&lt;h2 id=&#34;audio-samples&#34;&gt;Audio samples&lt;/h2&gt;
&lt;p&gt;There are 8 different systems, that include 6 parallel waveform generation systems (Student-*) trained by different optimization criteria as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Ground truth&lt;/strong&gt;: Recorded speech.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Teacher&lt;/strong&gt;: Teacher Gaussian WaveNet &lt;a href=&#34;https://arxiv.org/abs/1807.07281&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[1]&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Student-AX&lt;/strong&gt;: STFT auxiliary loss.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Student-AXAD&lt;/strong&gt;: STFT and adversarial losses.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Student-KL&lt;/strong&gt;: KLD loss (Ablation study; not used for subjective evaluations).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Student-KLAX&lt;/strong&gt;: KLD and STFT auxiliary losses.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Student-KLAXAD&lt;/strong&gt;: KLD, STFT, and adversarial losses (proposed).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Student-KLAXAD&lt;/strong&gt;*: Weights optimized version of the above (proposed).&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;copy-synthesis&#34;&gt;Copy-synthesis&lt;/h3&gt;
&lt;h4 id=&#34;japanese-female-speaker&#34;&gt;Japanese female speaker&lt;/h4&gt;
&lt;p&gt;Sample 1&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;Teacher&lt;/th&gt;&lt;th&gt;Student-AX&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample01]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample01]-2-Teacher.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample01]-3-Student-AX (AuxLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Student-AXAV&lt;/th&gt;&lt;th&gt;Student-KL&lt;/th&gt;&lt;th&gt;Student-KLAX&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample01]-4-Student-AXAV (AuxLoss + AdvLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample01]-5-Student-KL (KLD only; ablation study).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample01]-6-Student-KLAX (KLD + AuxLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Student-KLAXAD&lt;/th&gt;&lt;th&gt;Student-KLAXAD*&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample01]-7-Student-KLAXAD (Proposed; KLD + AuxLoss + AdvLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample01]-8-Student-KLAXAD (Proposed; weights optimized version).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;Teacher&lt;/th&gt;&lt;th&gt;Student-AX&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample02]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample02]-2-Teacher.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample02]-3-Student-AX (AuxLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Student-AXAV&lt;/th&gt;&lt;th&gt;Student-KL&lt;/th&gt;&lt;th&gt;Student-KLAX&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample02]-4-Student-AXAV (AuxLoss + AdvLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample02]-5-Student-KL (KLD only; ablation study).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample02]-6-Student-KLAX (KLD + AuxLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Student-KLAXAD&lt;/th&gt;&lt;th&gt;Student-KLAXAD*&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample02]-7-Student-KLAXAD (Proposed; KLD + AuxLoss + AdvLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample02]-8-Student-KLAXAD (Proposed; weights optimized version).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;Teacher&lt;/th&gt;&lt;th&gt;Student-AX&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample03]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample03]-2-Teacher.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample03]-3-Student-AX (AuxLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Student-AXAV&lt;/th&gt;&lt;th&gt;Student-KL&lt;/th&gt;&lt;th&gt;Student-KLAX&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample03]-4-Student-AXAV (AuxLoss + AdvLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample03]-5-Student-KL (KLD only; ablation study).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample03]-6-Student-KLAX (KLD + AuxLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Student-KLAXAD&lt;/th&gt;&lt;th&gt;Student-KLAXAD*&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample03]-7-Student-KLAXAD (Proposed; KLD + AuxLoss + AdvLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample03]-8-Student-KLAXAD (Proposed; weights optimized version).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 4&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;Teacher&lt;/th&gt;&lt;th&gt;Student-AX&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample04]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample04]-2-Teacher.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample04]-3-Student-AX (AuxLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Student-AXAV&lt;/th&gt;&lt;th&gt;Student-KL&lt;/th&gt;&lt;th&gt;Student-KLAX&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample04]-4-Student-AXAV (AuxLoss + AdvLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample04]-5-Student-KL (KLD only; ablation study).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample04]-6-Student-KLAX (KLD + AuxLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Student-KLAXAD&lt;/th&gt;&lt;th&gt;Student-KLAXAD*&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample04]-7-Student-KLAXAD (Proposed; KLD + AuxLoss + AdvLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample04]-8-Student-KLAXAD (Proposed; weights optimized version).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 5&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;Teacher&lt;/th&gt;&lt;th&gt;Student-AX&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample05]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample05]-2-Teacher.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample05]-3-Student-AX (AuxLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Student-AXAV&lt;/th&gt;&lt;th&gt;Student-KL&lt;/th&gt;&lt;th&gt;Student-KLAX&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample05]-4-Student-AXAV (AuxLoss + AdvLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample05]-5-Student-KL (KLD only; ablation study).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample05]-6-Student-KLAX (KLD + AuxLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Student-KLAXAD&lt;/th&gt;&lt;th&gt;Student-KLAXAD*&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample05]-7-Student-KLAXAD (Proposed; KLD + AuxLoss + AdvLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample05]-8-Student-KLAXAD (Proposed; weights optimized version).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;[1]: W. Ping, K. Peng, and J. Chen, “ClariNet: Parallel wave generation in end-to-end text-to-speech,” in Proc. ICLR, 2019 (&lt;a href=&#34;https://arxiv.org/abs/1807.07281&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;Work performed with nVoice, Clova Voice, Naver Corp.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;@inproceedings{Yamamoto2019,
  author={Ryuichi Yamamoto and Eunwoo Song and Jae-Min Kim},
  title={{Probability Density Distillation with Generative Adversarial Networks for High-Quality Parallel Waveform Generation}},
  year=2019,
  booktitle={Proc. Interspeech 2019},
  pages={699--703},
  doi={10.21437/Interspeech.2019-1965},
  url={http://dx.doi.org/10.21437/Interspeech.2019-1965}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>LJSpeech は価値のあるデータセットですが、ニューラルボコーダの品質比較には向かないと思います</title>
      <link>https://r9y9.github.io/blog/2019/06/11/ljspeech/</link>
      <pubDate>Tue, 11 Jun 2019 00:00:30 +0900</pubDate>
      <guid>https://r9y9.github.io/blog/2019/06/11/ljspeech/</guid>
      <description>&lt;p&gt;LJSpeech Dataset: &lt;a href=&#34;https://keithito.com/LJ-Speech-Dataset/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://keithito.com/LJ-Speech-Dataset/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;
&lt;p&gt;最近いろんな研究で &lt;a href=&#34;https://keithito.com/LJ-Speech-Dataset/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LJSpeech&lt;/a&gt; が使われていますが、合成音の品質を比べるならクリーンなデータセットを使ったほうがいいですね。でないと、合成音声に含まれるノイズがモデルの限界からくるノイズなのかコーパスの音声が含むノイズ（LJSpeechの場合リバーブっぽい音）なのか区別できなくて、公平に比較するのが難しいと思います。&lt;/p&gt;
&lt;p&gt;例えば、LJSpeechを使うと、ぶっちゃけ &lt;a href=&#34;https://nv-adlr.github.io/WaveGlow&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WaveGlow&lt;/a&gt; がWaveNetと比べて品質がいいかどうかわかんないですよね…&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.
例えば最近のNICT岡本さんの研究 (&lt;a href=&#34;https://www.slideshare.net/Takuma_OKAMOTO/ss-135604814&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;基本周波数とメルケプストラムを用いたリアルタイムニューラルボコーダに関する検討&lt;/a&gt;) を引用すると、実際にクリーンなデータで実験すれば（Noise shaping なしで）MOS は WaveNet (&lt;strong&gt;4.19&lt;/strong&gt;) &amp;gt; WaveGlow (3.27) と、結構な差が出たりします。LJSpeechを使った場合の WaveGlow (&lt;strong&gt;3.961&lt;/strong&gt;) &amp;gt; WaveNet (3.885) と比べると大きな差ですね。&lt;/p&gt;
&lt;p&gt;とはいえ、End-to-end音声合成を試すにはとてもいいデータセットであると思うので、積極的に活用しましょう。最近 &lt;a href=&#34;https://arxiv.org/abs/1904.02882&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LibriTTS&lt;/a&gt; が公開されたので、そちらも合わせてチェックするといいですね。&lt;/p&gt;
&lt;h2 id=&#34;why-ljspeech&#34;&gt;Why LJSpeech&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://keithito.com/LJ-Speech-Dataset/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LJSpeech&lt;/a&gt; は、&lt;a href=&#34;https://keithito.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito&lt;/a&gt; さんによって2017年に公開された、単一女性話者によって録音された24時間程度の英語音声コーパスです。なぜ近年よく使われて始めているのかと言うと（2019年6月時点で&lt;a href=&#34;https://scholar.google.co.jp/scholar?cites=8632543993730273058&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google scholarで27件の引用&lt;/a&gt;）、End-to-end 音声合成の研究に用いるデータセットとして、LJSpeechは最もといっていいほど手軽に手に入るからだと考えています。LJSpeech は public domainで配布されており、利用に制限もありませんし、企業、教育機関、個人など様々な立場から自由に使用することができます。End-to-end 音声合成（厳密にはseq2seq モデルの学習）は一般に大量のデータが必要なことが知られていますが、その要件も満たしていることから、特にEnd-to-end音声合成の研究で用いられている印象を受けます。最近だと、&lt;a href=&#34;https://speechresearch.github.io/fastspeech/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FastSpeech: Fast, Robust and Controllable Text to Speech&lt;/a&gt; にも使われていましたね。&lt;/p&gt;
&lt;h2 id=&#34;個人的な経験&#34;&gt;個人的な経験&lt;/h2&gt;
&lt;p&gt;個人的には、過去に以下のブログ記事の内容で使用してきました。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/10/15/tacotron/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tacotron: Towards End-to-End Speech Synthesis / arXiv:1703.10135 [cs.CL]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/12/13/deepvoice3/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;【単一話者編】Deep Voice 3: 2000-Speaker Neural Text-to-Speech / arXiv:1710.07654 [cs.SD]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/12/22/deepvoice3_multispeaker/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;【108 話者編】Deep Voice 3: 2000-Speaker Neural Text-to-Speech / arXiv:1710.07654 [cs.SD]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/11/23/dctts/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention. [arXiv:1710.08969]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2018/01/28/wavenet_vocoder/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WaveNet vocoder をやってみましたので、その記録です / WaveNet: A Generative Model for Raw Audio [arXiv:1609.03499]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2018/05/20/tacotron2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WN-based TTSやりました / Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions [arXiv:1712.05884]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;この記事を書くにあたって整理してみて、ずいぶんとたくさんお世話になっていることが改めてわかりました。keithitoさん本当にありがとうございます。&lt;/p&gt;
&lt;p&gt;2017年、僕がTacotronで遊び始めた当時、End-to-end音声合成が流行ってきていたのですが、フリーで手に入って、End-to-end 音声合成にも使えるような程々に大きな（&amp;gt; 20時間）コーパスって、あんまりなかったんですよね。今でこそ &lt;a href=&#34;https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;M-AILABS&lt;/a&gt; 、&lt;a href=&#34;https://arxiv.org/abs/1904.02882&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LibriTTS&lt;/a&gt;、日本語なら &lt;a href=&#34;https://sites.google.com/site/shinnosuketakamichi/publication/jsut&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JSUT&lt;/a&gt; もありますが、当時は選択肢は少なかったと記憶しています。今はいい時代になってきていますね。&lt;/p&gt;
&lt;h2 id=&#34;最後に&#34;&gt;最後に&lt;/h2&gt;
&lt;p&gt;久しぶりに短いですがブログを書きました。LJSpeechは良いデータセットですので、積極的に活用しましょう。ただ、データセットの特徴として、録音データが若干リバーブがかかったような音になっていることから、ニューラルボコーダの品質比較には（例えば WaveGlow vs WaveNet）あんまり向かないかなと思っています。&lt;/p&gt;
&lt;p&gt;2017年に、End-to-end音声合成を気軽に試そうと思った時にはLJSpeechは最有力候補でしたが、現在は他にもいろいろ選択肢がある気がします。以下、僕がぱっと思いつくものをまとめておきますので、参考までにどうぞ。&lt;/p&gt;
&lt;h2 id=&#34;end-to-end音声合成に使える手軽に手に入るデータセット&#34;&gt;End-to-end音声合成に使える手軽に手に入るデータセット&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;LJSpeech Dataset: &lt;a href=&#34;https://keithito.com/LJ-Speech-Dataset/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://keithito.com/LJ-Speech-Dataset/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LibriTTS: &lt;a href=&#34;https://arxiv.org/abs/1904.02882&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/1904.02882&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;JSUT: &lt;a href=&#34;https://sites.google.com/site/shinnosuketakamichi/publication/jsut&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://sites.google.com/site/shinnosuketakamichi/publication/jsut&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;M-AILABS: &lt;a href=&#34;https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VCTK: &lt;a href=&#34;https://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;WaveNet: &lt;a href=&#34;https://deepmind.com/blog/wavenet-generative-model-raw-audio/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://deepmind.com/blog/wavenet-generative-model-raw-audio/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;WaveGlow: &lt;a href=&#34;https://nv-adlr.github.io/WaveGlow&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://nv-adlr.github.io/WaveGlow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FastSpeech: &lt;a href=&#34;https://speechresearch.github.io/fastspeech/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://speechresearch.github.io/fastspeech/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;岡本拓磨，戸田智基，志賀芳則，河井恒，&amp;ldquo;基本周波数とメルケプストラムを用いたリアルタイムニューラルボコーダに関する検討&amp;rdquo;，日本音響学会講演論文集，2019年春季, pp. 1057–1060, Mar. 2019. &lt;a href=&#34;https://www.slideshare.net/Takuma_OKAMOTO/ss-135604814&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;僕の実装 をbest publicly availableWaveNet implementation として比較に使っていただいて恐縮ですが…。&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;</description>
    </item>
    
    <item>
      <title> WN-based TTSやりました / Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions [arXiv:1712.05884]</title>
      <link>https://r9y9.github.io/blog/2018/05/20/tacotron2/</link>
      <pubDate>Sun, 20 May 2018 14:21:30 +0900</pubDate>
      <guid>https://r9y9.github.io/blog/2018/05/20/tacotron2/</guid>
      <description>&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;Thank you for coming to see my blog post about WaveNet text-to-speech.&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron2/intro.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;ul&gt;
&lt;li&gt;論文リンク: &lt;a href=&#34;https://arxiv.org/abs/1712.05884&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/1712.05884&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;オンラインデモ: &lt;a href=&#34;https://colab.research.google.com/github/r9y9/Colaboratory/blob/master/Tacotron2_and_WaveNet_text_to_speech_demo.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tacotron2: WaveNet-based text-to-speech demo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;コード &lt;a href=&#34;https://github.com/r9y9/wavenet_vocoder&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/wavenet_vocoder&lt;/a&gt;, &lt;a href=&#34;https://github.com/Rayhane-mamah/Tacotron-2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rayhane-mamah/Tacotron-2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;音声サンプル: &lt;a href=&#34;https://r9y9.github.io/wavenet_vocoder/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://r9y9.github.io/wavenet_vocoder/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;三行まとめ&#34;&gt;三行まとめ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;自作WaveNet (&lt;strong&gt;WN&lt;/strong&gt;) と既存実装Tacotron 2 (WNを除く) を組み合わせて、英語TTSを作りました&lt;/li&gt;
&lt;li&gt;LJSpeechを学習データとした場合、自分史上 &lt;strong&gt;最高品質&lt;/strong&gt; のTTSができたと思います&lt;/li&gt;
&lt;li&gt;Tacotron 2と Deep Voice 3 のabstractを読ませた音声サンプルを貼っておきますので、興味のある方はどうぞ&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;なお、Tacotron 2 の解説はしません。申し訳ありません（なぜなら僕がまだ十分に読み込んでいないため）&lt;/p&gt;
&lt;h2 id=&#34;背景&#34;&gt;背景&lt;/h2&gt;
&lt;p&gt;過去に、WaveNetを実装しました（参考: &lt;a href=&#34;https://r9y9.github.io/blog/2018/01/28/wavenet_vocoder/&#34;&gt;WaveNet vocoder をやってみましたので、その記録です / WaveNet: A Generative Model for Raw Audio [arXiv:1609.03499]&lt;/a&gt;）。過去記事から引用します。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Tacotron2 は、あとはやればほぼできる感じですが、直近では僕の中で優先度が低めのため、しばらく実験をする予定はありません。興味のある方はやってみてください。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;やりたいことの一つとしてあったとはいえ、当初の予定通り、スクラッチでTacotron 2を実装する時間は取れなかったのですが、既存実装を使ってみたところ十分に上手く動いているように思えたので、ありがたく使わせていただき、WaveNet TTSを実現することができました。というわけで、結果をここにカジュアルに残しておこうという趣旨の記事になります。&lt;/p&gt;
&lt;p&gt;オープンなデータセット、コードを使って、実際どの程度の品質が得られるのか？学習/推論にどのくらい時間がかかるのか？いうのが気になる方には、参考になるかもしれませんので、よろしければ続きをどうぞ。&lt;/p&gt;
&lt;h2 id=&#34;実験条件&#34;&gt;実験条件&lt;/h2&gt;
&lt;p&gt;細かい内容はコードに譲るとして、重要な点だけリストアップします&lt;/p&gt;
&lt;h3 id=&#34;pre-trained-modelshyper-parameters-へのリンク&#34;&gt;Pre-trained models、hyper parameters へのリンク&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Tacotron2 (mel-spectrogram prediction part): trained 189k steps on LJSpeech dataset (&lt;a href=&#34;https://www.dropbox.com/s/vx7y4qqs732sqgg/pretrained.tar.gz?dl=0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pre-trained model&lt;/a&gt;, &lt;a href=&#34;https://github.com/r9y9/Tacotron-2/blob/9ce1a0e65b9217cdc19599c192c5cd68b4cece5b/hparams.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hyper params&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;WaveNet: trained over 1000k steps on LJSpeech dataset (&lt;a href=&#34;https://www.dropbox.com/s/zdbfprugbagfp2w/20180510_mixture_lj_checkpoint_step000320000_ema.pth?dl=0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pre-trained model&lt;/a&gt;, &lt;a href=&#34;https://www.dropbox.com/s/0vsd7973w20eskz/20180510_mixture_lj_checkpoint_step000320000_ema.json?dl=0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hyper params&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;wavenet&#34;&gt;WaveNet&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;1000k step以上訓練されたモデル (2018/1/27に作ったもの、10日くらい&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;学習した）をベースに、さらに 320k step学習（約3日）しました。再学習したのは、以前のコードには &lt;a href=&#34;https://github.com/r9y9/wavenet_vocoder/issues/33&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;wavenet_vocoder/issues/33&lt;/a&gt; こんなバグがあったからです。&lt;/li&gt;
&lt;li&gt;評価には、exponential moving averagingされたパラメータを使いました。decay パラメータはTaco2論文と同じ 0.9999&lt;/li&gt;
&lt;li&gt;学習には、Mel-spectrogram prediction networkにより出力される Ground-truth-aligned (GTA) なメルスペクトログラムではなく、生音声から計算されるメルスペクトログラムを使いました。時間の都合上そうしましたが、GTAを使うとより品質が向上すると考えられます&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tacotron-2-mel-spectrogram-prediction&#34;&gt;Tacotron 2 (mel-spectrogram prediction)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Rayhane-mamah/Tacotron-2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/Rayhane-mamah/Tacotron-2&lt;/a&gt; にはWaveNet実装も含まれていますが、mel-spectrogram prediction の部分だけ使用しました&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Rayhane-mamah/Tacotron-2/issues/30#issue-317360759&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/Rayhane-mamah/Tacotron-2/issues/30#issue-317360759&lt;/a&gt; で公開されている 182k step学習されたモデルを、さらに7k stepほど（数時間くらい）学習させました。再学習させた理由は、自分の実装とRayhane氏の実装で想定するメルスペクトログラムのレンジが異なっていたためです（僕: &lt;code&gt;[0, 1]&lt;/code&gt;, Rayhane: &lt;code&gt;[-4, 4]&lt;/code&gt;）。そういう経緯から、&lt;code&gt;[-4, 4]&lt;/code&gt; のレンジであったところ，&lt;code&gt;[0, 4]&lt;/code&gt; にして学習しなおしました。直接 &lt;code&gt;[0, 1]&lt;/code&gt; にして学習しなかったのは（それでも動く、と僕は思っているのですが）、mel-spectrogram のレンジを大きく取った方が良い、という報告がいくつかあったからです（例えば &lt;a href=&#34;https://github.com/Rayhane-mamah/Tacotron-2/issues/4#issuecomment-377728945&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/Rayhane-mamah/Tacotron-2/issues/4#issuecomment-377728945&lt;/a&gt; )。Attention seq2seq は経験上学習が難しいので、僕の直感よりも先人の知恵を優先することにした次第です。WNに入力するときには、 Taco2が出力するメルスペクトログラムを &lt;code&gt;c = np.interp(c, (0, 4), (0, 1))&lt;/code&gt; とレンジを変換して与えました&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;デモ音声&#34;&gt;デモ音声&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://r9y9.github.io/wavenet_vocoder/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://r9y9.github.io/wavenet_vocoder/&lt;/a&gt; にサンプルはたくさんあります。が、ここでは違うサンプルをと思い、Tacotron 2 と Deep Voice 3の abstract を読ませてみました。
学習データに若干残響が乗っているので（ノイズっぽい）それが反映されてしまっているのですが、個人的にはまぁまぁよい結果が得られたと思っています。興味がある方は、DeepVoice3など僕の過去記事で触れているTTS結果と比べてみてください。&lt;/p&gt;
&lt;p&gt;なお、推論の計算速度は,、僕のローカル環境（GTX 1080Ti, i7-7700K）でざっと 170 timesteps / second といった感じでした。これは、Parallel WaveNet の論文で触れられている数字とおおまかに一致します。&lt;/p&gt;
&lt;p&gt;This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text.&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron2/20180510_mixture_lj_checkpoint_step000320000_ema_speech-mel-00001.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize timedomain waveforms from those spectrograms.&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron2/20180510_mixture_lj_checkpoint_step000320000_ema_speech-mel-00002.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;Our model achieves a mean opinion score of 4.53 comparable to a MOS of 4.58 for professionally recorded speech.&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron2/20180510_mixture_lj_checkpoint_step000320000_ema_speech-mel-00003.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the input to WaveNet instead of linguistic, duration, and F0 features.&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron2/20180510_mixture_lj_checkpoint_step000320000_ema_speech-mel-00004.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;We further demonstrate that using a compact acoustic intermediate representation enables significant simplification of the WaveNet architecture.&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron2/20180510_mixture_lj_checkpoint_step000320000_ema_speech-mel-00005.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;We present Deep Voice 3, a fully-convolutional attention-based neural text-to-speech system.&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron2/20180510_mixture_lj_checkpoint_step000320000_ema_speech-mel-00006.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;Deep Voice 3 matches state-of-the-art neural speech synthesis systems in naturalness while training ten times faster.&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron2/20180510_mixture_lj_checkpoint_step000320000_ema_speech-mel-00007.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;We scale Deep Voice 3 to data set sizes unprecedented for TTS, training on more than eight hundred hours of audio from over two thousand speakers.&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron2/20180510_mixture_lj_checkpoint_step000320000_ema_speech-mel-00008.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;In addition, we identify common error modes of attention-based speech synthesis networks, demonstrate how to mitigate them, and compare several different waveform synthesis methods.&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron2/20180510_mixture_lj_checkpoint_step000320000_ema_speech-mel-00009.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;We also describe how to scale inference to ten million queries per day on one single-GPU server.&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron2/20180510_mixture_lj_checkpoint_step000320000_ema_speech-mel-00010.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;h2 id=&#34;オンラインデモ&#34;&gt;オンラインデモ&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/r9y9/Colaboratory/blob/master/Tacotron2_and_WaveNet_text_to_speech_demo.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tacotron2: WaveNet-based text-to-speech demo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Google Colabで動かせるようにデモノートブックを作りました。環境構築が不要なので、手軽にお試しできるかと思います。&lt;/p&gt;
&lt;h2 id=&#34;雑記&#34;&gt;雑記&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;WaveNetを学習するときに、Mel-spectrogram precition networkのGTAな出力でなく、生メルスペクトログラムをそのまま使っても品質の良い音声合成ができるのは個人的に驚きでした。これはつまり、Taco2が　(non teacher-forcingな条件で) 十分良いメルスペクトログラムを予測できている、ということなのだと思います。&lt;/li&gt;
&lt;li&gt;収束性を向上させるために、出力を127.5 倍するとよい、という件ですが、僕はやっていません。なぜなら、僕がまだこの方法の妥当性を理解できていないからです。&lt;a href=&#34;https://twitter.com/__dhgrs__/status/995962302896599040&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@__dhgrs__さんの報告&lt;/a&gt; によると、やはり有効に働くようですね…&lt;/li&gt;
&lt;li&gt;これまた &lt;a href=&#34;http://www.monthly-hack.com/entry/2018/02/23/203208&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@__dhgrs__さんのブログ記事&lt;/a&gt; にも書かれていますが、Mixture of Logistic distributions (MoLとします) を使った場合は、categoricalを考えてsoftmaxを使う場合に比べると十分な品質を得るのに大幅に計算時間が必要になりますね、、体験的には10倍程度です。計算にあまりに時間がかかるので、スクラッチで何度も学習するのは厳しく、学習済みモデルを何度も繰り返しfine turningしていくという、秘伝のタレ方式で学習を行いました（再現性なしです、懺悔）&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Rayhane-mamah/Tacotron-2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/Rayhane-mamah/Tacotron-2&lt;/a&gt; 今回使わせてもらったTaco2実装は、僕の実装も一部使われているようでした。これとは別の NVIDIA から出た &lt;a href=&#34;https://github.com/NVIDIA/tacotron2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/NVIDIA/tacotron2&lt;/a&gt; の謝辞には僕の名前を入れていただいていたり、他にもそういうケースがそれなりにあって、端的にいって光栄であり、うれしいお思いです。&lt;/li&gt;
&lt;li&gt;非公開のデータセットを使って学習/生成したWaveNet TTS のサンプルもあります。公開できないのでここにはあげていませんが、とても高品質な音声合成（主観ですが）ができることを確認しています&lt;/li&gt;
&lt;li&gt;このプロジェクトをはじめたことで、なんと光栄にも&lt;a href=&#34;http://www.nict.go.jp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NICT&lt;/a&gt;でのトークの機会をもらうことができました。オープソースについて是非はあると思いますが、個人的には良いことがとても多いなと思います。プレゼン資料は、https://github.com/r9y9/wavenet_vocoder/issues/57 に置いてあります（が、スライドだけで読み物として成立するものではないと思います、すみません）&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;
&lt;p&gt;WaveNet TTSをようやく作ることができました。Sample-levelでautoregressive modelを考えるというアプローチが本当に動かくのか疑問だったのですが、実際に作ってみて、上手く行くということを体感することができました。めでたし。&lt;/p&gt;
&lt;p&gt;Googleの研究者さま、素晴らしい研究をありがとうございます。WaveNetは本当にすごかった&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1609.03499&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Aaron van den Oord, Sander Dieleman, Heiga Zen, et al, &amp;ldquo;WaveNet: A Generative Model for Raw Audio&amp;rdquo;, 	arXiv:1609.03499, Sep 2016.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1711.10433&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Aaron van den Oord, Yazhe Li, Igor Babuschkin, et al, &amp;ldquo;Parallel WaveNet: Fast High-Fidelity Speech Synthesis&amp;rdquo;, 	arXiv:1711.10433, Nov 2017.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.isca-speech.org/archive/Interspeech_2017/pdfs/0314.PDF&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tamamori, Akira, et al. &amp;ldquo;Speaker-dependent WaveNet vocoder.&amp;rdquo; Proceedings of Interspeech. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1712.05884&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jonathan Shen, Ruoming Pang, Ron J. Weiss, et al, &amp;ldquo;Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions&amp;rdquo;, arXiv:1712.05884, Dec 2017.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1710.07654&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wei Ping, Kainan Peng, Andrew Gibiansky, et al, &amp;ldquo;Deep Voice 3: 2000-Speaker Neural Text-to-Speech&amp;rdquo;, arXiv:1710.07654, Oct. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1611.09482&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tom Le Paine, Pooya Khorrami, Shiyu Chang, et al, &amp;ldquo;Fast Wavenet Generation Algorithm&amp;rdquo;, arXiv:1611.09482, Nov. 2016&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.monthly-hack.com/entry/2018/02/23/203208&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VQ-VAEの追試で得たWaveNetのノウハウをまとめてみた。 - Monthly Hacker&amp;rsquo;s Blog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;曖昧な表現で申し訳ございません&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;僕が使った当時は、WNの部分は十分にテストされていなかったのと、WNのコードは僕のコードをtfにtranslateした感じな（著者がそういってます）ので、WNは自分の実装を使った次第です&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;</description>
    </item>
    
    <item>
      <title>WaveNet vocoder をやってみましたので、その記録です / WaveNet: A Generative Model for Raw Audio [arXiv:1609.03499]</title>
      <link>https://r9y9.github.io/blog/2018/01/28/wavenet_vocoder/</link>
      <pubDate>Sun, 28 Jan 2018 00:14:35 +0900</pubDate>
      <guid>https://r9y9.github.io/blog/2018/01/28/wavenet_vocoder/</guid>
      <description>&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;コード: &lt;a href=&#34;https://github.com/r9y9/wavenet_vocoder&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/wavenet_vocoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;音声サンプル: &lt;a href=&#34;https://r9y9.github.io/wavenet_vocoder/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://r9y9.github.io/wavenet_vocoder/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;三行まとめ&#34;&gt;三行まとめ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Local / global conditioning を最低要件と考えて、WaveNet を実装しました&lt;/li&gt;
&lt;li&gt;DeepVoice3 / Tacotron2 の一部として使えることを目標に作りました&lt;/li&gt;
&lt;li&gt;PixelCNN++ の旨味を少し拝借し、16-bit linear PCMのscalarを入力として、（まぁまぁ）良い22.5kHzの音声を生成させるところまでできました&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Tacotron2 は、あとはやればほぼできる感じですが、直近では僕の中で優先度が低めのため、しばらく実験をする予定はありません。興味のある方はやってみてください。&lt;/p&gt;
&lt;h2 id=&#34;音声サンプル&#34;&gt;音声サンプル&lt;/h2&gt;
&lt;p&gt;左右どちらかが合成音声です^^&lt;/p&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/wavenet_vocoder/mixture_lj/0_checkpoint_step000410000_ema_predicted.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/wavenet_vocoder/mixture_lj/0_checkpoint_step000410000_ema_target.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/wavenet_vocoder/mixture_lj/1_checkpoint_step000410000_ema_predicted.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/wavenet_vocoder/mixture_lj/1_checkpoint_step000410000_ema_target.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/wavenet_vocoder/mixture_lj/2_checkpoint_step000410000_ema_predicted.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/wavenet_vocoder/mixture_lj/2_checkpoint_step000410000_ema_target.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/wavenet_vocoder/mixture_lj/3_checkpoint_step000410000_ema_predicted.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/wavenet_vocoder/mixture_lj/3_checkpoint_step000410000_ema_target.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/wavenet_vocoder/mixture_lj/4_checkpoint_step000410000_ema_predicted.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/wavenet_vocoder/mixture_lj/4_checkpoint_step000410000_ema_target.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/wavenet_vocoder/mixture_lj/5_checkpoint_step000410000_ema_predicted.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/wavenet_vocoder/mixture_lj/5_checkpoint_step000410000_ema_target.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/wavenet_vocoder/mixture_lj/6_checkpoint_step000410000_ema_predicted.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/wavenet_vocoder/mixture_lj/6_checkpoint_step000410000_ema_target.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/wavenet_vocoder/mixture_lj/7_checkpoint_step000410000_ema_predicted.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/wavenet_vocoder/mixture_lj/7_checkpoint_step000410000_ema_target.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/wavenet_vocoder/mixture_lj/8_checkpoint_step000410000_ema_predicted.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/wavenet_vocoder/mixture_lj/8_checkpoint_step000410000_ema_target.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/wavenet_vocoder/mixture_lj/9_checkpoint_step000410000_ema_predicted.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/wavenet_vocoder/mixture_lj/9_checkpoint_step000410000_ema_target.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;自分で書いた背景&#34;&gt;自分で書いた背景&lt;/h2&gt;
&lt;p&gt;WaveNetが発表されたのは、一年以上前 (&lt;a href=&#34;https://deepmind.com/blog/wavenet-generative-model-raw-audio/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;記事&lt;/a&gt;) のことです。発表後すぐに、いくつかオープンソースの実装が出ていたように記憶しています。
一方で、僕が確認していた限りでは、local / global conditioningを十分にサポートした実装がなかったように思います。
例えば、Githubで一番スターが付いている &lt;a href=&#34;https://github.com/ibab/tensorflow-wavene&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ibab/tensorflow-wavenet&lt;/a&gt; では、いまだに十分にサポートされていません（&lt;a href=&#34;https://github.com/ibab/tensorflow-wavenet/issues/112&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#112&lt;/a&gt;）。
これはつまり、生成モデルとしては使えても、TTSには使えない、ということで、僕の要望を満たしてくれるものではありませんでした。また、ちょうど最近、Parallel WaveNetが発表されたのもあり、勉強も兼ねて、local / global conditioningを最低要件として置いて、自分で実装してみようと思った次第です。&lt;/p&gt;
&lt;p&gt;実装を通して僕が一番知りたかった（体感したかった）のは、WaveNetで本当に自然音声並みの品質の音声を生成できるのか？ということなので、Parallel WaveNetで提案されているような推論を高速化するための工夫に関しては手を付けていませんので、あしからず。&lt;/p&gt;
&lt;h2 id=&#34;実験を通して得た知見&#34;&gt;実験を通して得た知見&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Dropoutの有無については、WaveNetの論文に書いていませんが、僕は5%をゼロにする形で使いました。問題なく動いていそうです。PixelCNN++にはDropoutを使う旨が書かれていたので、WaveNetでも使われているのかなと推測しています。&lt;/li&gt;
&lt;li&gt;Gradient clippingの有無は、両方試しましたが、なくてもあっても学習は安定していました。&lt;/li&gt;
&lt;li&gt;条件付けする特徴量と音声サンプルの時間解像度を合わせるのには、（少なくともLJSpeechを使う場合には）同じ値をduplicateするのではなく、Transposed convolutionを使うほうが良さそうです。 ref: &lt;a href=&#34;https://github.com/r9y9/wavenet_vocoder/issues/1#issuecomment-357486766&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/wavenet_vocoder/#1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;初期のWaveNetでは、音声サンプルを256階調にmu-law quantizeして入力します。僕もはじめそうしていたのですが、22.5kHzのLJSpeechのデータを扱っていた時、そもそもmulaw / inv-mulaw で明らかに品質が劣化していることに気づきました。512階調にすればまだましになりましたが、どうせならと思ってPixelCNN++で提案されているMixture of logistic distributionsを使った次第です。&lt;/li&gt;
&lt;li&gt;Mixture of logistic distributionsを使う場合は、分散の下限を小さくするのが重要な気がしました (PixelCNN++でいう&lt;a href=&#34;https://github.com/openai/pixel-cnn/blob/2b03725126c580a07af47c498d456cec17a9735e/pixel_cnn_pp/nn.py#L54&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pixel_cnn_pp/nn.py#L54&lt;/a&gt; の部分)。でないと、生成される音声がノイジーになりやすい印象を受けました。直感的には、external featureで条件付けする場合は特に、logistic distributionがかなりピーキー（分散がすごく小さく）なり得るので、そのピーキーな分布を十分表現できる必要があるのかなと思っています。生成時には確率分布からサンプリングすることになるので、分散の下限値を大きくとってしまった場合、ノイジーになりえるのは想像がつきます。 ref: &lt;a href=&#34;https://github.com/r9y9/wavenet_vocoder/issues/7#issuecomment-360011074&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/wavenet_vocoder/#7&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;WaveNetの実装は（比較的）簡単だったので、人のコード読むのツライ…という方は、（僕のコードを再利用なんてせずに）自分で実装するのも良いかなと思いました。勉強にもなりました。&lt;/li&gt;
&lt;li&gt;WaveNetが発表された当時は、個人レベルの計算環境でやるのは無理なんじゃないかと思って手を出していなかったのですが、最近はそれが疑問に思えてきたので、実際にやってみました。僕のPCには1台しかGPUがついていませんが (GTX 1080 Ti)、個人でも可能だと示せたかと思います。&lt;/li&gt;
&lt;li&gt;実験をはじめた当初、バッチサイズ1でもGPUメモリ (12GB) を使いきってしまう…とつらまっていたのですが、Parallel WaveNetの論文でも言及されている通り、音声の一部を短く（7680サンプルとか）切り取って使っても、品質には影響しなさそうなことを確認しました。参考までに、この記事に貼ったサンプルは、バッチサイズ2、一音声あたりの長さ8000に制限して、実験して得たものです。学習時間は、パラメータを変えながら重ね重ねファインチューニングしていたので正確なことは言えないのですが、トータルでいえば10日くらい学習したかもしれません。ただ、1日くらいで、それなりにまともな音声はでます。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;WaveNetのすごさを実際に体感することができました。まだやりたいことは残っていますが、僕はそこそこ満足しました。&lt;/li&gt;
&lt;li&gt;今後のTODO及び過去/現在の進捗は、 &lt;a href=&#34;https://github.com/r9y9/wavenet_vocoder/issues/1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/wavenet_vocoder/#1&lt;/a&gt; にまとめています。海外の方との議論も見つかるので、興味のある方は見てください。&lt;/li&gt;
&lt;li&gt;実装をはじめた当初からコードを公開していたのですが、どうやら興味を持った方が複数いたようで、上記issueにて有益なコメントをたくさんもらいました。感謝感謝&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;参考にした論文&#34;&gt;参考にした論文&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1609.03499&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Aaron van den Oord, Sander Dieleman, Heiga Zen, et al, &amp;ldquo;WaveNet: A Generative Model for Raw Audio&amp;rdquo;, arXiv:1609.03499, Sep 2016.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1711.10433&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Aaron van den Oord, Yazhe Li, Igor Babuschkin, et al, &amp;ldquo;Parallel WaveNet: Fast High-Fidelity Speech Synthesis&amp;rdquo;, arXiv:1711.10433, Nov 2017.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.isca-speech.org/archive/Interspeech_2017/pdfs/0314.PDF&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tamamori, Akira, et al. &amp;ldquo;Speaker-dependent WaveNet vocoder.&amp;rdquo; Proceedings of Interspeech. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1712.05884&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jonathan Shen, Ruoming Pang, Ron J. Weiss, et al, &amp;ldquo;Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions&amp;rdquo;, arXiv:1712.05884, Dec 2017.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1710.07654&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wei Ping, Kainan Peng, Andrew Gibiansky, et al, &amp;ldquo;Deep Voice 3: 2000-Speaker Neural Text-to-Speech&amp;rdquo;, arXiv:1710.07654, Oct. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1701.05517&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tim Salimans, Andrej Karpathy, Xi Chen, Diederik P. Kingma, &amp;ldquo;PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications&amp;rdquo;, arXiv:1701.05517, Jan. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;参考になったコード&#34;&gt;参考になったコード&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/tensorflow/magenta/tree/master/magenta/models/nsynth/wavenet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tensorflow/magenta/nsynth/wavenet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/musyoku/wavenet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;musyoku/wavenet&lt;/a&gt; コードはもちろん、こちら &lt;a href=&#34;https://github.com/musyoku/wavenet/issues/4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#4&lt;/a&gt;  のイシューも役に立ちました。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ibab/tensorflow-wavenet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ibab/tensorflow-wavenet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/openai/pixel-cnn&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;openai/pixel-cnn&lt;/a&gt; PixelCNN++の公式実装です&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/pclucas14/pixel-cnn-pp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pclucas14/pixel-cnn-pp&lt;/a&gt; PixelCNN++のPyTorch実装です&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;参考になりそうなコード&#34;&gt;参考になりそうなコード&lt;/h2&gt;
&lt;p&gt;※僕は参考にしませんでしたが、役に立つかもしれません&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kan-bayashi/PytorchWaveNetVocoder&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/kan-bayashi/PytorchWaveNetVocoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/tomlepaine/fast-wavenet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/tomlepaine/fast-wavenet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/vincentherrmann/pytorch-wavenet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/vincentherrmann/pytorch-wavenet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/dhpollack/fast-wavenet.pytorch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/dhpollack/fast-wavenet.pytorch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>An open-source implementation of WaveNet vocoder</title>
      <link>https://r9y9.github.io/projects/wavenet/</link>
      <pubDate>Wed, 10 Jan 2018 16:12:53 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/wavenet/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An open-source implementation of Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning</title>
      <link>https://r9y9.github.io/projects/deepvoice3/</link>
      <pubDate>Wed, 27 Dec 2017 16:17:42 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/deepvoice3/</guid>
      <description></description>
    </item>
    
    <item>
      <title>【108 話者編】Deep Voice 3: 2000-Speaker Neural Text-to-Speech / arXiv:1710.07654 [cs.SD]</title>
      <link>https://r9y9.github.io/blog/2017/12/22/deepvoice3_multispeaker/</link>
      <pubDate>Fri, 22 Dec 2017 15:30:00 +0900</pubDate>
      <guid>https://r9y9.github.io/blog/2017/12/22/deepvoice3_multispeaker/</guid>
      <description>&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;論文リンク: &lt;a href=&#34;https://arxiv.org/abs/1710.07654&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1710.07654&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;コード: &lt;a href=&#34;https://github.com/r9y9/deepvoice3_pytorch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/deepvoice3_pytorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VCTK: &lt;a href=&#34;https://datashare.ed.ac.uk/handle/10283/2950&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://datashare.ed.ac.uk/handle/10283/2950&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;音声サンプルまとめ: &lt;a href=&#34;https://r9y9.github.io/deepvoice3_pytorch/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://r9y9.github.io/deepvoice3_pytorch/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;三行まとめ&#34;&gt;三行まとめ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1710.07654&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1710.07654: Deep Voice 3: 2000-Speaker Neural Text-to-Speech&lt;/a&gt; を読んで、複数話者の場合のモデルを実装しました&lt;/li&gt;
&lt;li&gt;論文のタイトル通りの2000話者とはいきませんが、&lt;a href=&#34;https://datashare.ed.ac.uk/handle/10283/2950&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VCTK&lt;/a&gt; を使って、108 話者対応の英語TTSモデルを作りました（学習時間1日くらい）&lt;/li&gt;
&lt;li&gt;入力する話者IDを変えることで、一つのモデルでバリエーションに富んだ音声サンプルを生成できることを確認しました&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;概要&#34;&gt;概要&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/12/13/deepvoice3/&#34;&gt;【単一話者編】Deep Voice 3: 2000-Speaker Neural Text-to-Speech / arXiv:1710.07654 [cs.SD]&lt;/a&gt; の続編です。&lt;/p&gt;
&lt;p&gt;論文概要は前回紹介したものと同じなので、話者の条件付けの部分についてのみ簡単に述べます。なお、話者の条件付けに関しては、DeepVoice2の論文 (&lt;a href=&#34;https://arxiv.org/abs/1705.08947&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1705.08947 [cs.CL]&lt;/a&gt;) の方が詳しいです。&lt;/p&gt;
&lt;p&gt;まず基本的に、話者の情報は trainable embedding としてモデルに組み込みます。text embeddingのうようにネットワークの入力の一箇所に入れるような設計では学習が上手くかない（話者情報を無視するようになってしまうのだと思います）ため、ネットワークのあらゆるところに入れるのがポイントのようです。具体的には、Encoder, Decoder (+ Attention), Converterのすべてに入れます。さらに具体的には、ネットワークの基本要素である Gated linear unit + Conv1d のすべてに入れます。詳細は論文に記載のarchitectureの図を参照してください。&lt;/p&gt;
&lt;p&gt;話者の条件付けに関して、一つ注意を加えるとすれば、本論文には明示的に書かれていませんが、 speaker embeddingは各時間stepすべてにexpandして用いるのだと思います（でないと実装するときに困る）。DeepVoice2の論文にはその旨が明示的に書かれています。&lt;/p&gt;
&lt;h2 id=&#34;vctk-の前処理&#34;&gt;VCTK の前処理&lt;/h2&gt;
&lt;p&gt;実験に入る前に、VCTKの前処理について、簡単にまとめたいと思います。VCTKの音声データには、数秒に渡る無音区間がそれなりに入っているので、それを取り除く必要があります。以前、&lt;a href=&#34;https://r9y9.github.io/blog/2017/11/12/jsut_ver1/&#34;&gt;日本語 End-to-end 音声合成に使えるコーパス JSUT の前処理&lt;/a&gt; で書いた内容と同じように、音素アライメントを取って無音区間を除去します。僕は以下の二つの方法をためしました。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/lowerquality/gentle&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gentle&lt;/a&gt; (&lt;a href=&#34;https://github.com/kaldi-asr/kaldi&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kaldi&lt;/a&gt;ベース)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/CSTR-Edinburgh/merlin&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Merlin&lt;/a&gt; 付属のアライメントツール (&lt;a href=&#34;http://festvox.org/cmu_arctic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;festvox&lt;/a&gt;ベース) (&lt;a href=&#34;https://gist.github.com/kastnerkyle/cc0ac48d34860c5bb3f9112f4d9a0300&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;便利スクリプト&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;論文中には、（無音除去のため、という文脈ではないのですが&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;）Gentleを使った旨が書かれています。しかし、試したところアライメントが失敗するケースがそれなりにあり、&lt;a href=&#34;https://github.com/facebookresearch/loop&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;loop&lt;/a&gt; は後者の方法を用いており良い結果も出ていることから、結論としては僕は後者を採用しました。なお、両方のコードは残してあるので、気になる方は両方ためしてみてください。&lt;/p&gt;
&lt;h2 id=&#34;実験&#34;&gt;実験&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://datashare.ed.ac.uk/handle/10283/2950&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VCTK&lt;/a&gt; の108話者分のすべて&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;を使用して、20時間くらい（30万ステップ x 2）学習しました。30万ステップ学習した後できたモデルをベースに、さらに30万ステップ学習しました&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;。モデルは、単一話者の場合とほとんど同じですが、変更を加えた点を以下にまとめます。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;共通&lt;/strong&gt;: Speaker embedding を追加しました。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;共通&lt;/strong&gt;: Speaker embeddingをすべての時間ステップにexpandしたあと、Dropoutを適用するようにしました（論文には書いていませんが、結論から言えば重要でした…）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Decoder&lt;/strong&gt;: アテンションのレイヤー数を2から1に減らしました&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;計算速度は、バッチサイズ16で、8.6 step/sec くらいでした。GPUメモリの使用量は9GB程度でした。Convolution BlockごとにLinearレイヤーが追加されるので、それなりにメモリ使用量が増えます。PyTorch v0.3.0を使いました。&lt;/p&gt;
&lt;p&gt;学習に使用したコマンドは以下です。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python train.py --data-root=./data/vctk --checkpoint-dir=checkpoints_vctk \
   --hparams=&amp;quot;preset=deepvoice3_vctk,builder=deepvoice3_multispeaker&amp;quot; \
   --log-event-path=log/deepvoice3_multispeaker_vctk_preset \
   --load-embedding=20171221_deepvoice3_checkpoint_step000300000.pth
 # &amp;lt;&amp;lt; 30万ステップで一旦打ち切り &amp;gt;&amp;gt;
 # もう一度0から30万ステップまで学習しなおし
 python train.py --data-root=./data/vctk --checkpoint-dir=checkpoints_vctk_fineturn \
   --hparams=&amp;quot;preset=deepvoice3_vctk,builder=deepvoice3_multispeaker&amp;quot; \
   --log-event-path=log/deepvoice3_multispeaker_vctk_preset_fine \
   --restore-parts=./checkpoints_vctk/checkpont_step000300000.pth
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;学習を高速化するため、LJSpeechで30万ステップ学習したモデルのembeddingの部分を再利用しました。また、cyclic annealingのような効果が得られることを期待して、一度学習を打ち切って、さらに0stepからファインチューニングしてみました。&lt;/p&gt;
&lt;p&gt;コードのコミットハッシュは &lt;a href=&#34;https://github.com/r9y9/deepvoice3_pytorch/tree/0421749af908905d181f089f06956fddd0982d47&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;0421749&lt;/a&gt; です。正確なハイパーパラメータが知りたい場合は、ここから辿れると思います。&lt;/p&gt;
&lt;h3 id=&#34;アライメントの学習過程-30万ステップ&#34;&gt;アライメントの学習過程 (~30万ステップ)&lt;/h3&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/deepvoice3_multispeaker/alignments.gif&#34; /&gt;&lt;/div&gt;
&lt;h3 id=&#34;学習された-speaker-embedding-の可視化&#34;&gt;学習された Speaker embedding の可視化&lt;/h3&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/deepvoice3_multispeaker/speaker_embedding.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;論文のappendixに書かれているのと同じように、学習されたEmbeddingに対してPCAをかけて可視化しました。論文の図とは少々異なりますが、期待通り、男女はほぼ線形分離できるようになっていることは確認できました。&lt;/p&gt;
&lt;h3 id=&#34;音声サンプル&#34;&gt;音声サンプル&lt;/h3&gt;
&lt;p&gt;最初に僕の感想を述べておくと、LJSpeechで単一話者モデルを学習した場合と比べると、汎化しにくい印象がありました。文字がスキップされるといったエラーケースも比較して多いように思いました。
たくさんサンプルを貼るのは大変なので、興味のある方は自分で適当な未知テキストを与えて合成してみてください。学習済みモデルは &lt;a href=&#34;https://github.com/r9y9/deepvoice3_pytorch#pretrained-models&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;deepvoice3_pytorch#pretrained-models&lt;/a&gt; からダウンロードできるようにしてあります。&lt;/p&gt;
&lt;h3 id=&#34;loophttpsytaigmangithubioloopnetwork-3-multiple-speakers-from-vctk-と同じ文章&#34;&gt;&lt;a href=&#34;https://ytaigman.github.io/loop/#network-3-multiple-speakers-from-vctk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Loop&lt;/a&gt; と同じ文章&lt;/h3&gt;
&lt;p&gt;Some have accepted this as a miracle without any physical explanation&lt;/p&gt;
&lt;p&gt;(69 chars, 11 words)&lt;/p&gt;
&lt;p&gt;speaker IDが若い順に12サンプルの話者ID を与えて、合成した結果を貼っておきます。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;225, 23,  F,    English,    Southern,  England&lt;/strong&gt; (ID, AGE,  GENDER,  ACCENTS,  REGION)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/loop/0_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker0.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;226,  22,  M,    English,    Surrey&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/loop/0_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker1.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;227,  38,  M,    English,    Cumbria&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/loop/0_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker2.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;228,  22,  F,    English,    Southern  England&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/loop/0_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker3.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;229,  23,  F,    English,    Southern  England&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/loop/0_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker4.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;230,  22,  F,    English,    Stockton-on-tees&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/loop/0_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker5.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;231,  23,  F,    English,    Southern  England&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/loop/0_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker6.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;232,  23,  M,    English,    Southern  England&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/loop/0_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker7.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;233,  23,  F,    English,    Staffordshire&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/loop/0_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker8.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;234,  22,  F,    Scottish,  West  Dumfries&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/loop/0_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker9.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;236,  23,  F,    English,    Manchester&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/loop/0_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker10.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;237,  22,  M,    Scottish,  Fife&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/loop/0_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker11.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;声質だけでなく、話速にもバリエーションが出ているのがわかります。&lt;code&gt;231&lt;/code&gt; の最初で一部音が消えています（こういったエラーケースはよくあります）。&lt;/p&gt;
&lt;h4 id=&#34;keithitotacotron-のサンプルhttpskeithitogithubioaudio-samples-と同じ文章&#34;&gt;&lt;a href=&#34;https://keithito.github.io/audio-samples/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron のサンプル&lt;/a&gt; と同じ文章&lt;/h4&gt;
&lt;p&gt;簡単に汎化性能をチェックするために、未知文章でテストします。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;男性 (292,  23,  M,    NorthernIrish,  Belfast)&lt;/li&gt;
&lt;li&gt;女性 (288,  22,  F,    Irish,  Dublin)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;の二つのサンプルを貼っておきます。&lt;/p&gt;
&lt;p&gt;Scientists at the CERN laboratory say they have discovered a new particle.&lt;/p&gt;
&lt;p&gt;(74 chars, 13 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/0_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker62.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/0_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker62_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/0_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker61.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/0_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker61_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;There&amp;rsquo;s a way to measure the acute emotional intelligence that has never gone out of style.&lt;/p&gt;
&lt;p&gt;(91 chars, 18 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/1_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker62.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/1_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker62_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/1_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker61.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/1_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker61_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;President Trump met with other leaders at the Group of 20 conference.&lt;/p&gt;
&lt;p&gt;(69 chars, 13 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/2_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker62.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/2_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker62_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/2_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker61.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/2_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker61_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;The Senate&amp;rsquo;s bill to repeal and replace the Affordable Care Act is now imperiled.&lt;/p&gt;
&lt;p&gt;(81 chars, 16 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/3_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker62.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/3_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker62_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/3_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker61.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/3_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker61_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;Generative adversarial network or variational auto-encoder.&lt;/p&gt;
&lt;p&gt;(59 chars, 7 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/4_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker62.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/4_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker62_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/4_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker61.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/4_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker61_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;The buses aren&amp;rsquo;t the problem, they actually provide a solution.&lt;/p&gt;
&lt;p&gt;(63 chars, 13 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/5_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker62.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/5_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker62_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/5_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker61.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/5_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker61_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;ところどころ音が抜けているのが目立ちます。色々実験しましたが、やはり単一話者 24hのデータで学習したモデルに比べると、一話者あたり30分~1h程度のデータでは、汎化させるのが難しい印象を持ちました。&lt;/p&gt;
&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;複数話者版のDeepVoice3を実装して、実際に108話者のデータセットで学習し、それなりに動くことを確認できました&lt;/li&gt;
&lt;li&gt;複数話者版のDeepVoice3では、アテンションの学習が単一話者の場合と比べて難しい印象でした。アテンションレイヤーの数を2から1に減らすと、アライメントがくっきりする傾向にあることを確認しました。&lt;/li&gt;
&lt;li&gt;VCTKの前処理大事、きちんとしましょう&lt;/li&gt;
&lt;li&gt;Speaker embedding にDropoutをかけるのは、論文には記載されていませんが、結果から言って重要でした。ないと、音声の品質以前の問題として、文字が正しく発音されない、といった現象に遭遇しました。&lt;/li&gt;
&lt;li&gt;Speaker embedding をすべての時刻に同一の値をexpandしてしまうと過学習しやすいのではないかいう予測を元に、各時刻でランダム性をいれることでその問題を緩和できないかと考え、Dropoutを足してみました。上手く言ったように思います&lt;/li&gt;
&lt;li&gt;論文の内容について詳しく触れていませんが、実はけっこう雑というか、文章と図に不一致があったりします（例えば図1にあるEncoder PreNet/PostNet は文章中で説明がない）。著者に連絡して確認するのが一番良いのですが、どういうモデルなら上手くいくか考えて試行錯誤するのも楽しいので、今回は雰囲気で実装しました。それなりに上手く動いているように思います&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;次は、DeepVoice3、Tacotron 2 (&lt;a href=&#34;https://arxiv.org/abs/1712.0588&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1712.05884 [cs.CL]&lt;/a&gt;) で有効性が示されている WaveNet Vocoder を実装して、品質を改善してみようと思っています。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1710.07654&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wei Ping, Kainan Peng, Andrew Gibiansky, et al, &amp;ldquo;Deep Voice 3: 2000-Speaker Neural Text-to-Speech&amp;rdquo;, arXiv:1710.07654, Oct. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1705.08947&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sercan Arik, Gregory Diamos, Andrew Gibiansky,, et al, &amp;ldquo;Deep Voice 2: Multi-Speaker Neural Text-to-Speech&amp;rdquo;, arXiv:1705.08947, May 2017.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1712.05884&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jonathan Shen, Ruoming Pang, Ron J. Weiss, et al, &amp;ldquo;Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions&amp;rdquo;, arXiv:1712.05884, Dec 2017.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;関連記事&#34;&gt;関連記事&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/12/13/deepvoice3/&#34;&gt;【単一話者編】Deep Voice 3: 2000-Speaker Neural Text-to-Speech / arXiv:1710.07654 [cs.SD] | LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/11/23/dctts/&#34;&gt;Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention. [arXiv:1710.08969] | LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/11/12/jsut_ver1/&#34;&gt;日本語 End-to-end 音声合成に使えるコーパス JSUT の前処理 | LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;VCTKの無音区間除去のためという文脈ではなく、テキストにshort pause / long pause を挿入するためです&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;transcriptionがない1話者 (p315) のデータは除いています&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Dropoutをきつくするとロスが下がりにくく、一方でゆるくすると汎化しにくい印象がありました。ので、Dropoutきつめである程度汎化させたあと、Dropoutをゆるめにしてfine turningする、といった戦略を取ってみました。&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Interactive C&#43;&#43;: Jupyter上で対話的にC&#43;&#43;を使う方法の紹介 [Jupyter Advent Calendar 2017]</title>
      <link>https://r9y9.github.io/blog/2017/12/21/jupyter-cxx/</link>
      <pubDate>Thu, 21 Dec 2017 00:00:00 +0900</pubDate>
      <guid>https://r9y9.github.io/blog/2017/12/21/jupyter-cxx/</guid>
      <description>&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://r9y9.github.io/images/jupyter-cxx/jupyter-cxx-demo.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://qiita.com/advent-calendar/2017/jupyter&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jupyter Advent Calendar 2017&lt;/a&gt; 21日目の記事です。&lt;/p&gt;
&lt;p&gt;C++をJupyterで使う方法はいくつかあります。この記事では、僕が試したことのある以下の4つの方法について、比較しつつ紹介したいと思います。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/root-project/cling&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;root/cling&lt;/a&gt; 付属のカーネル&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/root-project/cling&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;root/root&lt;/a&gt; 付属のカーネル&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/QuantStack/xeus-cling&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;xeus-cling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Keno/Cxx.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Keno/Cxx.jl&lt;/a&gt; をIJuliaで使う&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;まとめとして、簡単に特徴などを表にまとめておきますので、選ぶ際の参考にしてください。詳細な説明は後に続きます。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;a href=&#34;https://github.com/root-project/cling&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cling&lt;/a&gt;&lt;/th&gt;
&lt;th&gt;&lt;a href=&#34;https://github.com/root-project/root&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ROOT&lt;/a&gt;&lt;/th&gt;
&lt;th&gt;&lt;a href=&#34;https://github.com/QuantStack/xeus-cling&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;xeus-cling&lt;/a&gt;&lt;/th&gt;
&lt;th&gt;&lt;a href=&#34;%28https://github.com/Keno/Cxx.jl%29&#34;&gt;Cxx.jl&lt;/a&gt; + &lt;a href=&#34;https://github.com/JuliaLang/IJulia.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IJulia&lt;/a&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;C++インタプリタ実装&lt;/td&gt;
&lt;td&gt;C++&lt;/td&gt;
&lt;td&gt;C++&lt;/td&gt;
&lt;td&gt;C++&lt;/td&gt;
&lt;td&gt;Julia + C++&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;(Tab) Code completion&lt;/td&gt;
&lt;td&gt;○&lt;/td&gt;
&lt;td&gt;○&lt;/td&gt;
&lt;td&gt;○&lt;/td&gt;
&lt;td&gt;x&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cインタプリタ&lt;/td&gt;
&lt;td&gt;△&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;△&lt;/td&gt;
&lt;td&gt;△&lt;/td&gt;
&lt;td&gt;○&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;%magics&lt;/td&gt;
&lt;td&gt;x&lt;/td&gt;
&lt;td&gt;%%cpp, %%jsroot, その他&lt;/td&gt;
&lt;td&gt;x&lt;/td&gt;
&lt;td&gt;△&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;他言語との連携&lt;/td&gt;
&lt;td&gt;x&lt;/td&gt;
&lt;td&gt;Python, R &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;x&lt;/td&gt;
&lt;td&gt;Julia&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;バイナリ配布&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://root.cern.ch/download/cling/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;公式リンク&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://root.cern.ch/downloading-root&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;公式リンク&lt;/a&gt; (python2系向け）&lt;/td&gt;
&lt;td&gt;condaで提供&lt;/td&gt;
&lt;td&gt;△&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;オンラインデモ&lt;/td&gt;
&lt;td&gt;x&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://swanserver.web.cern.ch/swanserver/rootdemo/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;rootdemo&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://mybinder.org/v2/gh/QuantStack/xeus-cling/0.0.7-binder?filepath=notebooks%2Fxcpp.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;binderリンク&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;x&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;共通事項&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;すべて、clang/llvmをC++インタプリタのベースにしています。Cxx.jl以外は、C++インタプリタであるclingをベースに、さらにその上にjupyterカーネルを実装しています。&lt;/p&gt;
&lt;h2 id=&#34;1-cling&#34;&gt;1. cling&lt;/h2&gt;
&lt;p&gt;clingは、いわずとしれた（？）C++インタプリタ実装です。後述するROOTという data analysis framework の一部として、CERNによって開発されています。(20年くらい前の) 古くからあったCINTというC++インタプリタ実装が、clangを使って書き直された、という歴史があります。clingプロジェクトの一環としてJupyterカーネルが開発されています。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;良いところ&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;clingの起動が速いのでストレスが少ない &lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;イマイチなところ&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;IPythonだと使える &lt;code&gt;%time&lt;/code&gt; のようなマジックはない&lt;/li&gt;
&lt;li&gt;cling本体で良いのでは？感が否めない。cling本体のREPLを使えば、Ctrl+Rによるヒストリ検索も使えるし…&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;使ってみた感想まとめ&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;個人的には、Jupyterは可視化と組み合わせてこそ良さがあると思っているのもありますが、あえてJupyterで使う必要性を僕は感じませんでした。cling自体はとても素晴らしいのと、ノートブックとして実行結果ごとコードを保存したい、といった目的でjupyterを使う場合には、良いと思いました。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;#include &amp;lt;iostream&amp;gt;&lt;/code&gt; のあとにcode completionをしようとするとclingが落ちる、というバグがあります。Jupyterの場合はカーネルがリスタートします。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/vgvassilev/cling/issues/152&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/vgvassilev/cling/issues/152&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;参考リンク&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;公式web: &lt;a href=&#34;https://cdn.rawgit.com/root-project/cling/master/www/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cdn.rawgit.com/root-project/cling/master/www/index.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Github: &lt;a href=&#34;https://github.com/root-project/cling&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/root-project/cling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;紹介スライド: &lt;a href=&#34;http://llvm.org/devmtg/2010-11/Naumann-Cling.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LLVM Developers&#39; Meeting, &amp;ldquo;Creating cling, an interactive interpreter interface for clang&amp;rdquo;, Axel Naumann, Philippe Canal, Paul Russo, Vassil Vassilev, 04.11.2010, San Jose, CA, United States&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;2-root&#34;&gt;2. ROOT&lt;/h2&gt;
&lt;p&gt;ROOTの説明を公式ページから引用します：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A modular scientific software framework. It provides all the functionalities needed to deal with big data processing, statistical analysis, visualisation and storage. It is mainly written in C++ but integrated with other languages such as Python and R.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;日本語の情報が少ない印象ですが、&lt;a href=&#34;https://github.com/akira-okumura/RHEA/wiki/ROOT-%E8%AC%9B%E7%BF%92%E4%BC%9A-2017&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ROOT 講習会 2017 | 高エネルギー宇宙物理学のための ROOT 入門 – ROOT for High-Energy Astrophysics (RHEA)&lt;/a&gt; によると、実験系素粒子物理学では標準的なデータ解析ソフトウェア・ライブラリ群のようです。&lt;/p&gt;
&lt;p&gt;ROOTプロジェクト自体にclingを含みますが、clingが提供するjupyterカーネルとは別で、&lt;a href=&#34;https://github.com/root-project/root/tree/master/bindings/pyroot/JupyROOT&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JupyROOT&lt;/a&gt; というカーネルが開発されています。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;良いところ&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PythonとC++をミックスできる。&lt;code&gt;%%cpp&lt;/code&gt; magicでC++関数を定義して、ホスト（python) 側から呼び出せる&lt;/li&gt;
&lt;li&gt;&lt;code&gt;%%jsroot&lt;/code&gt; magic により、グラフをインタラクティブに動かせる&lt;/li&gt;
&lt;li&gt;IPythonで使えるmagicが使える（&lt;code&gt;%timeit&lt;/code&gt;, &lt;code&gt;%time&lt;/code&gt;, &lt;code&gt;%load_ext&lt;/code&gt;等）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;イマイチなところ&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Numpyやmatplotlibなど、Pythonを用いた数値計算ではデファクトに近いツールとの連携は微妙に思いました &lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;。cythonのように、手軽にnumpy arrayをC++に渡す、といった方法はなさそう・・・？（あったら教えてください）&lt;/li&gt;
&lt;li&gt;ROOTの（でかい）APIを覚えないと使えなさそうで、ハードルが高い&lt;/li&gt;
&lt;li&gt;公式のbinderのデモノートブック、ちょいちょいカーネルが落ちる…&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;使ってみた感想まとめ&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Jupyterカーネルはclingのものよりも良いと思いました。PythonとC++をミックスできるのが特に良いと思います。個人的には、ROOTが機能もりもりのデカイソフトウェアなことがあまり好きになれず、使い込んでいないのですが、ROOTのAPIに慣れた人、あるいは好きになれる人には、良いと思います。&lt;/p&gt;
&lt;p&gt;clingだと &lt;code&gt;#include &amp;lt;iostream&amp;gt;&lt;/code&gt;のあとにcode completionで落ちる、というバグがありまたが、ROOT付属のcling (&lt;code&gt;ROOT 6.10/08&lt;/code&gt; をソースからビルドして使いました) ではそのバグはありませんでした。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;参考リンク&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;公式ページ: &lt;a href=&#34;https://root.cern.ch/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://root.cern.ch/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Github: &lt;a href=&#34;https://github.com/root-project/root&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/root-project/root&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;オンラインデモ: &lt;a href=&#34;https://swanserver.web.cern.ch/swanserver/rootdemo/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://swanserver.web.cern.ch/swanserver/rootdemo/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;3-xeus-cling&#34;&gt;3. xeus-cling&lt;/h2&gt;
&lt;p&gt;先月 11月30日に、&lt;a href=&#34;https://blog.jupyter.org/interactive-workflows-for-c-with-jupyter-fe9b54227d92&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jupyter blog で紹介&lt;/a&gt; されたカーネルです。名前の通りclingをベースにしています。C++インタプリタとしては機能的にcling付属カーネルと同じですが、&lt;a href=&#34;https://github.com/QuantStack/xeus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;xeus&lt;/a&gt; というJupyter kernel protocolのC++実装をベースにしている点が異なります。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;良いところ&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;condaでパッケージとして提供されているので、インストールが楽。clang/clingも併せてインストールしてくれます&lt;/li&gt;
&lt;li&gt;同じ開発元が、&lt;a href=&#34;https://github.com/QuantStack/xplot&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;xplot&lt;/a&gt; という可視化ライブラリを提供している（ただしalphaバージョン）&lt;/li&gt;
&lt;li&gt;標準ライブラリのヘルプが &lt;code&gt;?&lt;/code&gt; コマンドで確認できます (例. &lt;code&gt;?std::vector&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;イマイチなところ&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;外部ライブラリをロードしようとしたら動きませんでした（なので &lt;a href=&#34;https://github.com/QuantStack/xeus-cling/pull/94&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;プルリク&lt;/a&gt; 投げました（が、いい方法ではなかったようでcloseされました&lt;/li&gt;
&lt;li&gt;&lt;code&gt;%timeit&lt;/code&gt; の実装があったので試してみましたが、エラーが出て動きませんでした&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;使ってみた感想まとめ&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;少しalphaバージョンの印象を受けました。xplotなど周辺ツールへの期待がありますが、個人的にはmatplotlib等pythonの可視化ツールでいいのでは…という気持ちになりました。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;参考リンク&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Github: &lt;a href=&#34;https://github.com/QuantStack/xeus-cling&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/QuantStack/xeus-cling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;紹介記事: &lt;a href=&#34;https://blog.jupyter.org/interactive-workflows-for-c-with-jupyter-fe9b54227d92&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://blog.jupyter.org/interactive-workflows-for-c-with-jupyter-fe9b54227d92&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;4-cxxjl--ijuliajl&#34;&gt;4. Cxx.jl + IJulia.jl&lt;/h2&gt;
&lt;p&gt;Cxx.jlは、clangをベースにしたJuliaのC++インタフェースです。JuliaにはIJuliaというJupyterカーネルの実装があるので、IJuliaとCxx.jlを使えば、Jupyter上でC++を使うことができます。過去にCxx.jlに関する記事をいくつか書きましたので、そのリンクを貼っておきます。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2016/01/24/passing-julia-expressions-to-cxx/&#34;&gt;Cxx.jlを用いてJulia expression/value をC++に埋め込む実験 | LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2015/12/22/cxx-jl/&#34;&gt;Cxx.jl を使ってみた感想 + OpenCV.jl, Libfreenect2.jl の紹介 | LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;良いところ&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;JuliaとC++をミックスできます。過去記事に書きましたが、例えばC++関数内でJuliaのプログレスバーを使ったりできます&lt;/li&gt;
&lt;li&gt;C++インタプリタとCインタプリタを切り替えられます&lt;/li&gt;
&lt;li&gt;&lt;code&gt;icxx&lt;/code&gt; と &lt;code&gt;cxx&lt;/code&gt; マクロで、それぞれローカル/グローバルスコープを切り替えられます。&lt;/li&gt;
&lt;li&gt;Juliaの配列をC++に渡すのは非常に簡単にできます。例を以下に示します&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-jl&#34;&gt;C++ &amp;gt; #include &amp;lt;iostream&amp;gt;
true

julia&amp;gt; cxx&amp;quot;&amp;quot;&amp;quot;
       template &amp;lt;class T&amp;gt;
       void f(T x, int n) {
           for (int i = 0; i &amp;lt; n; ++i) {
               std::cout &amp;lt;&amp;lt; x[i] &amp;lt;&amp;lt; std::endl;
           }
       }&amp;quot;&amp;quot;&amp;quot;;

julia&amp;gt; x = rand(5)
10-element Array{Float64,1}:
 0.593086
 0.736548
 0.344246
 0.390799
 0.226175

julia&amp;gt; icxx&amp;quot;f($(pointer(x)), $(length(x)));&amp;quot;
0.593086
0.736548
0.344246
0.390799
0.226175
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;イマイチなところ&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cxxパッケージを読み込むのに多少時間がかかります。僕の環境では（プリコンパイルされた状態で）2.5秒程度でした&lt;/li&gt;
&lt;li&gt;(Tab) Code completionは実装されていません &lt;a href=&#34;https://github.com/Keno/Cxx.jl/issues/61&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#61&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;icxx&lt;/code&gt; or &lt;code&gt;cxx&lt;/code&gt; で囲まないといけず、syntax highlightはされません&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;使ってみた感想まとめ&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;僕は一年以上Cxx.jlを使っているので、バイアスも入っていると思いますが、かなり使いやすいと思います。パッケージのロードに時間がかかるのは、何度もカーネルやjuliaを再起動したりしなければ、まったく気になりません。&lt;a href=&#34;https://github.com/JuliaLang/IJulia.jl/blob/42d103eaa89d8cf1ab3bc0a8ee0e298bb9a91f80/src/magics.jl#L1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IJuliaの設計上の理由&lt;/a&gt; により、magicはありませんが、例えば &lt;code&gt;%time&lt;/code&gt; は &lt;code&gt;@time&lt;/code&gt; マクロで十分であり、不便に感じません。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;参考リンク&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;IJulia: &lt;a href=&#34;https://github.com/JuliaLang/IJulia.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/JuliaLang/IJulia.jl&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Cxx : &lt;a href=&#34;https://github.com/Keno/Cxx.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/Keno/Cxx.jl&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;C++と他言語のやりとりのスムースさの観点から、やはり僕は対話環境でC++を使うならCxx.jlが最高だと思いました。Cxx + JuliaのREPLも便利ですが、Cxx + IJuliaも良いと思います。&lt;/li&gt;
&lt;li&gt;ただし、C++単体でしか使わない、ということであれば、cling or xeus-clingが良いと思います。ただし xeus-clingは、前述の通り外部ライブラリを読みこもうとするとエラーになる問題があったので、外部ライブラリを読み込んで使用したい場合はパッチ (&lt;a href=&#34;https://github.com/QuantStack/xeus-cling/pull/94&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;xeus-cling/#94&lt;/a&gt;) を当てた方がよいかもしれません&lt;/li&gt;
&lt;li&gt;xeus-clingには、Jupyterブログにのっていたのでどんなものかと思って試してみましたが、周辺ツール含め思ってたよりalpha版のようでした。また、他と比べての機能的な優位性はあまり感じませんでした。ただし、condaパッケージとして提供されているので、敷居が一番低いのは嬉しいですね&lt;/li&gt;
&lt;li&gt;ROOTのjupyter kernelは、C++とpythonをミックスできるのが特に良く、素晴らしいと思いました。また &lt;code&gt;%%cpp&lt;/code&gt; magicの他にも、ipythonで使える &lt;code&gt;%timeit&lt;/code&gt; などのmagicも使えるのは、ユーザにとっては嬉しいです。Cxx.jlを除けば、ROOTのカーネルが一番良いと思いました。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.jupyter.org/interactive-workflows-for-c-with-jupyter-fe9b54227d92&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interactive Workflows for C++ with Jupyter – Jupyter Blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://qiita.com/mugwort_rc/items/b8087d1b6f9498b037d5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;C++11/14/17インタプリタ環境 Jupyter-Cling - Qiita&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://qiita.com/sasaki77/items/f6253e1d6638fba0e744&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JupyterにC++のノートのためのclingカーネルを追加する [Mac] - Qiita&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/akira-okumura/RHEA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;高エネルギー宇宙物理学のための ROOT 入門 – ROOT for High-Energy Astrophysics (RHEA)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;clangをベースにしているので原理的には可能だと思いますが、少なくともjupyterカーネルとしてはありません&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Juliaのmacroを使えばよい、というスタンスで、提供していません。 &lt;a href=&#34;https://github.com/JuliaLang/IJulia.jl/blob/42d103eaa89d8cf1ab3bc0a8ee0e298bb9a91f80/src/magics.jl#L1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;参考リンク&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://root.cern.ch/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;公式ホームページ&lt;/a&gt; より引用: It is mainly written in C++ but integrated with other languages such as Python and R.&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;linux向け&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Cxx.jlだと、パッケージのコンパイルに10秒かかる、とか過去にありました。最近は改善されていますが&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/rootpy/rootpy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/rootpy/rootpy&lt;/a&gt; ライブラリはありますが、結局このライブラリのAPIを覚えないといけないという…はい…&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>ニューラルネットの学習過程の可視化を題材に、Jupyter &#43; Bokeh で動的な描画を行う方法の紹介 [Jupyter Advent Calendar 2017]</title>
      <link>https://r9y9.github.io/blog/2017/12/14/jupyter-bokeh/</link>
      <pubDate>Thu, 14 Dec 2017 00:00:30 +0900</pubDate>
      <guid>https://r9y9.github.io/blog/2017/12/14/jupyter-bokeh/</guid>
      <description>&lt;p&gt;Line &lt;a href=&#34;https://bokeh.pydata.org/en/latest/docs/reference/models/glyphs/line.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://bokeh.pydata.org/en/latest/docs/reference/models/glyphs/line.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://r9y9.github.io/images/jupyter_with_bokeh_files/jupyter_with_bokeh_3_1.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;VBar &lt;a href=&#34;https://bokeh.pydata.org/en/latest/docs/reference/models/glyphs/vbar.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://bokeh.pydata.org/en/latest/docs/reference/models/glyphs/vbar.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://r9y9.github.io/images/jupyter_with_bokeh_files/jupyter_with_bokeh_3_3.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;HBar &lt;a href=&#34;https://bokeh.pydata.org/en/latest/docs/reference/models/glyphs/hbar.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://bokeh.pydata.org/en/latest/docs/reference/models/glyphs/hbar.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://r9y9.github.io/images/jupyter_with_bokeh_files/jupyter_with_bokeh_3_5.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;ImageRGBA &lt;a href=&#34;https://bokeh.pydata.org/en/latest/docs/reference/models/glyphs/image_rgba.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://bokeh.pydata.org/en/latest/docs/reference/models/glyphs/image_rgba.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://r9y9.github.io/images/jupyter_with_bokeh_files/jupyter_with_bokeh_3_7.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;ImageRGBA &lt;a href=&#34;https://bokeh.pydata.org/en/latest/docs/reference/models/glyphs/image_rgba.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://bokeh.pydata.org/en/latest/docs/reference/models/glyphs/image_rgba.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://r9y9.github.io/images/jupyter_with_bokeh_files/jupyter_with_bokeh_3_9.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;前置き&#34;&gt;前置き&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://qiita.com/advent-calendar/2017/jupyter&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jupyter Advent Calendar 2017&lt;/a&gt; 14日目の記事です。この記事は、Jupyter notebookで作成したものをnbconvertでmarkdownに変換し、手で少し修正して作りました。読み物としてはこの記事を、実行するにはノートブックの方を参照していただくのが良いかと思います。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://gist.github.com/r9y9/d57e797c28f6cdc4e44264411c21b76f&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ノートブック (gist)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://nbviewer.jupyter.org/gist/r9y9/d57e797c28f6cdc4e44264411c21b76f&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nbviewer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;概要&#34;&gt;概要&lt;/h2&gt;
&lt;p&gt;適当なニューラルネットの学習過程の可視化（ロス、正解率の遷移等）を題材にして、&lt;a href=&#34;https://bokeh.pydata.org/en/latest/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bokeh&lt;/a&gt;を使って動的にグラフを更新していくことによる可視化の実用例を紹介します。このノートブックの冒頭に、最後まで実行すると得られるグラフ一覧をまとめました。どうやってグラフを作るのか知りたい方は続きを読んでもらえればと思います。Bokehの詳細な使い方は、公式ドキュメントを参考にしてください。&lt;/p&gt;
&lt;p&gt;なお、ここで紹介する例は、僕が過去に出た機械学習のコンペ (&lt;a href=&#34;https://deepanalytics.jp/compe/36?tab=compedetail&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://deepanalytics.jp/compe/36?tab=compedetail&lt;/a&gt;) で実際に使用したコードからほぼ取ってきました（8/218位でした）。グラフを動的に更新する方法は &lt;a href=&#34;https://bokeh.pydata.org/en/latest/docs/user_guide/notebook.html#notebook-handles&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;公式ドキュメント&lt;/a&gt; に記述されていますが、そのサンプルの内容は「円を描画して色を変える」といった実用性皆無のものであること、またググっても例が多く見つからないことから、このような紹介記事を書くことにしました。参考になれば幸いです。&lt;/p&gt;
&lt;p&gt;余談ではありますが、こと機械学習に関しては、tensorboardを使ったほうが簡単で良いと思います。僕は最近そうしています。 &lt;a href=&#34;https://qiita.com/r9y9/items/d54162d37ec4f110f4b4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://qiita.com/r9y9/items/d54162d37ec4f110f4b4&lt;/a&gt;. 色なり位置なり大きさなりを柔軟にカスタマイズしたい、あるいはノートブックで処理を完結させたい、と言った場合には、ここで紹介する方法も良いかもしれません。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%pylab inline
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Populating the interactive namespace from numpy and matplotlib
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from IPython.display import HTML, Image
import IPython
from os.path import exists

def summary():
    baseurl = &amp;quot;https://bokeh.pydata.org/en/latest/docs/reference/models/glyphs/&amp;quot;
    for (name, figname, url) in [
        (&amp;quot;Line&amp;quot;, &amp;quot;line&amp;quot;, &amp;quot;line.html&amp;quot;),
        (&amp;quot;VBar&amp;quot;, &amp;quot;vbar&amp;quot;, &amp;quot;vbar.html&amp;quot;),
        (&amp;quot;HBar&amp;quot;, &amp;quot;hbar&amp;quot;, &amp;quot;hbar.html&amp;quot;),
        (&amp;quot;ImageRGBA&amp;quot;, &amp;quot;gray_image&amp;quot;, &amp;quot;image_rgba.html&amp;quot;),
        (&amp;quot;ImageRGBA&amp;quot;, &amp;quot;inferno_image&amp;quot;, &amp;quot;image_rgba.html&amp;quot;),
        ]:
        gif = &amp;quot;./fig/{}.gif&amp;quot;.format(figname)
        print(&amp;quot;\n&amp;quot;,name, baseurl + url)
        if exists(gif):
            with open(gif, &#39;rb&#39;) as f:
                IPython.display.display(Image(data=f.read()), format=&amp;quot;gif&amp;quot;)

summary()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(※ブログ先頭に貼ったので省略します)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# True にしてノートブックを実行すると、上記gifの元となる画像を保存し、最後にgifを生成する
save_img = False
if save_img:
    import os
    from os.path import exists
    if not exists(&amp;quot;./fig&amp;quot;):
        os.makedirs(&amp;quot;./fig&amp;quot;)
    toolbar_location = None
else:
    toolbar_location = &amp;quot;above&amp;quot;

# bokehで描画したグラフはnotebookに残らないので、Trueの場合は代わりに事前に保存してあるgifを描画する
# もしローカルで実行するときは、Falseにしてください
show_static = True
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;準備&#34;&gt;準備&lt;/h2&gt;
&lt;p&gt;先述の通り、ニューラルネットの学習過程の可視化を題材として、Jupyter上でのBokehの使い方を紹介していきたいと思います。今回は、PyTorch (v0.3.0) を使ってニューラルネットの学習のコードを書きました。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/pytorch/examples/tree/master/mnist&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/pytorch/examples/tree/master/mnist&lt;/a&gt; をベースに、可視化しやすいように少しいじりました。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.autograd import Variable
import numpy as np
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;data&#34;&gt;Data&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;use_cuda = torch.cuda.is_available()

batch_size = 128

kwargs = {&#39;num_workers&#39;: 1, &#39;pin_memory&#39;: True} if use_cuda else {}
train_loader = torch.utils.data.DataLoader(
    datasets.MNIST(&#39;./data&#39;, train=True, download=True,
                   transform=transforms.Compose([
                       transforms.ToTensor()
                   ])),
    batch_size=batch_size, shuffle=True, **kwargs)
test_loader = torch.utils.data.DataLoader(
    datasets.MNIST(&#39;./data&#39;, train=False, transform=transforms.Compose([
                       transforms.ToTensor(),
                   ])),
    batch_size=batch_size, shuffle=False, **kwargs)

data_loaders = {&amp;quot;train&amp;quot;: train_loader, &amp;quot;test&amp;quot;:test_loader}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;model&#34;&gt;Model&lt;/h3&gt;
&lt;p&gt;簡単な畳み込みニューラルネットです。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
        self.conv2_drop = nn.Dropout2d()
        self.fc1 = nn.Linear(320, 50)
        self.fc2 = nn.Linear(50, 10)

    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
        x = x.view(-1, 320)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, training=self.training)
        x = self.fc2(x)
        return F.log_softmax(x, dim=-1)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;train-loop&#34;&gt;Train loop&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from tqdm import tnrange
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;epochs = 20

def __train_loop(model, data_loaders, optimizer, epoch, phase):
    model = model.train() if phase == &amp;quot;train&amp;quot; else model.eval()
    running_loss = 0
    running_corrects = 0
    corrects = [0]*10
    counts = [0]*10
    for batch_idx, (x, y) in enumerate(data_loaders[phase]):
        x = x.cuda() if use_cuda else x
        y = y.cuda() if use_cuda else y
        x, y = Variable(x), Variable(y)
        optimizer.zero_grad()
        y_hat = model(x)

        # loss
        loss = F.nll_loss(y_hat, y)

        # update
        if phase == &amp;quot;train&amp;quot;:
            loss.backward()
            optimizer.step()
        running_loss += loss.data[0]

        # predict
        preds = torch.max(y_hat.data, 1)[1]
        match = (preds == y.data).cpu()
        running_corrects += match.sum()

        # カテゴリごとの正解率を出すのにほしい
        for i in range(len(match)):
            if match.view(-1)[i] &amp;gt; 0:
                corrects[y.data.view(-1)[i]] += 1
        for i in range(len(match)):
            counts[y.data.view(-1)[i]] += 1

    # epoch-wise metrics
    l = running_loss / len(data_loaders[phase])
    acc = running_corrects / len(data_loaders[phase].dataset)
    return {&amp;quot;loss&amp;quot;: l, &amp;quot;acc&amp;quot;: acc, &amp;quot;corrects&amp;quot;: corrects, &amp;quot;counts&amp;quot;: counts}

def train_loop(model, data_loaders, optimizer, epochs=12, callback=None):
    history = {&amp;quot;train&amp;quot;: {}, &amp;quot;test&amp;quot;: {}}
    for epoch in tnrange(epochs):
        for phase in [&amp;quot;train&amp;quot;, &amp;quot;test&amp;quot;]:
            d = __train_loop(model, data_loaders, optimizer, epoch, phase)
            for k,v in d.items():
                try:
                    history[phase][k].append(v)
                except KeyError:
                    history[phase][k] = [v]

            # ここでグラフの更新を呼ぶ想定です
            if callback is not None:
                callback.on_epoch_end(epoch, phase, history)
    return history
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;本編&#34;&gt;本編&lt;/h2&gt;
&lt;h3 id=&#34;0-matplotlib&#34;&gt;0. Matplotlib&lt;/h3&gt;
&lt;p&gt;まずはじめに、動的ではない（静的な）グラフの例を示します。手書き数字認識のような識別タスクにおいて、最も一般的であると思われる評価尺度として、ロスと正解率を可視化します。&lt;code&gt;train_loop&lt;/code&gt;関数は、返り値にロスと正解率のhistoryを返すようにしたので、それを使ってグラフを作ります。&lt;/p&gt;
&lt;p&gt;図の作成にはいろんなツールがあると思うのですが、matplotlibが定番で（僕は）大きな不満もないので、よく使っています。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = Net().cuda() if use_cuda else Net()
optimizer = optim.Adadelta(model.parameters())
history = train_loop(model, data_loaders, optimizer, epochs)
print(&amp;quot;Test loss: {:.3f}&amp;quot;.format(history[&amp;quot;train&amp;quot;][&amp;quot;loss&amp;quot;][-1]))
print(&amp;quot;Test acc: {:.3f}&amp;quot;.format(history[&amp;quot;test&amp;quot;][&amp;quot;acc&amp;quot;][-1]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Test loss: 0.111
Test acc: 0.989
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;matplotlib.pyplot.figure(figsize=(16,6))
subplot(1,2,1)
plot(history[&amp;quot;train&amp;quot;][&amp;quot;loss&amp;quot;], linewidth=3, color=&amp;quot;red&amp;quot;, label=&amp;quot;train loss&amp;quot;)
plot(history[&amp;quot;test&amp;quot;][&amp;quot;loss&amp;quot;], linewidth=3, color=&amp;quot;blue&amp;quot;, label=&amp;quot;test lsos&amp;quot;)
xlabel(&amp;quot;epoch&amp;quot;, fontsize=16)
legend(prop={&amp;quot;size&amp;quot;: 16})
subplot(1,2,2)
plot(history[&amp;quot;train&amp;quot;][&amp;quot;acc&amp;quot;], linewidth=3, color=&amp;quot;red&amp;quot;, label=&amp;quot;train acc&amp;quot;)
plot(history[&amp;quot;test&amp;quot;][&amp;quot;acc&amp;quot;], linewidth=3, color=&amp;quot;blue&amp;quot;, label=&amp;quot;test acc&amp;quot;)
xlabel(&amp;quot;epoch&amp;quot;, fontsize=16)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.text.Text at 0x7f16fa9ca438&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://r9y9.github.io/images/jupyter_with_bokeh_files/jupyter_with_bokeh_16_1.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;1-line&#34;&gt;1. Line&lt;/h3&gt;
&lt;p&gt;次に、上記の線グラフを、Bokehを使って作ってみます。これには、 &lt;a href=&#34;https://bokeh.pydata.org/en/latest/docs/reference/models/glyphs/line.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://bokeh.pydata.org/en/latest/docs/reference/models/glyphs/line.html&lt;/a&gt; が使えます。&lt;/p&gt;
&lt;p&gt;bokehで作ったグラフをnotebookにinline plotするためには、&lt;a href=&#34;https://bokeh.pydata.org/en/latest/docs/reference/io.html#bokeh.io.output_notebook&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bokeh.io.output_notebook&lt;/a&gt; を呼び出しておく必要があります。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import bokeh
import bokeh.io
from bokeh.io import push_notebook, show, output_notebook
from bokeh.plotting import figure
try:
    # 少し古いbokehだとこっち
    from bokeh.io import gridplot
except ImportError:
    from bokeh.layouts import gridplot

output_notebook()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;次に定義する &lt;code&gt;LinePlotsCallback&lt;/code&gt; は、グラフの情報をプロパティに保持し、&lt;code&gt;on_epoch_end&lt;/code&gt; で学習結果のhisotoryを受け取って、ロスと正解率のグラフを更新します。 historyには、今回の場合は、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ロス (float)&lt;/li&gt;
&lt;li&gt;正解率 (float)&lt;/li&gt;
&lt;li&gt;カテゴリ毎の総サンプル数 (list)&lt;/li&gt;
&lt;li&gt;カテゴリ毎の正解サンプル数 (list)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;の4つを含めるように実装しました (&lt;code&gt;train_loop&lt;/code&gt; 関数を参照)。&lt;code&gt;LinePlotsCallback&lt;/code&gt; では、このうちロスと正解率を随時受け取って、グラフを更新します。&lt;code&gt;Line&lt;/code&gt; オブジェクトの更新には、&lt;code&gt;data_source.data[&amp;quot;x&amp;quot;]&lt;/code&gt;, &lt;code&gt;data_source.data[&amp;quot;y&amp;quot;]&lt;/code&gt; に随時値を追加していくことで行います。&lt;/p&gt;
&lt;p&gt;以降示すグラフでも同じなのですが、グラフを生成する基本的な手順をまとめると、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://bokeh.pydata.org/en/latest/docs/reference/plotting.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bokeh.plotting.figure&lt;/a&gt; により、figureオブジェクトを生成する&lt;/li&gt;
&lt;li&gt;生成したfigureオブジェクトに対して、線グラフ、棒グラフといったパーツ (レンダラ、bokehでは&lt;a href=&#34;https://bokeh.pydata.org/en/latest/docs/reference/models/glyphs.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;glyphs&lt;/a&gt;と呼ぶ) を生成する&lt;/li&gt;
&lt;li&gt;(今回は格子状に図を配置したかったので）複数のfigureオブジェクトをgrid上にレイアウトする&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;となっています。格子状に配置しない場合は最後のステップは不要ですが、便利なので使います。&lt;/p&gt;
&lt;p&gt;グラフの更新は、レンダラに値をセットしたあとに、&lt;a href=&#34;https://bokeh.pydata.org/en/latest/docs/reference/io.html#bokeh.io.push_notebook&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bokeh.io.push_notebook&lt;/a&gt; を呼び出すことで行います。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class LinePlotsCallback(object):
    def __init__(self, epochs=epochs, batch_size=batch_size):
        # Epoch毎
        p1 = figure(title=&amp;quot;Loss&amp;quot;, plot_height=300, plot_width=350,
                        y_range=(0, 0.5), x_range=(-1, epochs+1))
        p2 = figure(title=&amp;quot;Acc&amp;quot;, plot_height=300, plot_width=350,
                        y_range=(0.8, 1.0), x_range=(-1, epochs+1))

        self.renderers = {&amp;quot;train&amp;quot;:{}, &amp;quot;test&amp;quot;:{}}

        # 赤: train, 青: テスト
        for phase, c in [(&amp;quot;train&amp;quot;, &amp;quot;red&amp;quot;), (&amp;quot;test&amp;quot;, &amp;quot;blue&amp;quot;)]:
            for (p, key) in [(p1, &amp;quot;loss&amp;quot;), (p2, &amp;quot;acc&amp;quot;)]:
                self.renderers[phase][key] = p.line([], [], color=c, line_width=3)

        self.graph = gridplot([p1, p2], ncols=2, toolbar_location=toolbar_location)


    def on_epoch_end(self, epoch, phase, history):
        for key in [&amp;quot;loss&amp;quot;, &amp;quot;acc&amp;quot;]:
            self.renderers[phase][key].data_source.data[&amp;quot;x&amp;quot;].append(epoch)
            self.renderers[phase][key].data_source.data[&amp;quot;y&amp;quot;].append(history[phase][key][-1])
        push_notebook()
        if save_img:
            bokeh.io.export_png(self.graph, &amp;quot;fig/{:02d}_line.png&amp;quot;.format(epoch))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;callback = LinePlotsCallback()
if show_static:
    if exists(&amp;quot;fig/line.gif&amp;quot;):
        with open(&amp;quot;fig/line.gif&amp;quot;, &#39;rb&#39;) as f:
            IPython.display.display(Image(data=f.read()), format=&amp;quot;gif&amp;quot;)
else:
    bokeh.io.show(callback.graph, notebook_handle=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://r9y9.github.io/images/jupyter_with_bokeh_files/jupyter_with_bokeh_21_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = Net().cuda() if use_cuda else Net()
optimizer = optim.Adadelta(model.parameters())
history = train_loop(model, data_loaders, optimizer, epochs, callback=callback)
print(&amp;quot;Test loss: {:.3f}&amp;quot;.format(history[&amp;quot;train&amp;quot;][&amp;quot;loss&amp;quot;][-1]))
print(&amp;quot;Test acc: {:.3f}&amp;quot;.format(history[&amp;quot;test&amp;quot;][&amp;quot;acc&amp;quot;][-1]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Test loss: 0.113
Test acc: 0.990
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;2-vbar&#34;&gt;2. VBar&lt;/h3&gt;
&lt;p&gt;データセット全体の正解率だけでなく、カテゴリ毎の正解率などの尺度を知りたい時がよくあります。次は、数字の各カテゴリごとにどのくらい正解しているのか、といった尺度を可視化するために、縦棒グラフを作ってみます。これには、 &lt;a href=&#34;https://bokeh.pydata.org/en/latest/docs/reference/models/glyphs/vbar.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://bokeh.pydata.org/en/latest/docs/reference/models/glyphs/vbar.html&lt;/a&gt; が使えます。on_epoch_endで渡されるhistoryのうち、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;カテゴリ毎の総サンプル数 (list)&lt;/li&gt;
&lt;li&gt;カテゴリ毎の正解サンプル数 (list)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;の二つを使って動的にグラフを更新します。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Line&lt;/code&gt; オブジェクトの更新は、&lt;code&gt;data_source.data[&amp;quot;x&amp;quot;]&lt;/code&gt;, &lt;code&gt;data_source.data[&amp;quot;y&amp;quot;]&lt;/code&gt; に値を追加していくことで行いましたが、&lt;code&gt;VBar&lt;/code&gt;オブジェクトの場合は、&lt;code&gt;data_source.data[&amp;quot;top&amp;quot;]&lt;/code&gt; に値をセットします。下向きの棒グラフが作りたい場合は、&lt;code&gt;data_source.data[&amp;quot;bottom&amp;quot;]&lt;/code&gt; に値をセットすればOKです。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class VBarPlotsCallback(object):
    def __init__(self, epochs=epochs, batch_size=batch_size):
        # Epoch毎
        p1 = figure(title=&amp;quot;Category-wise correctness (train)&amp;quot;,
                    plot_height=300, plot_width=350, y_range=(0, 7000))
        p2 = figure(title=&amp;quot;Category-wise correctness (test)&amp;quot;,
                    plot_height=300, plot_width=350, y_range=(0, 1500))

        self.renderers = {&amp;quot;train&amp;quot;:{}, &amp;quot;test&amp;quot;:{}}

        bar_opts = dict(width=0.8, alpha=0.5)
        for phase, p in [(&amp;quot;train&amp;quot;, p1), (&amp;quot;test&amp;quot;, p2)]:
            for (key, c) in [(&amp;quot;corrects&amp;quot;, &amp;quot;blue&amp;quot;), (&amp;quot;counts&amp;quot;, &amp;quot;red&amp;quot;)]:
                self.renderers[phase][key] = p.vbar(
                    x=np.arange(0,10), top=[0]*10, name=&amp;quot;test&amp;quot;, color=c, **bar_opts)

        self.graph = gridplot([p1, p2], ncols=2, toolbar_location=toolbar_location)

    def on_epoch_end(self, epoch, phase, history):
        for key in [&amp;quot;counts&amp;quot;, &amp;quot;corrects&amp;quot;]:
            self.renderers[phase][key].data_source.data[&amp;quot;top&amp;quot;] = history[phase][key][-1]
            push_notebook()
        if save_img:
            bokeh.io.export_png(self.graph, &amp;quot;fig/{:02d}_vbar.png&amp;quot;.format(epoch))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;callback = VBarPlotsCallback()
if show_static:
    if exists(&amp;quot;fig/vbar.gif&amp;quot;):
        with open(&amp;quot;fig/vbar.gif&amp;quot;, &#39;rb&#39;) as f:
            IPython.display.display(Image(data=f.read()), format=&amp;quot;gif&amp;quot;)
else:
    bokeh.io.show(callback.graph, notebook_handle=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://r9y9.github.io/images/jupyter_with_bokeh_files/jupyter_with_bokeh_25_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = Net().cuda() if use_cuda else Net()
optimizer = optim.Adadelta(model.parameters())
history = train_loop(model, data_loaders, optimizer, epochs, callback=callback)
print(&amp;quot;Test loss: {:.3f}&amp;quot;.format(history[&amp;quot;train&amp;quot;][&amp;quot;loss&amp;quot;][-1]))
print(&amp;quot;Test acc: {:.3f}&amp;quot;.format(history[&amp;quot;test&amp;quot;][&amp;quot;acc&amp;quot;][-1]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Test loss: 0.115
Test acc: 0.989
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;3-hbar&#34;&gt;3. HBar&lt;/h3&gt;
&lt;p&gt;HBarと非常に似たグラフとして、横向きの棒グラフである &lt;a href=&#34;https://bokeh.pydata.org/en/latest/docs/reference/models/glyphs/hbar.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://bokeh.pydata.org/en/latest/docs/reference/models/glyphs/hbar.html&lt;/a&gt; があります。VBarの場合と同様に、カテゴリ毎の正解サンプル数を可視化してみます。本質的に可視化する情報は変わりませんが、あくまでデモということで。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;HBar&lt;/code&gt;オブジェクトの更新は、&lt;code&gt;data_source.data[&amp;quot;right&amp;quot;]&lt;/code&gt; or &lt;code&gt;data_source.data[&amp;quot;left&amp;quot;]&lt;/code&gt; に値をセットすればOKです。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class HBarPlotsCallback(object):
    def __init__(self, epochs=epochs, batch_size=batch_size):
        # Epoch毎
        p1 = figure(title=&amp;quot;Category-wise correctness (train)&amp;quot;,
                    plot_height=300, plot_width=350, x_range=(0, 7000))
        p2 = figure(title=&amp;quot;Category-wise correctness (test)&amp;quot;,
                    plot_height=300, plot_width=350, x_range=(0, 1500))

        self.renderers = {&amp;quot;train&amp;quot;:{}, &amp;quot;test&amp;quot;:{}}

        bar_opts = dict(height=0.8, alpha=0.5)
        for phase, p in [(&amp;quot;train&amp;quot;, p1), (&amp;quot;test&amp;quot;, p2)]:
            for (key, c) in [(&amp;quot;corrects&amp;quot;, &amp;quot;blue&amp;quot;), (&amp;quot;counts&amp;quot;, &amp;quot;green&amp;quot;)]:
                self.renderers[phase][key] = p.hbar(
                    y=np.arange(0,10), right=[0]*10, name=&amp;quot;test&amp;quot;, color=c, **bar_opts)

        self.graph = gridplot([p1, p2], ncols=2, toolbar_location=toolbar_location)

    def on_epoch_end(self, epoch, phase, history):
        for key in [&amp;quot;counts&amp;quot;, &amp;quot;corrects&amp;quot;]:
            self.renderers[phase][key].data_source.data[&amp;quot;right&amp;quot;] = history[phase][key][-1]
            push_notebook()
        if save_img:
            bokeh.io.export_png(self.graph, &amp;quot;fig/{:02d}_hbar.png&amp;quot;.format(epoch))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;callback = HBarPlotsCallback()
if show_static:
    if exists(&amp;quot;fig/hbar.gif&amp;quot;):
        with open(&amp;quot;fig/hbar.gif&amp;quot;, &#39;rb&#39;) as f:
            IPython.display.display(Image(data=f.read()), format=&amp;quot;gif&amp;quot;)
else:
    bokeh.io.show(callback.graph, notebook_handle=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://r9y9.github.io/images/jupyter_with_bokeh_files/jupyter_with_bokeh_29_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = Net().cuda() if use_cuda else Net()
optimizer = optim.Adadelta(model.parameters())
history = train_loop(model, data_loaders, optimizer, epochs, callback=callback)
print(&amp;quot;Test loss: {:.3f}&amp;quot;.format(history[&amp;quot;train&amp;quot;][&amp;quot;loss&amp;quot;][-1]))
print(&amp;quot;Test acc: {:.3f}&amp;quot;.format(history[&amp;quot;test&amp;quot;][&amp;quot;acc&amp;quot;][-1]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Test loss: 0.116
Test acc: 0.989
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;5-image&#34;&gt;5. Image&lt;/h3&gt;
&lt;p&gt;最後に、https://bokeh.pydata.org/en/latest/docs/reference/models/glyphs/image_rgba.html を使って、画像を可視化する例を紹介します。例えば生成モデルを学習するときなど、学習の過程で、その生成サンプルを可視化したい場合がよくあるので、そういった場合に使えます。&lt;/p&gt;
&lt;p&gt;最初に実装したモデルは手書き数字認識のための識別モデルだったため、趣向を変えて、生成モデルである Variational Auto-encoder (VAE) を使います。識別モデルの学習と生成モデルの学習は少し毛色が違うので、（ほとんど同じですが、簡単のため）併せて学習用のコードを書き換えました。&lt;/p&gt;
&lt;h4 id=&#34;vae&#34;&gt;VAE&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/pytorch/examples/tree/master/vae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/pytorch/examples/tree/master/vae&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class VAE(nn.Module):
    def __init__(self):
        super(VAE, self).__init__()

        self.fc1 = nn.Linear(784, 400)
        self.fc21 = nn.Linear(400, 20)
        self.fc22 = nn.Linear(400, 20)
        self.fc3 = nn.Linear(20, 400)
        self.fc4 = nn.Linear(400, 784)

        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()

    def encode(self, x):
        h1 = self.relu(self.fc1(x))
        return self.fc21(h1), self.fc22(h1)

    def reparameterize(self, mu, logvar):
        if self.training:
            std = logvar.mul(0.5).exp_()
            eps = Variable(std.data.new(std.size()).normal_())
            return eps.mul(std).add_(mu)
        else:
            return mu

    def decode(self, z):
        h3 = self.relu(self.fc3(z))
        return self.sigmoid(self.fc4(h3))

    def forward(self, x):
        mu, logvar = self.encode(x.view(-1, 784))
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

def loss_function(recon_x, x, mu, logvar):
    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784))

    # see Appendix B from VAE paper:
    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014
    # https://arxiv.org/abs/1312.6114
    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    # Normalise by same number of elements as in reconstruction
    KLD /= batch_size * 784

    return BCE + KLD
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;training-loop&#34;&gt;Training loop&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def __train_loop_vae(model, data_loaders, optimizer, epoch, phase):
    model = model.train() if phase == &amp;quot;train&amp;quot; else model.eval()
    running_loss = 0
    recon_batch_first, target = None, None
    for batch_idx, (x, _) in enumerate(data_loaders[phase]):
        x = x.cuda() if use_cuda else x
        x = Variable(x)
        optimizer.zero_grad()
        y_hat = model(x)

        # loss
        recon_batch, mu, logvar = model(x)
        loss = loss_function(recon_batch, x, mu, logvar)

        # update
        if phase == &amp;quot;train&amp;quot;:
            loss.backward()
            optimizer.step()
        running_loss += loss.data[0]

        if target is None:
            target = x
            recon_batch_first = recon_batch

    # epoch-wise metrics
    l = running_loss / len(data_loaders[phase])
    return {&amp;quot;loss&amp;quot;: l, &amp;quot;recon&amp;quot;: recon_batch_first.data.cpu(), &amp;quot;target&amp;quot;: target.data.cpu()}

def train_loop_vae(model, data_loaders, optimizer, epochs=12, callback=None):
    history = {&amp;quot;train&amp;quot;: {}, &amp;quot;test&amp;quot;: {}}
    for epoch in tnrange(epochs):
        for phase in [&amp;quot;train&amp;quot;, &amp;quot;test&amp;quot;]:
            d = __train_loop_vae(model, data_loaders, optimizer, epoch, phase)
            for k,v in d.items():
                try:
                    history[phase][k].append(v)
                except KeyError:
                    history[phase][k] = [v]

            if callback is not None:
                callback.on_epoch_end(epoch, phase, history)
    return history
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;さて、準備は終わりです。&lt;/p&gt;
&lt;p&gt;次に示す &lt;code&gt;ImagePlotsCallback&lt;/code&gt; は、&lt;code&gt;on_epoch_end&lt;/code&gt; で学習結果のhisotoryを受け取って、VAEを通して復元した画像と、復元したい対象の画像を動的に更新します。ImageRGBA の場合は、&lt;code&gt;data_source.data[&amp;quot;image&amp;quot;]&lt;/code&gt; に配列をセットすることで、更新することができます。&lt;/p&gt;
&lt;p&gt;注意事項として、モノクロ画像を描画する際には、適当なカラーマップをかけて、(w, h) -&amp;gt; (w, h, 4) の配列にしておく必要があります。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from torchvision.utils import make_grid
from matplotlib.pyplot import cm

def _to_img(batch, cmap=cm.gray):
    # 128は多かったので半分にします
    _batch_size = batch_size // 2
    batch = batch[:_batch_size]

    batch = batch.view(-1,1,28,28)
    grid = make_grid(batch, nrow=int(np.sqrt(_batch_size)))[0].numpy()
    # Force squared
    l = np.min(grid.shape)
    grid = grid[:l, :l]
    img = np.uint8(cmap(grid) * 255)
    return img

class ImagePlotsCallback(object):
    def __init__(self, epochs=epochs, batch_size=batch_size, cmap=cm.gray):
        x_range, y_range = (-0.5, 10.5), (-0.5, 10.5)
        p1 = figure(title=&amp;quot;Reconstructed (train)&amp;quot;,
                    plot_height=350, plot_width=350, x_range=x_range, y_range=y_range)
        p2 = figure(title=&amp;quot;Target (train)&amp;quot;,
                    plot_height=350, plot_width=350, x_range=x_range, y_range=y_range)
        p3 = figure(title=&amp;quot;Reconstructed (test)&amp;quot;,
                    plot_height=350, plot_width=350, x_range=x_range, y_range=y_range)
        p4 = figure(title=&amp;quot;Target (test)&amp;quot;,
                    plot_height=350, plot_width=350, x_range=x_range, y_range=y_range)
        self.cmap = cmap

        self.renderers = {&amp;quot;train&amp;quot;:{}, &amp;quot;test&amp;quot;:{}}
        empty = torch.zeros(batch_size,1,28,28)
        empty = _to_img(empty, self.cmap)
        # to adjast aspect ratio
        r = empty.shape[0]/empty.shape[1]

        # https://github.com/bokeh/bokeh/issues/1666
        for k, p in [(&amp;quot;recon&amp;quot;, p1), (&amp;quot;target&amp;quot;, p2)]:
            self.renderers[&amp;quot;train&amp;quot;][k] = p.image_rgba(image=[empty[::-1]], x=[0], y=[0], dw=[10], dh=[r*10])
        for k, p in [(&amp;quot;recon&amp;quot;, p3), (&amp;quot;target&amp;quot;, p4)]:
            self.renderers[&amp;quot;test&amp;quot;][k] = p.image_rgba(image=[empty[::-1]], x=[0], y=[0], dw=[10], dh=[r*10])

        self.graph = gridplot([p1, p2, p3, p4], ncols=2, toolbar_location=toolbar_location)

    def on_epoch_end(self, epoch, phase, history):
        for k in [&amp;quot;recon&amp;quot;, &amp;quot;target&amp;quot;]:
            self.renderers[phase][k].data_source.data[&amp;quot;image&amp;quot;] = [_to_img(history[phase][k][-1], self.cmap)[::-1]]
        push_notebook()

        if save_img:
            bokeh.io.export_png(self.graph, &amp;quot;fig/{:02d}_{}_image.png&amp;quot;.format(epoch, self.cmap.name))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# グレースケール
callback = ImagePlotsCallback(cmap=cm.gray)
if show_static:
    if exists(&amp;quot;fig/gray_image.gif&amp;quot;):
        with open(&amp;quot;fig/gray_image.gif&amp;quot;, &#39;rb&#39;) as f:
            IPython.display.display(Image(data=f.read()), format=&amp;quot;gif&amp;quot;)
else:
    bokeh.io.show(callback.graph, notebook_handle=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://r9y9.github.io/images/jupyter_with_bokeh_files/jupyter_with_bokeh_38_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# gifを作ったときに見やすいように、shuffle=Falseにする
kwargs = {&#39;num_workers&#39;: 1, &#39;pin_memory&#39;: True} if use_cuda else {}
train_loader = torch.utils.data.DataLoader(
    datasets.MNIST(&#39;./data&#39;, train=True, download=True,
                   transform=transforms.Compose([
                       transforms.ToTensor()
                   ])),
    batch_size=batch_size, shuffle=False, **kwargs)
test_loader = torch.utils.data.DataLoader(
    datasets.MNIST(&#39;./data&#39;, train=False, transform=transforms.Compose([
                       transforms.ToTensor(),
                   ])),
    batch_size=batch_size, shuffle=False, **kwargs)

data_loaders = {&amp;quot;train&amp;quot;: train_loader, &amp;quot;test&amp;quot;:test_loader}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = VAE().cuda() if use_cuda else VAE()
optimizer = optim.Adam(model.parameters(), lr=1e-3)
history = train_loop_vae(model, data_loaders, optimizer, epochs, callback=callback)
print(&amp;quot;Test loss: {:.3f}&amp;quot;.format(history[&amp;quot;train&amp;quot;][&amp;quot;loss&amp;quot;][-1]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Test loss: 0.133
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;本質的な違いはありませんが、異なるカラーマップを試してみます。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;gradient = np.linspace(0,1,256)
gradient = np.vstack((gradient, gradient))
pyplot.figure(figsize=(16,0.5))
imshow(gradient, aspect=&amp;quot;auto&amp;quot;, cmap=cm.inferno)
axis(&amp;quot;off&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://r9y9.github.io/images/jupyter_with_bokeh_files/jupyter_with_bokeh_42_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;callback = ImagePlotsCallback(cmap=cm.inferno)
if show_static:
    if exists(&amp;quot;fig/inferno_image.gif&amp;quot;):
        with open(&amp;quot;fig/inferno_image.gif&amp;quot;, &#39;rb&#39;) as f:
            IPython.display.display(Image(data=f.read()), format=&amp;quot;gif&amp;quot;)
else:
    bokeh.io.show(callback.graph, notebook_handle=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://r9y9.github.io/images/jupyter_with_bokeh_files/jupyter_with_bokeh_43_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = VAE().cuda() if use_cuda else VAE()
optimizer = optim.Adam(model.parameters(), lr=1e-3)
history = train_loop_vae(model, data_loaders, optimizer, epochs, callback=callback)
print(&amp;quot;Test loss: {:.3f}&amp;quot;.format(history[&amp;quot;train&amp;quot;][&amp;quot;loss&amp;quot;][-1]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Test loss: 0.133
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;
&lt;p&gt;Bokehによるグラフ作成は、少しとっつきにくいかもしれませんが（matplotlibとかではレンダラとか意識しないですよね）、慣れれば柔軟性が高く、便利なのではないかと思います。&lt;/p&gt;
&lt;p&gt;今回の記事を書くにあたっては、bokeh v0.12.9 を使いました。もしローカルでnotebookを実行する場合は、バージョンを揃えることをおすすめします。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://qiita.com/driller/items/0730325bf5c1cd689979&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ipywidgetsとBokeh使ってインタラクティブな可視化をする - Qiita&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://qiita.com/y__sama/items/654ed8ab7464718876f9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jupyter Notebookを動的に使ってみる - Qiita&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://bokeh.pydata.org/en/latest/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Welcome to Bokeh — Bokeh 0.12.12 documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://qiita.com/r9y9/items/d54162d37ec4f110f4b4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyTorchで学習の過程を確認したいときはtensorboardXを使うのが良かったです -　Qiita&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>【単一話者編】Deep Voice 3: 2000-Speaker Neural Text-to-Speech / arXiv:1710.07654 [cs.SD]</title>
      <link>https://r9y9.github.io/blog/2017/12/13/deepvoice3/</link>
      <pubDate>Wed, 13 Dec 2017 12:15:00 +0900</pubDate>
      <guid>https://r9y9.github.io/blog/2017/12/13/deepvoice3/</guid>
      <description>&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;論文リンク: &lt;a href=&#34;https://arxiv.org/abs/1710.07654&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1710.07654&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;コード: &lt;a href=&#34;https://github.com/r9y9/deepvoice3_pytorch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/deepvoice3_pytorch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;三行まとめ&#34;&gt;三行まとめ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1710.07654&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1710.07654: Deep Voice 3: 2000-Speaker Neural Text-to-Speech&lt;/a&gt; を読んで、単一話者の場合のモデルを実装しました（複数話者の場合は、今実験中です (&lt;a href=&#34;https://github.com/r9y9/deepvoice3_pytorch/pull/6&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;deepvoice3_pytorch/#6&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1710.08969&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1710.08969&lt;/a&gt; と同じく、RNNではなくCNNを使うのが肝です&lt;/li&gt;
&lt;li&gt;例によって &lt;a href=&#34;https://keithito.com/LJ-Speech-Dataset/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LJSpeech Dataset&lt;/a&gt; を使って、英語TTSモデルを作りました（学習時間半日くらい）。論文に記載のハイパーパラメータでは良い結果が得られなかったのですが、&lt;a href=&#34;https://arxiv.org/abs/1710.08969&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1710.08969&lt;/a&gt; のアイデアをいくつか借りることで、良い結果を得ることができました。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;概要&#34;&gt;概要&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/11/23/dctts/&#34;&gt;Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention. [arXiv:1710.08969]&lt;/a&gt; で紹介した方法と、モチベーション、基本的な方法論はまったく同じのため省略します。モデルのアーキテクチャが異なりますが、その点についても前回述べたので、そちらを参照ください。
今回の記事では、DeepVoice3のアーキテクチャをベースにした方法での実験結果をまとめます。&lt;/p&gt;
&lt;h2 id=&#34;予備実験&#34;&gt;予備実験&lt;/h2&gt;
&lt;p&gt;はじめに、可能な限り論文に忠実に、論文に記載のモデルアーキテクチャ、ハイパーパラメータで、レイヤー数やConvレイヤーのカーネル数を若干増やしたモデルで試しました。（増やさないと、LJSpeechではイントネーションが怪しい音声が生成されてしまいました）。しかし、どうもビブラートがかかったような音声が生成される傾向にありました。色々試行錯誤して改良したのですが、詳細は後述するとして、改良前/改良後の音声サンプルを以下に示します。&lt;/p&gt;
&lt;p&gt;Generative adversarial network or variational auto-encoder.&lt;/p&gt;
&lt;p&gt;(59 chars, 7 words)&lt;/p&gt;
&lt;p&gt;改良前：&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3/yobi/3_checkpoint_step000530000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;改良後：&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3/yobi/4_checkpoint_step000210000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;いかがでしょうか。結構違いますよね。なお、改良前のモデルは53万イテレーション、改良後は21万イテレーション学習しました。回数を増やせばいいというものではないようです（当たり前ですが）。結論からいうと、モデルの自由度が足りなかったのが品質が向上しにくかった原因ではないかと考えています。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2017/12/21 追記&lt;/strong&gt;：すいません、21万イテレーションのモデルは、何かしら別の事前学習したモデルから、さらに学習したような気がしてきました…。ただ、合計で53万もイテレーションしていないのは間違いないと思います申し訳ございません&lt;/p&gt;
&lt;h2 id=&#34;実験&#34;&gt;実験&lt;/h2&gt;
&lt;p&gt;前回と同じく &lt;a href=&#34;https://keithito.com/LJ-Speech-Dataset/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LJSpeech Dataset&lt;/a&gt; を使って、11時間くらい（21万ステップ）学習しました。モデルは、DeepVoice3で提案されているものを少しいじりました。どのような変更をしたのか、以下にまとめます。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Encoder&lt;/strong&gt;: レイヤー数を増やし、チャンネル数を大きくしました。代わりにカーネル数は7から3に減らしました&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Decoder&lt;/strong&gt;: メル周波数スペクトログラムの複数フレームをDecoderの1-stepで予測するのではなく、&lt;a href=&#34;https://arxiv.org/abs/1710.08969&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1710.08969&lt;/a&gt; で述べられているように、1-stepで（粗い）1フレームを予測して、ConvTransposed1d により元の時間解像度までアップサンプリングする（要は時間方向のアップサンプリングをモデルで学習する）ようにしました&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Decoder&lt;/strong&gt;: アテンションの前に、いくつかConv1d + ReLUを足しました&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Converter&lt;/strong&gt;: ConvTransposed1dを二つ入れて、時間解像度を4倍にアップサンプリングするようにしました&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Converter&lt;/strong&gt;: チャンネル数を大きくしました&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Decoder/Converter&lt;/strong&gt;: レイヤーの最後にSigmoidを追加しました&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Loss&lt;/strong&gt;: Guided attention lossを加えました&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Loss&lt;/strong&gt;: Binary divergenceを加えました&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;共通&lt;/strong&gt;: Linearを1x1 convolutionに変えました。Dilationを大きくとりました&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上記変更点について、本来ならば、Extensiveに実験して、どれがどの程度有効か調べるのが一番良いのですが、計算資源の都合により、部分的にしかやっていません（すいません）。部分的とはいえ、わかったことは最後にまとめておきます。&lt;/p&gt;
&lt;p&gt;計算速度は、バッチサイズ16で、5.3 step/sec くらいの計算速度でした。&lt;a href=&#34;https://arxiv.org/abs/1710.08969&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1710.08969&lt;/a&gt; よりは若干速いくらいです。GPUメモリの使用量は5 ~ 6GB程度でした。PyTorch v0.3.0を使いました。&lt;/p&gt;
&lt;p&gt;学習に使用したコマンドは以下です。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python train.py --checkpoint-dir=checkpoints_deepvoice3 \
    --hparams=&amp;quot;use_preset=True,builder=deepvoice3&amp;quot; \
    --log-event-path=log/deepvoice3_preset
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;コードのコミットハッシュは &lt;a href=&#34;https://github.com/r9y9/deepvoice3_pytorch/tree/7bcf1d070448b4127b41bdf3a1e34c9fea382054&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;7bcf1d0704&lt;/a&gt; です。正確なハイパーパラメータが知りたい場合は、ここから辿れると思います。&lt;/p&gt;
&lt;h3 id=&#34;アライメントの学習過程&#34;&gt;アライメントの学習過程&lt;/h3&gt;
&lt;p&gt;今回の実験ではアテンションレイヤーは二つ（最初と最後）ありますが、以下に平均を取ったものを示します。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/deepvoice3/alignment.gif&#34; /&gt;&lt;/div&gt;
&lt;h3 id=&#34;各種ロスの遷移&#34;&gt;各種ロスの遷移&lt;/h3&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/deepvoice3/deepvoice3_tensorboard.png&#34; /&gt;&lt;/div&gt;
&lt;h3 id=&#34;音声サンプル&#34;&gt;音声サンプル&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/11/23/dctts/&#34;&gt;前回の記事&lt;/a&gt; で貼ったサンプルとまったく同じ文章を用いました。興味のある方は聴き比べてみてください。&lt;/p&gt;
&lt;h4 id=&#34;httpstachi-higithubiotts_samples-より&#34;&gt;&lt;a href=&#34;https://tachi-hi.github.io/tts_samples/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://tachi-hi.github.io/tts_samples/&lt;/a&gt; より&lt;/h4&gt;
&lt;p&gt;icassp stands for the international conference on acoustics, speech and signal processing.&lt;/p&gt;
&lt;p&gt;(90 chars, 14 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3/nyanko/0_checkpoint_step000210000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3/nyanko/0_checkpoint_step000210000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;a matrix is positive definite, if all eigenvalues are positive.&lt;/p&gt;
&lt;p&gt;(63 chars, 12 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3/nyanko/2_checkpoint_step000210000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3/nyanko/2_checkpoint_step000210000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;a spectrogram is obtained by applying es-tee-ef-tee to a signal.&lt;/p&gt;
&lt;p&gt;(64 chars, 11 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3/nyanko/6_checkpoint_step000210000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3/nyanko/6_checkpoint_step000210000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;h4 id=&#34;keithitotacotron-のサンプルhttpskeithitogithubioaudio-samples-と同じ文章&#34;&gt;&lt;a href=&#34;https://keithito.github.io/audio-samples/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron のサンプル&lt;/a&gt; と同じ文章&lt;/h4&gt;
&lt;p&gt;Scientists at the CERN laboratory say they have discovered a new particle.&lt;/p&gt;
&lt;p&gt;(74 chars, 13 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3/3_keithito/0_checkpoint_step000210000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3/3_keithito/0_checkpoint_step000210000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;There&amp;rsquo;s a way to measure the acute emotional intelligence that has never gone out of style.&lt;/p&gt;
&lt;p&gt;(91 chars, 18 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3/3_keithito/1_checkpoint_step000210000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3/3_keithito/1_checkpoint_step000210000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;President Trump met with other leaders at the Group of 20 conference.&lt;/p&gt;
&lt;p&gt;(69 chars, 13 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3/3_keithito/2_checkpoint_step000210000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3/3_keithito/2_checkpoint_step000210000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;The Senate&amp;rsquo;s bill to repeal and replace the Affordable Care Act is now imperiled.&lt;/p&gt;
&lt;p&gt;(81 chars, 16 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3/3_keithito/3_checkpoint_step000210000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3/3_keithito/3_checkpoint_step000210000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;Generative adversarial network or variational auto-encoder.&lt;/p&gt;
&lt;p&gt;(59 chars, 7 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3/3_keithito/4_checkpoint_step000210000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3/3_keithito/4_checkpoint_step000210000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;The buses aren&amp;rsquo;t the problem, they actually provide a solution.&lt;/p&gt;
&lt;p&gt;(63 chars, 13 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3/3_keithito/5_checkpoint_step000210000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3/3_keithito/5_checkpoint_step000210000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;
&lt;p&gt;以下、知見をまとめますが、あくまでその傾向がある、という程度に受け止めてください。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tacotron, DeepVoice3で述べられているようにメル周波数スペクトログラムの複数フレームをDecoderの1-stepで予測するよりも、&lt;a href=&#34;https://arxiv.org/abs/1710.08969&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1710.08969&lt;/a&gt; で述べられているように、1-stepで（粗い）1フレームを予測して、ConvTransposed1d により元の時間解像度までアップサンプリングする方が良い。生成された音声のビブラートのような現象が緩和されるように感じた&lt;/li&gt;
&lt;li&gt;Dilationを大きくしても、大きな品質の変化はないように感じた&lt;/li&gt;
&lt;li&gt;Guided-attentionは、アテンションが早くmonotonicになるという意味で良い。ただし、品質に大きな影響はなさそうに感じた&lt;/li&gt;
&lt;li&gt;Encoderのレイヤー数を大きくするのは効果あり&lt;/li&gt;
&lt;li&gt;Converterのチャンネル数を大きくするのは効果あり&lt;/li&gt;
&lt;li&gt;Binary divergence lossは、学習を安定させるために、DeepVoice3風のアーキテクチャでも有効だった&lt;/li&gt;
&lt;li&gt;Encoder/Converterは &lt;a href=&#34;https://arxiv.org/abs/1710.08969&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1710.08969&lt;/a&gt; のものを、DecoderはDeepVoice3のものを、というパターンで試したことがありますが、&lt;a href=&#34;https://arxiv.org/abs/1710.08969&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1710.08969&lt;/a&gt;に比べて若干品質が落ちたように感じたものの、ほぼ同等と言えるような品質が得られました。&lt;a href=&#34;https://arxiv.org/abs/1710.08969&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1710.08969&lt;/a&gt; ではDecoderに20レイヤー以上使っていますが、10未満でもそれなりの品質になったように思います（上で貼った音声サンプルがまさにその例です）&lt;/li&gt;
&lt;li&gt;品質を改良するために、&lt;a href=&#34;https://arxiv.org/abs/1710.08969&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1710.08969&lt;/a&gt; から色々アイデアを借りましたが、逆にDeepVoice3のアイデアで良かったと思えるものに、Decoderの入力に、(メル周波数の次元まで小さくして、Sigmoidを通して得られる）メル周波数スペクトログラムを使うのではなくその前のhidden stateを使う、といったことがありました。勾配がサチりやすいSigmoidをかまないからか、スペクトログラムに対するL1 Lossの減少が確実に速くなりました (&lt;a href=&#34;https://github.com/r9y9/deepvoice3_pytorch/commit/22a674803f2994af2b818635a0501e4417834936&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;22a6748&lt;/a&gt;)。&lt;/li&gt;
&lt;li&gt;この記事に貼った音声サンプルにおいて、先頭のaが抜けている例が目立ちますが、過去にやった実験ではこういう例は稀だったので、何かハイパーパラメータを誤っていじったんだと思います（闇&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1710.07654&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wei Ping, Kainan Peng, Andrew Gibiansky, et al, &amp;ldquo;Deep Voice 3: 2000-Speaker Neural Text-to-Speech&amp;rdquo;, arXiv:1710.07654, Oct. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1705.03122&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jonas Gehring, Michael Auli, David Grangier, et al, &amp;ldquo;Convolutional Sequence to Sequence Learning&amp;rdquo;, arXiv:1705.03122, May 2017.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/11/23/dctts/&#34;&gt;Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention. [arXiv:1710.08969] | LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;関連記事&#34;&gt;関連記事&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/11/23/dctts/&#34;&gt;Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention. [arXiv:1710.08969] | LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention. [arXiv:1710.08969]</title>
      <link>https://r9y9.github.io/blog/2017/11/23/dctts/</link>
      <pubDate>Thu, 23 Nov 2017 19:30:00 +0900</pubDate>
      <guid>https://r9y9.github.io/blog/2017/11/23/dctts/</guid>
      <description>&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;論文リンク: &lt;a href=&#34;https://arxiv.org/abs/1710.08969&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1710.08969&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;コード: &lt;a href=&#34;https://github.com/r9y9/deepvoice3_pytorch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/deepvoice3_pytorch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;三行まとめ&#34;&gt;三行まとめ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1710.08969&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1710.08969: Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention.&lt;/a&gt; を読んで、実装しました&lt;/li&gt;
&lt;li&gt;RNNではなくCNNを使うのが肝で、オープンソースTacotronと同等以上の品質でありながら、&lt;strong&gt;高速に (一日程度で) 学習できる&lt;/strong&gt; のが売りのようです。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://keithito.com/LJ-Speech-Dataset/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LJSpeech Dataset&lt;/a&gt; を使って、英語TTSモデルを作りました（学習時間一日くらい）。完全再現とまではいきませんが、大まかに論文の主張を確認できました。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;前置き&#34;&gt;前置き&lt;/h2&gt;
&lt;p&gt;本当は &lt;a href=&#34;https://arxiv.org/abs/1710.07654&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepVoice3&lt;/a&gt; の実装をしていたのですが、どうも上手くいかなかったので気分を変えてやってみました。
以前 Tacotronに関する長いブログ記事 (&lt;a href=&#34;https://r9y9.github.io/blog/2017/10/15/tacotron/&#34;&gt;リンク&lt;/a&gt;) を書いてしまったのですが、読む方も書く方もつらいので、簡潔にまとめることにしました。興味のある人は続きもどうぞ。&lt;/p&gt;
&lt;h2 id=&#34;概要&#34;&gt;概要&lt;/h2&gt;
&lt;p&gt;End-to-endテキスト音声合成 (Text-to-speech synthesis; TTS) のための &lt;strong&gt;Attention付き畳み込みニューラルネット (CNN)&lt;/strong&gt; が提案されています。SampleRNN, Char2Wav, Tacotronなどの従来提案されてきたRNNをベースとする方法では、モデルの構造上計算が並列化しにくく、
学習/推論に時間がかかることが問題としてありました。本論文では、主に以下の二つのアイデアによって、従来法より速く学習できるモデルを提案しています。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;RNNではなくCNNを使うこと (参考論文: &lt;a href=&#34;https://arxiv.org/abs/1705.03122&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1705.03122&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Attentionがmotonicになりやすくする効果を持つLossを考えること (&lt;strong&gt;Guided attention&lt;/strong&gt;)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;実験では、オープンソースTacotron (&lt;a href=&#34;https://github.com/keithito/tacotron&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron&lt;/a&gt;) の12日学習されたモデルと比較し、主観評価により同等以上の品質が得られたことが示されています。&lt;/p&gt;
&lt;h3 id=&#34;deepvoice3httpsarxivorgabs171007654-との違い&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/1710.07654&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepVoice3&lt;/a&gt; との違い&lt;/h3&gt;
&lt;p&gt;ほぼ同時期に発表されたDeepVoice3も同じく、CNNをベースとするものです。論文を読みましたが、モチベーションとアプローチの基本は DeepVoice3 と同じに思いました。しかし、ネットワーク構造は DeepVoice3とは大きく異なります。いくつか提案法の特徴を挙げると、以下のとおりです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ネットワークが深い（DeepVoice3だとEncoder, Decoder, Converter それぞれ10未満ですが、この論文ではDecoderだけで20以上）。すべてにおいて深いです。カーネルサイズは3と小さいです&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;Fully-connected layer ではなく1x1 convolutionを使っています&lt;/li&gt;
&lt;li&gt;チャンネル数が大きい（256とか512とか、さらにネットワーク内で二倍になっていたりする）。DeepVoice3だとEncoderは64です&lt;/li&gt;
&lt;li&gt;レイヤーの深さに対して指数上に大きくなるDilationを使っています（DeepVoiceではすべてdilation=1）&lt;/li&gt;
&lt;li&gt;アテンションレイヤーは一つ（DeepVoice3は複数&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;DeepVoice3は、&lt;a href=&#34;https://arxiv.org/abs/1705.03122&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1705.03122&lt;/a&gt; のモデル構造とかなり似通っている一方で、本論文では（参考文献としてあげられていますが）影も形もないくらい変わっている、という印象を受けます。&lt;/p&gt;
&lt;p&gt;ロスに関しては、Guided attentionに関するロスが加わるのに加えて、TacotronやDeepVoice3とは異なり、スペクトログラム/メルスペクトログラムに関して binary divergence (定義は論文参照) をロスに加えているという違いがあります。&lt;/p&gt;
&lt;h2 id=&#34;実験&#34;&gt;実験&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://keithito.com/LJ-Speech-Dataset/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LJSpeech Dataset&lt;/a&gt; を使って、17時間くらい（26.5万ステップ）学習しました。計算資源の都合上、SSRNのチャンネル数は512ではなくその半分の256にしました。&lt;/p&gt;
&lt;p&gt;なお、実装するにあたっては、厳密に再現しようとはせず、色々雰囲気でごまかしました。もともとDeepVoice3の実装をしていたのもあり、アイデアをいくつか借りています。例えば、デコーダの出力をいつ止めるか、というdone flag predictionをネットワークに入れています。Dropoutについて言及がありませんが、ないと汎化しにくい印象があったので&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;、足しました。&lt;/p&gt;
&lt;p&gt;計算速度は、バッチサイズ16で、4.3 step/sec くらいの計算速度でした。僕のマシンのGPUはGTX 1080Ti です。使用したハイパーパラメータは&lt;a href=&#34;https://github.com/r9y9/deepvoice3_pytorch/blob/70dc880fae185d96effaee97f0ce55b5c0d13b61/hparams.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;こちら&lt;/a&gt;です。学習に使用したコマンドは以下です（メモ）。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python train.py --data-root=./data/ljspeech --checkpoint-dir=checkpoints_nyanko \
    --hparams=&amp;quot;use_preset=True,builder=nyanko&amp;quot; \
    --log-event-path=log/nyanko_preset
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;アライメントの学習過程&#34;&gt;アライメントの学習過程&lt;/h3&gt;
&lt;p&gt;数万ステップで、綺麗にmonotonicになりました。GIFは、同じ音声に対するアライメントではなく、毎度違う（ランダムな）音声サンプルに対するアライメントを計算して、くっつけたものです（わかりずらくすいません&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/dctts/alignment.gif&#34; /&gt;&lt;/div&gt;
&lt;h3 id=&#34;各種ロスの遷移&#34;&gt;各種ロスの遷移&lt;/h3&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/dctts/dctts_tensorboard.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;見づらくて申し訳ありませんという感じですが、僕のための簡易ログということで貼っておきます。binary divergenceは、すぐに収束したようでした。&lt;/p&gt;
&lt;h3 id=&#34;音声サンプル&#34;&gt;音声サンプル&lt;/h3&gt;
&lt;h4 id=&#34;公式音声サンプルhttpstachi-higithubiotts_samples-と同じ文章抜粋&#34;&gt;&lt;a href=&#34;https://tachi-hi.github.io/tts_samples/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;公式音声サンプル&lt;/a&gt; と同じ文章（抜粋）&lt;/h4&gt;
&lt;p&gt;公式サンプルとの比較です。11/23時点で、公式のサンプル数が15個と多いので、適当に3つ選びました。公式と比べると少し異なっている印象を受けますが、まぁまぁ良いかなと思いました（曖昧ですが&lt;/p&gt;
&lt;p&gt;icassp stands for the international conference on acoustics, speech and signal processing.&lt;/p&gt;
&lt;p&gt;(90 chars, 14 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nyanko/6_nyanko/0_checkpoint_step000265000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/nyanko/6_nyanko/0_checkpoint_step000265000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;a matrix is positive definite, if all eigenvalues are positive.&lt;/p&gt;
&lt;p&gt;(63 chars, 12 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nyanko/6_nyanko/2_checkpoint_step000265000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/nyanko/6_nyanko/2_checkpoint_step000265000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;a spectrogram is obtained by applying es-tee-ef-tee to a signal.&lt;/p&gt;
&lt;p&gt;(64 chars, 11 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nyanko/6_nyanko/6_checkpoint_step000265000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/nyanko/6_nyanko/6_checkpoint_step000265000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;h4 id=&#34;keithitotacotron-のサンプルhttpskeithitogithubioaudio-samples-と同じ文章&#34;&gt;&lt;a href=&#34;https://keithito.github.io/audio-samples/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron のサンプル&lt;/a&gt; と同じ文章&lt;/h4&gt;
&lt;p&gt;Scientists at the CERN laboratory say they have discovered a new particle.&lt;/p&gt;
&lt;p&gt;(74 chars, 13 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nyanko/3_keithito/0_checkpoint_step000265000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/nyanko/3_keithito/0_checkpoint_step000265000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;There&amp;rsquo;s a way to measure the acute emotional intelligence that has never gone out of style.&lt;/p&gt;
&lt;p&gt;(91 chars, 18 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nyanko/3_keithito/1_checkpoint_step000265000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/nyanko/3_keithito/1_checkpoint_step000265000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;President Trump met with other leaders at the Group of 20 conference.&lt;/p&gt;
&lt;p&gt;(69 chars, 13 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nyanko/3_keithito/2_checkpoint_step000265000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/nyanko/3_keithito/2_checkpoint_step000265000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;The Senate&amp;rsquo;s bill to repeal and replace the Affordable Care Act is now imperiled.&lt;/p&gt;
&lt;p&gt;(81 chars, 16 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nyanko/3_keithito/3_checkpoint_step000265000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/nyanko/3_keithito/3_checkpoint_step000265000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;Generative adversarial network or variational auto-encoder.&lt;/p&gt;
&lt;p&gt;(59 chars, 7 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nyanko/3_keithito/4_checkpoint_step000265000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/nyanko/3_keithito/4_checkpoint_step000265000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;The buses aren&amp;rsquo;t the problem, they actually provide a solution.&lt;/p&gt;
&lt;p&gt;(63 chars, 13 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nyanko/3_keithito/5_checkpoint_step000265000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/nyanko/3_keithito/5_checkpoint_step000265000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;h2 id=&#34;まとめ--わかったことなど&#34;&gt;まとめ &amp;amp; わかったことなど&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Tacotronでは学習に何日もかかっていましたが（計算も遅く1日で10万step程度）、1日でそれなりの品質になりました。&lt;/li&gt;
&lt;li&gt;Guided atetntionがあると、確かに速くattentionがmonotonicになりました。&lt;/li&gt;
&lt;li&gt;2時間程度の学習では &lt;a href=&#34;https://tachi-hi.github.io/tts_samples/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ここ&lt;/a&gt; にあるのと同程度の品質にはなりませんでした…&lt;/li&gt;
&lt;li&gt;DeepVoice3のモデルアーキテクチャで学習した場合と比べると、品質は向上しました&lt;/li&gt;
&lt;li&gt;DeepVoice3と比べると、深いせいなのか学習が難しいように思いました。重みの初期化のパラメータをちょっといじると、sigmoidの出力が0 or 1になって学習が止まる、といったことがありました。重みの初期化はとても重要でした&lt;/li&gt;
&lt;li&gt;上記にも関連して、勾配のノルムが爆発的に大きくなることがしばしばあり、クリッピングを入れました（重要でした）&lt;/li&gt;
&lt;li&gt;Binary divergenceをロスにいれても品質には影響がないように感じました。ただしないと学習初期に勾配が爆発しやすかったです&lt;/li&gt;
&lt;li&gt;提案法は色々なアイデアが盛り込まれているのですが、実際のところどれが重要な要素なのか、といった点に関しては、論文では明らかにされていなかったように思います。今後その辺りを明らかにする論文があってもいいのではないかと思いました。&lt;/li&gt;
&lt;li&gt;学習に使うGPUメモリ量、Tacotronより多い（SSRNのチャンネル数512, バッチサイズ16で &lt;del&gt;8GBくらい&lt;/del&gt; 5~6GB くらいでした）……厳しい……&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;2017/12/19追記: Dropoutなしだと、入力テキストとは無縁の英語らしき何かが生成されるようになってしまいました。Dropoutはやはり重要でした&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一番の学びは、ネットワークの重みの初期化方法は重要、ということでした。おしまい&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1710.08969&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hideyuki Tachibana, Katsuya Uenoyama, Shunsuke Aihara, &amp;ldquo;Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention&amp;rdquo;. arXiv:1710.08969, Oct 2017.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1710.07654&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wei Ping, Kainan Peng, Andrew Gibiansky, et al, &amp;ldquo;Deep Voice 3: 2000-Speaker Neural Text-to-Speech&amp;rdquo;, arXiv:1710.07654, Oct. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1705.03122&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jonas Gehring, Michael Auli, David Grangier, et al, &amp;ldquo;Convolutional Sequence to Sequence Learning&amp;rdquo;, arXiv:1705.03122, May 2017.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;He, Kaiming, et al. &amp;ldquo;Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.&amp;rdquo; Proceedings of the IEEE international conference on computer vision. 2015.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;DeepVoice3でカーネルサイズ3で試すと、全然うまくいきませんでした&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;推論時にアテンションの制約をいれても、「ふぁふぁふぁふぁふぁ」みたいな繰り返しが起きてしまいました&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;論文ではエンコーダデコーダの学習とSRNNの学習を別々でおこなっていますが、僕は一緒にやりました。そのせいもあります&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>日本語 End-to-end 音声合成に使えるコーパス JSUT の前処理 [arXiv:1711.00354]</title>
      <link>https://r9y9.github.io/blog/2017/11/12/jsut_ver1/</link>
      <pubDate>Sun, 12 Nov 2017 03:00:00 +0900</pubDate>
      <guid>https://r9y9.github.io/blog/2017/11/12/jsut_ver1/</guid>
      <description>&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;コーパス配布先リンク: &lt;a href=&#34;https://sites.google.com/site/shinnosuketakamichi/publication/jsut&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JSUT (Japanese speech corpus of Saruwatari Lab, University of Tokyo) - Shinnosuke Takamichi (高道 慎之介)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;論文リンク: &lt;a href=&#34;https://arxiv.org/abs/1711.00354&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1711.00354&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;三行まとめ&#34;&gt;三行まとめ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;日本語End-to-end音声合成に使えるコーパスは神、ありがとうございます&lt;/li&gt;
&lt;li&gt;クリーンな音声であるとはいえ、冒頭/末尾の無音区間は削除されていない、またボタンポチッみたいな音も稀に入っているので注意&lt;/li&gt;
&lt;li&gt;僕が行った無音区間除去の方法（Juliusで音素アライメントを取って云々）を記録しておくので、必要になった方は参考にどうぞ。ラベルファイルだけほしい人は連絡ください&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;jsut-とは&#34;&gt;JSUT とは&lt;/h2&gt;
&lt;p&gt;ツイート引用：&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;フリーの日本語音声コーパス（単一話者による10時間データ）を公開しました．音声研究等にお役立てください．&lt;a href=&#34;https://t.co/94ShJY44mA&#34;&gt;https://t.co/94ShJY44mA&lt;/a&gt; &lt;a href=&#34;https://t.co/T0etDwD7cS&#34;&gt;pic.twitter.com/T0etDwD7cS&lt;/a&gt;&lt;/p&gt;&amp;mdash; Shinnosuke Takamichi (高道 慎之介) (@forthshinji) &lt;a href=&#34;https://twitter.com/forthshinji/status/923547202865131520?ref_src=twsrc%5Etfw&#34;&gt;October 26, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;つい先月、JSUT という、日本語 End-to-end 音声合成の研究に使えることを前提に作られた、フリーの大規模音声コーパスが公開されました。詳細は上記リンク先を見てもらうとして、簡単に特徴をまとめると、以下のとおりです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;単一日本語女性話者の音声10時間&lt;/li&gt;
&lt;li&gt;無響室で収録されている、クリーンな音声コーパス &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;非営利目的で無料で使える&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;僕の知る限り、日本語 End-to-end 音声合成に関する研究はまだあまり発展していないように感じていたのですが、その理由の一つに誰でも自由に使えるコーパスがなかったことがあったように思います。このデータセットはとても貴重なので、ぜひ使っていきたいところです。
高道氏およびコーパスを整備してくださった方、本当にありがとうございます。&lt;/p&gt;
&lt;p&gt;この記事では、僕が実際に日本語End-to-end音声合成の実験をしようと思った時に、必要になった前処理（最初と最後の&lt;strong&gt;無音区間の除去&lt;/strong&gt;）について書きたいと思います。&lt;/p&gt;
&lt;h2 id=&#34;問題&#34;&gt;問題&lt;/h2&gt;
&lt;p&gt;まずはじめに、最初と最後の無音区間を除去したい理由には、以下の二つがありました。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Tacotronのようなattention付きseq2seqモデルにおいて、アライメントを学習するのに不都合なこと。句読点に起因する無音区間ならともかく、最初/最後の無音区間は、テキスト情報からはわからないので、直感的には不要であると思われます。参考までに、&lt;a href=&#34;https://arxiv.org/abs/1705.08947&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepVoice2の論文のsection 4.2&lt;/a&gt; では、無音区間をトリミングするのがよかったと書かれています。&lt;/li&gt;
&lt;li&gt;発話の前、発話の後に、微妙にノイズがある（息を大きく吸う音、ボタンをポチッ、みたいな機械音等）データがあり、そのノイズが不都合なこと。例えばTacotronのようなモデルでは、テキスト情報とスペクトログラムの関係性を学習したいので、テキストに関係のないノイズは可能な限り除去しておきたいところです。参考までに、ボタンポチノイズは 例えば &lt;code&gt;basic5000/wav/BASIC5000_0008.wav&lt;/code&gt; に入っています&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;最初何も考えずに（ダメですが）データを入れたら、アライメントが上手く学習されないなーと思い、データを見ていたところ、後者に気づいた次第です。&lt;/p&gt;
&lt;h2 id=&#34;方法&#34;&gt;方法&lt;/h2&gt;
&lt;p&gt;さて、無音区間を除去する一番簡単な方法は、適当にパワーで閾値処理をすることです。しかし、前述の通りボタンをポチッと押したようなノイズは、この方法では難しそうでした。というわけで、少し手間はかかりますが、Juliusで音素アライメントを取って、無音区間を推定することにしました。
以下、Juliusを使ってアライメントファイル（.lab) を作る方法です。コードは、 &lt;a href=&#34;https://github.com/r9y9/segmentation-kit/tree/jsut&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/segmentation-kit/tree/jsut&lt;/a&gt; にあります。&lt;/p&gt;
&lt;p&gt;自分で準備するのが面倒だから結果のラベルファイルだけほしいという方がいれば、連絡をいただければお渡しします。Linux環境での実行を想定しています。僕はUbuntu 16.04で作業しています。&lt;/p&gt;
&lt;h3 id=&#34;準備&#34;&gt;準備&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/julius-speech/julius&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Julius&lt;/a&gt; をインストールする。&lt;code&gt;/usr/local/bin/julius&lt;/code&gt; にバイナリがあるとします&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://taku910.github.io/mecab/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MeCab&lt;/a&gt;をインストールする&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/neologd/mecab-ipadic-neologd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;mecab-ipadic-neologd&lt;/a&gt; をインストールする&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/nnmnkwii&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nnmnkwii&lt;/a&gt; のmasterブランチを入れる&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pip install mecab-python3 jaconv&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sudo apt-get install sox&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/segmentation-kit/tree/jsut&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Juliusの音素セグメンテーションツールキットのフォーク (jsutブランチ)&lt;/a&gt; をクローンする。クローン先を作業ディレクトリとします&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;コーパスの場所を設定&#34;&gt;コーパスの場所を設定&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;params.py&lt;/code&gt; というファイルに、コーパスの場所を指定する変数 (&lt;code&gt;in_dir&lt;/code&gt;) があるので、設定します。僕の場合、以下のようになっています。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# coding: utf-8
in_dir = &amp;quot;/home/ryuichi/data/jsut_ver1&amp;quot;
dst_dir = &amp;quot;jsut&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;音素アライメントの実行&#34;&gt;音素アライメントの実行&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;bash run.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;でまるっと実行できるようにしました。MeCabで読みを推定するなどの処理は、この記事を書いている時点では &lt;code&gt;a.py&lt;/code&gt;, &lt;code&gt;b.py&lt;/code&gt;, &lt;code&gt;c.py&lt;/code&gt;, &lt;code&gt;d.py&lt;/code&gt;というファイルに書かれています。 適当なファイル名で申し訳ありませんという気持ちですが、自分のための書いたコードはこうなってしまいがちです、申し訳ありません。&lt;/p&gt;
&lt;p&gt;7000ファイル以上処理するので、三十分くらいかかります。&lt;code&gt;./jsut&lt;/code&gt; というディレクトリに、labファイルができていれば正常に実行完了です。最後に、&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Failed number of utterances: 87
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;のように、アライメントに失敗したファイル数が表示されるようになっています。失敗の理由には、MeCabでの読みの推定に失敗した（特に数字）などがあります。手で直すことも可能なのですが（実際に一度はやろうとした）非常に大変なので、多少失敗してもよいので大雑把にアライメントを取ることを目的として、スクリプトを作りました。&lt;/p&gt;
&lt;p&gt;なお、juliusはwavesurferのフォーマットでラベルファイルを吐きますが、HTKのラベルフォーマットの方が僕には都合がよかったので、変換するようにしました。&lt;/p&gt;
&lt;h3 id=&#34;コーパスにパッチ&#34;&gt;コーパスにパッチ&lt;/h3&gt;
&lt;p&gt;便宜上、下記のようにwavディレクトリと同じ階層にラベルファイルがあると都合がよいので、僕はそのようにします。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tree ~/data/jsut_ver1/ -d -L 2
/home/ryuichi/data/jsut_ver1/
├── basic5000
│   ├── lab
│   └── wav
├── countersuffix26
│   ├── lab
│   └── wav
├── loanword128
│   ├── lab
│   └── wav
├── onomatopee300
│   ├── lab
│   └── wav
├── precedent130
│   ├── lab
│   └── wav
├── repeat500
│   ├── lab
│   └── wav
├── travel1000
│   ├── lab
│   └── wav
├── utparaphrase512
│   ├── lab
│   └── wav
└── voiceactress100
    ├── lab
    └── wav
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;以下のコマンドにより、生成されたラベルファイルをコーパス配下にコピーします。この処理は、&lt;code&gt;run.sh&lt;/code&gt; では実行しないようになっているので、必要であれば自己責任でおこなってください。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python d.py
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;ラベル活用例&#34;&gt;ラベル活用例&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://gist.github.com/r9y9/db6b5484a6a5deca24e81e76cb17e046&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gist.github.com/r9y9/db6b5484a6a5deca24e81e76cb17e046&lt;/a&gt; のようなコードを書いて、ボタンポチ音が末尾に入っている &lt;code&gt;basic5000/wav/BASIC5000_0008.wav&lt;/code&gt; に対して無音区間削除を行ってみると、結果は以下のようになります。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/jsut_basic5000_08.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;パワーベースの閾値処理では上手くいかない一方で&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;、音素アライメントを使った方法では上手く無音区間除去ができています。その他、数十サンプルを目視で確認しましたが、僕の期待どおり上手くいっているようでした。めでたし。&lt;/p&gt;
&lt;h2 id=&#34;おわり&#34;&gt;おわり&lt;/h2&gt;
&lt;p&gt;以上です。End-to-end系のモデルにとってはデータは命であり、このコーパスは神であります。このコーパスを使って、同じように前処理をしたい人の参考になれば幸いです。&lt;/p&gt;
&lt;p&gt;いま僕はこのコーパスを使って、日本語end-to-end音声合成の実験も少しやっているので、まとまったら報告しようと思っています。&lt;/p&gt;
&lt;div =align=&#34;center&#34;&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;これさ、ASJとかで発表しない？絶対に価値あると思う。諸々のサポートはしますよ。&lt;/p&gt;&amp;mdash; Shinnosuke Takamichi (高道 慎之介) (@forthshinji) &lt;a href=&#34;https://twitter.com/forthshinji/status/928303639478747136?ref_src=twsrc%5Etfw&#34;&gt;November 8, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;コーパスを作った本人氏にASJで発表しないかと勧誘を受けていますが、現在の予定は未定です^q^&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1711.00354&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ryosuke Sonobe, Shinnosuke Takamichi and Hiroshi Saruwatari,
&amp;ldquo;JSUT corpus: free large-scale Japanese speech corpus for end-to-end speech synthesis,&amp;rdquo;
arXiv preprint, 1711.00354, 2017.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1705.08947&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sercan Arik, Gregory Diamos, Andrew Gibiansky, et al, &amp;ldquo;Deep Voice 2: Multi-Speaker Neural Text-to-Speech&amp;rdquo;, 	arXiv:1705.08947, 2017.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;以前ブログでEnd-to-end英語音声合成に使えると書いた &lt;a href=&#34;https://keithito.com/LJ-Speech-Dataset/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LJSpeech&lt;/a&gt;はクリーンではないんですねー&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;このサンプルで上手くいくように閾値を調整すると、他のサンプルでトリミングしすぎてしまうようになってしまいます&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Tacotron: Towards End-to-End Speech Synthesis / arXiv:1703.10135 [cs.CL]</title>
      <link>https://r9y9.github.io/blog/2017/10/15/tacotron/</link>
      <pubDate>Sun, 15 Oct 2017 14:00:00 +0900</pubDate>
      <guid>https://r9y9.github.io/blog/2017/10/15/tacotron/</guid>
      <description>&lt;p&gt;Googleが2017年4月に発表したEnd-to-Endの音声合成モデル &lt;a href=&#34;https://arxiv.org/abs/1703.10135&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tacotron: Towards End-to-End Speech Synthesis / arXiv:1703.10135 [cs.CL]&lt;/a&gt; に興味があったので、自分でも同様のモデルを実装して実験してみました。結果わかったことなどをまとめておこうと思います。&lt;/p&gt;
&lt;p&gt;GoogleによるTacotronの音声サンプルは、 &lt;a href=&#34;https://google.github.io/tacotron/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://google.github.io/tacotron/&lt;/a&gt; から聴けます。僕の実装による音声サンプルはこの記事の真ん中くらいから、あるいは  &lt;a href=&#34;http://nbviewer.jupyter.org/github/r9y9/tacotron_pytorch/blob/f98eda7336726cdfe4ab97ae867cc7f71353de50/notebooks/Test%20Tacotron.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Test Tacotron.ipynb | nbviewer&lt;/a&gt;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; から聴くことができます。&lt;/p&gt;
&lt;p&gt;とても長い記事になってしまったので、結論のみ知りたい方は、一番最後まで飛ばしてください。最後の方のまとめセクションに、実験した上で僕が得た知見がまとまっています。&lt;/p&gt;
&lt;h2 id=&#34;概要&#34;&gt;概要&lt;/h2&gt;
&lt;p&gt;論文のタイトルにもある通り、End-to-Endを目指しています。典型的な（複雑にあなりがちな）音声合成システムの構成要素である、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;言語依存のテキスト処理フロントエンド&lt;/li&gt;
&lt;li&gt;言語特徴量と音響特徴量のマッピング (HMMなりDNNなり)&lt;/li&gt;
&lt;li&gt;波形合成のバックエンド&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;を一つのモデルで達成しようとする、&lt;strong&gt;attention付きseq2seqモデル&lt;/strong&gt; を提案しています。ただし、&lt;strong&gt;Toward&lt;/strong&gt; とあるように、完全にEnd-to-Endではなく、ネットワークは波形ではなく &lt;strong&gt;振幅スペクトログラム&lt;/strong&gt; を出力し、Griffin limの方法によって位相を復元し、逆短時間フーリエ変換をすることによって、最終的な波形を得ます。根本にあるアイデア自体はシンプルですが、そのようなEnd-to-Endに近いモデルで高品質な音声合成を実現するのは困難であるため、論文では学習を上手くいくようするためのいくつかのテクニックを提案する、といった主張です。以下にいくつかピックアップします。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;エンコーダに &lt;strong&gt;CBFG&lt;/strong&gt; (1-D convolution bank + highway network + bidirectional GRU) というモジュールを使う&lt;/li&gt;
&lt;li&gt;デコーダの出力をスペクトログラムではなく（より低次元の）&lt;strong&gt;メル周波数スペクトログラム&lt;/strong&gt; にする。スペクトログラムはアライメントを学習するには冗長なため。&lt;/li&gt;
&lt;li&gt;スペクトログラムは、メル周波数スペクトログラムに対して &lt;strong&gt;CBFG&lt;/strong&gt; を通して得る&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;その他、BatchNormalizationを入れたり、Dropoutを入れたり、GRUをスタックしたり、と色々ありますが、正直なところ、どれがどのくらい効果があるのかはわかっていません（調べるには、途方もない時間がかかります）が、論文の主張によると、これらが有効なようです。&lt;/p&gt;
&lt;h2 id=&#34;既存実装&#34;&gt;既存実装&lt;/h2&gt;
&lt;p&gt;Googleは実装を公開していませんが、オープンソース実装がいくつかあります。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Kyubyong/tacotron&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/Kyubyong/tacotron&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/barronalex/Tacotron&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/barronalex/Tacotron&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/keithito/tacotron&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/keithito/tacotron&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;自分で実装する前に、上記をすべてを簡単に試したり、生成される音声サンプルを比較した上で、僕は &lt;a href=&#34;https://github.com/keithito/tacotron&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron&lt;/a&gt; が一番良いように思いました。最も良いと思った点は、keithito さんは、&lt;a href=&#34;https://keithito.com/LJ-Speech-Dataset/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LJ Speech Dataset&lt;/a&gt; という単一話者の英語読み上げ音声 &lt;strong&gt;約24時間のデータセットを構築&lt;/strong&gt; し、それを &lt;strong&gt;public domainで公開&lt;/strong&gt; していることです。このデータセットは貴重です。&lt;a href=&#34;https://keithito.github.io/audio-samples/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;デモ音声サンプル&lt;/a&gt;は、そのデータセットを使った結果でもあり、他と比べてとても高品質に感じました。自分でも試してみて、1時間程度で英語らしき音声が生成できるようになったのと、さらに数時間でアライメントも学習されることを確認しました。&lt;/p&gt;
&lt;p&gt;なお、上記3つすべてで学習スクリプトを回して音声サンプルを得る、程度のことは試しましたが、僕がコードレベルで読んだのは &lt;a href=&#34;https://github.com/keithito/tacotron&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron&lt;/a&gt; のみです。読んだコードは、TensorFlowに詳しくない僕でも読めるもので、とても構造化されていて読みやすかったです。&lt;/p&gt;
&lt;h2 id=&#34;自前実装&#34;&gt;自前実装&lt;/h2&gt;
&lt;p&gt;勉強も兼ねて、PyTorchでスクラッチから書きました。その結果が &lt;a href=&#34;https://github.com/r9y9/tacotron_pytorch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/tacotron_pytorch&lt;/a&gt; です。&lt;/p&gt;
&lt;p&gt;先にいくつか結論を書いておくと、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;音の品質は、&lt;a href=&#34;https://github.com/keithito/tacotron&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron&lt;/a&gt; の方が良く感じました（同じモデルの実装を心がけたのに…つらい…）。ただ、データセットの音声には残響が乗っていて、生成された音声が元音声に近いのかというのは、僕には判断がつきにくいです。記事の後半に比較できるようにサンプルを貼っておきますので、気になる方はチェックしてみてください&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/keithito/tacotron&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron&lt;/a&gt; では長い入力だと合成に失敗する一方で&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;、僕の実装では比較的長くてもある程度合成できるようです。なぜのかを突き詰めるには、TensorFlowのseq2seq APIの &lt;strong&gt;コード&lt;/strong&gt; (APIは抽象化されすぎていてdocstringからではよくわからないので…) を読みとく必要があるかなと思っています（やっていませんすいません&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;実験&#34;&gt;実験&lt;/h2&gt;
&lt;p&gt;基本的には &lt;a href=&#34;https://github.com/keithito/tacotron&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron&lt;/a&gt; の学習スクリプトと同じで、&lt;a href=&#34;https://keithito.com/LJ-Speech-Dataset/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LJ Speech Dataset&lt;/a&gt; を使って学習させました。テキスト処理、音声処理 (Griffin lim等) には既存のコードをそのまま使用し、モデル部分のみ自分で置き換えました。実験では、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;attention付きseq2seqの肝である、アライメントがどのように学習されていくのか&lt;/li&gt;
&lt;li&gt;学習が進むにつれて、生成される音声はどのように変わっていくのか&lt;/li&gt;
&lt;li&gt;学習されたモデルは、汎化性能はどの程度なのか（未知文章、長い文章、スペルミスに対してパフォーマンスはどう変わるのか、等）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;を探っていきました。&lt;/p&gt;
&lt;h3 id=&#34;アライメントの学習過程の可視化&#34;&gt;アライメントの学習過程の可視化&lt;/h3&gt;
&lt;p&gt;通常のseq2seqは、エンコーダRNNによって得た最後のタイムステップにおける隠れ層の状態を、デコーダのRNNの初期状態として渡します。一方attentiont付きのseq2seqモデルでは、デコーダRNNは各タイムステップで、エンコーダRNNの各タイムステップにおける隠れ層の状態を重みづけて使用し、その重みも学習します。attention付きのseq2seqでは、アライメントがきちんと（曖昧な表現ですが）学習されているかを可視化してチェックするのが、学習がきちんと進んでいるのか確認するのに便利です。&lt;/p&gt;
&lt;p&gt;以下に、47000 step (epochではありません。僕の計算環境 GTX 1080Ti で半日かからないくらい) iterationしたときのアライメント結果と、47000 stepの時点での予測された音声サンプルを示します。なお、gifにおける各画像は、データセットをランダムにサンプルした際のアライメントであり、ある同じ音声に対するアライメントではありません。Tacotron論文には、Bahdanau Attentionを使用したとありますが、&lt;a href=&#34;https://github.com/keithito/tacotron/issues/24&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron #24 Try Monotonic Attention&lt;/a&gt; によると、Tacotron論文の第一著者は新しいバージョンのTacotronでは Monotonic attentionを使用しているらしいということから、Monotonic Attentionでも試してみました。あとでわかったのですが、長文（200文字、数文とか）を合成しようとすると途中でアライメントがスキップすることが多々見受けられたので、そういった場合に、monotonicという制約が上手く働くのだと思います。&lt;/p&gt;
&lt;p&gt;以下の順でgifを貼ります。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/keithito/tacotron&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron&lt;/a&gt;, Bahdanau attention&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/keithito/tacotron&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron&lt;/a&gt;, Bahdanau-style monotonic attention&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/tacotron_pytorch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/tacotron_pytorch&lt;/a&gt;, Bahdanau attention&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;keithito: Bahdanau Attention&lt;/strong&gt;&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/tacotron-tf-alignment_47000steps.gif&#34; /&gt;&lt;/div&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/step-47000-audio-tf.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;keithito: (Bahdanau-style) Monotonic Attention&lt;/strong&gt;&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/tacotron-tf-monotonic-alignment_47000steps.gif&#34; /&gt;&lt;/div&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/step-47000-audio-tf-monotonic.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;自前実装: Bahdanau Attention&lt;/strong&gt;&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/tacotron-alignment_47000steps.gif&#34; /&gt;&lt;/div&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/step-47000-audio-pt.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;Monotonicかどうかで比較すると、Monotonic attentionの方がアライメントがかなり安定しているように見えます。しかし、Githubのスレッドにあった音声サンプルを聴くと、音質的な意味では大きな違いがないように思ったのと、収束速度（簡単に試したところ、アライメントがまともになりだすstepは20000くらいで、ほぼ同じでした）も同じに思えました。一方で自前実装は、アライメントがまともになるstepが10000くらいとやや早く、またシャープに見えます。&lt;/p&gt;
&lt;p&gt;音声サンプルの方ですが、既存実装は両者ともそれなりにまともです。一方自前実装では、まだかなりノイジーです。できるだけtf実装と同じようにつくり、実験条件も同じにしたつもりですが、何か間違っているかもしれません。が、イテレーションを十分に回すと、一応音声はそれなりに出るようになります。&lt;/p&gt;
&lt;p&gt;音声サンプルに関する注意点としては、これはデコードの際に教師データを使っているので、この時点でのモデル使って、同等の音質の音声を生成できるとは限りません。学習時には、デコーダの各タイムステップで教師データのスペクトログラム（正確には、デコーダの出力はメル周波数スペクトログラム）を入力とする一方で、評価時には、デコーダ自身が出力したスペクトログラムを次のタイムステップの入力に用います。評価時には、一度変なスペクトログラムを出力してしまったら、エラーが蓄積していってどんどん変な出力をするようになってしまうことは想像に難しくないと思います。seq2seqモデルのデコードにはビームサーチが代表的なものとしてありますが、Tacotronでは単純にgreedy decodingをします。&lt;/p&gt;
&lt;h3 id=&#34;学習が進むにつれて生成される音声はどのように変わっていくのか&#34;&gt;学習が進むにつれて、生成される音声はどのように変わっていくのか&lt;/h3&gt;
&lt;p&gt;さて、ここからは自前実装のみでの実験結果です。約10日、70万step程度学習させましたので、5000, 10000, 50000, そのあとは10万から10万ステップごとに70万ステップまでそれぞれで音声を生成して、どのようになっているのかを見ていきます。&lt;/p&gt;
&lt;h4 id=&#34;例文1&#34;&gt;例文1&lt;/h4&gt;
&lt;p&gt;Hi, my name is Tacotron. I&amp;rsquo;m still learning a lot from data.&lt;/p&gt;
&lt;p&gt;(56 chars, 14 words)&lt;/p&gt;
&lt;p&gt;step 5000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/0_step5000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 10000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/0_step10000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 50000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/0_step50000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 100000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/0_step100000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 200000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/0_step200000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 300000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/0_step300000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 400000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/0_step400000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 500000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/0_step500000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 600000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/0_step600000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 700000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/0_step700000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;だいたい20万ステップ（学習二日くらい）から、まともな音声になっているように感じます。細かいところでは、&lt;code&gt;Hi,&lt;/code&gt; &lt;code&gt;Tacotron&lt;/code&gt; という部分が少し発音しにくそうです。データセットにはこのような話し言葉のようなものが少ないのと、&lt;code&gt;Tacotron&lt;/code&gt; という単語が英語らしさ的な意味で怪しいから（造語ですよね、たぶん）と考えられます。&lt;/p&gt;
&lt;h4 id=&#34;例文2&#34;&gt;例文2&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Python_%28programming_language%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://en.wikipedia.org/wiki/Python_(programming_language)&lt;/a&gt; より引用：&lt;/p&gt;
&lt;p&gt;Python is a widely used high-level programming language for general-purpose programming, created by Guido van Rossum and first released in 1991.&lt;/p&gt;
&lt;p&gt;(144 chars, 23 words)&lt;/p&gt;
&lt;p&gt;step 5000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/1_step5000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 10000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/1_step10000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 50000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/1_step50000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 100000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/1_step100000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 200000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/1_step200000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 300000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/1_step300000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 400000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/1_step400000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 500000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/1_step500000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 600000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/1_step600000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 700000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/1_step700000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;だいたい20万ステップから、まともな音声になっているように思います。&lt;/p&gt;
&lt;h3 id=&#34;モデルの汎化性能について調査&#34;&gt;モデルの汎化性能について調査&lt;/h3&gt;
&lt;p&gt;以下、72万ステップ（一週間くらい）学習させたモデルを使って、いろんな入力でテストした結果です。音声と合わせてアライメントも貼っておきます。&lt;/p&gt;
&lt;h4 id=&#34;適当な未知入力&#34;&gt;適当な未知入力&lt;/h4&gt;
&lt;p&gt;データセットには存在しない文章を使ってテストしてみました。ところどころ（非ネイティブの僕にでも）不自然だと感じるところが見られますが、とはいえまぁまぁいい感じではないでしょうか。(google translateで同じ文章を合成してみて比べても、そんなに悪くない気がしました)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/PyPy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://en.wikipedia.org/wiki/PyPy&lt;/a&gt; より：&lt;/p&gt;
&lt;p&gt;PyPy is an alternate implementation of the Python programming language written in Python.&lt;/p&gt;
&lt;p&gt;(89 chars, 14 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/0_unknown/0_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/0_unknown/0_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/NumPy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://en.wikipedia.org/wiki/NumPy&lt;/a&gt; より：&lt;/p&gt;
&lt;p&gt;NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.&lt;/p&gt;
&lt;p&gt;(215 chars, 35 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/0_unknown/1_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/0_unknown/1_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&#34;https://numba.pydata.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://numba.pydata.org/&lt;/a&gt; より：&lt;/p&gt;
&lt;p&gt;Numba gives you the power to speed up your applications with high performance functions written directly in Python.&lt;/p&gt;
&lt;p&gt;(115 chars, 19 words)&lt;/p&gt;
&lt;p&gt;&lt;audio controls=&#34;controls&#34; &gt;は&lt;/p&gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/0_unknown/2_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/0_unknown/2_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;h4 id=&#34;スペルミス&#34;&gt;スペルミス&lt;/h4&gt;
&lt;p&gt;スペルミスがある場合に、合成結果はどうなるのか、といったテストです。&lt;a href=&#34;https://google.github.io/tacotron/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Googleのデモ&lt;/a&gt;にあるように、ある程度ロバスト（少なくとも全体が破綻するといったことはない）のように思いました。&lt;/p&gt;
&lt;p&gt;Thisss isrealy awhsome.&lt;/p&gt;
&lt;p&gt;(23 chars, 4 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/1_spell/0_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/1_spell/0_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;This is really awesome.&lt;/p&gt;
&lt;p&gt;(23 chars, 5 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/1_spell/1_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/1_spell/1_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;I cannnnnot believe it.&lt;/p&gt;
&lt;p&gt;(23 chars, 5 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/1_spell/2_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/1_spell/2_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;I cannot believe it.&lt;/p&gt;
&lt;p&gt;(20 chars, 6 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/1_spell/3_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/1_spell/3_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;h4 id=&#34;中少し長めの文章&#34;&gt;中〜少し長めの文章&lt;/h4&gt;
&lt;p&gt;だいたい250文字を越えたくらいで、単語がスキップされるなどの現象が多く確認されました。データセットは基本的に短い文章の集まりなのが理由に思います。前述の通り、monotonic attentionを使えば、原理的にはスキップされにくくなると思います。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1703.10135&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/1703.10135&lt;/a&gt; より引用：&lt;/p&gt;
&lt;p&gt;A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module.&lt;/p&gt;
&lt;p&gt;(155 chars, 26 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/2_long/1_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/2_long/1_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&#34;https://americanliterature.com/childrens-stories/little-red-riding-hood&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://americanliterature.com/childrens-stories/little-red-riding-hood&lt;/a&gt; より引用：&lt;/p&gt;
&lt;p&gt;Once upon a time there was a dear little girl who was loved by every one who looked at her, but most of all by her grandmother, and there was nothing that she would not have given to the child.&lt;/p&gt;
&lt;p&gt;(193 chars, 43 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/2_long/0_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/2_long/0_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1703.10135&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/1703.10135&lt;/a&gt; より引用：&lt;/p&gt;
&lt;p&gt;A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices.&lt;/p&gt;
&lt;p&gt;(263 chars, 41 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/2_long/2_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/2_long/2_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&#34;https://americanliterature.com/childrens-stories/little-red-riding-hood&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://americanliterature.com/childrens-stories/little-red-riding-hood&lt;/a&gt; より引用：&lt;/p&gt;
&lt;p&gt;Once upon a time there was a dear little girl who was loved by every one who looked at her, but most of all by her grandmother, and there was nothing that she would not have given to the child. Once she gave her a little cap of red velvet, which suited her so well
that she would never wear anything else. So she was always called Little Red Riding Hood.&lt;/p&gt;
&lt;p&gt;(354 chars, 77 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/2_long/3_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/2_long/3_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;h3 id=&#34;googleのデモと比較&#34;&gt;Googleのデモと比較&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://google.github.io/tacotron/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://google.github.io/tacotron/&lt;/a&gt; の音声サンプルと同じ文章で試します。大文字小文字の区別は今回学習したモデルでは区別しないので、一部例文は除いています。いくつか気づいたことを挙げておくと、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;He has read the whole thing. / He reads book. のように、readの読みが動詞の活用形によって変わるような場合なのですが、上手く行くときといかないときがありました。イテレーションを進めていく上で、ロスは下がり続ける一方で、きちんと区別して発音できるようになったりできなくなってしまったり、というのを繰り返していました。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;?&lt;/code&gt; が文末につくことで、イントネーションが変わってくれることを期待しましたが、データセット中に &lt;code&gt;?&lt;/code&gt; が少なすぎたのか、あまりうまくいかなかったように思います。&lt;/li&gt;
&lt;li&gt;out-of-domainの文章にもロバストのように思いましたが、二個目の例文のような、（複雑な？）専門用語の発音は、厳しい感じがしました。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Generative adversarial network or variational auto-encoder.&lt;/p&gt;
&lt;p&gt;(59 chars, 7 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/0_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/0_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;Basilar membrane and otolaryngology are not auto-correlations.&lt;/p&gt;
&lt;p&gt;(62 chars, 8 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/1_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/1_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;He has read the whole thing.&lt;/p&gt;
&lt;p&gt;(28 chars, 7 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/2_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/2_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;He reads books.&lt;/p&gt;
&lt;p&gt;(15 chars, 4 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/3_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/3_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;Thisss isrealy awhsome.&lt;/p&gt;
&lt;p&gt;(23 chars, 4 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/4_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/4_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;This is your personal assistant, Google Home.&lt;/p&gt;
&lt;p&gt;(45 chars, 9 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/5_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/5_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;This is your personal assistant Google Home.&lt;/p&gt;
&lt;p&gt;(44 chars, 8 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/6_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/6_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;The quick brown fox jumps over the lazy dog.&lt;/p&gt;
&lt;p&gt;(44 chars, 10 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/7_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/7_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;Does the quick brown fox jump over the lazy dog?&lt;/p&gt;
&lt;p&gt;(51 chars, 11 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/8_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/8_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;h3 id=&#34;keithitotacotron-との比較&#34;&gt;keithito/tacotron との比較&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://keithito.github.io/audio-samples/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://keithito.github.io/audio-samples/&lt;/a&gt; の audio samples で使われている文章に対するテストです。比較しやすいように、比較対象の音声も合わせて貼っておきます。自前実装で生成したもの、&lt;a href=&#34;https://github.com/keithito/tacotron&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron&lt;/a&gt; で生成したもの、の順です。&lt;/p&gt;
&lt;p&gt;Scientists at the CERN laboratory say they have discovered a new particle.&lt;/p&gt;
&lt;p&gt;(74 chars, 13 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/3_keithito/0_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/keithito/eval-877000-0.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/3_keithito/0_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;There&amp;rsquo;s a way to measure the acute emotional intelligence that has never gone out of style.&lt;/p&gt;
&lt;p&gt;(91 chars, 18 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/3_keithito/1_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/keithito/eval-877000-1.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/3_keithito/1_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;President Trump met with other leaders at the Group of 20 conference.&lt;/p&gt;
&lt;p&gt;(69 chars, 13 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/3_keithito/2_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/keithito/eval-877000-2.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/3_keithito/2_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;The Senate&amp;rsquo;s bill to repeal and replace the Affordable Care Act is now imperiled.&lt;/p&gt;
&lt;p&gt;(81 chars, 16 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/3_keithito/3_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/keithito/eval-877000-3.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/3_keithito/3_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;Generative adversarial network or variational auto-encoder.&lt;/p&gt;
&lt;p&gt;(59 chars, 7 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/3_keithito/4_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/keithito/eval-877000-4.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/3_keithito/4_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;The buses aren&amp;rsquo;t the problem, they actually provide a solution.&lt;/p&gt;
&lt;p&gt;(63 chars, 13 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/3_keithito/5_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/keithito/eval-877000-5.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/3_keithito/5_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;h3 id=&#34;ground-truth-との比較&#34;&gt;Ground truth との比較&lt;/h3&gt;
&lt;p&gt;最後に、元のデータセットとの比較です。学習データからサンプルを取ってきて比較します。自前実装で生成したもの、ground truthの順に貼ります。&lt;/p&gt;
&lt;p&gt;Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition.&lt;/p&gt;
&lt;p&gt;(152 chars, 30 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/5_ljspeech_sample/0_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/lj/LJ001-0001.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/5_ljspeech_sample/0_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;in being comparatively modern.&lt;/p&gt;
&lt;p&gt;(30 chars, 5 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/5_ljspeech_sample/1_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/lj/LJ001-0002.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/5_ljspeech_sample/1_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;For although the Chinese took impressions from wood blocks engraved in relief for centuries before the woodcutters of the Netherlands, by a similar process.&lt;/p&gt;
&lt;p&gt;(156 chars, 26 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/5_ljspeech_sample/2_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/lj/LJ001-0003.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/5_ljspeech_sample/2_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;produced the block books, which were the immediate predecessors of the true printed book,&lt;/p&gt;
&lt;p&gt;(89 chars, 16 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/5_ljspeech_sample/3_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/lj/LJ001-0004.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/5_ljspeech_sample/3_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;the invention of movable metal letters in the middle of the fifteenth century may justly be considered as the invention of the art of printing.&lt;/p&gt;
&lt;p&gt;(143 chars, 26 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/5_ljspeech_sample/4_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/lj/LJ001-0005.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/5_ljspeech_sample/4_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;元音声があまり良いクリーンな音声ではないとはいえ、まー元音声とは大きな違いがありますねー、、厳しいです。スペクトログラムを見ている限りでは（貼ってないですが、すいません）、明らかに高周波数成分の予測が上手く言っていないことはわかっています。ナイーブなアイデアではありますが、GANを導入すると良くなるのではないかと思っています。&lt;/p&gt;
&lt;h3 id=&#34;おまけ生成する度に変わる音声&#34;&gt;おまけ：生成する度に変わる音声&lt;/h3&gt;
&lt;p&gt;実験する過程で副次的に得られた結果ではあるのですが、テスト時に一部dropoutを有効にしていると&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;、生成する度に音声が異なる（韻律が微妙に変わる）、といった現象を経験しています。以下、前に検証した際の実験ノートのリンクを貼っておきます。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://nbviewer.jupyter.org/gist/r9y9/fe1945b73cd5b98e97c61410fe26a851#Try-same-input-multiple-times&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://nbviewer.jupyter.org/gist/r9y9/fe1945b73cd5b98e97c61410fe26a851#Try-same-input-multiple-times&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;まとめ--感想など&#34;&gt;まとめ &amp;amp; 感想など&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Tacotronを実装しました &lt;a href=&#34;https://github.com/r9y9/tacotron_pytorch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/tacotron_pytorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;24時間のデータセットに対して、20万ステップ程度（数日くらい）学習させたらそれなりにまともな音声が生成できるようになりました。70万ステップ（一週間と少しくらい）学習させましたが、ずっとロスは下がり続ける一方で、50万くらいからはあまり大きな品質改善は見られなかったように思います。&lt;/li&gt;
&lt;li&gt;Googleの論文と（ほぼ&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;）同じように実装したつもりですが、品質はそこまで高くならなかったように思います。End-to-end では、 &lt;strong&gt;データの量と品質&lt;/strong&gt; がかなり重要なので、それが主な原因だと思っています。（僕の実装に、多少バグがあるかもしれませんが、、、&lt;/li&gt;
&lt;li&gt;EOS (End-of-sentence) では、理想的には要素がすべて0のスペクトログラムが出力されるはずなのですが、実際にはやはりそうもいかないので、判定には以下のようなしきい値処理を用いました。ここで貼った音声は全部この仕組みで動いており、単純ですがそれなりに上手く機能しているようです。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;def is_end_of_frames(output, eps=0.2):
    return (output.data &amp;lt;= eps).all()
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;論文からは非自明な点の一つとして、エンコーダの出力のうち、入力のゼロ詰めした部分をマスキングするかどうか、といった点があります。これは、既存実装によってもまちまちで、例えば &lt;a href=&#34;https://github.com/keithito/tacotron&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron&lt;/a&gt; ではマスキングしていませんが、&lt;a href=&#34;https://github.com/barronalex/Tacotron/blob/2de9e507456cbe2b680cbc6b2beb6a761bd2eebd/models/tacotron.py#L51&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;barronalex/Tacotron&lt;/a&gt; ではマスクしています。僕はマスクする場合としない場合と両方試したのですが（ここに貼った結果は、マスクしていない場合のものです）、マスクしないほうが若干良くなったような気もします。理想的にはマスクするべきだと思ったのですが、実際に試したところどちらかが圧倒的に悪いという結果ではありませんでした。発見した大きな違いの一つは、マスクなしの場合はアテンションは大まかにmonotonicになる一方で、マスクありの場合は、無音区間ではエンコーダ出力の冒頭にアテンションの重みが大きくなる（ので、monotonicではない）、と言ったことがありました。マスクありの音声サンプル、アライメントの可視化は、（少し古いですが）&lt;a href=&#34;http://nbviewer.jupyter.org/github/r9y9/tacotron_pytorch/blob/bdad19fdff22016c7457a979707655bb7a605cd8/notebooks/Test%20Tacotron.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ここ&lt;/a&gt; にあります。参考までに、Tensorflowでエンコーダの出力マスクする場合は、&lt;code&gt;memory_sequence_length&lt;/code&gt; を指定します &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BahdanauAttention&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BahdanauAttention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;日本語でやったり、multi-speaker でやったりしたかったのですが、とにかく実験に時間がかかるので、今のところ僕の中では優先度が低めになってしまいました。時間と計算資源に余裕があれば、やりたいのですが…&lt;/li&gt;
&lt;li&gt;日本語でやるには、英語と同じようにはいきません。というのも、char-levelで考えた際に、語彙が大きすぎるので。やるならば、十分大きな日本語テキストコーパスからembeddingを別途学習して（Tacotronでは、モデル自体にembeddingが入っています）、その他の部分を音声つきコーパスで学習する、といった方法が良いかなと思います。CSJコーパスは結構向いているんじゃないかと思っています。&lt;/li&gt;
&lt;li&gt;multi-speakerモデルを考える場合、どこにembeddingを差し込むのか、といったことが重要になってきますが、&lt;a href=&#34;https://github.com/keithito/tacotron/issues/18&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron/issues/18&lt;/a&gt; や &lt;a href=&#34;https://github.com/keithito/tacotron/issues/24&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron/issues/24&lt;/a&gt; に少し議論があるので、興味のある人は見てみるとよいかもしれません。DeepVoiceの論文も参考になるかと思います&lt;/li&gt;
&lt;li&gt;最新のTensorFlowでは、griffin lim や stft（GPUで走る、勾配が求められる）が実装されているので、tacotronモデルを少し拡張して、サンプルレベルでロスを考える、といったことが簡単に試せると思います（ある意味WaveNetです）。ただし、ものすごく計算リソースを必要とするのが容易に想像がつくので、僕はやっていません。GPU落ちてこないかな、、、&lt;/li&gt;
&lt;li&gt;Tacotronの拡張として、speaker embedding以外にも、いろんな潜在変数を埋め込んでみると、楽しそうに思いました。例えば話速、感情とか。&lt;/li&gt;
&lt;li&gt;TensorFlowのseq2seqあたりのドキュメント/コードをよく読んでいたのですが、APIが抽象化されすぎていてつらいなと思いました。例えばAttentionWrapper、コードを読まずに挙動を理解するのは無理なのではと思いました &lt;a href=&#34;https://github.com/r9y9/tacotron_pytorch/issues/2#issuecomment-334255759&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/tacotron_pytorch/issues/2#issuecomment-334255759&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/keithito/tacotron&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron&lt;/a&gt; は本当によく書かれているなと思ったので、TensorFlowに長けている方には、おすすめです&lt;/li&gt;
&lt;li&gt;僕の実装では、バッチサイズ32でGPUメモリ5GB程度しか食わないので、Tacotronは比較的軽いモデルなのだなーと思いました。物体検出で有名な single shot multibox detector (通称SSD) なんかは、バッチサイズ16とかでも平気で12GBとか使ってくるので（一年近く前の経験ですが）、無限にGPUリソースがほしくなってきます&lt;/li&gt;
&lt;li&gt;これが僕にとって、はじめてまともにseq2seqを実装した経験でした。色々勉強したのですが、Attention mechanism に関しては、 &lt;a href=&#34;http://colinraffel.com/blog/online-and-linear-time-attention-by-enforcing-monotonic-alignments.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://colinraffel.com/blog/online-and-linear-time-attention-by-enforcing-monotonic-alignments.html&lt;/a&gt; がとても参考になりました。あとで知ったのですが、monotonic attentionの著者は僕が昔から使っている音楽信号処理のライブラリ &lt;a href=&#34;https://github.com/librosa/librosa&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;librosa&lt;/a&gt; のコミッタでした（僕も弱小コミッタの一人）。とても便利で、よくテストされているので、おすすめです。オープンソースのTacotron実装でも、音声処理にも使われています&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;
&lt;p&gt;End-to-End 音声合成は、言語処理のフロントエンドを（最低限の前処理を除き）必要としないという素晴らしさがあります。SampleRNN、Char2wavと他にも色々ありますが、今後もっと発展していくのではないかと思っています。おしまい。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1703.10135&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tacotron: Towards End-to-End Speech Synthesis / arXiv:1703.10135&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;URLには現時点のgitのコミットハッシュが入っています。最新版は、 &lt;a href=&#34;https://github.com/r9y9/tacotron_pytorch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/tacotron_pytorch&lt;/a&gt; から直接辿ってください。&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/keithito/tacotron/pull/43#issuecomment-332068107&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/keithito/tacotron/pull/43#issuecomment-332068107&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;dropoutを切ってしまうと、アライメントが死んでしまうというバグ？に苦しんでおり…未だ原因を突き止められていません&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;たとえばロスはちょっと違って、高周波数帯域に比べて低周波数帯域の重みを少し大きくしていたりしています。これは既存のtf実装に従いました。&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>GAN 日本語音声合成 [arXiv:1709.08041]</title>
      <link>https://r9y9.github.io/blog/2017/10/10/gantts-jp/</link>
      <pubDate>Tue, 10 Oct 2017 11:45:32 +0900</pubDate>
      <guid>https://r9y9.github.io/blog/2017/10/10/gantts-jp/</guid>
      <description>&lt;p&gt;&lt;strong&gt;10/11 追記&lt;/strong&gt;: IEEE TASLPのペーパー (Open access) が公開されたようなので、リンクを貼っておきます: &lt;a href=&#34;https://ieeexplore.ieee.org/document/8063435/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ieeexplore.ieee.org/document/8063435/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;arXiv論文リンク: &lt;a href=&#34;https://arxiv.org/abs/1709.08041&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1709.08041&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/10/09/gantts/&#34;&gt;前回の記事&lt;/a&gt; の続きです。これでこのシリーズは終わりの予定です。&lt;/p&gt;
&lt;p&gt;前回は英語音声合成でしたが、以前書いた &lt;a href=&#34;https://r9y9.github.io/blog/2017/08/16/japanese-dnn-tts/&#34;&gt;DNN日本語音声合成の記事&lt;/a&gt; で使ったデータと同じものを使い、日本語音声合成をやってみましたので、結果を残しておきます。&lt;/p&gt;
&lt;h2 id=&#34;実験&#34;&gt;実験&lt;/h2&gt;
&lt;h3 id=&#34;実験条件&#34;&gt;実験条件&lt;/h3&gt;
&lt;p&gt;HTSのNIT-ATR503のデモデータ (&lt;a href=&#34;https://github.com/r9y9/nnmnkwii_gallery/blob/4899437e22528399ca50c34097a2db2bed782f8b/data/NIT-ATR503_COPYING&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ライセンス&lt;/a&gt;) から、wavデータ503発話を用います。442を学習用、56を評価用、残り5をテスト用にします（※英語音声とtrain/evalの比率は同じです）。継続長モデルは、state-levelではなくphone-levelです。サンプリング周波数が48kHzなので、mgcの次元を25から60に増やしました。モデル構造は、すべて英語音声合成の場合と同じです。ADV loss は0次を除くmgcを用いて計算しました。F0は入れていません。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/gantts&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gantts&lt;/a&gt; の &lt;a href=&#34;https://github.com/r9y9/gantts/tree/jp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;jpブランチ&lt;/a&gt; をチェックアウトして、以下のシェルを実行すると、ここに貼った結果が得られます。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; ./jp_tts_demo.sh jp_tts_order59
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ただし、シェル中に、&lt;code&gt;HTS_ROOT&lt;/code&gt; という変数があり、シェル実行前に、環境に合わせてディレクトリを指定する必要があります。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;diff --git a/jp_tts_demo.sh b/jp_tts_demo.sh
index 7a8f12c..b18e604 100755
--- a/jp_tts_demo.sh
+++ b/jp_tts_demo.sh
@@ -8,7 +8,7 @@ experiment_id=$1
 fs=48000

 # Needs adjastment
-HTS_DEMO_ROOT=~/local/HTS-demo_NIT-ATR503-M001
+HTS_DEMO_ROOT=HTS日本語デモの場所を指定してください

 # Flags
 run_duration_training=1
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;変換音声の比較&#34;&gt;変換音声の比較&lt;/h3&gt;
&lt;h4 id=&#34;音響モデルのみ適用&#34;&gt;音響モデルのみ適用&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;自然音声&lt;/li&gt;
&lt;li&gt;ベースライン&lt;/li&gt;
&lt;li&gt;GAN&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;の順に音声を貼ります。聴きやすいように、soxで音量を正規化しています。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;nitech_jp_atr503_m001_j49&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nit-atr503/nitech_jp_atr503_m001_j49.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/acoustic_only/baseline/test/nitech_jp_atr503_m001_j49.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/acoustic_only/gan/test/nitech_jp_atr503_m001_j49.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;nitech_jp_atr503_m001_j50&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nit-atr503/nitech_jp_atr503_m001_j50.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/acoustic_only/baseline/test/nitech_jp_atr503_m001_j50.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/acoustic_only/gan/test/nitech_jp_atr503_m001_j50.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;nitech_jp_atr503_m001_j51&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nit-atr503/nitech_jp_atr503_m001_j51.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/acoustic_only/baseline/test/nitech_jp_atr503_m001_j51.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/acoustic_only/gan/test/nitech_jp_atr503_m001_j51.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;nitech_jp_atr503_m001_j52&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nit-atr503/nitech_jp_atr503_m001_j52.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/acoustic_only/baseline/test/nitech_jp_atr503_m001_j52.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/acoustic_only/gan/test/nitech_jp_atr503_m001_j52.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;nitech_jp_atr503_m001_j53&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nit-atr503/nitech_jp_atr503_m001_j53.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/acoustic_only/baseline/test/nitech_jp_atr503_m001_j53.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/acoustic_only/gan/test/nitech_jp_atr503_m001_j53.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;h4 id=&#34;音響モデル継続長モデルを適用&#34;&gt;音響モデル＋継続長モデルを適用&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;nitech_jp_atr503_m001_j49&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nit-atr503/nitech_jp_atr503_m001_j49.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/duration_acoustic/baseline/test/nitech_jp_atr503_m001_j49.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/duration_acoustic/gan/test/nitech_jp_atr503_m001_j49.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;nitech_jp_atr503_m001_j50&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nit-atr503/nitech_jp_atr503_m001_j50.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/duration_acoustic/baseline/test/nitech_jp_atr503_m001_j50.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/duration_acoustic/gan/test/nitech_jp_atr503_m001_j50.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;nitech_jp_atr503_m001_j51&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nit-atr503/nitech_jp_atr503_m001_j51.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/duration_acoustic/baseline/test/nitech_jp_atr503_m001_j51.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/duration_acoustic/gan/test/nitech_jp_atr503_m001_j51.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;nitech_jp_atr503_m001_j52&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nit-atr503/nitech_jp_atr503_m001_j52.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/duration_acoustic/baseline/test/nitech_jp_atr503_m001_j52.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/duration_acoustic/gan/test/nitech_jp_atr503_m001_j52.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;nitech_jp_atr503_m001_j53&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nit-atr503/nitech_jp_atr503_m001_j53.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/duration_acoustic/baseline/test/nitech_jp_atr503_m001_j53.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/duration_acoustic/gan/test/nitech_jp_atr503_m001_j53.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;どうでしょうか。ちょっと早口になってしまっている箇所もありますが、全体的には明瞭性が上がって、品質が改善されたような感じがします。若干ノイジーな感じは、音響モデルにRNNを使えば改善されるのですが、今回は計算リソースの都合上、Feed-forward型のサンプルとなっています。&lt;/p&gt;
&lt;h3 id=&#34;gv&#34;&gt;GV&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;nitech_jp_atr503_m001_j49&lt;/code&gt; に対して計算した結果です。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/jp-gantts/nitech_jp_atr503_m001_j49_gv.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;英語音声合成の実験でも確認しているのですが、mgcの次元を大きく取ると、高次元でGVが若干落ちる傾向にあります。ただし、&lt;a href=&#34;https://twitter.com/r9y9/status/915213687891169280&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;一週間前の僕のツイート&lt;/a&gt; によると、なぜかそんなこともなく（当時ばりばりのプロトタイピングの時期だったので、コードが残っておらず、いまは再現できないという、、すいません）、僕が何かミスをしている可能性もあります。ただ、品質はそんなに悪くないように思います。&lt;/p&gt;
&lt;h3 id=&#34;変調スペクトル&#34;&gt;変調スペクトル&lt;/h3&gt;
&lt;p&gt;評価用セットで平均を取ったものです。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/jp-gantts/ms.png&#34; /&gt;&lt;/div&gt;
&lt;h3 id=&#34;特徴量の分布&#34;&gt;特徴量の分布&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;nitech_jp_atr503_m001_j49&lt;/code&gt; に対して計算した結果です。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/jp-gantts/nitech_jp_atr503_m001_j49_scatter.png&#34; /&gt;&lt;/div&gt;
&lt;h3 id=&#34;おまけ-htsデモと聴き比べ&#34;&gt;おまけ: HTSデモと聴き比べ&lt;/h3&gt;
&lt;p&gt;HTSデモを実行すると生成されるサンプルとの聴き比べです。注意事項ですが、&lt;strong&gt;実験条件がまったく異なります&lt;/strong&gt;。あくまで参考程度にどうぞ。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;HTSデモ&lt;/li&gt;
&lt;li&gt;ベースライン&lt;/li&gt;
&lt;li&gt;GAN&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;の順に音声を貼ります。&lt;/p&gt;
&lt;p&gt;1 こんにちは&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/hts_nit_atr503_2mix/phrase01.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/test/baseline/phrase01.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/test/gan/phrase01.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;2 それではさようなら&lt;/p&gt;
&lt;p&gt;HTS&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/hts_nit_atr503_2mix/phrase02.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/test/baseline/phrase02.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/test/gan/phrase02.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;3 はじめまして&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/hts_nit_atr503_2mix/phrase03.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/test/baseline/phrase03.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/test/gan/phrase03.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;4 ようこそ名古屋工業大学へ&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/hts_nit_atr503_2mix/phrase04.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/test/baseline/phrase04.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/test/gan/phrase04.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;5 今夜の名古屋の天気は雨です&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/hts_nit_atr503_2mix/phrase05.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/test/baseline/phrase05.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/test/gan/phrase05.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;
&lt;p&gt;アイデアはシンプル、効果は素晴らしいという、僕の好きな（試したくなる）研究の紹介でした。ありがとうございました。&lt;/p&gt;
&lt;p&gt;GANシリーズのその他記事へのリンクは以下の通りです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/10/05/ganvc/&#34;&gt;GAN 声質変換 (en) 編はこちら&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/10/09/gantts/&#34;&gt;GAN 音声合成 (en) 編はこちら&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;追記: 図を作るのに使ったノートブックは &lt;a href=&#34;http://nbviewer.jupyter.org/gist/r9y9/185a56417cee27d9f785b8caf1c9f5ec&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;こちら&lt;/a&gt; においておきました。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1709.08041&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yuki Saito, Shinnosuke Takamichi, Hiroshi Saruwatari, &amp;ldquo;Statistical Parametric Speech Synthesis Incorporating Generative Adversarial Networks&amp;rdquo;, arXiv:1709.08041 [cs.SD], Sep. 2017&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>【音声合成編】Statistical Parametric Speech Synthesis Incorporating Generative Adversarial Networks [arXiv:1709.08041]</title>
      <link>https://r9y9.github.io/blog/2017/10/09/gantts/</link>
      <pubDate>Mon, 09 Oct 2017 02:00:00 +0900</pubDate>
      <guid>https://r9y9.github.io/blog/2017/10/09/gantts/</guid>
      <description>&lt;p&gt;&lt;strong&gt;10/11 追記&lt;/strong&gt;: IEEE TASLPのペーパー (Open access) が公開されたようなので、リンクを貼っておきます: &lt;a href=&#34;https://ieeexplore.ieee.org/document/8063435/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ieeexplore.ieee.org/document/8063435/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;arXiv論文リンク: &lt;a href=&#34;https://arxiv.org/abs/1709.08041&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1709.08041&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/10/05/ganvc/&#34;&gt;前回の記事&lt;/a&gt; の続きです。音響モデルの学習にGANを使うというアイデアは、声質変換だけでなく音声合成にも応用できます。&lt;a href=&#34;http://festvox.org/cmu_arctic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CMU ARCTIC&lt;/a&gt; を使った英語音声合成の実験を行って、ある程度良い結果がでたので、まとめようと思います。音声サンプルだけ聴きたい方は真ん中の方まで読み飛ばしてください。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;コードはこちら: &lt;a href=&#34;https:github.com/r9y9/gantts&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/gantts | PyTorch implementation of GAN-based text-to-speech and voice conversion (VC) &lt;/a&gt; (VCのコードも一緒です)&lt;/li&gt;
&lt;li&gt;音声サンプル付きデモノートブックはこちら: &lt;a href=&#34;http://nbviewer.jupyter.org/github/r9y9/gantts/blob/master/notebooks/Test%20TTS.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The effects of adversarial training in text-to-speech synthesis | nbviewer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;前回の記事でも書いた注意書きですが、厳密に同じ結果を再現しようとは思っていません。同様のアイデアを試す、といったことに主眼を置いています。&lt;/p&gt;
&lt;h2 id=&#34;実験&#34;&gt;実験&lt;/h2&gt;
&lt;h3 id=&#34;実験条件&#34;&gt;実験条件&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://festvox.org/cmu_arctic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CMU ARCTIC&lt;/a&gt; から、話者 &lt;code&gt;slt&lt;/code&gt; のwavデータそれぞれ1131発話すべてを用います。
&lt;a href=&#34;https://github.com/CSTR-Edinburgh/merlin&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Merlin&lt;/a&gt;  の slt デモの条件と同様に、1000を学習用、126を評価用、残り5をテスト用にします。継続長モデル（state-level）には &lt;strong&gt;Bidirectional-LSTM RNN&lt;/strong&gt; を、音響モデルには &lt;strong&gt;Feed-forward型&lt;/strong&gt; のニューラルネットを使用しました&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;。継続長モデル、音響モデルの両方にGANを取り入れました。論文の肝である &lt;strong&gt;ADV loss&lt;/strong&gt; についてですが、mgcのみ（0次は除く）を使って計算するパターンと、mgc + lf0で計算するパターンとで比較しました&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;p&gt;実験の結果 (ADV loss: mgcのみ) は、 &lt;a href=&#34;https://github.com/r9y9/gantts/tree/a5ec247ba7ee1a160875661f8899f56f8010be5b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a5ec247&lt;/a&gt; をチェックアウトして、下記のシェルを実行すると再現できます。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;./tts_demo.sh tts_test
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;データのダウンロード、特徴抽出、モデル学習、音声サンプル合成まで一通り行われます。&lt;code&gt;tts_test&lt;/code&gt; の部分は何でもよいです。tensorboard用に吐くログイベント名、モデル出力先、音声サンプル出力先の決定に使われます。詳細はコードを参照ください。 (ADV loss: mgc + lf0) の結果は、&lt;a href=&#34;https://github.com/r9y9/gantts/blob/a5ec247ba7ee1a160875661f8899f56f8010be5b/hparams.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ハイパーパラメータ&lt;/a&gt;を下記のように変更してシェルを実行すると再現できます。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;diff --git a/hparams.py b/hparams.py
index d82296c..e73dc57 100644
--- a/hparams.py
+++ b/hparams.py
@@ -175,7 +175,7 @@ tts_acoustic = tf.contrib.training.HParams(
     # Streams used for computing adversarial loss
     # NOTE: you should probably change discriminator&#39;s `in_dim`
     # if you change the adv_streams
-    adversarial_streams=[True, False, False, False],
+    adversarial_streams=[True, True, False, False],
     # Don&#39;t switch this on unless you are sure what you are doing
     # If True, you will need to adjast `in_dim` for discriminator.
     # Rationale for this is that power coefficients are less meaningful
@@ -202,7 +202,7 @@ tts_acoustic = tf.contrib.training.HParams(
     # Discriminator
     discriminator=&amp;quot;MLP&amp;quot;,
     discriminator_params={
-        &amp;quot;in_dim&amp;quot;: 24,
+        &amp;quot;in_dim&amp;quot;: 25,
         &amp;quot;out_dim&amp;quot;: 1,
         &amp;quot;num_hidden&amp;quot;: 2,
         &amp;quot;hidden_dim&amp;quot;: 256,
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;変換音声の比較&#34;&gt;変換音声の比較&lt;/h3&gt;
&lt;h4 id=&#34;音響モデルのみ適用-adv-loss-mgcのみ&#34;&gt;音響モデルのみ適用 (ADV loss: mgcのみ)&lt;/h4&gt;
&lt;p&gt;継続長モデルを適用しない、かつ ADV lossにmgcのみを用いる場合です。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;自然音声&lt;/li&gt;
&lt;li&gt;ベースライン&lt;/li&gt;
&lt;li&gt;GAN&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;の順に音声を貼ります。聴きやすいように、soxで音量を正規化しています。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;arctic_b0535&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/slt/arctic_b0535.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/acoustic_only/baseline/test/arctic_b0535.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/acoustic_only/gan/test/arctic_b0535.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;arctic_b0536&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/slt/arctic_b0536.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/acoustic_only/baseline/test/arctic_b0536.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/acoustic_only/gan/test/arctic_b0536.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;arctic_b0537&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/slt/arctic_b0537.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/acoustic_only/baseline/test/arctic_b0537.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/acoustic_only/gan/test/arctic_b0537.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;arctic_b0538&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/slt/arctic_b0538.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/acoustic_only/baseline/test/arctic_b0538.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/acoustic_only/gan/test/arctic_b0538.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;arctic_b0539&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/slt/arctic_b0539.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/acoustic_only/baseline/test/arctic_b0539.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/acoustic_only/gan/test/arctic_b0539.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;VCの場合と同じように、音声の明瞭性が上がったように思います。&lt;/p&gt;
&lt;h4 id=&#34;音響モデル継続長モデルを適用-adv-loss-mgcのみ&#34;&gt;音響モデル＋継続長モデルを適用 (ADV loss: mgcのみ)&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;arctic_b0535&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/slt/arctic_b0535.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/duration_acoustic/baseline/test/arctic_b0535.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/duration_acoustic/gan/test/arctic_b0535.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;arctic_b0536&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/slt/arctic_b0536.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/duration_acoustic/baseline/test/arctic_b0536.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/duration_acoustic/gan/test/arctic_b0536.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;arctic_b0537&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/slt/arctic_b0537.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/duration_acoustic/baseline/test/arctic_b0537.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/duration_acoustic/gan/test/arctic_b0537.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;arctic_b0538&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/slt/arctic_b0538.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/duration_acoustic/baseline/test/arctic_b0538.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/duration_acoustic/gan/test/arctic_b0538.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;arctic_b0539&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/slt/arctic_b0539.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/duration_acoustic/baseline/test/arctic_b0539.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/duration_acoustic/gan/test/arctic_b0539.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;音声の明瞭性が上がっているとは思いますが、継続長に関しては、ベースライン/GANで差異がほとんどないように感じられると思います。これは、（僕が実験した範囲では少なくとも）DiscriminatorがGeneartorに勝ちやすくて (音響モデルの場合は、そんなことはない)、 ADV lossが下がるどころか上がってしまい、結果 MGE lossを最小化する場合とほとんど変わっていない、という結果になっています。論文に記載の内容とは異なり、state-levelの継続長モデルではあるものの、ハイパーパラメータなどなどいろいろ変えて試したのですが、上手くいきませんでした。&lt;/p&gt;
&lt;h4 id=&#34;adv-loss-mgc-vs-mgc--lf0&#34;&gt;ADV loss: mgc vs mgc + lf0&lt;/h4&gt;
&lt;p&gt;次に、ロスの比較です。F0の変化に着目しやすいように、継続長モデルを使わず、音響モデルのみを適用します。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;自然音声&lt;/li&gt;
&lt;li&gt;ADV loss (mgcのみ, 24次元)&lt;/li&gt;
&lt;li&gt;ADV loss (mgc + lf0, 25次元)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;の順に音声を貼ります。また、WORLD (dio + stonemask) で分析したF0の可視化結果も併せて貼っておきます。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;arctic_b0535&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/slt/arctic_b0535.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/acoustic_only/gan/test/arctic_b0535.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24_advf0/acoustic_only/gan/test/arctic_b0535.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/gantts/arctic_b0535_f0.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;arctic_b0536&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/slt/arctic_b0536.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/acoustic_only/gan/test/arctic_b0536.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24_advf0/acoustic_only/gan/test/arctic_b0536.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/gantts/arctic_b0536_f0.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;arctic_b0537&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/slt/arctic_b0537.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/acoustic_only/gan/test/arctic_b0537.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24_advf0/acoustic_only/gan/test/arctic_b0537.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/gantts/arctic_b0537_f0.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;arctic_b0538&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/slt/arctic_b0538.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/acoustic_only/gan/test/arctic_b0538.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24_advf0/acoustic_only/gan/test/arctic_b0538.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/gantts/arctic_b0538_f0.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;arctic_b0539&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/slt/arctic_b0539.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/acoustic_only/gan/test/arctic_b0539.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24_advf0/acoustic_only/gan/test/arctic_b0539.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/gantts/arctic_b0539_f0.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;どうでしょうか。上手くいっている場合も（arctic_b537とか）あれば、上手くいっていない場合 (arctic_b539とか) もあるように思います。僕にはF0が不自然に揺れているように感じ場合が多くありました。ここでは5つしか音声を貼っていませんが、その他126個の評価用音声でも聴き比べていると、ADV lossにF0を入れない方がよい気がしました（あくまで僕の主観ですが&lt;/p&gt;
&lt;p&gt;このあたりは、F0の抽出法、補間法に強く依存しそうです。今回は、F0抽出のパラメータをまったくチューニングしていないので、そのせいもあった（f0分析エラーに引っ張られて悪くなった）のかもしれません。&lt;/p&gt;
&lt;h3 id=&#34;global-variance-は補償されているのか&#34;&gt;Global variance は補償されているのか？&lt;/h3&gt;
&lt;p&gt;F0の話は終わりで、スペクトル特徴量に着目して結果を分析していきます。以下、ADV loss (mgcのみ)、継続長モデル＋音響モデルを適用したサンプルでの分析結果です。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/gantts/arctic_b0537_gv.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;大まかに、論文で示されているのと同様の結果が得られました。なお、これは &lt;code&gt;arctic_b0537&lt;/code&gt; の一発話に対して計算したもので、テストセットの平均ではありません（すいません）。また、これはテストセット中のサンプルの中で、GVが補償されていることがわかりやすい例をピックアップしました。ただし、他のテストサンプルにおいても同様の傾向が見られるのは確認しています。&lt;/p&gt;
&lt;h3 id=&#34;modulation-spectrum-変調スペクトル-は補償されているのか&#34;&gt;Modulation spectrum (変調スペクトル) は補償されているのか？&lt;/h3&gt;
&lt;p&gt;評価用の音声126発話それぞれで変調スペクトルを計算し、それらの平均を取り、適当な特徴量の次元をピックアップしたものを示します。横軸は変調周波数です。一番右端が50Hzです。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/gantts/ms.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;arctic_b0537&lt;/code&gt; の一発話に対して計算したものです。&lt;strong&gt;VCの場合とは異なり&lt;/strong&gt;、ベースライン、GANともに、低次元であっても10Hzを越えた辺りから自然音声とは大きく異っています。これはなぜなのか、僕にはまだわかっていません。また、VCの場合と同様に、高次元になるほど、GANベースの方が変調スペクトルは自然音声に近いこともわかります。GANによって、変調スペクトルはある程度補償されていると言えると思います。&lt;/p&gt;
&lt;h3 id=&#34;特徴量の分布&#34;&gt;特徴量の分布&lt;/h3&gt;
 &lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/gantts/arctic_b0537_scatter.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;arctic_b0537&lt;/code&gt; の一発話に対して計算したものです。論文で示されているほど顕著ではない気がしますが、おおまかに同様の結果が得られました。&lt;/p&gt;
&lt;h2 id=&#34;感想&#34;&gt;感想&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;GANのチューニングは難しい。人力（直感）ハイパーパラメータのチューニングを試しましたが、大変でした。そしてあまりうまくできた自信がありません。潤沢な計算資源でなんとかしたい…&lt;/li&gt;
&lt;li&gt;GANの学習は不安定（に感じる）が、通常の MSE loss の学習は安定で、かつBidirectional LSTM RNNは安定してよいです（結果をここに貼っていなくて申し訳ですが）。ただし、計算にものすごく時間がかかるのと、GPUメモリをかなり使うので、とりあえず通常のfeed forward型で実験した結果をまとめました&lt;/li&gt;
&lt;li&gt;state-levelの継続長モデルに、GANを使うのはあまり上手くできませんでした。ここに貼ったサンプルからはわからないのですが（すいません）、GとDが上手く競い合わず、Dが勝ってしまう場合がほとんどでした（結果それが一番まし）。上手く競い合わせるようとすると、早口音声が生成されてしまったり、と失敗がありました。&lt;/li&gt;
&lt;li&gt;F0を ADV lossに加えると、より自然音声に近づくと感じる場合もあるが、一方でF0が不自然に揺れてしまう場合もありました。これはF0の抽出法、補間法にも依存するので、調査が必要です&lt;/li&gt;
&lt;li&gt;mgc, lf0, vuv, bapすべてで ADV lossに加えると、残念な結果を見ることになりました。理想的にはこれでも上手くいくと思って最初に試したのですが、だめでした。興味のある人はためしてみてください&lt;/li&gt;
&lt;li&gt;mgcの0次（パワー成分）は、ADV lossに加えない方がよい。考えてみると、特にフレーム単位のモデルの場合（RNNではなく）、パワー情報はnatural/generated の識別にはほとんど寄与しなさそうです。これはArxivの方の論文には書いていないのですが（僕の見逃しでなければ）、&lt;a href=&#34;http://sython.org/papers/ASJ/saito2017asja.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ASJの原稿&lt;/a&gt; には書いてあるんですよね。一つのハマりどころでした&lt;/li&gt;
&lt;li&gt;DにRNNを使った実験も少しやってみたのですが、うまく競い合わせるのが難しそうでした。DにRNNを使うのは本質的には良いと思っているので、この辺りはもう少し色々試行錯誤したいと思っています&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;
&lt;p&gt;GANの学習は大変でしたが、上手く学習できれば品質向上につながることを確認できました。今後、計算リソースが空き次第、RNNでの実験も進めようと思うのと、日本語でやってみようと思っています。&lt;/p&gt;
&lt;p&gt;GANシリーズのその他記事へのリンクは以下の通りです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/10/05/ganvc/&#34;&gt;GAN 声質変換 (en) 編はこちら&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/10/10/gantts-jp/&#34;&gt;GAN 音声合成 (ja) 編はこちら&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;p&gt;Arxivにあるペーパーだけでなく、その他いろいろ参考にしました。ありがとうございます。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1709.08041&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yuki Saito, Shinnosuke Takamichi, Hiroshi Saruwatari, &amp;ldquo;Statistical Parametric Speech Synthesis Incorporating Generative Adversarial Networks&amp;rdquo;, arXiv:1709.08041 [cs.SD], Sep. 2017&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sython.org/papers/SIG-SLP/saito201702slp.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yuki Saito, Shinnosuke Takamichi, and Hiroshi Saruwatari, &amp;ldquo;Training algorithm to deceive anti-spoofing verification for DNN-based text-to-speech synthesis,&amp;rdquo; IPSJ SIG Technical Report, 2017-SLP-115, no. 1, pp. 1-6, Feb., 2017. (in Japanese)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jstage.jst.go.jp/article/transinf/E100.D/8/E100.D_2017EDL8034/_article&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yuki Saito, Shinnosuke Takamichi, and Hiroshi Saruwatari, &amp;ldquo;Voice conversion using input-to-output highway networks,&amp;rdquo; IEICE Transactions on Information and Systems, Vol.E100-D, No.8, pp.1925&amp;ndash;1928, Aug. 2017&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.slideshare.net/ShinnosukeTakamichi/dnnantispoofing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.slideshare.net/ShinnosukeTakamichi/dnnantispoofing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.slideshare.net/YukiSaito8/Saito2017icassp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.slideshare.net/YukiSaito8/Saito2017icassp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/SythonUK/whisperVC&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/SythonUK/whisperVC&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sython.org/papers/ASJ/saito2017asja.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yuki Saito, Shinnosuke Takamichi, and Hiroshi Saruwatari, &amp;ldquo;Experimental investigation of divergences in adversarial DNN-based speech synthesis,&amp;rdquo; Proc. ASJ, Spring meeting, 1-8-7, &amp;ndash;, Sep., 2017. (in Japanese)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;継続長モデル、音響モデルともにRNNを使うと良くなることがわかっているのですが、計算リソースの都合上、今回は音響モデルはFeed-forwardにしました。Feed-forwardだと30分で終わる計算が、RNNだと数時間かかってしまうので…&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;他も色々やったのですが、だいたい失敗でした。&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>【声質変換編】Statistical Parametric Speech Synthesis Incorporating Generative Adversarial Networks [arXiv:1709.08041]</title>
      <link>https://r9y9.github.io/blog/2017/10/05/ganvc/</link>
      <pubDate>Thu, 05 Oct 2017 23:25:36 +0900</pubDate>
      <guid>https://r9y9.github.io/blog/2017/10/05/ganvc/</guid>
      <description>&lt;p&gt;&lt;strong&gt;10/11 追記&lt;/strong&gt;: IEEE TASLPのペーパー (Open access) が公開されたようなので、リンクを貼っておきます: &lt;a href=&#34;https://ieeexplore.ieee.org/document/8063435/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ieeexplore.ieee.org/document/8063435/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;arXiv論文リンク: &lt;a href=&#34;https://arxiv.org/abs/1709.08041&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1709.08041&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;2017年9月末に、表題の &lt;a href=&#34;https://arxiv.org/abs/1709.08041&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;論文&lt;/a&gt; が公開されたのと、&lt;a href=&#34;https://github.com/r9y9/nnmnkwii&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nnmnkwii&lt;/a&gt; という designed for easy and fast prototyping を目指すライブラリを作ったのもあるので、実装してみました。僕が実験した限りでは、声質変換 (Voice conversion; VC) では安定して良くなりました（音声合成ではまだ実験中です）。この記事では、声質変換について僕が実験した結果をまとめようと思います。音声合成については、また後日まとめます&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;コードはこちら: &lt;a href=&#34;https:github.com/r9y9/gantts&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/gantts | PyTorch implementation of GAN-based text-to-speech and voice conversion (VC) &lt;/a&gt; (TTSのコードも一緒です)&lt;/li&gt;
&lt;li&gt;音声サンプルを聴きたい方はこちら: &lt;a href=&#34;http://nbviewer.jupyter.org/github/r9y9/gantts/blob/master/notebooks/Test%20VC.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The effects of adversarial training in voice conversion | nbviewer&lt;/a&gt; (※解説はまったくありませんのであしからず)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;なお、厳密に同じ結果を再現しようとは思っていません。同様のアイデアを試す、といったことに主眼を置いています。コードに関しては、ここに貼った結果を再現できるように気をつけました。&lt;/p&gt;
&lt;h2 id=&#34;概要&#34;&gt;概要&lt;/h2&gt;
&lt;p&gt;一言でいえば、音響モデルの学習に Generative Adversarial Net (&lt;strong&gt;GAN&lt;/strong&gt;) を導入する、といったものです。少し具体的には、&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;音響モデル（生成モデル）が生成した音響特徴量を偽物か本物かを見分けようとする識別モデルと、&lt;/li&gt;
&lt;li&gt;生成誤差を小さくしつつ (Minimum Generation Error loss; &lt;strong&gt;MGE loss&lt;/strong&gt; の最小化) 、生成した特徴量を識別モデルに本物だと誤認識させようとする (Adversarial loss; &lt;strong&gt;ADV loss&lt;/strong&gt; の最小化) 生成モデル&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;を交互に学習することで、自然音声の特徴量と生成した特徴量の分布を近づけるような、より良い音響モデルを獲得する、といった方法です。&lt;/p&gt;
&lt;h2 id=&#34;ベースライン&#34;&gt;ベースライン&lt;/h2&gt;
&lt;p&gt;ベースラインとしては、 &lt;strong&gt;MGE training&lt;/strong&gt; が挙げられています。DNN音声合成でよくあるロス関数として、音響特徴量 (静的特徴量 + 動的特徴量) に対する Mean Squared Error (&lt;strong&gt;MSE loss&lt;/strong&gt;) というものがあります。これは、特徴量の各次元毎に誤差に正規分布を考えて、その対数尤度を最大化することを意味します。
しかし、&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;静的特徴量と動的特徴量の間には本来 deterministic な関係があることが無視されていること&lt;/li&gt;
&lt;li&gt;ロスがフレーム単位で計算されるので、 (動的特徴量が含まれているとはいえ) 時間構造が無視されてしまっていること&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;から、それらの問題を解決するために、系列単位で、かつパラメータ生成後の静的特徴量の領域でロスを計算する方法、MGE training が提案されています。&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h2 id=&#34;実験&#34;&gt;実験&lt;/h2&gt;
&lt;h3 id=&#34;実験条件&#34;&gt;実験条件&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://festvox.org/cmu_arctic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CMU ARCTIC&lt;/a&gt; から、話者 &lt;code&gt;clb&lt;/code&gt; と &lt;code&gt;slt&lt;/code&gt; のwavデータそれぞれ500発話を用います。439を学習用、56を評価用、残り5をテスト用にします。音響特徴量には、WORLDを使って59次のメルケプストラムを抽出し、0次を除く59次元のベクトルを各フレーム毎の特徴量とします。F0、非周期性指標に関しては、元話者のものをそのまま使い、差分スペクトル法を用いて波形合成を行いました。F0の変換はしていません。音響モデルには、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jstage.jst.go.jp/article/transinf/E100.D/8/E100.D_2017EDL8034/_article&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yuki Saito, Shinnosuke Takamichi, and Hiroshi Saruwatari, &amp;ldquo;Voice conversion using input-to-output highway networks,&amp;rdquo; IEICE Transactions on Information and Systems, Vol.E100-D, No.8, pp.1925&amp;ndash;1928, Aug. 2017&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;で述べられている highway network を用います。ただし、活性化関数をReLUからLeakyReLUにしたり、Dropoutを入れたり、アーキテクチャは微妙に変えています。前者は、調べたら勾配が消えにくくて学習の不安定なGANに良いと書いてある記事があったので（ちゃんと理解しておらず安直ですが、実験したところ悪影響はなさそうでしたので様子見）、後者は、GANの学習の安定化につながった気がします（少なくともTTSでは）。Discriminatorには、Dropout付きの多層ニューラルネットを使いました。MGE loss と ADV loss をバランスする重み &lt;code&gt;w_d&lt;/code&gt; は、 1.0 にしました。層の数、ニューロンの数等、その他詳細が知りたい方は、コードを参照してください。実験に使用したコードの正確なバージョンは  &lt;a href=&#34;https://github.com/r9y9/gantts/tree/ccbb51b51634b272f0a71f29ad4c28edd8ce3429&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ccbb51b&lt;/a&gt; です。ハイパーパラメータは &lt;a href=&#34;https://github.com/r9y9/gantts/blob/ccbb51b51634b272f0a71f29ad4c28edd8ce3429/hparams.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;こちら&lt;/a&gt; です。&lt;/p&gt;
&lt;p&gt;ここで示す結果を再現したい場合は、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;コードをチェックアウト&lt;/li&gt;
&lt;li&gt;パッケージと依存関係をインストール&lt;/li&gt;
&lt;li&gt;&lt;code&gt;clb&lt;/code&gt; と &lt;code&gt;slt&lt;/code&gt; のデータをダウンロード（僕の場合は、 &lt;code&gt;~/data/cmu_arctic&lt;/code&gt; にあります&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;そして、以下のスクリプトを実行すればOKです。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;./vc_demo.sh ~/data/cmu_arctic
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;なお実行には、GPUメモリが4GBくらいは必要です（バッチサイズ32の場合）。GTX 1080Ti + i7-7700K の計算環境で、約1時間半くらいで終わります。スクリプト実行が完了すれば、&lt;code&gt;generated&lt;/code&gt; ディレクトリに、ベースライン/GAN それぞれで変換した音声が出力されます。以下に順に示す図については、&lt;a href=&#34;http://nbviewer.jupyter.org/github/r9y9/gantts/blob/master/notebooks/Test%20VC.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;デモノートブック&lt;/a&gt; を実行すると作ることができます。&lt;/p&gt;
&lt;h3 id=&#34;変換音声の比較&#34;&gt;変換音声の比較&lt;/h3&gt;
&lt;p&gt;テストセットの5つのデータに対しての変換音声、およびその元音声、ターゲット音声を比較できるように貼っておきます。下記の順番です。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;元話者の音声&lt;/li&gt;
&lt;li&gt;ターゲット話者の音声&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MGE Loss&lt;/strong&gt; を最小化して得られたモデルによる変換音声&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MGE loss + ADV loss&lt;/strong&gt; を最小化して得られたモデルによる変換音声&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;比較しやすいように、音量はsoxで正規化しました。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;arctic_a0496&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/src/arctic_a0496.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/tgt/arctic_a0496.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/baseline/test/arctic_a0496.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/gan/test/arctic_a0496.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;arctic_a0497&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/src/arctic_a0497.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/tgt/arctic_a0497.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/baseline/test/arctic_a0497.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/gan/test/arctic_a0497.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;arctic_a0498&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/src/arctic_a0498.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/tgt/arctic_a0498.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/baseline/test/arctic_a0498.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/gan/test/arctic_a0498.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;arctic_a0499&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/src/arctic_a0499.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/tgt/arctic_a0499.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/baseline/test/arctic_a0499.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/gan/test/arctic_a0499.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;arctic_a0500&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/src/arctic_a0500.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/tgt/arctic_a0500.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/baseline/test/arctic_a0500.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/gan/test/arctic_a0500.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;code&gt;clb&lt;/code&gt;, &lt;code&gt;slt&lt;/code&gt; は違いがわかりにくいと以前誰かから指摘されたのですが、これに慣れてしまいました。わかりづらかったらすいません。僕の耳では、明瞭性が上がって、良くなっているように思います。&lt;/p&gt;
&lt;h3 id=&#34;global-variance-は補償されているのか&#34;&gt;Global variance は補償されているのか？&lt;/h3&gt;
&lt;p&gt;統計ベースの手法では、変換音声の &lt;strong&gt;Global variance (GV)&lt;/strong&gt; が落ちてしまい、品質が劣化してしまう問題がよく知られています。GANベースの手法によって、この問題に対処できているのかどうか、実際に確認しました。以下に、データセット中の一サンプルを適当にピックアップして、GVを計算したものを示します。縦軸は対数、横軸はメルケプストラムの次元です。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/ganvc/gv.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;おおおまか、論文で示されているのと同等の結果を得ることができました。&lt;/p&gt;
&lt;h3 id=&#34;modulation-spectrum-変調スペクトル-は補償されているのか&#34;&gt;Modulation spectrum (変調スペクトル) は補償されているのか？&lt;/h3&gt;
&lt;p&gt;GVをより一般化ものとして、変調スペクトルという概念があります。端的に言えば、パラメータ系列の時間方向に対する離散フーリエ変換の二乗（の対数※定義によるかもですが、ここでは対数をとったもの）です。統計処理によって劣化した変換音声は、変調スペクトルが自然音声と比べて小さくなっていることが知られています。というわけで、GANベースの方法によって、変調スペクトルは補償されているのか？ということを調べてみました。これは、論文には書いていません（が、きっとされていると思います）。以下に、評価用の音声56発話それぞれで変調スペクトルを計算し、それらの平均を取り、適当な特徴量の次元をピックアップしたものを示します。横軸は変調周波数です。一番右端が50Hzです。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/ganvc/ms.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;特に高次元の変調スペクトルに対して、ベースラインは大きく落ちている一方で、GANベースでは比較的自然音声と近いことがわかります。しかし、高次元になるほど、自然音声とGANベースでも違いが出ているのがわかります。改善の余地はありそうですね。&lt;/p&gt;
&lt;h3 id=&#34;特徴量の分布&#34;&gt;特徴量の分布&lt;/h3&gt;
&lt;p&gt;論文で示されているscatter plotですが、同じことをやってみました。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/ganvc/scatter.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;概ね、論文通りの結果となっています。&lt;/p&gt;
&lt;h3 id=&#34;詐称率について&#34;&gt;詐称率について&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;w_d&lt;/code&gt; を変化させて、詐称率がどうなるかは実験していないのですが、&lt;code&gt;w_d = 1.0&lt;/code&gt; の場合に、だいたい0.7 ~ 0.9 くらいに収まることを確認しました。TTSでは0.99くらいの、論文と同様の結果が出ました。くらい、というのは、どのくらい Discriminator を学習させるか、初期化としてのMGE学習（例えば25epochくらい）のあと生成された特徴量に対して学習させるのか、それとも初期化とは別でベースライン用のモデル（100epochとか）を使って生成された特徴量に対して学習させるのか、によって変わってくるのと、その辺りが論文からではあまりわからなかったのと、学習率や最適化アルゴリズムやデータによっても変わってくるのと、詐称率の計算は品質にはまったく関係ないのもあって、あまり真面目にやっていません。すいません&lt;/p&gt;
&lt;h2 id=&#34;感想&#34;&gt;感想&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;効果は劇的、明らかに良くなりました。素晴らしいですね&lt;/li&gt;
&lt;li&gt;論文で書かれている反復回数 (25epochとか)よりも、100, 200と多く学習させる方がよかったです（知覚的な差は微妙ですが）ロスは下がり続けていました。&lt;/li&gt;
&lt;li&gt;実装はそんなに大変ではなかったですが、GANの学習が難しい感じがしました（VCではあまり失敗しないが、TTSではよく失敗する。落とし所を探し中&lt;/li&gt;
&lt;li&gt;Adam は学習は速いが、過学習ししやすい。GANも不安定になりがちな気がしました&lt;/li&gt;
&lt;li&gt;Adagrad は収束は遅いが、安定&lt;/li&gt;
&lt;li&gt;MGE loss と ADV loss の重みの計算は、適当にclipするようにしました。しなくてもだいたい収束しますが、バグがあると簡単に発散しますね〜haha&lt;/li&gt;
&lt;li&gt;gradient clipping をいれました。TTSでは少なくとも良くなった気がします。VCはなしでも安定しているようです。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;
&lt;p&gt;とても良くなりました。素晴らしいです。今回もWORLDにお世話になりました。続いて、TTSでも実験を進めていきます。&lt;/p&gt;
&lt;p&gt;GANシリーズのその他記事へのリンクは以下の通りです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/10/09/gantts/&#34;&gt;GAN 音声合成 (en) 編はこちら&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/10/10/gantts-jp/&#34;&gt;GAN 音声合成 (ja) 編はこちら&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;p&gt;Arxivにあるペーパーだけでなく、その他いろいろ参考にしました。ありがとうございます。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1709.08041&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yuki Saito, Shinnosuke Takamichi, Hiroshi Saruwatari, &amp;ldquo;Statistical Parametric Speech Synthesis Incorporating Generative Adversarial Networks&amp;rdquo;, arXiv:1709.08041 [cs.SD], Sep. 2017&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sython.org/papers/SIG-SLP/saito201702slp.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yuki Saito, Shinnosuke Takamichi, and Hiroshi Saruwatari, &amp;ldquo;Training algorithm to deceive anti-spoofing verification for DNN-based text-to-speech synthesis,&amp;rdquo; IPSJ SIG Technical Report, 2017-SLP-115, no. 1, pp. 1-6, Feb., 2017. (in Japanese)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jstage.jst.go.jp/article/transinf/E100.D/8/E100.D_2017EDL8034/_article&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yuki Saito, Shinnosuke Takamichi, and Hiroshi Saruwatari, &amp;ldquo;Voice conversion using input-to-output highway networks,&amp;rdquo; IEICE Transactions on Information and Systems, Vol.E100-D, No.8, pp.1925&amp;ndash;1928, Aug. 2017&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.slideshare.net/ShinnosukeTakamichi/dnnantispoofing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.slideshare.net/ShinnosukeTakamichi/dnnantispoofing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.slideshare.net/YukiSaito8/Saito2017icassp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.slideshare.net/YukiSaito8/Saito2017icassp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/SythonUK/whisperVC&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/SythonUK/whisperVC&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Kobayashi, Kazuhiro, et al. &amp;ldquo;Statistical Singing Voice Conversion with Direct Waveform Modification based on the Spectrum Differential.&amp;rdquo; Fifteenth Annual Conference of the International Speech Communication Association. 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;論文では有効性が示されていますが、僕が試した範囲内で、かつ僕の耳にによれば、あまり大きな改善は確認できていません。客観的な評価は、そのうちする予定です。&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>DNN音声合成のためのライブラリの紹介とDNN日本語音声合成の実装例</title>
      <link>https://r9y9.github.io/blog/2017/08/16/japanese-dnn-tts/</link>
      <pubDate>Wed, 16 Aug 2017 23:10:56 +0900</pubDate>
      <guid>https://r9y9.github.io/blog/2017/08/16/japanese-dnn-tts/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/nnmnkwii&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nnmnkwii&lt;/a&gt; というDNN音声合成のためのライブラリを公開しましたので、その紹介をします。&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://t.co/p8MnOxkVoH&#34;&gt;https://t.co/p8MnOxkVoH&lt;/a&gt; Library to build speech synthesis systems designed for easy and fast prototyping. Open sourced:)&lt;/p&gt;&amp;mdash; 山本りゅういち (@r9y9) &lt;a href=&#34;https://twitter.com/r9y9/status/897117170265501696&#34;&gt;August 14, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;ドキュメントの最新版は &lt;a href=&#34;https://r9y9.github.io/nnmnkwii/latest/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://r9y9.github.io/nnmnkwii/latest/&lt;/a&gt; です。以下に、いくつかリンクを貼っておきます。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/nnmnkwii/v0.0.1/design_jp.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;なぜ作ったのか、その背景の説明と設計 (日本語)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/nnmnkwii/v0.0.1/nnmnkwii_gallery/notebooks/00-Quick%20start%20guide.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;クイックガイド&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/nnmnkwii/v0.0.1/nnmnkwii_gallery/notebooks/tts/01-DNN-based%20statistical%20speech%20synthesis%20%28en%29.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DNN英語音声合成のチュートリアル&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;よろしければご覧ください&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;p&gt;ドキュメントは、だいたい英語でお硬い雰囲気で書いたので、この記事では、日本語でカジュアルに背景などを説明しようと思うのと、（ドキュメントには英語音声合成の例しかないので）HTSのデモに同梱のATR503文のデータセットを使って、&lt;strong&gt;DNN日本語音声合成&lt;/strong&gt; を実装する例を示したいと思います。結果だけ知りたい方は、音声サンプルが下の方にあるので、適当に読み飛ばしてください。&lt;/p&gt;
&lt;h2 id=&#34;なぜ作ったのか&#34;&gt;なぜ作ったのか&lt;/h2&gt;
&lt;p&gt;一番大きな理由は、僕が &lt;strong&gt;対話環境（Jupyter, IPython等）&lt;/strong&gt; で使えるツールがほしかったからです&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;。
僕は結構前からREPL (Read-Eval-Print-Loop) 信者で、プログラミングのそれなりの時間をREPLで過ごします。
IDEも好きですし、emacsも好きなのですが、同じくらいJupyterや&lt;a href=&#34;https://github.com/JuliaLang/julia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Julia&lt;/a&gt;のREPLが好きです。
用途に応じて使い分けますが、特に何かデータを分析する必要があるような時に、即座にデータを可視化できるJupyter notebookは、僕にとってプログラミングに欠かせないものになっています。&lt;/p&gt;
&lt;p&gt;ところが、HTSの後継として生まれたDNN音声合成ツールである &lt;a href=&#34;https://github.com/CSTR-Edinburgh/merlin&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Merlin&lt;/a&gt; は、コマンドラインツールとして使われる想定のもので、僕の要望を満たしてくれるものではありませんでした。
とはいえ、Merlinは優秀な音声研究者たちの産物であり、当然役に立つ部分も多く、使っていました。しかし、ことプロトタイピングにおいては、やはり対話環境でやりたいなあという思いが強まっていきました。&lt;/p&gt;
&lt;p&gt;新しく作るのではなく、Merlinを使い続ける、Merlinを改善する方針も考えました。僕がMerlinを使い始めた頃、Merlinはpython3で動かなかったので、動くように &lt;a href=&#34;https://github.com/CSTR-Edinburgh/merlin/pull/141&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;プルリク&lt;/a&gt; を出したこともあるのですが、まぁレビューに数カ月もかかってしまったので、これは新しいものを作った方がいいな、と思うに至りました。&lt;/p&gt;
&lt;p&gt;以上が、僕が新しくツール作ろうと思った理由です。&lt;/p&gt;
&lt;h2 id=&#34;特徴&#34;&gt;特徴&lt;/h2&gt;
&lt;p&gt;さて、Merlinに対する敬意と不満から生まれたツールでありますが、その特徴を簡単にまとめます。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;対話環境&lt;/strong&gt; での使用を前提に、設計されています。コマンドラインツールはありません。ユーザが必要に応じて作ればよい、という考えです。&lt;/li&gt;
&lt;li&gt;DNN音声合成のデモをノートブック形式で提供しています。&lt;/li&gt;
&lt;li&gt;大規模データでも扱えるように、データセットとデータセットのイテレーション（フレーム毎、発話毎の両方）のユーティリティが提供されています&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Merlinとは異なり、音響モデルは提供しません&lt;/strong&gt;。自分で実装する必要があります（が、今の時代簡単ですよね、lstmでも数行で書けるので&lt;/li&gt;
&lt;li&gt;任意の深層学習フレームワークと併せて使えるように、設計されています&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;（&lt;a href=&#34;https://r9y9.github.io/nnmnkwii/latest/references/autograd.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;autogradパッケージ&lt;/a&gt;のみ、今のところPyTorch依存です&lt;/li&gt;
&lt;li&gt;言語特徴量の抽出の部分は、Merlinのコードをリファクタして用いています。そのせいもあって、Merlinのデモと同等のパフォーマンスを簡単に実現できます。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;対象ユーザ&#34;&gt;対象ユーザ&lt;/h2&gt;
&lt;p&gt;まずはじめに、大雑把にいって、音声合成の研究（or その真似事）をしてみたい人が主な対象です。
自前のデータを元に、ブラックボックスでいいので音声合成エンジンを作りたい、という人には厳しいかもしれません。その前提を元に、少し整理します。&lt;/p&gt;
&lt;h3 id=&#34;こんな人におすすめです&#34;&gt;こんな人におすすめです&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Jupyter notebookが好きな人&lt;/li&gt;
&lt;li&gt;REPLが好きな人&lt;/li&gt;
&lt;li&gt;Pythonで処理を完結させたい人&lt;/li&gt;
&lt;li&gt;オープンソースの文化に寛容な人&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;音声合成の研究を始めてみたい人&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;こんな人には向かないかも&#34;&gt;こんな人には向かないかも&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;コマンドラインツールこそ至高な人&lt;/li&gt;
&lt;li&gt;パイプライン処理こそ至高な人&lt;/li&gt;
&lt;li&gt;SPTKのコマンドラインツール至高な人&lt;/li&gt;
&lt;li&gt;信頼のある機関が作ったツールしか使わない人&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;音声研究者ガチ勢で、自前のツールで満足している人&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;dnn日本語音声合成の実装例&#34;&gt;DNN日本語音声合成の実装例&lt;/h2&gt;
&lt;p&gt;さて、前置きはこのくらいにして、日本語音声合成の実装例を示します。シンプルなFeed forwardなネットワークと、Bi-directional LSTM RNNの2パターンを、ノートブック形式で作成しました。&lt;/p&gt;
&lt;p&gt;ソースコードは、 &lt;a href=&#34;https://github.com/r9y9/nnmnkwii_gallery&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/nnmnkwii_gallery&lt;/a&gt; にあります。以下に、現状点での直リンク（gitのコミットハッシュがURLに入っています）を貼っておきます。nbviewerに飛びます。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://nbviewer.jupyter.org/github/r9y9/nnmnkwii_gallery/blob/bd4bd260eb22d0000ac2776b204b3a5afb693c49/notebooks/tts/jp-01-DNN-based%20statistical%20speech%20synthesis.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Feed forwardなネットワークを使った音声合成のノートブックへの直リンク&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://nbviewer.jupyter.org/github/r9y9/nnmnkwii_gallery/blob/bd4bd260eb22d0000ac2776b204b3a5afb693c49/notebooks/tts/jp-02-Bidirectional-LSTM%20based%20RNNs%20for%20speech%20synthesis.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bi-directional LSTM RNNを使った音声合成のノートブックへの直リンク&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;興味のある人は、ローカルに落として実行してみてください。CUDA環境があることが前提ですが、通常のFeed forwardのネットワークを用いたデモは、
特徴抽出の時間（初回実行時に必要）を除けば、5分で学習&amp;amp;波形生成が終わります。Bi-directional LSTMのデモは、僕の環境 (i7-7700K, GTX 1080Ti) では、約2時間で終わります。GPUメモリが少ない場合は、バッチサイズを小さくしなければならず、より時間がかかるかもしれません。&lt;/p&gt;
&lt;h3 id=&#34;データセット&#34;&gt;データセット&lt;/h3&gt;
&lt;p&gt;今回は、HTSのNIT-ATR503のデモデータ (&lt;a href=&#34;https://github.com/r9y9/nnmnkwii_gallery/blob/4899437e22528399ca50c34097a2db2bed782f8b/data/NIT-ATR503_COPYING&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ライセンス&lt;/a&gt;) を拝借します。ライブラリを使って音声合成を実現するためのデータとして、最低限以下が必要です。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(state or phone level) フルコンテキストラベル&lt;/li&gt;
&lt;li&gt;Wavファイル&lt;/li&gt;
&lt;li&gt;質問ファイル（言語特徴量の抽出に必要）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上2つは、今回はHTSのデモスクリプトからまるまるそのまま使います（※HTSのデモスクリプトを回す必要はありません）。質問ファイルは、コンテキストクラスタリングに使われる質問ファイルを元に、質問数を（本当に）適当に減らして、Merlinのデモの質問ファイルからCQSに該当する質問を加えて、作成しました。
フルコンテキストラベルには、phone-levelでアライメントされたものを使いますが、
state-levelでアライメントされたものを使えば、性能は上がると思います。今回は簡単のためにphone-levelのアライメントを使います。&lt;/p&gt;
&lt;p&gt;質問の選定には、改善の余地があることがわかっていますが、あくまでデモということで、ご了承ください。&lt;/p&gt;
&lt;h3 id=&#34;音声合成の結果&#34;&gt;音声合成の結果&lt;/h3&gt;
&lt;p&gt;全体の処理に興味がある場合は別途ノートブックを見てもらうとして、ここでは結果だけ貼っておきます。
HTSのデモからとってきた例文5つに対して、それぞれ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Feed forward neural networks (MyNetとします) で生成したもの&lt;/li&gt;
&lt;li&gt;Bi-directional LSTM recurrent neural networks (MyRNNとします)で生成したもの&lt;/li&gt;
&lt;li&gt;HTSデモで生成したもの (HTSとします)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;の順番に、音声ファイルを添付しておきます。聴きやすいように、soxで正規化しています。それではどうぞ。&lt;/p&gt;
&lt;p&gt;1 こんにちは&lt;/p&gt;
&lt;p&gt;MyNet&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-01-tts/phrase01.wav&#34; type=&#34;audio/wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;MyRNN&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-02-tts/phrase01.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;HTS&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/hts_nit_atr503_2mix/phrase01.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;2 それではさようなら&lt;/p&gt;
&lt;p&gt;MyNet&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-01-tts/phrase02.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;MyRNN&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-02-tts/phrase02.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;HTS&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/hts_nit_atr503_2mix/phrase02.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;3 はじめまして&lt;/p&gt;
&lt;p&gt;MyNet&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-01-tts/phrase03.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;MyRNN&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-02-tts/phrase03.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;HTS&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/hts_nit_atr503_2mix/phrase03.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;4 ようこそ名古屋工業大学へ&lt;/p&gt;
&lt;p&gt;MyNet&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-01-tts/phrase04.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;MyRNN&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-02-tts/phrase04.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;HTS&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/hts_nit_atr503_2mix/phrase04.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;5 今夜の名古屋の天気は雨です&lt;/p&gt;
&lt;p&gt;MyNet&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-01-tts/phrase05.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;MyRNN&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-02-tts/phrase05.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;HTS&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/hts_nit_atr503_2mix/phrase05.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;一応HTSで生成された音声も貼りましたが、そもそも実験条件が違いすぎる&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;ので、単純に比較することはできません。
せめて HTS ＋ STRAIGHTと比較したかったところですが、僕はSTRAIGHTを持っていないので、残念ながらできません、悲しみ。&lt;/p&gt;
&lt;p&gt;しかし、それなりにまともな音声が出ているような気がします。&lt;/p&gt;
&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;
&lt;p&gt;いままでさんざん、汎用性とは程遠いクソコードを書いてきましたが、今回こそは少しはマシなものを作ろうと思って作りました。僕以外の人にも役に立てば幸いです。あと、この記事を書いた目的は、いろんな人に使ってみてほしいのと、使ってみた結果のフィードバックがほしい（バグ見つけた、そもそもエラーで動かん、ここがクソ、等）ということなので、フィードバックをくださると助かります。よろしくお願いします。&lt;/p&gt;
&lt;p&gt;ちなみに名前ですが、ななみ or しちみと読んでください。何でもいいのですが、常識的に考えてあぁ確かに読めないなぁと思いました（小並感）。ドキュメントにあるロゴは、昔三次元物体追跡の実験をしていたときに撮ったく某モンのポイントクラウドですが、そのうち七味的な画像に変えようと思っています。適当ですいません&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;リンク切れが怖いので、v0.0.1のリンクを貼りました。できれば、最新版をご覧ください。 &lt;a href=&#34;https://r9y9.github.io/nnmnkwii/latest/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://r9y9.github.io/nnmnkwii/latest/&lt;/a&gt; こちらからたどれます&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;知っている人にはまたか、と言われそう&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;音響モデルの提供をライブラリの範囲外とすることで、間接的に達成されています&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;バグにエンカウントしたらすぐに使うのをやめてしまう人には、向いていないかもしれません。&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Merlinは、エジンバラ大学の優秀な研究者の方々によって作られています&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;f0分析、スペクトル包絡抽出、非周期性成分の抽出法がすべてことなる、またポストフィルタの種類も異なる。条件をある程度揃えて比較するのが面倒そうだったので（なにせHTSを使ったモデルの学習は数時間かかるし…）、手を抜きました、すいません&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>DNN統計的音声合成ツールキット Merlin の中身を理解をする</title>
      <link>https://r9y9.github.io/blog/2017/08/16/trying-to-understand-merlin/</link>
      <pubDate>Wed, 16 Aug 2017 03:00:00 +0900</pubDate>
      <guid>https://r9y9.github.io/blog/2017/08/16/trying-to-understand-merlin/</guid>
      <description>&lt;p&gt;この記事では、音声合成ツールキットであるMerlinが、具体的に何をしているのか（特徴量の正規化、無音区間の削除、ポストフィルタなど、コードを読まないとわからないこと）、その中身を僕が理解した範囲でまとめます。
なお、HMM音声合成について簡単に理解していること（HMMとは、状態とは、フルコンテキストラベルとは、くらい）を前提とします。&lt;/p&gt;
&lt;h2 id=&#34;はじめに&#34;&gt;はじめに&lt;/h2&gt;
&lt;p&gt;Merlinの概要については以下をご覧ください。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://ssw9.net/papers/ssw9_PS2-13_Wu.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wu, Zhizheng, Oliver Watts, and Simon King. &amp;ldquo;Merlin: An open source neural network speech synthesis system.&amp;rdquo; Proc. SSW, Sunnyvale, USA (2016).&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://ssw9.net/papers/ssw9_DS-3_Ronanki.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;A Demonstration of the
Merlin Open Source Neural Network Speech Synthesis System&amp;rdquo;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cstr-edinburgh.github.io/merlin/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;公式ドキュメント&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Merlinにはデモスクリプトがついています。基本的にユーザが使うインタフェースはrun_merlin.pyというコマンドラインスクリプトで、
デモスクリプトではrun_merlin.pyに用途に応じた設定ファイルを与えることで、継続長モデルの学習/音響モデルの学習/パラメータ生成など、音声合成に必要なステップを実現しています。&lt;/p&gt;
&lt;p&gt;デモスクリプトを実行すると、音声データ (wav) と言語特徴量（HTSのフルコンテキストラベル）から、変換音声が合成されるところまでまるっとやってくれるのですが、それだけでは内部で何をやっているのか、理解することはできません。
ツールキットを使う目的が、自分が用意したデータセットで音声合成器を作りたい、といった場合には、特に内部を知る必要はありません。
また、設定ファイルをちょこっといじるだけでこと済むのであれば、知る必要はないかもしれません。
しかし、モデル構造を変えたい、学習アルゴリズムを変えたい、ポストフィルタを入れたい、といったように、少し進んだ使い方をしようとすれば、内部構造を理解しないとできないことも多いと思います。&lt;/p&gt;
&lt;p&gt;run_merlin.py はあらゆる処理 (具体的にはあとで述べます) のエントリーポイントになっているがゆえに、コードはなかなかに複雑になっています&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;。この記事では、run_merlin.pyがいったい何をしているのかを読み解いた結果をまとめます。&lt;/p&gt;
&lt;h2 id=&#34;merlinでは提供しないこと&#34;&gt;Merlinでは提供しないこと&lt;/h2&gt;
&lt;p&gt;Merlinが何を提供してくれるのかを理解する前に、何を提供しないのか、をざっくりと整理します。以下のとおりです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Text-processing (&lt;strong&gt;Frontend&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;Speech analysis/synthesis (&lt;strong&gt;Backend&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;HTSと同様に、frontend, backendといった部分は提供していません。Merlinの論文にもあるように、HTSの影響を受けているようです。&lt;/p&gt;
&lt;p&gt;Frontendには、英語ならFestival、BackendにはWORLDやSTRAIGHTを使ってよろしくやってね、というスタンスです。
Backendに関しては、Merlinのインストールガイドにあるように、WOLRDをインストールするように促されます。&lt;/p&gt;
&lt;p&gt;デモスクリプトでは、Frontendによって生成されたフルコンテキストラベル（HTS書式）が事前に同梱されているので、Festivalをインストールする必要はありません。
misc以下に、Festivalを使ってフルコンテキストラベルを作るスクリプト (make_labels) があるので、デモデータ以外のデータセットを使う場合は、それを使います。&lt;/p&gt;
&lt;h2 id=&#34;steps&#34;&gt;Steps&lt;/h2&gt;
&lt;p&gt;本編です。slt_arcticのデモスクリプトに従い、いくらかのステップに分けて、詳細に見ていきます。なお、以下デモスクリプトと書いた際には、slt_arcticのデモスクリプトを指すものとします。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;継続長モデルの学習&lt;/li&gt;
&lt;li&gt;音響モデルの学習&lt;/li&gt;
&lt;li&gt;変換音声の合成&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;なお、Merlinのスクリプトによってはかれるデータは、基本的に&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;x.astype(np.float32).tofile(&amp;quot;foobar.bin&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;といった感じで、32bit浮動小数点のnumpyの配列がヘッダなしのバイナリフォーマットで保存されています。デバッグ時には、&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;np.fromfile(&amp;quot;foobar.bin&amp;quot;, dtype=np.float32)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;として、ファイルを読み込んでインスペクトするのが便利です。注意事項として、ややこしいことに、拡張子は信頼できません。&lt;code&gt;.lab&lt;/code&gt; という拡張子であっても、フルコンテキストラベルのテキストファイルである場合もあれば、上述のようにバイナリフォーマットである可能性もあります。つらいですね！&lt;/p&gt;
&lt;h3 id=&#34;継続長モデルの学習&#34;&gt;継続長モデルの学習&lt;/h3&gt;
&lt;p&gt;継続長モデルとは、言語特徴量から、継続長を予測するモデルです。Merlinでは、phone-level / state-level のどちらかを選択可能です。Merlinの提供するDNN音声合成では、継続長の予測→音響特徴量の予測→合成、といったスタイルをとります。
デフォルトでは、state-levelで継続長（具体的には一状態当たりの継続フレーム数）を予測します。状態レベルのアライメントのほうが、時間解像度の高いコンテキストを得られ、結果音声合成の品質が良くなるので、デフォルトになっているのだと思います。 &lt;a href=&#34;https://github.com/CSTR-Edinburgh/merlin/issues/18&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/CSTR-Edinburgh/merlin/issues/18&lt;/a&gt; に少し議論があります。&lt;/p&gt;
&lt;p&gt;デモスクリプトを実行すると、 &lt;code&gt;experiments/slt_arctic_demo/duration_model/&lt;/code&gt; 以下に継続長モデル用のデータがは出力されます。いくつか重要なものについて、以下に示します。&lt;/p&gt;
&lt;h4 id=&#34;data&#34;&gt;data&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;label_phone_align&lt;/code&gt;: 音素レベルでのフルコンテキストラベルです&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dur&lt;/code&gt;: 状態別継続長です。正確には、&lt;code&gt;T&lt;/code&gt; をフルコンテキストラベル中の音素数として、&lt;code&gt;(T, 5)&lt;/code&gt; の配列が発話ごとに保存されます。5は音素あたりのHMMの状態数で、慣例的に？5が使用されているような気がします。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;inter_module&#34;&gt;inter_module&lt;/h4&gt;
&lt;p&gt;中間結果のファイル群です&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;binary_label_416/&lt;/code&gt;: HTS形式の質問ファイルを元に生成した、言語特徴量行列です。デモスクリプトでは、416個の質問があるので、一状態あたり416次元の特徴ベクトルになります。binaryな特徴量（母音か否か）と連続的な特徴量（単語中のsylalbleの数等）があります。&lt;code&gt;(T, 416)&lt;/code&gt; の行列が、発話ごとに保存されています。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;label_norm_HTS_416.dat&lt;/code&gt;: 416次元の特徴ベクトルの正規化に必要な情報です。デモスクリプトでは、言語特徴量に関してはmin/max正規化が行われるので、minおよびmaxの416次元のベクトル（計416*2次元）が保存されています。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nn_dur_5/&lt;/code&gt;: 無音区間が除去された、状態別継続長です。フォルダ名からは察することは難しいですが、無音区間が除去されています。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nn_no_silence_lab_416/&lt;/code&gt;: 無音区間が除去された、言語特徴量行列です。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nn_no_silence_lab_norm_416/&lt;/code&gt;: 無音区間が除去された、min/max正規化された言語特徴量行列です。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nn_norm_dur_5/&lt;/code&gt; 無音区間が除去された、mean/variance正規化された状態別継続長です。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;norm_info_dur_5_MVN.dat&lt;/code&gt;: 継続長の正規化に必要な情報です。具体的には、Mean-variance正規化（N(0, 1)になるようにする）が行われるので、平均と標準偏差（not 分散）が入っています。状態レベルでのアライメントを使用する場合は、5*2で計10次元のベクトルです。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ref_data/&lt;/code&gt;: RMSEなどの評価基準を計算する際に使われる継続長のテストデータです。&lt;code&gt;data/dur&lt;/code&gt; ディレクトリの継続長データを元に、無音区間が除去されたものです&lt;/li&gt;
&lt;li&gt;&lt;code&gt;var/&lt;/code&gt;: 継続長の分散（not 標準偏差）です。パラメータ生成 (MLPG) に使われる想定のデータです&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;けっこうたくさんありますね。これだけでも、いかに多くのことがrun_merlin.pyによってなされているか、わかるかと思います。&lt;/p&gt;
&lt;h4 id=&#34;入力出力&#34;&gt;入力/出力&lt;/h4&gt;
&lt;p&gt;中間ファイルがたくさんあってややこしいですが、整理すると、ネットワーク学習に用いる入力と出力は以下になります。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;入力: &lt;code&gt;nn_no_silence_lab_norm_416&lt;/code&gt;, 一発話あたりの特徴量のshape: &lt;code&gt;(T, 416)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;出力: &lt;code&gt;nn_norm_dur_5&lt;/code&gt;, 一発話あたりの特徴量のshape: &lt;code&gt;(T, 5)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;学習されたモデルは、 &lt;code&gt;nnets_model&lt;/code&gt;というフォルダに保存されます。&lt;/p&gt;
&lt;h3 id=&#34;音響モデルの学習&#34;&gt;音響モデルの学習&lt;/h3&gt;
&lt;p&gt;音響モデルとは、言語特徴量からメルケプストラム、F0、非周期性成分などの音響特徴量を予測するモデルです。Merlinのデモスクリプトでは、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;メルケプストラム: 60次元（動的特徴量を合わせると、180次元)&lt;/li&gt;
&lt;li&gt;対数F0: 1次元（動的特徴量を合わせると、3次元)&lt;/li&gt;
&lt;li&gt;有声 or 無声フラグ (voiced/unvoiced; vuv): 1次元&lt;/li&gt;
&lt;li&gt;非周期性成分: 1次元（動的特徴量を合わせると、3次元)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;の計187次元の音響特徴量を予測するモデルを考えます。継続長モデルのときと同様に、出力されるファイルについていくらか説明します。&lt;/p&gt;
&lt;h4 id=&#34;data-1&#34;&gt;data&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;bap&lt;/code&gt;: 発話毎に計算された非周期性成分が入っています。bapはband averaged aperiodicityの略です（専門家の人にとっては当たり前かと思いますが、一応&lt;/li&gt;
&lt;li&gt;&lt;code&gt;label_phone_align&lt;/code&gt;: phone-levelでアライメントされたHTSのコンテキストラベルが入っています。デフォルトの設定では使いません。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;label_state_align&lt;/code&gt;: state-levelでアライメントされたHTSのコンテキストラベルが入っています&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lf0&lt;/code&gt;: 対数F0です。なお、WORLDではかれるF0は無声区間で0を取りますが、無声区間の部分を線形補間することによって、非ゼロの値で補完しています。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mgc&lt;/code&gt;: メルケプストラムです（フォルダ名は、慣習的にメル一般化ケプストラムを表す &lt;code&gt;mgc&lt;/code&gt;となっていますが、デモスクリプトでは実際にはメルケプストラムです）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;inter_module-1&#34;&gt;inter_module&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;binary_label_425/&lt;/code&gt;: 言語特徴量の行列です。継続長モデルの場合と違って、フレーム単位で生成されているのと、フレーム単位ならではの特徴量（音素中の何フレーム目なのか、等）が追加されています。フレーム数を &lt;code&gt;T&lt;/code&gt; として、 &lt;code&gt;(T, 425)&lt;/code&gt; の配列が発話ごとに保存されています。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;label_norm_HTS_425.dat&lt;/code&gt;: 言語特徴量のmin/max正規化に必要なmin/maxのベクトルです。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nn_mgc_lf0_vuv_bap_187/&lt;/code&gt;: mgc, lf0, vuv, bapを結合した音響特徴量です。よくcmp (composed featureから来ていると思われる) と表されるものです。ディレクトリ名からは判別が付きませんが、無音区間は削除されています。ややこしい&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nn_no_silence_lab_425/&lt;/code&gt;: &lt;code&gt;binary_label_425&lt;/code&gt; の言語特徴量から無音区間を削除したものです&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nn_no_silence_lab_norm_425/&lt;/code&gt;: それをさらにmin/max正規化したものです&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nn_norm_mgc_lf0_vuv_bap_187/&lt;/code&gt;: &lt;code&gt;nn_mgc_lf0_vuv_bap_187/&lt;/code&gt;の音響特徴量をN(0, 1)になるようにmean/variance正規化したものです&lt;/li&gt;
&lt;li&gt;&lt;code&gt;norm_info_mgc_lf0_vuv_bap_187_MVN.dat&lt;/code&gt;: 音響特徴量の正規化に必要な、平均と標準偏差です&lt;/li&gt;
&lt;li&gt;&lt;code&gt;var/&lt;/code&gt;: mgc, lf0, bap, vuvそれぞれの分散です。このうちvuvは、パラメータ生成時にMLPGを行いませんが、保存はされています。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;入力出力-1&#34;&gt;入力/出力&lt;/h4&gt;
&lt;p&gt;継続長モデルの場合と同様の中間特徴量が出力されています。改めて整理すると、音響モデルの学習に使用する入力と出力は、以下のとおりです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;入力: &lt;code&gt;nn_no_silence_lab_norm_425/&lt;/code&gt;, 一発話あたりの特徴量のshape: &lt;code&gt;(T, 425)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;出力: &lt;code&gt;nn_norm_mgc_lf0_vuv_bap_187&lt;/code&gt;, 一発話あたりの特徴量のshape: &lt;code&gt;(T, 187)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;学習されたモデルは、 &lt;code&gt;nnets_model&lt;/code&gt;というフォルダに保存されます。&lt;/p&gt;
&lt;h3 id=&#34;波形生成&#34;&gt;波形生成&lt;/h3&gt;
&lt;p&gt;得られた継続長モデルと音響モデルから、波形を生成する処理は、大雑把にいって以下の手順で行われます。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;フルコンテキストラベルから得られる言語特徴量を元に、継続長モデルを使って継続長を予測する&lt;/li&gt;
&lt;li&gt;予測された継続長を使って、フルコンテキストラベルを書き換える。より具体的には、状態毎の start_time, end_time の部分を書き換える。&lt;/li&gt;
&lt;li&gt;書き換えられたフルコンテキストラベルから、音響モデル用のフレームレベルの言語特徴量を計算し、音響モデルを使って音響特徴量を予測する&lt;/li&gt;
&lt;li&gt;予測された音響特徴量（static + delta + delta-delta) から、静的特徴量をMLPGによって生成する。MLPGによって生成するのは、mgc, lf0, bapのみで、vuvについてはそのまま使う。波形合成にはvuvを直接使うのではなく、vuv &amp;lt; 0.5以下のf0を0として扱う。&lt;/li&gt;
&lt;li&gt;生成されたメルケプストラムに対して、Merlinお手製ポストフィルタを掛ける&lt;/li&gt;
&lt;li&gt;得られた音響特徴量 (mgc, f0, bap) から、WORLDを使って波形合成をする&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以上です。Merlinの良い所の一つに、ログをたくさんはいてくれるというのがあります。しかし、このうちポストフィルタ（デフォルトでONです）に関しては一切（デフォルトでは）ログがはかれず、気づくのに時間がかかりました。&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;また、個人的な感覚ですが、このポストフィルタの影響は絶大に思いました。コードを見て、何をしているのか僕には理解できませんでしたが、ヒューリスティックな方法も含んでいるように思いました。興味のある方は、 波形合成用のconfファイルを開いて、&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[Waveform]
do_post_filtering: False
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;のように、&lt;code&gt;[Waveform]&lt;/code&gt; セクションに &lt;code&gt;do_post_filtering&lt;/code&gt; という項目を加えて、生成結果を聴き比べてみることをおすすめします。ポストフィルタによって劇的に音質が改善されているのがわかると思います。さらに興味のある方は、コードを読んでみてください。参考文献も探しましたが、僕には見つかりませんでした。ご存知の方がいれば教えていただきたいです。&lt;/p&gt;
&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;
&lt;p&gt;Merlin、最初は使いにくいなと思っていましたが、頑張って読んでみれば、とても勉強になりました（使いやすいとは言っていない）。後半はだれて、適当なまとめになってしまったかもしれません、すいません。もろもろの不満から&lt;a href=&#34;https://github.com/r9y9/nnmnkwii&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;新しいツール&lt;/a&gt;を作りましたが、それはまた別の機会に紹介したいと思います。ありがとうございました。&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://jrmeyer.github.io/merlin/2017/02/14/Installing-Merlin.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://jrmeyer.github.io/merlin/2017/02/14/Installing-Merlin.html&lt;/a&gt; によれば、This is a very clearly written Python script だそうです…。僕に読解力がないだけの可能性があります&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;自分で作ったモデルが、どうしてもmerlinに勝てない、なぜだ、と悩んでいたとき、Merlinに言及している論文の一つに、ポストフィルタを使っているとの記述があり、探ってみるとたしかにあった、という感じでした。&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>言語処理100本ノック 2015 をすべてやりました</title>
      <link>https://r9y9.github.io/blog/2017/06/09/nlp100/</link>
      <pubDate>Fri, 09 Jun 2017 21:58:50 +0900</pubDate>
      <guid>https://r9y9.github.io/blog/2017/06/09/nlp100/</guid>
      <description>&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/nlp100_summary.png&#34; /&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;本家サイト: &lt;a href=&#34;http://www.cl.ecei.tohoku.ac.jp/nlp100/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cl.ecei.tohoku.ac.jp/nlp100/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;僕が書いたコード: &lt;a href=&#34;https://github.com/r9y9/nlp100&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/nlp100&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;最近、自然言語処理(NLP)を勉強しようという熱が出ました。ある自然言語処理の問題を解きたかったのですが、
無知のためにか直感がまったく働かず、これはまずいと感じたので、
入門的なのに手を出そうと思った次第です。
結果、毎日やりつづけて、12日かかりました（上図は、横軸が日付、縦軸が達成した問題数です。図は&lt;a href=&#34;https://github.com/r9y9/nlp100/blob/master/summary.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;seabornで適当に作りました&lt;/a&gt;）。
速度重視&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;で問題を解きましたが、思ったよりうまく進まず大変だった、というのが正直な感想です。以下、雑多な感想です。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mecab, cabocha, CoreNLPの解析結果をパースするコードを書くのは、ただただ面倒に感じた&lt;/li&gt;
&lt;li&gt;NER実装しろ、みたいな問題があったらより楽しかったかなと思った&lt;/li&gt;
&lt;li&gt;正規表現をまったく使いこなせていなかったことがわかったので、勉強し直せてよかった&lt;/li&gt;
&lt;li&gt;全体を通して、第9章のword embeddingを自前で作る部分が一番楽しかった&lt;/li&gt;
&lt;li&gt;うろ覚えですが&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;、問題文中に表現が正確でない（と感じる）部分があって、困惑したことがあった&lt;/li&gt;
&lt;li&gt;9割python、1割juliaで書きましたが、sklearn, numpy, scipyなどを使わなくてよい、かつ速度が重要な場合は、簡単に速くできるのでjulia良い&lt;/li&gt;
&lt;li&gt;python、ライブラリが充実しすぎていて本当に楽&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://qiita.com/segavvy/items/fb50ba8097d59475f760&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;素人の言語処理100本ノック:まとめ - Qiita&lt;/a&gt; がとても丁寧で、解いたはいいものの自信がないときなどに、ちょくちょく見ていました。参考になりました&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;今後&#34;&gt;今後&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.amazon.co.jp/dp/4061529242&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;深層学習による自然言語処理&lt;/a&gt;を買ったので&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;、それを読んで、自然言語処理の勉強を続けようと思います。&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;ナイーブな実装多し、コピペ多し、descriptiveでない変数名多し、等&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;掘り返して探す気力がない・・・&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Amazonによると、僕は5/29に買っている模様。なお現在の進捗は0&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Juliaをソースからビルドする / Building Julia from source</title>
      <link>https://r9y9.github.io/blog/2016/12/23/julia-advent-calender-2016-customize-source-build/</link>
      <pubDate>Fri, 23 Dec 2016 18:06:08 +0900</pubDate>
      <guid>https://r9y9.github.io/blog/2016/12/23/julia-advent-calender-2016-customize-source-build/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://qiita.com/advent-calendar/2016/julialang&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Julia advent calendar 2016&lt;/a&gt; 23日目の記事です。&lt;/p&gt;
&lt;h2 id=&#34;はじめに&#34;&gt;はじめに&lt;/h2&gt;
&lt;p&gt;Juilaを最も簡単にインストールする方法は、&lt;a href=&#34;http://julialang.org/downloads/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;公式のダウンロードページ&lt;/a&gt;からバイナリ or インストーラを使用することだと思います。多くの人は、処理系をソースからビルドして使用することはめったにないと思いますが&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;、自分好みにビルドをカスタマイズしてJuliaを使いたいというコアな方向けに、僕がよく使うビルド時のオプションや便利そうなオプション、ビルド時のTipsなどを紹介しようと思います。&lt;/p&gt;
&lt;p&gt;僕がソースからビルドすることになった主な理由は、ソースからビルドしないと使えないパッケージがあったからです&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;h2 id=&#34;下準備&#34;&gt;下準備&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/JuliaLang/julia &amp;amp;&amp;amp; cd julia
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;ビルドのカスタマイズ方法&#34;&gt;ビルドのカスタマイズ方法&lt;/h2&gt;
&lt;p&gt;Juliaのビルドシステムでは、Make.userというファイルで、ユーザがいくらかカスタマイズすることを許可しています。プロジェクトトップにMake.userを作成し、そこに &lt;code&gt;override LLVM_VER=3.7.1&lt;/code&gt; のような書き方で記述することで、カスタマイズ可能です（詳細は&lt;a href=&#34;https://github.com/JuliaLang/julia#source-download-and-compilation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;公式の説明&lt;/a&gt;をご覧ください）。例えば僕の場合、主な開発環境であるmacOSではMake.userを以下のように記述しています（項目の説明は後ほどします）。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;override LLVM_VER=3.7.1
override LLVM_ASSERTIONS=1
override BUILD_LLVM_CLANG=1
override USE_LLVM_SHLIB=1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;あとは、通常通りmakeコマンドを走らせることで、ビルドを行います。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make -j4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;コア数は適当に指定します。llvm, openblasあたりのビルドが結構重いので、並列ビルドがオススメです。&lt;/p&gt;
&lt;h2 id=&#34;僕がよく使うオプション&#34;&gt;僕がよく使うオプション&lt;/h2&gt;
&lt;p&gt;ここから、僕がよく使うオプションをいくつか解説します。&lt;/p&gt;
&lt;h3 id=&#34;llvm_ver&#34;&gt;LLVM_VER&lt;/h3&gt;
&lt;p&gt;llvmのバージョンを表します。Julia上でC++を使いたいというcrazyな人に激推しの &lt;a href=&#34;https://github.com/Keno/Cxx.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Keno/Cxx.jl&lt;/a&gt; というパッケージがあるのですが、このパッケージはclangとllvmの3.7.1以上を必要とします（Cxx.jlについては、過去に何度か記事を書いたので、例えば &lt;a href=&#34;http://r9y9.github.io/blog/2016/01/24/passing-julia-expressions-to-cxx/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cxx.jlを用いてJulia expression/value をC++に埋め込む実験&lt;/a&gt; をご覧ください）。llvm 3.3がデフォルトだったJulia v0.4時代では、明示的に3.7.1と指定する必要がありました。いまは、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Julia v0.5の公式配布バイナリでも、&lt;code&gt;Pkg.add(&amp;quot;Cxx&amp;quot;)&lt;/code&gt;でインストールできるとされている（&lt;a href=&#34;https://github.com/Keno/Cxx.jl/issues/287&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Keno/Cxx.jl/#287&lt;/a&gt;）&lt;/li&gt;
&lt;li&gt;かつ現状のデフォルトバージョンが3.7.1 (もうすぐ3.9.1になりそうですが &lt;a href=&#34;https://github.com/JuliaLang/julia/pull/19678/files&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JuliaLang/julia/#19768&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;なので、僕の場合は明示的にLLVM_VERを指定する必要はなくなってきましたが、例えば、LLVMのNVPTX backendを使ってJuliaでCUDAカーネルを書けるようにする &lt;a href=&#34;https://github.com/JuliaGPU/CUDAnative.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JuliaGPU/CUDAnative.jl&lt;/a&gt; （要 llvm 3.9）のような、experimentalなパッケージを試したい場合など、LLVM_VERを指定したくなる場合もあるかと思います。&lt;/p&gt;
&lt;h3 id=&#34;llvm_assertions&#34;&gt;LLVM_ASSERTIONS&lt;/h3&gt;
&lt;p&gt;LLVMをassert付きでビルドするかどうかを表します。ONにするとビルドかかる時間が長くなり、LLVMのパフォーマンスが若干落ちますが、デバッグには便利です。Juliaのコード生成周りでエラーを起こしやすいようなコードを書くときには、ONにしておくと便利です。&lt;/p&gt;
&lt;h3 id=&#34;build_llvm_clang&#34;&gt;BUILD_LLVM_CLANG&lt;/h3&gt;
&lt;p&gt;llvmとあわせて、clangをビルドするかどうか、というオプションです。Cxx.jlに必要なので、僕はそのためにONにしています。その他必要なケースとしては、clangのaddress/memory sanitizerを使いたい場合が考えられます。詳細は&lt;a href=&#34;http://docs.julialang.org/en/stable/devdocs/sanitizers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;devdocs/sanitizers&lt;/a&gt; をご覧ください。&lt;/p&gt;
&lt;h2 id=&#34;cc-cxx&#34;&gt;CC, CXX&lt;/h2&gt;
&lt;p&gt;コンパイラの指定です。僕の場合 ubuntu 14.04では、（Cxx.jlのために）以下のように設定しています。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;override CC=gcc-6
override CXX=g++-6
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;参考: &lt;a href=&#34;https://github.com/r9y9/julia-cxx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/julia-cxx&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;macOS では特に設定していませんが、Julia以外のプロジェクトをビルドするときに、たまに&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CXX=usr/local/bin/clang++ cmake ${path_to_project}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;のように、xcode付属のclangではなく、自前でビルドしたclangを使いたい場合などに、CC, CXXを指定したりします。&lt;/p&gt;
&lt;h3 id=&#34;use_clang&#34;&gt;USE_CLANG&lt;/h3&gt;
&lt;p&gt;clangを使ってビルドするかどうかを表します。gccを使いたくない、というときにオンにします。&lt;/p&gt;
&lt;h3 id=&#34;use_llvm_shlib&#34;&gt;USE_LLVM_SHLIB&lt;/h3&gt;
&lt;p&gt;llvmを共有ライブラリとしてビルドするかどうかを表します。v0.4ではデフォルトがオフで、v0.5からはオンになっています。llvmの共有ライブラリをdlopenして色々いじりたい場合（何度もアレですが、Cxx.jlを使いたい場合とか）は、オンにする必要があります。&lt;/p&gt;
&lt;h3 id=&#34;use_system_lib_name&#34;&gt;USE_SYSTEM_${LIB_NAME}&lt;/h3&gt;
&lt;p&gt;Juliaでは、デフォルトで依存ライブラリをソースからビルドします。システムにインストールされたライブラリを使用したい場合、USE_SYSTEM_XXX （e.g. &lt;code&gt;USE_SYSTEM_BLAS&lt;/code&gt;）をオンにします。ビルド時間を短縮することが可能です。&lt;/p&gt;
&lt;p&gt;USE_SYSTEM_xxx にどのようなものがあるのかは、&lt;a href=&#34;https://github.com/JuliaLang/julia/blob/d8ecebe1a47fd401ef63a80250c096a21843a82d/Make.inc#L25-L47&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Make.inc&lt;/a&gt; をご覧ください。&lt;/p&gt;
&lt;h2 id=&#34;便利そうなオプション&#34;&gt;便利そうなオプション&lt;/h2&gt;
&lt;h3 id=&#34;use_intel_mkl&#34;&gt;USE_INTEL_MKL&lt;/h3&gt;
&lt;p&gt;MKLを使うかどうかを表します。MKLを持っている場合にオンにすれば、一部パフォーマンスが向上しそうですね。&lt;/p&gt;
&lt;h3 id=&#34;use_gpl_libs&#34;&gt;USE_GPL_LIBS&lt;/h3&gt;
&lt;p&gt;GPLのライブラリ（FFTWなど）を使用するかどうかを表します。使ったことはありませんが、Juliaを組み込みで使用したい場合に、便利かもしれません。&lt;/p&gt;
&lt;h2 id=&#34;ビルド時のtips&#34;&gt;ビルド時のTips&lt;/h2&gt;
&lt;p&gt;Juliaは依存関係が多く、cloneした直後の状態からのビルドには一時間以上かかることもあります&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;。また、masterを追いかけている場合は、途中でビルドにこけてしまうことも珍しくありません。個人的な経験で言えば、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;llvm&lt;/li&gt;
&lt;li&gt;openblas&lt;/li&gt;
&lt;li&gt;libgit2&lt;/li&gt;
&lt;li&gt;mbettls&lt;/li&gt;
&lt;li&gt;libunwind&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;あたりの依存関係のビルドで、何度も失敗しています。僕がソースビルドをし始めたころ、よく調べずに &lt;code&gt;make clean &amp;amp;&amp;amp; make&lt;/code&gt; をして、案の定駄目で、よくわからずに &lt;code&gt;make distcleannall&lt;/code&gt; してしまうこともありました（&lt;code&gt;distcleanall&lt;/code&gt;が必要なケースは稀であり、そうでない場合は非常に時間を無駄にします）&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;。過去の失敗から、僕が学んできたTipsを紹介します。&lt;/p&gt;
&lt;h3 id=&#34;プロジェクトトップmakefileのcleanコマンドを適切に使い分ける&#34;&gt;プロジェクトトップMakefileのcleanコマンドを適切に使い分ける&lt;/h3&gt;
&lt;p&gt;cleanコマンドにはさまざまなものがあります。ビルドのし直しが不要なものまでcleanして、無駄に時間を消費しないように、正しく使い分けましょう。以下、基本的なcleanコマンドを簡単にまとめます。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;コマンド&lt;/th&gt;
&lt;th&gt;説明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;clean&lt;/td&gt;
&lt;td&gt;Julia本体のclean&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;cleanall&lt;/td&gt;
&lt;td&gt;Julia本体、flisp、libuvのclean&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;distclean&lt;/td&gt;
&lt;td&gt;binary配布用の成果物をclean&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;distcleanall&lt;/td&gt;
&lt;td&gt;deps以下の依存関係をすべてclean&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;よほどのことがない限り、 &lt;code&gt;make distcleanall&lt;/code&gt; を使わないようにしましょう。&lt;code&gt;make distclean&lt;/code&gt; はほとんど使う必要はないと思います。&lt;/p&gt;
&lt;p&gt;コマンドの詳細、その他コマンドについては、&lt;a href=&#34;https://github.com/JuliaLang/julia/blob/d8ecebe1a47fd401ef63a80250c096a21843a82d/Makefile&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;julia/Makefile&lt;/a&gt; をご覧ください&lt;/p&gt;
&lt;h3 id=&#34;サブディレクトリのmakefileを使いわける&#34;&gt;サブディレクトリのMakefileを使いわける&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;deps&lt;/strong&gt;: 依存関係&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;src&lt;/strong&gt;: コンパイラ (C, C++, flisp)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;base&lt;/strong&gt;: 標準ライブラリ (Julia)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;doc&lt;/strong&gt;: ドキュメント&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;など（一部です）、Makefileは復数のサブディレクトリにわかれています。依存関係のビルドに失敗した場合には、depsディレクトリ以下のMakefileが使えます。&lt;/p&gt;
&lt;p&gt;depsディレクトリ以下、依存関係のcleanコマンドには、大きく以下の二種類があります。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;コマンド&lt;/th&gt;
&lt;th&gt;説明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;clean-xxx&lt;/td&gt;
&lt;td&gt;xxx の clean&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;distclean-xxx&lt;/td&gt;
&lt;td&gt;xxx の clean と &lt;code&gt;rm -rf ビルドディレクトリ&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;例えばlibgit2のバージョンが変わってエラーが出たからといって、すべてをビルドし直す必要は基本的にはありません。まずは、&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;make -C deps clean-libgit2 &amp;amp;&amp;amp; make
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;としてビルドし直し、それでも駄目な場合は、&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;make -C deps distclean-libgit2 &amp;amp;&amp;amp; make
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;といった具合に、軽いcleanコマンドから順に試しましょう。&lt;/p&gt;
&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Make.user をプロジェクトトップに配置することで、ビルドをカスタマイズできます&lt;/li&gt;
&lt;li&gt;ビルドに失敗したとき、良く考えずに &lt;code&gt;make distcleanall&lt;/code&gt; するのをやめましょう（自戒&lt;/li&gt;
&lt;li&gt;サブディレクトリの Makefile を使い分けて、rebuildは最小限にしましょう&lt;/li&gt;
&lt;/ul&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;大変ですよね&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Keno/Cxx.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/Keno/Cxx.jl&lt;/a&gt; です&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;環境によります&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;僕があほなだけの可能性が大いにあります&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Cxx.jlを用いてJulia expression/value をC&#43;&#43;に埋め込む実験</title>
      <link>https://r9y9.github.io/blog/2016/01/24/passing-julia-expressions-to-cxx/</link>
      <pubDate>Sun, 24 Jan 2016 22:32:08 +0900</pubDate>
      <guid>https://r9y9.github.io/blog/2016/01/24/passing-julia-expressions-to-cxx/</guid>
      <description>&lt;p&gt;Keno氏によるJuliaCon 2015 の発表 &lt;a href=&#34;https://www.youtube.com/watch?v=OB8BclL_Tmo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Keno Fischer: Shaving the Yak&lt;/a&gt; でタイトルの内容が一部紹介されていて、便利そうだなと思い、色々試してみました。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/OB8BclL_Tmo&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;br/&gt;
&lt;p&gt;発表の内容は大まかに、Keno氏がなぜ&lt;a href=&#34;https://github.com/Keno/Cxx.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cxx.jl&lt;/a&gt;を作ったのか、なぜJuliaを始めたのか、といったモチベーションの話から、Cxx.jlでできることについてlive programmingを交えての紹介、といった話になっています。50分とけっこう長いですが、面白いので興味のある方はどうぞ。この記事は、上の動画を見たあと、Cxx.jlと戯れた結果をまとめたものです。&lt;/p&gt;
&lt;p&gt;以下、この記事の目次です。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;前置き：C++をJulia上で使う&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;本編：JuliaのexpressionやvalueをC++に埋め込む&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;前置きが若干長いので、タイトルの内容が知りたい方は、飛ばして下さい。&lt;/p&gt;
&lt;h2 id=&#34;前置きcをjulia上で使う&#34;&gt;前置き：C++をJulia上で使う&lt;/h2&gt;
&lt;p&gt;Cxx.jlを使えば、C++をJulia上で非常にスムーズに扱うことができます。例えば、C++の&lt;code&gt;std::vector&amp;lt;T&amp;gt;&lt;/code&gt;を使いたい、さらにはJuliaの&lt;code&gt;filter&lt;/code&gt;関数を&lt;code&gt;std::vector&amp;lt;T&amp;gt;&lt;/code&gt;に対して使えるようにしたい、といった場合は、以下に示すように、ほんのすこしのコードを書くだけでできます。&lt;/p&gt;
&lt;p&gt;準備：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;using Cxx
import CxxStd: StdVector
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;filter&lt;/code&gt;関数：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-jl&#34;&gt;function Base.filter{T}(f, v::StdVector{T})
    r = icxx&amp;quot;std::vector&amp;lt;$T&amp;gt;();&amp;quot;
    for i in 0:length(v)-1
        if f(T(v[i]))
            push!(r, v[i])
        end
    end
    r
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;なお、&lt;code&gt;filter&lt;/code&gt;関数に出てくる、&lt;code&gt;length&lt;/code&gt;, &lt;code&gt;getindex&lt;/code&gt;, &lt;code&gt;push!&lt;/code&gt; は、Cxx..jlにそれぞれ以下のように定義されています。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-jl&#34;&gt;Base.getindex(it::StdVector,i) = icxx&amp;quot;($(it))[$i];&amp;quot;
Base.length(it::StdVector) = icxx&amp;quot;$(it).size();&amp;quot;
Base.push!(v::StdVector,i) = icxx&amp;quot;$v.push_back($i);&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;計算結果を見やすくするために、&lt;code&gt;show&lt;/code&gt; 関数も定義しておきます。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;function Base.show{T}(io::IO, v::StdVector{T})
    println(io, &amp;quot;$(length(v))-element StdVector{$T}:&amp;quot;)
    for i = 0:length(v)-1
        println(io, T(v[i]))
    end
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;実行結果：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-jlcon&#34;&gt;julia&amp;gt; v = icxx&amp;quot;std::vector&amp;lt;double&amp;gt;{1,2,3,4,5,6,7,8,9,10};&amp;quot;
10-element StdVector{Float64}:
1.0
2.0
3.0
4.0
5.0
6.0
7.0
8.0
9.0
10.0

julia&amp;gt; filter(x -&amp;gt; x &amp;gt; 5, v)
5-element StdVector{Float64}:
6.0
7.0
8.0
9.0
10.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;簡単にできました。とても強力です。&lt;/p&gt;
&lt;p&gt;さて、以降本編に入りたいと思いますが、Julia上でC++を使うのは簡単かつ、Cxx.jlの主な用途だとは思うのですが（少なくとも自分がそうでした）、逆はどうなのでしょうか？実は、limitationはあるものの、かなり面白いことができます。&lt;/p&gt;
&lt;h2 id=&#34;juliaのexpressionやvalueをcに埋め込む&#34;&gt;JuliaのexpressionやvalueをC++に埋め込む&lt;/h2&gt;
&lt;p&gt;まず簡単に、基本的な使い方を整理します。&lt;/p&gt;
&lt;h3 id=&#34;valueを埋める&#34;&gt;valueを埋める&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;$(some_value)&lt;/code&gt; という書き方をします&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-jlcon&#34;&gt;
julia&amp;gt; cxx&amp;quot;&amp;quot;&amp;quot;
       int getRandom() {
           return $(rand(1:10));
       }
       &amp;quot;&amp;quot;&amp;quot;
true
julia&amp;gt; @cxx getRandom()
2
julia&amp;gt; @cxx getRandom()
2
julia&amp;gt; @cxx getRandom()
2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;rand関数を評価したvalueを埋め込んでいるので、何度&lt;code&gt;getRandom&lt;/code&gt;を呼び出しても結果は同じになります。&lt;/p&gt;
&lt;h3 id=&#34;expressionを埋める&#34;&gt;expressionを埋める&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;$:(some_expression)&lt;/code&gt; という書き方をします。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-jlcon&#34;&gt;julia&amp;gt; cxx&amp;quot;&amp;quot;&amp;quot;
       int getReallyRandom() {
           return $:(rand(1:10));
       }
       &amp;quot;&amp;quot;&amp;quot;
true
julia&amp;gt; @cxx getReallyRandom()
1
julia&amp;gt; @cxx getReallyRandom()
9
julia&amp;gt; @cxx getReallyRandom()
2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;期待した通りの動作になっていますね。&lt;/p&gt;
&lt;h2 id=&#34;発展例&#34;&gt;発展例&lt;/h2&gt;
&lt;p&gt;さて、以下、もう少し発展的な例です。&lt;/p&gt;
&lt;h3 id=&#34;c-expressionの中にjuila-expressionを埋めてさらにその中にc-expressionを埋める-1&#34;&gt;C++ expressionの中にJuila expressionを埋めて、さらにその中にC++ expressionを埋める (1)&lt;/h3&gt;
&lt;p&gt;言葉にするとややこしいですが、例を見ればすぐにわかると思います。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-jlcon&#34;&gt;julia&amp;gt; cxx&amp;quot;&amp;quot;&amp;quot;
       void test4(int N) {
           for (int i = 0; i &amp;lt; N; ++i) {
               $:(println(icxx&amp;quot;return i;&amp;quot;); nothing);
           }
       }
       &amp;quot;&amp;quot;&amp;quot;
true

julia&amp;gt; @cxx test4(10)
0
1
2
3
4
5
6
7
8
9
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;簡単に説明すると、C++のfor分の中で、Juliaのprintln関数を読んでいて、さらにprintlnの引数に、C++ expressionが渡されています。&lt;code&gt;icxx&amp;quot;return i;&amp;quot;&lt;/code&gt;という部分が重要で、これは C++ lambda&lt;code&gt;[&amp;amp;](){return i;)}&lt;/code&gt; に相当しています。中々キモい表記ですが、こんなこともできるようです。&lt;/p&gt;
&lt;h3 id=&#34;c-expressionの中にjuila-expressionを埋めてさらにその中にc-expressionを埋める-2&#34;&gt;C++ expressionの中にJuila expressionを埋めて、さらにその中にC++ expressionを埋める (2)&lt;/h3&gt;
&lt;p&gt;もう少し実用的な例です。C++関数の中で、Juliaのプログレスバーを使ってみます。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-jlcon&#34;&gt;julia&amp;gt; using ProgressMeter
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-jlcon&#34;&gt;julia&amp;gt; cxx&amp;quot;&amp;quot;&amp;quot;
       #include &amp;lt;iostream&amp;gt;
       #include &amp;lt;cmath&amp;gt;

       double FooBar(size_t N) {
           double result = 0.0;
           $:(global progress_meter = Progress(icxx&amp;quot;return N;&amp;quot;, 1); nothing);
           for (size_t i = 0; i &amp;lt; N; ++i) {
               result = log(1+i) + log(2+i);
               $:(next!(progress_meter); nothing);
           }
           return result;
       }
       &amp;quot;&amp;quot;&amp;quot;
true

julia&amp;gt; @cxx FooBar(100000000)
Progress: 100% Time: 0:00:18
36.84136149790473
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;プログレスバーについては、&lt;a href=&#34;http://qiita.com/bicycle1885/items/6c7cd3d853e00ddfc250&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Juliaでプログレスバーの表示をする | qiitq&lt;/a&gt; を参考にどうぞ。このコードもなかなかきもいですが、期待した通りに、プログレスバーが表示されます。&lt;/p&gt;
&lt;p&gt;さて、この例からは、Cxx.jlの（現在の）limitationが垣間見えるのですが、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Juliaのexpressionで定義したローカル変数は、C++的には同じ関数スコープであっても、Julia expressionからはアクセス不可（上記例では、&lt;code&gt;progress_meter&lt;/code&gt;をglobalにしないと、for文内のjulia expressionからは&lt;code&gt;progress_meter&lt;/code&gt; にアクセスできません）&lt;/li&gt;
&lt;li&gt;随所にある&lt;code&gt;nothing&lt;/code&gt;にお気づきの人もいると思うのですが、C++ expression内のJulia expressionにさらにC++ expressionを埋め込む場合（※そういったexpressionのことを、&lt;strong&gt;nested expressions&lt;/strong&gt; と呼ぶんだと思います）、返り値は&lt;code&gt;Void&lt;/code&gt;型しか受け付けられません（&lt;code&gt;nothing&lt;/code&gt; をJulia expressionの末尾に置くことで、Julia expressionの返り値を&lt;code&gt;Void&lt;/code&gt;にしています）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;後者について、簡単に例をあげておきます。&lt;/p&gt;
&lt;h4 id=&#34;ネストしていないからok&#34;&gt;ネストしていないからOK&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-jlcon&#34;&gt;julia&amp;gt; cxx&amp;quot;&amp;quot;&amp;quot;
       int getRandom2() {
           int r = $:(rand(1:10));
           return r;
       }
       &amp;quot;&amp;quot;&amp;quot;
true

julia&amp;gt; @cxx getRandom2()
2
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;ネストしているからダメ&#34;&gt;ネストしているからダメ&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-jlcon&#34;&gt;
julia&amp;gt; cxx&amp;quot;&amp;quot;&amp;quot;
       int getRandom3(int hi) {
           int r = $:(rand(1:icxx&amp;quot;return hi;&amp;quot;));
           return r;
       }
       &amp;quot;&amp;quot;&amp;quot;
In file included from :1:
__cxxjl_10.cpp:2:9: error: cannot initialize a variable of type &#39;int&#39; with an rvalue of type &#39;void&#39;
    int r = __julia::call2([&amp;amp;](){ return hi; });
        ^   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
ERROR: Currently only `Void` is supported for nested expressions
 in InstantiateSpecializations at /Users/ryuyamamoto/.julia/v0.5/Cxx/src/cxxstr.jl:268
 [inlined code] from /Users/ryuyamamoto/.julia/v0.5/Cxx/src/cxxstr.jl:723
 in anonymous at /Users/ryuyamamoto/.julia/v0.5/Cxx/src/cxxstr.jl:759
 in eval at ./boot.jl:265
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;nested expressionsで、返り値が&lt;code&gt;Void&lt;/code&gt;以外も取れるようになると、嬉しいなーと思います。&lt;/p&gt;
&lt;h3 id=&#34;c-lambda-に-julia-expressionを埋める&#34;&gt;C++ lambda に Julia expressionを埋める&lt;/h3&gt;
&lt;p&gt;これは現在、間接的にしかできませんでした。以下に例をあげます。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-jlcon&#34;&gt;ulia&amp;gt; for f in [&amp;quot;iostream&amp;quot;, &amp;quot;thread&amp;quot;] cxxinclude(f) end

julia&amp;gt; cxx&amp;quot;&amp;quot;&amp;quot;
       int getRandom() { return $:(rand(1:10)); }
       &amp;quot;&amp;quot;&amp;quot;
true

julia&amp;gt; th = icxx&amp;quot;&amp;quot;&amp;quot;
           std::thread([]{
               for (size_t i = 0; i &amp;lt; 10; ++i) {
                   std::cout &amp;lt;&amp;lt; getRandom() &amp;lt;&amp;lt; std::endl;
               }
            });
       &amp;quot;&amp;quot;&amp;quot;
6
10
5
6
5
3
7
2
6
9
(class std::__1::thread) {
}

julia&amp;gt; @cxx th-&amp;gt;join()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;threadである必要はない例ですが、lambdaの例ということで。間接的にというのは、一度Julia関数をC++関数に埋め込んで、そのC++関数をlambdaの中で呼び出す、という意味です。&lt;/p&gt;
&lt;p&gt;以下のようにJulia expressionを直接埋めようとすると、assertion failureで落ちるてしまうので、注意&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-jlcon&#34;&gt;julia&amp;gt; th = icxx&amp;quot;&amp;quot;&amp;quot;
           std::thread([]{
               for (size_t i = 0; i &amp;lt; 10; ++i) {
                   std::cout &amp;lt;&amp;lt; $:(rand(1:10)) &amp;lt;&amp;lt; std::endl;
               }
            });
       &amp;quot;&amp;quot;&amp;quot;

In file included from :1:
:4:36: error: variable &#39;__juliavar1&#39; cannot be implicitly captured in a lambda with no capture-default specified
            std::cout &amp;lt;&amp;lt; jl_apply0(__juliavar1) &amp;lt;&amp;lt; std::endl;
                                   ^
:1:1: note: &#39;__juliavar1&#39; declared here
^
:2:17: note: lambda expression begins here
    std::thread([]{
                ^
Assertion failed: (V &amp;amp;&amp;amp; &amp;quot;DeclRefExpr not entered in LocalDeclMap?&amp;quot;), function EmitDeclRefLValue, file /Users/ryuyamamoto/julia/deps/srccache/llvm-3.7.1/tools/clang/lib/CodeGen/CGExpr.cpp, line 2000.
zsh: abort      julia-master
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;例はこれで以上です。&lt;/p&gt;
&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;
&lt;p&gt;C++にJuliaを埋め込むといったことは今までほとんどしなかったのですが、今回色々試してみて、いくつかlimitationはあるものの非常に強力だと思いました。興味のある人は、C++にJuliaを埋め込む例は、&lt;a href=&#34;https://github.com/Keno/Gallium.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Keno/Gallium.jl&lt;/a&gt; にいくつか見つかるので、参考になるかもしれません。&lt;/p&gt;
&lt;p&gt;Keno氏の発表、とてもおもしろかったです。先週半ば頃、午前2時半くらいから見始めたのですが、面白くて一気に見てしまいました。いまllvm/clangについて勉強しているので、limitationの部分は、できれば自分でも解決可能かどうか、挑戦してみたいなと思っています。おしまい&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Cxx.jlの著者 &lt;a href=&#34;https://github.com/Keno&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Keno&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Keno/Cxx.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cxx.jl&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=OB8BclL_Tmo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Keno Fischer: Shaving the Yak&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>対話環境でPoint Cloud Library (PCL) を使いたい</title>
      <link>https://r9y9.github.io/blog/2016/01/18/trying-to-use-pcl-in-dynamic-language/</link>
      <pubDate>Mon, 18 Jan 2016 00:44:46 +0900</pubDate>
      <guid>https://r9y9.github.io/blog/2016/01/18/trying-to-use-pcl-in-dynamic-language/</guid>
      <description>&lt;p&gt;新年はじめての記事ということで、少し遅いですが、あけましておめでとうございます。PCLを対話環境で使いたかったので、お正月の間にPCLのラッパーを作りました&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;。なぜ作ったのか、どうやって作ったのか、少し整理して書いてみようと思います。&lt;/p&gt;
&lt;h2 id=&#34;point-cloud-library-pcl-とは&#34;&gt;Point Cloud Library (PCL) とは&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://www.pointclouds.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.pointclouds.org/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;問題&#34;&gt;問題&lt;/h2&gt;
&lt;p&gt;PCL はboost、Eigenに依存している、かつtemplateを多く使用しているため、PCLを使用したプロジェクトのコンパイル時間は非常に長くなるという問題があります。twitterで [PCL コンパイル] として検索すると、例えば以下の様なツイートが見つかりますが、完全に同意です。&lt;/p&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; lang=&#34;en&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;PCLリンクしてるコードのコンパイルに一分半くらいかかる。つらい&lt;/p&gt;&amp;mdash; がらえもん(プログラム書く (@garaemon_coder) &lt;a href=&#34;https://twitter.com/garaemon_coder/status/632064713816305664&#34;&gt;August 14, 2015&lt;/a&gt;&lt;/blockquote&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; lang=&#34;en&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;PCLはC++だしコンパイル遅いしで色々めんどくさい&lt;/p&gt;&amp;mdash; 動かないで点P (@initial_D_0601) &lt;a href=&#34;https://twitter.com/initial_D_0601/status/636013899486105600&#34;&gt;August 25, 2015&lt;/a&gt;&lt;/blockquote&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; lang=&#34;en&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;PCLを使うプロジェクトのコンパイル時間かかりすぎて辛いわ&lt;/p&gt;&amp;mdash; kato tetsuro (@tkato_) &lt;a href=&#34;https://twitter.com/tkato_/status/662545461362847744&#34;&gt;November 6, 2015&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;boostへの依存関係が必須かどうかについては疑問が残りますが、点群処理ではパフォーマンスが求められることが多いと思われるので、C++で書かれていることは合理的に思います。とはいえ、コンパイル時間が長いのは試行錯誤するにはつらいです。&lt;/p&gt;
&lt;h2 id=&#34;ではどうするか&#34;&gt;ではどうするか&lt;/h2&gt;
&lt;p&gt;試行錯誤のサイクルを速く回せるようにすることは僕にとって非常に重要だったのと、 C++で書かなければいけないという制約もなかった（※組み込み用途ではない）ので、対話的にPCLを使うために、僕は動的型付け言語でラッパーを作ることにしました。&lt;/p&gt;
&lt;p&gt;参考までに、対話環境を使うことによるメリットは、下記スライドが参考になります。PCLの紹介もされています&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/vMvYpKqA5aLtI8&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt; &lt;div style=&#34;margin-bottom:5px&#34;&gt; &lt;strong&gt; &lt;a href=&#34;//www.slideshare.net/payashim/ssii-2015-hayashi&#34; title=&#34;コンピュータビジョンの最新ソフトウェア開発環境 SSII2015 チュートリアル hayashi&#34; target=&#34;_blank&#34;&gt;コンピュータビジョンの最新ソフトウェア開発環境 SSII2015 チュートリアル hayashi&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href=&#34;//www.slideshare.net/payashim&#34; target=&#34;_blank&#34;&gt;Masaki Hayashi&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;何で書くか&#34;&gt;何で書くか&lt;/h2&gt;
&lt;p&gt;世の中には色んなプログラミング言語があります。C++ライブラリのラッパー作るぞとなったとき、僕にとって選択肢は、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python&lt;/li&gt;
&lt;li&gt;Julia&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;の二択でした。それぞれ、以下のプロジェクトに頼れば templateを多用したライブラリのラップができそうだと思いました。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://cython.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cython&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Keno/Cxx.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cxx.jl&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;pythonに関しては、すでに cythonで書かれた &lt;a href=&#34;https://github.com/strawlab/python-pcl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;strawlab/python-pcl&lt;/a&gt; というラッパーがあります。しかし、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;現在あまりメンテされていない&lt;/li&gt;
&lt;li&gt;サポートされている機能も多くはない&lt;/li&gt;
&lt;li&gt;templateを多用したライブラリのラップをcythonで十分にできるかどうか自信がなかった &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;Juliaは関数や型がパラメータを持てるため、templateを多用したライブラリのラップが簡単にできそうだと思った（i.e. &lt;code&gt;pcl::PointCloud&amp;lt;T&amp;gt;&lt;/code&gt; は &lt;code&gt;PointCloud{T}&lt;/code&gt; と書ける&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;）&lt;/li&gt;
&lt;li&gt;Cxx.jl を使えば JITライクに C++ を使える（試行錯誤できる）し、Juliaのほうがいいかな&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;といった理由から、Juliaで書くことにしました。&lt;/p&gt;
&lt;h2 id=&#34;成果物&#34;&gt;成果物&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/PCL.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/PCL.jl&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://nbviewer.jupyter.org/gist/r9y9/6ed9a1d0b46993d374f5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;StatisticalOutlierRemovalのデモ | nbviewer&lt;/a&gt; こんな感じで、jupyter上で試行錯誤できるようになりましたとさ&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;。&lt;a href=&#34;https://github.com/strawlab/python-pcl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;strawlab/python-pcl&lt;/a&gt; よりも多くのことができると思います。&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;PCLは非常に大きなライブラリのため、全ての機能をラップするつもりはありませんが、今後必要に応じて機能を追加するかもしれません。&lt;/p&gt;
&lt;h2 id=&#34;適当なスクショ&#34;&gt;適当なスクショ&lt;/h2&gt;
&lt;p&gt;PCL.jl で、少なくとも最低限以下はできますということで。ソースコードは &lt;a href=&#34;https://github.com/r9y9/PCL.jl/tree/master/examples&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/PCL.jl/examples&lt;/a&gt; にあります。&lt;/p&gt;
&lt;h3 id=&#34;pclvisualizer&#34;&gt;PCLVisualizer&lt;/h3&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/milk_cartoon_all_small_clorox.gif&#34; /&gt;&lt;/div&gt;
&lt;h3 id=&#34;3d-object-recognition-based-on-correspondence-grouping&#34;&gt;3D Object Recognition based on Correspondence Grouping&lt;/h3&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/correspondence_grouping.png&#34; /&gt;&lt;/div&gt;
&lt;h3 id=&#34;hypothesis-verification-for-3d-object-recognition&#34;&gt;Hypothesis Verification for 3D Object Recognition&lt;/h3&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/global_hypothesis_verification.png&#34; /&gt;&lt;/div&gt;
&lt;h3 id=&#34;extracting-indices-from-a-pointcloud&#34;&gt;Extracting indices from a PointCloud&lt;/h3&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/extract_indices.png&#34; /&gt;&lt;/div&gt;
&lt;h3 id=&#34;kinect-v2で遊ぶ&#34;&gt;Kinect v2で遊ぶ&lt;/h3&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/rGdsNoK3n9Q&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;&lt;/div&gt;
&lt;br/&gt;
&lt;p&gt;画質低い &amp;amp; クロップが適当で一部しか見えませんが、諸々の処理を含めて fpsは15くらいでしょうか。depthとrgb imageのregistration、その結果の点群への変換に関しては、&lt;del&gt;20~30fps程度でした&lt;/del&gt; 測りなおしたら平均40fpsくらいはでてました。real-timeで点群を処理するようなアプリケーションを書く場合は、現実的にはC++で書くことになるかと思います。&lt;/p&gt;
&lt;h3 id=&#34;余談&#34;&gt;余談&lt;/h3&gt;
&lt;p&gt;Kinect v2 から得たデータを点群に変換するのに、Juliaではパフォーマンスを出すのに苦労したのですが、結果面白い（キモい？）コードができたので、少し話はそれますが簡単に紹介しておきたいと思います。&lt;/p&gt;
&lt;h4 id=&#34;depthとcolorを点群に変換する関数&#34;&gt;Depthとcolorを点群に変換する関数&lt;/h4&gt;
&lt;p&gt;まず、コードを以下に示します。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-jl&#34;&gt;function getPointCloudXYZRGB(registration, undistorted, registered)
    w = width(undistorted)
    h = height(undistorted)
    cloud = pcl.PointCloud{pcl.PointXYZRGB}(w, h)
    icxx&amp;quot;$(cloud.handle)-&amp;gt;is_dense = false;&amp;quot;
    pointsptr = icxx&amp;quot;&amp;amp;$(cloud.handle)-&amp;gt;points[0];&amp;quot;
    icxx&amp;quot;&amp;quot;&amp;quot;
    for (size_t ri = 0; ri &amp;lt; $h; ++ri) {
        for (size_t ci = 0; ci &amp;lt; $w; ++ci) {
            auto p = $(pointsptr) + $w * ri + ci;
            $(registration)-&amp;gt;getPointXYZRGB($(undistorted.handle),
                $(registered.handle), ri, ci, p-&amp;gt;x, p-&amp;gt;y, p-&amp;gt;z, p-&amp;gt;rgb);
        }
    }
    &amp;quot;&amp;quot;&amp;quot;
    cloud
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/PCL.jl/blob/bd6aefc72537761fa81244da512e2002bb1c4817/examples/libfreenect2_grabbar.jl#L12-L29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/PCL.jl/examples/libfreenect2_grabbar.jl#L12-L29&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;syntax highlightとは何だったのか、と言いたくなるようなコードですが、performance heavy な部分は &lt;code&gt;icxx&amp;quot;&amp;quot;&amp;quot;...&amp;quot;&amp;quot;&amp;quot;&lt;/code&gt; という形で、C++ で記述しています。Juliaのコード中で、こんなに自由にC++を使えるなんて、何というかキモいけど書いていて楽しいです。&lt;/p&gt;
&lt;p&gt;なお、最初に書いたコードは、以下の様な感じでした。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-jl&#34;&gt;function getPointCloudXYZRGB(registration, undistorted, registered)
    w = width(undistorted)
    h = height(undistorted)
    cloud = pcl.PointCloud{pcl.PointXYZRGB}(w, h)
    icxx&amp;quot;$(cloud.handle)-&amp;gt;is_dense = true;&amp;quot;
    pointsptr = icxx&amp;quot;&amp;amp;$(cloud.handle)-&amp;gt;points[0];&amp;quot;
    for ri in 0:h-1
        for ci in 0:w-1
            p = icxx&amp;quot;$(pointsptr) + $w * $ri + $ci;&amp;quot;
            x,y,z,r,g,b = getPointXYZRGB(registration, undistorted,
                registered, ri, ci)
            isnan(z) &amp;amp;&amp;amp; icxx&amp;quot;$(cloud.handle)-&amp;gt;is_dense = false;&amp;quot;
            icxx&amp;quot;&amp;quot;&amp;quot;
            $p-&amp;gt;x = $x;
            $p-&amp;gt;y = $y;
            $p-&amp;gt;z = $z;
            $p-&amp;gt;r = $r;
            $p-&amp;gt;g = $g;
            $p-&amp;gt;b = $b;
            &amp;quot;&amp;quot;&amp;quot;
        end
    end
    cloud
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/PCL.jl/blob/bd6aefc72537761fa81244da512e2002bb1c4817/examples/libfreenect2_grabbar.jl#L12-L29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/PCL.jl/examples/libfreenect2_grabbar.jl#L12-L29&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;このコードだと、forループの中でJulia関数の呼びだしが発生するため、実は重たい処理になっています。このコードだと、確かfps 3 とかそのくらいでした。関数呼び出しがボトルネックだと気づいて、&lt;code&gt;icxx&amp;quot;&amp;quot;&amp;quot;...&amp;quot;&amp;quot;&amp;quot;&lt;/code&gt; でくるんで（一つの関数にすることで）高速化を図った次第です。&lt;/p&gt;
&lt;h2 id=&#34;雑記&#34;&gt;雑記&lt;/h2&gt;
&lt;p&gt;以下、僕のmacbook proで &lt;code&gt;tic(); using PCL; toc()&lt;/code&gt; をした結果：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-jl&#34;&gt;julia&amp;gt; tic(); using PCL; toc()
INFO: vtk include directory found: /usr/local/include/vtk-6.3
INFO: Loading Cxx.jl...
INFO: dlopen...
INFO: vtk version: 6.3.0
INFO: Including headers from system path: /usr/local/include
INFO: pcl_version: 1.8
INFO: Include pcl top-level headers
  1.053026 seconds (91 allocations: 4.266 KB)
INFO: Include pcl::common headers
  5.433219 seconds (91 allocations: 4.078 KB)
INFO: adding vtk and visualization module headers
INFO: Include pcl::io headers
  0.389614 seconds (195 allocations: 11.034 KB)
INFO: Include pcl::registration headers
  1.428106 seconds (195 allocations: 11.065 KB)
INFO: Include pcl::recognition headers
  1.154518 seconds (136 allocations: 6.141 KB)
INFO: Include pcl::features headers
  0.033937 seconds (181 allocations: 8.094 KB)
INFO: Include pcl::filters headers
  0.070545 seconds (316 allocations: 14.125 KB)
INFO: Include pcl::kdtree headers
  0.022809 seconds (91 allocations: 4.078 KB)
INFO: Include pcl::sample_consensus headers
  0.014600 seconds (91 allocations: 4.141 KB)
INFO: Include pcl::segmentation headers
  0.010710 seconds (46 allocations: 2.094 KB)
INFO: FLANN version: 1.8.4
elapsed time: 39.194405845 seconds
39.194405845
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/PCL.jl/blob/9760565dd3b744e16733c54992551e4e0babc7ee/src/PCL.jl#L90-L101&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/PCL.jl/src/PCL.jl#L90-L101&lt;/a&gt; pcl/pcl_base.h. pcl/common/common_headers.h 当たりのパースに大分時間かかってますね、、。まぁ一度ロードしてしまえば、Juliaのプロセスをkillしないかぎり問題ないのですが。開発中は、頻繁にreloadする必要があって、辛かったです。&lt;/p&gt;
&lt;p&gt;ロード時間が長い問題は、Cxx.jlにプリコンパイル（&lt;a href=&#34;https://github.com/Keno/Cxx.jl/issues/181&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Keno/Cxx.jl/issues/181&lt;/a&gt;）がサポートされれば、改善するかもしれません。&lt;/p&gt;
&lt;h2 id=&#34;さいごに&#34;&gt;さいごに&lt;/h2&gt;
&lt;p&gt;PCLを対話環境で使えるようになりました。快適です。また今回のラッピングを通して、PCLとは関係ありませんが、&lt;a href=&#34;https://github.com/Keno/Cxx.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cxx.jl&lt;/a&gt; でできないことはほぼないという所感を持ちました。C++ の対話環境（REPL）も付いているので、最強すぎますね。Cythonでもできるぞってことであれば、教えて下さい。僕もpythonから使えるのであれば使いたいです（でも作るのは面倒過ぎる気がするので手を出せない）。&lt;/p&gt;
&lt;p&gt;僕にとって快適な環境はできましたが、&lt;a href=&#34;https://github.com/Keno/Cxx.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cxx.jl&lt;/a&gt; のビルドはかなり面倒なので（Juliaの開発版も必要ですし…）、きっと誰も使わないんだろうなー、、、&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Keno/Cxx.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Keno/Cxx.jl&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://r9y9.github.io/blog/2015/12/22/cxx-jl/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cxx.jl を使ってみた感想 + OpenCV.jl, Libfreenect2.jl の紹介&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;僕、ラッパー作ってばっかり…&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;opencvはpythonラッパーについて触れられているのに、PCLのラッパーは無いだと？うーむ、じゃあ作ってみるかーと、思った気もします。&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;公式にサポートはされていますが、過去にcythonではまったことがあるので、懐疑的&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;cythonでも同じようにかけますが、pythonだと&lt;code&gt;PointCloud(dtype=T)&lt;/code&gt;みたいに書くことになるんですかね&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;PCLVisualizerはGUIで使った方が便利なので、JuliaのREPLから使うことが多いですが&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;python-pclよりもインストールは大変だと思いますが…&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Cxx.jl を使ってみた感想 &#43; OpenCV.jl, Libfreenect2.jl の紹介</title>
      <link>https://r9y9.github.io/blog/2015/12/22/cxx-jl/</link>
      <pubDate>Tue, 22 Dec 2015 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2015/12/22/cxx-jl/</guid>
      <description>&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/opencvjl_demo.jpg &#34;OpenCV.jl based on Cxx.jl&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;h2 id=&#34;はじめに&#34;&gt;はじめに&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://qiita.com/advent-calendar/2015/julialang&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Julia Advent Calendar 2015&lt;/a&gt; 22日目の記事です。&lt;/p&gt;
&lt;p&gt;Julia の C++ FFI (Foreign Function Interface) である &lt;a href=&#34;https://github.com/Keno/Cxx.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cxx.jl&lt;/a&gt; をしばらく使ってみたので、その感想を書きます。加えて、Cxx.jl を使って作った成果物の話も簡単に書こうと思います（冒頭に貼った画像は、OpenCV.jl でテキトーにカメラから画像をキャプチャしてthresholdingしたやつです）。
Cxx.jl の動作原理については、僕の理解が不十分なため簡単にしか紹介できませんが、そもそも使ったことがある人が稀だと思われるので、感想程度でも役に立てば幸いです。&lt;/p&gt;
&lt;h2 id=&#34;cxxjl-とは&#34;&gt;Cxx.jl とは&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Keno/Cxx.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/Keno/Cxx.jl&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;簡単に説明すると、Cxx.jl とは、Julia から C++ を使用する（e.g. 関数呼び出し、メソッド呼び出し、メンバ変数へのアクセス、etc) ための機能を提供するパッケージです。C++のライブラリを活用したい、あるいはパフォーマンスがシビアな箇所で一部 C++ 使いたい（Cインタフェースを作りたくない&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;）、といった場合に便利です。&lt;/p&gt;
&lt;p&gt;Cxx.jl の原理についてざっくりといえば、clang を用いて C++ から LLVM IR を生成し、llvmcall を用いて（Just in time に）コードを実行する、という方式のようです&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Cxx.jl の原理について知りたい場合は、Cxx.jl のソースコード（+コメント）を、Cxx.jl を使うと何ができるのか知りたい場合は、Cxx.jl の README を御覧ください。&lt;/p&gt;
&lt;p&gt;以下、過去を思い出しながら感想を書いてみます&lt;/p&gt;
&lt;h2 id=&#34;実際に使う前に&#34;&gt;実際に使う前に&lt;/h2&gt;
&lt;h3 id=&#34;pkgbuildcxx-を成功させることが困難&#34;&gt;Pkg.build(&amp;ldquo;Cxx&amp;rdquo;) を成功させることが困難&lt;/h3&gt;
&lt;p&gt;そもそも使いはじめる前に、ビルドすることが困難でした。Cxx.jl を動作させるためには、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;julia&lt;/li&gt;
&lt;li&gt;llvm&lt;/li&gt;
&lt;li&gt;clang&lt;/li&gt;
&lt;li&gt;lldb&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;の開発版が必要ですが、ビルドが難しい大きな原因は、動作することが保証された&lt;strong&gt;明確な revision が存在しない&lt;/strong&gt;ことにあります。（なんじゃそれ、と思うかもしれませんが、まぁまだ安定版はリリースされていないので、、）&lt;/p&gt;
&lt;p&gt;今でこそ、llvm, clang, lldbは、Keno氏の fork の kf/gallium ブランチ使えばいいよと README に書いてありますが、僕が使い始めた二ヶ月ほど前は、開発版のllvmが必要だよ、くらいにしか書いていませんでした（参考: &lt;a href=&#34;https://github.com/Keno/Cxx.jl/blob/3897e8720b683fe35e407f2128d14e41cec8e0dd/README.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cxx.jl/README&lt;/a&gt;）。何度もllvmをビルドし直すのは、本当に苦行でした…&lt;/p&gt;
&lt;p&gt;参考：&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; lang=&#34;en&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;今日だけでllvmをn回ビルドしてる（ビルドできたとは言ってない&lt;/p&gt;&amp;mdash; 山本りゅういち (@r9y9) &lt;a href=&#34;https://twitter.com/r9y9/status/655000313112367104&#34;&gt;October 16, 2015&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;br&gt;
&lt;p&gt;思考停止の様子：&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;make -C deps clean-llvm &amp;amp; make -j4&lt;/p&gt;&amp;mdash; 山本りゅういち (@r9y9) &lt;a href=&#34;https://twitter.com/r9y9/status/670571501658251264&#34;&gt;November 28, 2015&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;br&gt;
&lt;h3 id=&#34;cxxjl-のビルドはどうするのが一番簡単なのか&#34;&gt;Cxx.jl のビルドはどうするのが一番簡単なのか&lt;/h3&gt;
&lt;p&gt;さて、さらっと書きましたが、今では llvm, clang, lldb　のkf/gallium　ブランチを使えば、比較的簡単に、多少の試行錯誤&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; で Cxx.jl をビルドして使えます。&lt;/p&gt;
&lt;h3 id=&#34;開発版-llvm-と一緒に-julia-をビルドする&#34;&gt;開発版 llvm と一緒に Julia をビルドする&lt;/h3&gt;
&lt;p&gt;Juliaをクローンしたディレクトリで、以下の様な &lt;code&gt;Make.user&lt;/code&gt; ファイルを作成して make します。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;override LLDB_VER=master
override LLVM_VER=svn
override LLVM_ASSERTIONS=1
override BUILD_LLVM_CLANG=1
override BUILD_LLDB=1
override USE_LLVM_SHLIB=1
override LLDB_DISABLE_PYTHON=1

override LLVM_GIT_URL_LLVM=https://github.com/JuliaLang/llvm.git
override LLVM_GIT_URL_LLDB=https://github.com/JuliaLang/lldb.git
override LLVM_GIT_URL_CLANG=https://github.com/JuliaLang/clang.git
override LLVM_GIT_VER=kf/gallium
override LLVM_GIT_VER_LLDB=kf/gallium
override LLVM_GIT_VER_CLANG=kf/gallium
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;一部、LLVM_ASSERSONS を有効にするなど、必ずしも必須でないものも含まれていますが、こちらが現状の推奨のようです。この設定で、僕はubuntu 14.04, osx 10.10 でビルドが通ることを確認しました&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;注意：すでに llvm や clang がローカルにクローン済の場合、&lt;code&gt;deps/srccache&lt;/code&gt; 以下からクローン済みのソースを消してからビルドすることをおすすめします。すでにクローンされていて、upstream  の変更を取り入れたい場合は、&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make -C deps update-llvm
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;とすると便利です。&lt;/p&gt;
&lt;h3 id=&#34;cxxjl-のインストール&#34;&gt;Cxx.jl のインストール&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-jl&#34;&gt;Pkg.clone(&amp;quot;https://github.com/Keno/Cxx.jl.git&amp;quot;)
Pkg.build(&amp;quot;Cxx&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;エラーがでなければ、インストール完了&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;です。&lt;/p&gt;
&lt;h2 id=&#34;実際に使ってみたあと&#34;&gt;実際に使ってみたあと&lt;/h2&gt;
&lt;p&gt;さて、ようやくビルドもできて、ここからは使ってしばらくしての感想です。&lt;/p&gt;
&lt;h3 id=&#34;julia-上で-c-の-syntax-がそのまま使える&#34;&gt;Julia 上で C++ の syntax がそのまま使える&lt;/h3&gt;
&lt;p&gt;まず、簡単に Cxx.jl の機能を挙げると、重要なのは&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;cxx&amp;quot;...&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;icxx&amp;quot;...&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;@cxx&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;の三つです。以下、簡単に例をあげると、&lt;code&gt;cxx&amp;quot;...&amp;quot;&lt;/code&gt; でC++ syntax を評価して：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;cxx&amp;quot;#include &amp;lt;iostream&amp;gt;&amp;quot;

cxx&amp;quot;&amp;quot;&amp;quot;
namespace test {
void f() {
    std::cout &amp;lt;&amp;lt; &amp;quot;Hello C++&amp;quot; &amp;lt;&amp;lt; std::endl;
}
}
&amp;quot;&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;@cxx&lt;/code&gt; マクロで C++ 関数を呼び出す：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-jl&#34;&gt;@cxx test::f()  # Hello C++
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;cxx&amp;quot;...&amp;quot;&lt;/code&gt;はグローバルスコープで評価されますが、&lt;code&gt;icxx&amp;quot;...&amp;quot;&lt;/code&gt; を使えば、特定のスコープ内で C++ を使用することもできます。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-jl&#34;&gt;for i in 1:10
    icxx&amp;quot;&amp;quot;&amp;quot;std::cout &amp;lt;&amp;lt; $i &amp;lt;&amp;lt; std::endl;&amp;quot;&amp;quot;&amp;quot;
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;ccall&lt;/code&gt; のように、返り値、引数の型などを指定して実行するのではなく、C++ のsyntax をそのまま使ってコードが書ける、という点にびっくりしました。&lt;/p&gt;
&lt;h3 id=&#34;template-も使える&#34;&gt;template も使える&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;cxx&amp;quot;&amp;quot;&amp;quot;
template &amp;lt;typename T&amp;gt;
T add(T x, T y) { return x + y; }
&amp;quot;&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;こんな感じで特殊化も可能&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;cxx&amp;quot;&amp;quot;&amp;quot;
template &amp;lt;&amp;gt;
int add&amp;lt;int&amp;gt;(int x, int y) { return x + y; }
&amp;quot;&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;書いてて気付きましたが、README には template について言及されていませんね。僕は、今のところ問題なく使えています。例には出していませんが、template class ももちろん使えます（例. &lt;code&gt;std::vector&amp;lt;T&amp;gt;&lt;/code&gt;）。&lt;/p&gt;
&lt;h3 id=&#34;その他雑記&#34;&gt;その他雑記&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Cxx.jl で使える C++ には制約がある（はず）だが、ここ二ヶ月使用した限りでは、大きな制約に出会ってないし、快適&lt;/li&gt;
&lt;li&gt;共有ライブラリの呼び出しは、&lt;code&gt;ccall&lt;/code&gt; と違ってライブラリだけでなくヘッダーファイルも必要&lt;/li&gt;
&lt;li&gt;&lt;code&gt;using Cxx&lt;/code&gt; にはけっこう時間がかかる。僕の環境では約15秒だった&lt;/li&gt;
&lt;li&gt;たまに llvm error を吐いて落ちる。デバッグするには llvm, clang についてある程度知識がないと難しそう&lt;/li&gt;
&lt;li&gt;C++ REPL 便利&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;という感じですかね。書き進むに連れて適当になってすいません、、、&lt;/p&gt;
&lt;h2 id=&#34;cxxjl-を使って作った成果物&#34;&gt;Cxx.jl を使って作った成果物&lt;/h2&gt;
&lt;p&gt;まとめに入る前に、Cxx.jl を使って遊ぶ過程で作った成果物を、簡単なコメント付きで紹介します。&lt;/p&gt;
&lt;h3 id=&#34;opencvjl&#34;&gt;OpenCV.jl&lt;/h3&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/video_thresholding.gif &#34;OpenCV.jl demo&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/OpenCV.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/OpenCV.jl&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Cxx.jl の学習の題材として作りました。途中から cv::Mat 周りを真面目に作り始めたので、それなりに使えると思います。&lt;/p&gt;
&lt;p&gt;デザインポリシーとして、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cv::Mat を &lt;code&gt;AbstractArray{T,N}&lt;/code&gt; の subtype として Julia ライクに使えること&lt;/li&gt;
&lt;li&gt;cv::Mat と Julia の Array の相互変換をサポートすること&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;を念頭において作りました。&lt;/p&gt;
&lt;h3 id=&#34;libfreenect2jl&#34;&gt;LibFreenect2.jl&lt;/h3&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/depth_streaming_example.gif &#34;LibFreenect2.jl demo&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/LibFreenect2.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/LibFreenect2.jl&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ちょうど仕事で kinect v2 を触っていたので、遊びがてらやってみました。&lt;/p&gt;
&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Cxx.jl をビルドするのはけっこう面倒ですが、C++を（主観ですが）ほとんど不自由なくJITライクに使えるので、非常に便利です&lt;/li&gt;
&lt;li&gt;二つほど C++ ライブラリのラッパーを作ってみましたが、簡単にできるので、みなさんもお試しあれ&lt;/li&gt;
&lt;li&gt;レッツ・トライ Cxx.jl！&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;おまけ&#34;&gt;おまけ&lt;/h2&gt;
&lt;p&gt;現在 Julia community では、llvm 3.3 から llvm 3.7.1 に移行しようとする動きがあるので（ref: &lt;a href=&#34;https://github.com/JuliaLang/julia/issues/9336&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;julia/issues/9336&lt;/a&gt;, &lt;a href=&#34;https://github.com/JuliaLang/julia/pull/14430&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;julia/pull/14430&lt;/a&gt;）、移行後は、もう少しビルドが楽になるかもしれません。&lt;/p&gt;
&lt;p&gt;実は、この記事を読んでも、きっと今は　Cxx.jl をビルドできないんじゃないかなと思うんですが、もしどうしてもビルドしたい、ということであれば、僕のローカルの llvm, clang, lldb, julia の revision を調べて教えるので、言ってください。&lt;/p&gt;
&lt;h3 id=&#34;20151228-追記&#34;&gt;2015/12/28 追記&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://qiita.com/r9y9/items/37633ed37e22612b5224&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cxx.jl を動作させるための julia, llvm, clang, lldb のコミットハッシュ | qiita&lt;/a&gt;&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;例えば template を多用している場合、Cインタフェースを作るのは面倒です&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;※正確に理解していないため、あまり宛てにしないでください）&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;多少の試行錯誤、というのは、Julia と Cxx.jl のリビジョンは、経験的には必ずしも（特にJuliaの）masterで動作しないので、Julia と Cxx.jl を master から少し遡って、ビルドできるか試行錯誤する、という意味です&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;ビルドが通ったことがある、の方が正確ですが&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;なお、現状のJulia masterとCxx.jl masterでは、エラーが出ると踏んでおります、、、&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Julia: 値と変数に対する Type Annotation の違い</title>
      <link>https://r9y9.github.io/blog/2015/12/08/julia-type-annotations/</link>
      <pubDate>Tue, 08 Dec 2015 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2015/12/08/julia-type-annotations/</guid>
      <description>&lt;h2 id=&#34;はじめに&#34;&gt;はじめに&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://qiita.com/advent-calendar/2015/julialang&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Julia Advent Calendar 2015&lt;/a&gt; 8日目の記事です。&lt;/p&gt;
&lt;p&gt;この記事では、値 (value) と変数 (variable) に対する type annotation の違いを、問題とそれに対する解答を用意する形式で説明しようと思います。そんなの知ってるぜ！という方は、問題だけ解いてみて自分の理解度を試してもらえればと思います。&lt;/p&gt;
&lt;p&gt;記事に出てくるJuliaコードは、Julia 0.5-dev, 0.4.0 で動作確認しました。&lt;/p&gt;
&lt;h2 id=&#34;問題&#34;&gt;問題&lt;/h2&gt;
&lt;p&gt;新規REPLセッションを開いて、A、B それぞれを実行したときの挙動はどうなるでしょうか？エラーの発生の有無と、エラーが発生しない場合は返り値の値、型を答えてください。&lt;/p&gt;
&lt;h3 id=&#34;a&#34;&gt;A&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-jl&#34;&gt;function f()
    x = (1.0 + 2.0)::Int
    return x
end

f()
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;b&#34;&gt;B&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-jl&#34;&gt;function g()
    x::Int = (1.0 + 2.0)
    return x
end

g()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;なお、一方ではエラーが起き、もう一方はエラー無く実行されます。一見似たような書き方ですが、二つは異なる意味を持ちます。この記事ではそれぞれを解説しようと思います。&lt;/p&gt;
&lt;p&gt;この問題の答えがわからなかった方は、この記事を読むと正解がわかるはずなので、続きをご覧ください。下の方に、簡潔な問題の解答とおまけ問題を書いておきました。&lt;/p&gt;
&lt;h2 id=&#34;a-値に対する-type-annotation&#34;&gt;A: 値に対する type annotation&lt;/h2&gt;
&lt;p&gt;Aの2行目では、値に対して type annotation をしています。これは typeassert とも呼びます。Aで使った type annotation を日本語で説明してみると、「&lt;code&gt;(1.0 + 2.0)&lt;/code&gt; という式を評価した値は、Int 型であることを保証する」となります。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;(1.0 + 2.0)&lt;/code&gt; を評価した値は &lt;code&gt;3.0&lt;/code&gt; であり、 Float64の型を持ちます。したがって &lt;code&gt;Float64 != Int&lt;/code&gt; であるため、&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ERROR: TypeError: typeassert: expected Int64, got Float64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;のような typeassert のエラーが吐かれます。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;(1.0 + 2.0)&lt;/code&gt;を評価した値の型は一見して明らかため、実用的な例ではありませんが、例えば関数の返り値の型は一見してわからないことがあるので、例えば以下のような書き方は有用な場合もあると思います。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-jl&#34;&gt;x = f(y)::Int
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;b-変数に対する-type-annotation&#34;&gt;B: 変数に対する type annotation&lt;/h2&gt;
&lt;p&gt;Bの2行目では、変数に対して type annotation をしています。同じく日本語で説明すると、「&lt;code&gt;x&lt;/code&gt;という変数に入る値は、Int 型であることを保証する」となります。また、値に対する annotation とは異なり&lt;strong&gt;スコープ&lt;/strong&gt;を持ちます。&lt;/p&gt;
&lt;p&gt;前述したとおり、&lt;code&gt;(1.0 + 2.0)&lt;/code&gt; を評価した値は &lt;code&gt;3.0&lt;/code&gt; であり、Float64の型を持ちます。一方で、&lt;code&gt;x&lt;/code&gt; は Int型の値を持つ変数として宣言されているため、この場合、Float64型である &lt;code&gt;(1.0 + 2.0)&lt;/code&gt; を、Int 型に変換するような処理が&lt;strong&gt;暗黙的に&lt;/strong&gt;行われます。したがって、変換可能な場合には（B の例がそうです）、エラーは起きません。暗黙的に処理が行われるというのは、知らないと予期せぬバグに遭遇することになるため、気をつける必要があります。&lt;/p&gt;
&lt;p&gt;では、変数に対する type annotation はどのような場合に使うかというと、あるスコープの範囲で、代入によって変数の型が変わってしまうのを防ぐために使います。ある変数の型がスコープの範囲で不変というのはコンパイラにとっては嬉しい事で、パフォーマンスの向上に繋がります。Performance tips にもありますね（参考: &lt;a href=&#34;http://docs.julialang.org/en/release-0.4/manual/performance-tips/#avoid-changing-the-type-of-a-variable&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Performance tips / Avoid changing the type of a variable&lt;/a&gt;）&lt;/p&gt;
&lt;h2 id=&#34;違いまとめ&#34;&gt;違いまとめ&lt;/h2&gt;
&lt;p&gt;ここまでの話から、違いをまとめると、以下のようになります。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Type annotation の種類&lt;/th&gt;
&lt;th&gt;typeassert error　&lt;/th&gt;
&lt;th&gt;暗黙的な型変換　&lt;/th&gt;
&lt;th&gt;スコープ　&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;値に対する type annotation&lt;/td&gt;
&lt;td&gt;あり&lt;/td&gt;
&lt;td&gt;なし&lt;/td&gt;
&lt;td&gt;なし&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;変数に対する type annotation 　&lt;/td&gt;
&lt;td&gt;なし&lt;/td&gt;
&lt;td&gt;あり&lt;/td&gt;
&lt;td&gt;あり&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h2 id=&#34;最後に&#34;&gt;最後に&lt;/h2&gt;
&lt;p&gt;type annotation を使うときは、値と変数に対する annotation の違いを意識して、使い分けましょう&lt;/p&gt;
&lt;h2 id=&#34;問題の解答&#34;&gt;問題の解答&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A: typeassert に引っかかり、TypeError が吐かれる&lt;/li&gt;
&lt;li&gt;B: Int 型の 3 が返り値として得られる&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;おまけ問題&#34;&gt;おまけ問題&lt;/h2&gt;
&lt;h3 id=&#34;1&#34;&gt;1&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-jl&#34;&gt;function h()
    x::UInt8 = UInt8(0)
    x = Float64(0.0)
    x
end
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-jl&#34;&gt;# なんと表示されるでしょうか？
println(typeof(h()))
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;2&#34;&gt;2&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-jl&#34;&gt;function s()
    x::Int = Float64(0)
    x = UInt8(0)
    x = Float32(0.5)
    x
end
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-jl&#34;&gt;# なんと表示されるでしょうか？
s()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;解答は、各自REPLで実行して確認してみてください。長々と読んでくださりありがとうございました。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://docs.julialang.org/en/release-0.4/manual/types/?highlight=typeassert#type-declarations&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;公式ドキュメント / Type Declarations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>pysptk: SPTKのpythonラッパーを作った (part 2)</title>
      <link>https://r9y9.github.io/blog/2015/09/06/pysptk/</link>
      <pubDate>Sun, 06 Sep 2015 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2015/09/06/pysptk/</guid>
      <description>&lt;p&gt;2015/09/05:&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://t.co/WFBmYEIVce&#34;&gt;https://t.co/WFBmYEIVce&lt;/a&gt; SPTKのpythonラッパー（マシなやつ）完成&lt;br&gt;ドキュメント &lt;a href=&#34;http://t.co/jYhw1y3Bzg&#34;&gt;http://t.co/jYhw1y3Bzg&lt;/a&gt;&lt;br&gt;pip install pysptk でインストールできるようになりました。pypi童貞捨てれた&lt;/p&gt;&amp;mdash; 山本 龍一 / Ryuichi Yamamoto (@r9y9) &lt;a href=&#34;https://twitter.com/r9y9/status/639848868075560960?ref_src=twsrc%5Etfw&#34;&gt;September 4, 2015&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;ずいぶん前に、swig遊びをしがてらpythonのラッパーを書いていたんですが、cythonを使って新しく作りなおしました。かなりパワーアップしました。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install pysptk
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;でインストールできるので、よろしければどうぞ&lt;/p&gt;
&lt;h2 id=&#34;なぜ作ったのか&#34;&gt;なぜ作ったのか&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;cythonとsphinxで遊んでたらできた&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;使い方&#34;&gt;使い方&lt;/h2&gt;
&lt;p&gt;以下のデモを参考にどうぞ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/r9y9/pysptk/blob/51c103e5a7e9746c96cd78043df4e48fe2d6a3a8/examples/pysptk%20introduction.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Introduction to pysptk&lt;/a&gt;: メル一般化ケプストラム分析とか&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/r9y9/pysptk/blob/51c103e5a7e9746c96cd78043df4e48fe2d6a3a8/examples/Speech%20analysis%20and%20re-synthesis.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Speech analysis and re-synthesis&lt;/a&gt;: 音声の分析・再合成のデモ。合成音声はnotebook上で再生できます&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ドキュメント&#34;&gt;ドキュメント&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://pysptk.readthedocs.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://pysptk.readthedocs.org&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;ぼやき&#34;&gt;ぼやき&lt;/h2&gt;
&lt;p&gt;SPTKの関数、変な値入れるとexitしたりセグフォったりするので、ちゃんとテスト書いてほしいなあ&lt;/p&gt;
&lt;h2 id=&#34;関連&#34;&gt;関連&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://r9y9.github.io/blog/2014/08/10/sptk-from-python/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SPTKのPythonラッパーを書いた - LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>最近の音声信号処理遊びの進捗</title>
      <link>https://r9y9.github.io/blog/2015/08/23/speech-analysis-and-synthesis-in-julia/</link>
      <pubDate>Sun, 23 Aug 2015 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2015/08/23/speech-analysis-and-synthesis-in-julia/</guid>
      <description>&lt;h2 id=&#34;hello&#34;&gt;hello&lt;/h2&gt;
&lt;p&gt;遡ればもう約一年まえになるでしょうか、統計的声質遊びをしたいと思い、理論の勉強を始めたり、（特にJuliaで）コードを色々書いていました（お前ほんといろんな言語で遊んでるな、というツッコミはさておき）。&lt;a href=&#34;http://r9y9.github.io/blog/2014/11/12/statistical-voice-conversion-code/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;統計的声質変換クッソムズすぎワロタ（チュートリアル編） - LESS IS MORE&lt;/a&gt; を書いていた当初は、当然自分のためだけに書いていて、まぁアレな出来でしたが、最近気を取り直して多少マシに仕上げましたので、何となくブログに書いてみようかなーと思った次第です。というわけで、最近公式に登録したいくつかのパッケージを、まとめて簡単に紹介します。&lt;/p&gt;
&lt;p&gt;主な改善点は、windowsもちゃんとサポートするようにしたこと（誰得？）と、テストをきちんと書いたことと、julia的なインタフェースを意識するようにしたことですかね。3つ目はかなり曖昧ですが、まぁ気持ち使いやすくなったと思います。&lt;/p&gt;
&lt;h2 id=&#34;パッケージ&#34;&gt;パッケージ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/MelGeneralizedCepstrums.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MelGeneralizedCepstrums.jl&lt;/a&gt;: メル一般化ケプストラム分析&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/SynthesisFilters.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SynthesisFilters.jl&lt;/a&gt;: メル一般化ケプストラムからの音声波形合成&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/SPTK.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SPTK.jl&lt;/a&gt;: &lt;a href=&#34;http://sp-tk.sourceforge.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SPTK&lt;/a&gt;のラッパー&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;車輪の再発明はできるだけしたくなかったので、最初のほうはCライブラリのラッパーを書くことが多く、windowsとかめんどくさいしunix環境でしか動作確認してませんでしたが、&lt;a href=&#34;http://qiita.com/r9y9/items/e0567e2a21a5e3c36e51&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WindowsのJuliaから呼べるようなCライブラリの共有ライブラリ（DLL）を作る | qiita&lt;/a&gt; 重い腰を上げてwindowsでも動くように頑張ったことがあり（めんどくさいとか言って手を動かさないのホント良くないですね）、登録したパッケージはすべてwindowsでも動くようになりました。めでたし。&lt;a href=&#34;https://github.com/r9y9/WORLD.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WORLD.jl&lt;/a&gt; もwindowsで動くようにしました。&lt;/p&gt;
&lt;h2 id=&#34;melgeneralizedcepstrumsjl&#34;&gt;MelGeneralizedCepstrums.jl&lt;/h2&gt;
&lt;p&gt;メルケプストラムの推定とか。いくつか例を載せておきます&lt;/p&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/r9y9/MelGeneralizedCepstrums.jl/v0.0.1/examples/cepstrum.png&#34; alt=&#34;cepstrum based envelope.&#34; class=&#34;image&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/r9y9/MelGeneralizedCepstrums.jl/v0.0.1/examples/mel-cepstrum.png&#34; alt=&#34;mel-cepstrum based envelope.&#34; class=&#34;image&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/r9y9/MelGeneralizedCepstrums.jl/v0.0.1/examples/mel-generalized-cepstrum.png&#34; alt=&#34;mel-generalized-cepstrum based envelope.&#34; class=&#34;image&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/r9y9/MelGeneralizedCepstrums.jl/v0.0.1/examples/lpc-cepstrum.png&#34; alt=&#34;lpc-cepstrum based envelope.&#34; class=&#34;image&#34;&gt;
&lt;p&gt;詳細はこちらの&lt;a href=&#34;http://nbviewer.ipython.org/github/r9y9/MelGeneralizedCepstrums.jl/blob/v0.0.1/examples/Introduction%20to%20MelGeneralizedCeptrums.jl.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ノートブック&lt;/a&gt;へ&lt;/p&gt;
&lt;p&gt;メルケプストラム分析、メル一般化ケプストラム分析に関しては、SPTKの実装をjuliaで再実装してみました。結果、速度は1.0 ~ 1.5倍程度でおさまって、かつ数値的な安定性は増しています（メモリ使用量はお察し）。まぁ僕が頑張ったからというわけでなく、単にJuliaの線形方程式ソルバーがSPTKのものより安定しているというのが理由です。&lt;/p&gt;
&lt;h2 id=&#34;synthesisfiltersjl&#34;&gt;SynthesisFilters.jl&lt;/h2&gt;
&lt;p&gt;メルケプストラムからの波形合成とか。&lt;/p&gt;
&lt;p&gt;詳細はこちらの&lt;a href=&#34;http://nbviewer.ipython.org/github/r9y9/SynthesisFilters.jl/blob/v0.0.1/examples/Introduction%20to%20SynthesisFilters.jl.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ノートブック&lt;/a&gt;へ。いくつかの音声合成フィルタの合成音をノートブック上で比較することができます。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/r9y9/SynthesisFilters.jl/blob/mix-excitation/examples/Introduction%20to%20SynthesisFilters.jl.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;mixed excitation（っぽいの）を使ったバージョンのノートブック&lt;/a&gt;: 実装に自信がないので、そのうち消すかも。聴覚的にはこっちのほうが良いです。&lt;/p&gt;
&lt;h2 id=&#34;sptkjl&#34;&gt;SPTK.jl&lt;/h2&gt;
&lt;p&gt;公式のSPTKではなく、僕が少しいじったSPTK（windowsで動くようにしたり、APIとして使いやすいように関数内でexitしてた部分を適切なreturn code返すようにしたり、swipeというF0抽出のインタフェースをexposeしたり、など）をベースにしています。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/r9y9/SPTK.jl/blob/v0.0.1/examples/Introduction%20to%20SPTK.jl.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;デモ用のノートブック&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;MelGeneralizedCepstrums.jl と SynthesiFilters.jl は、ほとんどSPTK.jlで成り立っています。本質的に SPTK.jl にできて MelGeneralizedCepstrums.jl と SynthesiFilters.jlにできないことは基本的にないのですが、後者の方が、より簡単な、Julia的なインタフェースになっています。&lt;/p&gt;
&lt;p&gt;例えば、メルケプストラム、ケプストラム、LPCなど、スペクトルパラメータの型に応じて、適切なフィルタ係数に変換する、合成フィルタを選択するなど、multiple dispatchを有効に活用して、よりシンプルなインタフェースを提供するようにしました（というか自分がミスりたくなかったからそうしました）。&lt;/p&gt;
&lt;h2 id=&#34;おわり&#34;&gt;おわり&lt;/h2&gt;
&lt;p&gt;かなり適当に書きましたが、最近の進捗は、Juliaで書いていたパッケージ多少改善して、公式に登録したくらいでした。進捗まじ少なめ。あと些細なことですが、ipython（ijulia）に音埋め込むのクッソ簡単にできてびっくりしました（なんで今までやらなかったんだろう）。&lt;a href=&#34;https://github.com/jfsantos&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@jfsantos&lt;/a&gt; に感謝&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>JuliaTokyo #3 Speech Signal Processing in Julia</title>
      <link>https://r9y9.github.io/blog/2015/04/26/juliatokyo3-speech-signal-processing-in-julia/</link>
      <pubDate>Sun, 26 Apr 2015 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2015/04/26/juliatokyo3-speech-signal-processing-in-julia/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://juliatokyo.connpass.com/event/13218/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JuliaTokyo #3&lt;/a&gt;でLT発表してきました。前回の&lt;a href=&#34;https://juliatokyo.connpass.com/event/8010/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JuliaTokyo #2&lt;/a&gt;でも発表したので、二回目でした。&lt;/p&gt;
&lt;h2 id=&#34;スライド&#34;&gt;スライド&lt;/h2&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/h4geMoK1msYqdY&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt; &lt;div style=&#34;margin-bottom:5px&#34;&gt; &lt;strong&gt; &lt;a href=&#34;//www.slideshare.net/ryuichiy/juliatokyo-3-speech-signal-processing-in-julia-47403938&#34; title=&#34;JuliaTokyo #3 Speech Signal Processing in Julia&#34; target=&#34;_blank&#34;&gt;JuliaTokyo #3 Speech Signal Processing in Julia&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href=&#34;//www.slideshare.net/ryuichiy&#34; target=&#34;_blank&#34;&gt;Ryuichi YAMAMOTO&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;コード&#34;&gt;コード&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/JuliaTokyo3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/JuliaTokyo3&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;三行まとめ&#34;&gt;三行まとめ&lt;/h2&gt;
&lt;p&gt;発表の内容を三行でまとめると、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;音声ファイルの読み込み（or 書き込み）は[WAV.jl]((&lt;a href=&#34;https://github.com/dancasimiro/WAV.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/dancasimiro/WAV.jl&lt;/a&gt;)を使おう&lt;/li&gt;
&lt;li&gt;基本的なデジタル信号処理は &lt;a href=&#34;https://github.com/JuliaDSP/DSP.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JuliaDSP/DSP.jl&lt;/a&gt; をチェック（※JuliaDSPにはウェーブレットとかもあるよ）&lt;/li&gt;
&lt;li&gt;音声に特化した信号処理は、&lt;a href=&#34;https://github.com/r9y9/WORLD.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/WORLD.jl&lt;/a&gt; がオススメです&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;という感じです。&lt;/p&gt;
&lt;p&gt;応用例として、歌声を分離する話（&lt;a href=&#34;https://github.com/r9y9/RobustPCA.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;デモコード&lt;/a&gt;）、統計的声質変換（&lt;a href=&#34;http://r9y9.github.io/blog/2014/11/12/statistical-voice-conversion-code/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;統計的声質変換クッソムズすぎワロタ（チュートリアル編） - LESS IS MORE&lt;/a&gt;）、画像をスペクトログラムに足しこむ話とか、さっと紹介しました。&lt;/p&gt;
&lt;h2 id=&#34;補足&#34;&gt;補足&lt;/h2&gt;
&lt;p&gt;僕が使う/作ったパッケージを、あとで見返せるように最後のスライドにまとめておいたのですが、改めてここで整理しておきます。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/dancasimiro/WAV.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;dancasimiro/WAV&lt;/a&gt; WAVファイルの読み込み&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/JuliaDSP/DSP.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JuliaDSP/DSP&lt;/a&gt; 窓関数、スペクトログラム、デジタルフィルタ&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/WORLD.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/WORLD&lt;/a&gt; 音声分析・合成フレームワーク&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/MelGeneralizedCepstrums.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/MelGeneralizedCepstrums&lt;/a&gt; メル一般化ケプストラム分析&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/SynthesisFilters.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/SynthesisFilters&lt;/a&gt; メル一般化ケプストラムからの波形合成&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/SPTK.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/SPTK&lt;/a&gt; 音声信号処理ツールキット&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/RobustPCA.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/RobustPCA&lt;/a&gt; ロバスト主成分分析(歌声分離へ応用)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/REAPER.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/REAPER&lt;/a&gt; 基本周波数推定&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/VoiceConversion.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/VoiceConversion&lt;/a&gt; 統計的声質変換&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上から順に、&lt;del&gt;汎用的かなーと思います&lt;/del&gt;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;。僕が書いたパッケージの中では、&lt;strong&gt;WORLDのみ&lt;/strong&gt;公式パッケージにしています。理由は単純で、その他のパッケージはあまりユーザがいないだろうなーと思ったからです。かなりマニアックであったり、今後の方針が決まってなかったり（ごめんなさい）、応用的過ぎて全然汎用的でなかったり。WORLDは自信を持ってオススメできますので、Juliaで音声信号処理をやってみようかなと思った方は、ぜひお試しください。&lt;/p&gt;
&lt;h2 id=&#34;ざっくり感想&#34;&gt;ざっくり感想&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;＃Juliaわからん 本当に素晴らしいと思うので、僕も積極的に #Juliaわからん とつぶやいていこうと思います（詳しくは &lt;a href=&#34;https://twitter.com/chezou&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@chezou&lt;/a&gt; さんの記事をどうぞ &lt;a href=&#34;http://chezou.hatenablog.com/entry/2015/04/26/222518&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#JuliaTokyo で #juliaわからん という雑なレポジトリを立てた話をしたら julia.tokyo ができてた  - once upon a time,&lt;/a&gt;）。僕は、Julia に Theano が欲しいです。&lt;code&gt;T.grad&lt;/code&gt; 強力すぎる&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ccall&lt;/code&gt; かんたんとか言いましたが、ミスった書き方をしたときのエラーメッセージはあまり親切ではないので、つまずきやすいかも。僕は気合で何とかしています。&lt;/li&gt;
&lt;li&gt;Julia遅いんだけど？？？と言われたら、&lt;a href=&#34;https://twitter.com/bicycle1885&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@bicycle1885&lt;/a&gt; さんの &lt;a href=&#34;http://www.slideshare.net/KentaSato/whats-wrong-47403774&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;What&amp;rsquo;s wrong with this Julia?&lt;/a&gt; を投げつけようと思います。&lt;/li&gt;
&lt;li&gt;かなり聴衆が限定的になってしまう話をしてしまったので、次発表するならJulia 言語自体の話をしようかなと思いました&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;最後に&#34;&gt;最後に&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/sorami&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@sorami&lt;/a&gt;さんを筆頭とする運営の方々、本当にありがとうございました！楽しかったです。&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;とスライドに書いたけど、考えなおすと、僕が思う品質の高さ順、の方が正確です、失礼しました。MelGeneneralizedCepstrumsは一番気合入れて書いたけど、ユーザーがいるかといったらいないし、RobustPCAはさっと書いただけだけど、アルゴリズムとしては汎用的だし。またRobustPCAだけ毛色が違いますが、応用例で紹介したのでリストに入れておきました。&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>ccallにおけるポインタ周りのハマりどころとその解決法</title>
      <link>https://r9y9.github.io/blog/2014/12/09/julia-advent-calender-2014-poiner-tips/</link>
      <pubDate>Tue, 09 Dec 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/12/09/julia-advent-calender-2014-poiner-tips/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://qiita.com/advent-calendar/2014/julialang&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Julia Advent Calendar 2014&lt;/a&gt; 9日目の記事です。&lt;/p&gt;
&lt;h2 id=&#34;はじめに&#34;&gt;はじめに&lt;/h2&gt;
&lt;p&gt;CやFortranの関数をJuliaから呼ぶために使用する&lt;code&gt;ccall&lt;/code&gt;において、ポインタに関係するハマりどころとその解決法を紹介します。純粋なJuliaを使っている場合にはポインタを意識することはめったにないと思うので、&lt;code&gt;ccall&lt;/code&gt; を使う人（計算が重いボトルネック部分をCで書いてJuliaから呼びたい人、Cのライブラリのラッパーを書きたい/書いてる人）を主な読者と想定して記事を書きました（限定的でごめんなさい）。&lt;/p&gt;
&lt;p&gt;困った時は、公式ドキュメントの &lt;a href=&#34;http://docs.julialang.org/en/latest/manual/calling-c-and-fortran-code/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Calling C and Fortran Code&lt;/a&gt; を参考にしましょう。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;: 最新版の公式ドキュメントをいくつか引用していますが、ドキュメントは日々更新されていますので、この記事を読んで頂いた時点とは異なる可能性があることにご注意ください。&lt;/p&gt;
&lt;h2 id=&#34;こんなとき&#34;&gt;こんなとき&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;ccall&lt;/code&gt; を使う際に、ポインタに関する以下のような疑問を持つことがあります。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ポインタを引数に持つ（例. &lt;code&gt;double*&lt;/code&gt;）関数のラッピングはどうすればいいのか？&lt;/li&gt;
&lt;li&gt;構造体のポインタを引数に持つ関数のラッピングはどうすれば？&lt;/li&gt;
&lt;li&gt;ポインタのポインタを引数に持つ（例. &lt;code&gt;double**&lt;/code&gt;）関数のラッピングは？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一つ目は非常に簡単で、&lt;code&gt;Array&lt;/code&gt;（Cの関数が&lt;code&gt;double*&lt;/code&gt;を取るならば&lt;code&gt;Array{Float64,1}&lt;/code&gt;）をそのまま渡せばよいだけです。ドキュメントの&lt;a href=&#34;http://docs.julialang.org/en/latest/manual/calling-c-and-fortran-code/#array-conversions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Array Conversions&lt;/a&gt;にも書かれています。が、残りの二つに関してはハマりどころがあります。順に説明します。&lt;/p&gt;
&lt;h2 id=&#34;構造体のポインタを引数に持つ関数のラッピングはどうすれば&#34;&gt;構造体のポインタを引数に持つ関数のラッピングはどうすれば？&lt;/h2&gt;
&lt;p&gt;現状のドキュメントは少し不親切なので、引用した上で、整理します。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://docs.julialang.org/en/latest/manual/calling-c-and-fortran-code/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Calling C and Fortran Code&lt;/a&gt; より引用:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Currently, it is not possible to pass structs and other non-primitive types from Julia to C libraries. However, C functions that generate and use opaque struct types by passing pointers to them can return such values to Julia as Ptr{Void}, which can then be passed to other C functions as Ptr{Void}. Memory allocation and deallocation of such objects must be handled by calls to the appropriate cleanup routines in the libraries being used, just like in any C program.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;冒頭に it is not possible とあります。が、決して不可能なわけではありません。上記文章の要点をまとめると、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;opaqueな構造体はCからJuliaへポインタとして渡すことができる&lt;/li&gt;
&lt;li&gt;そのポインタは &lt;code&gt;Ptr{Void}&lt;/code&gt; としてCの関数に渡すことができる&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;と書かれています。つまり、一般には構造体は渡せないけどポインタ渡しはできるよ、ということです。&lt;/p&gt;
&lt;p&gt;じゃあnon-opaqueな構造体についてはどうなんだ？Juliaの型を渡せないのか？という疑問が出てきます。結論からいえば、non-opaqueな構造体についてもポインタ渡しは可能です。つまり、Cの構造体に相当するimmutableな型&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;をjuliaで宣言してあげれば、juliaの型をCに渡すことが可能です（値渡しはできません）&lt;/p&gt;
&lt;p&gt;例を示します。&lt;/p&gt;
&lt;h3 id=&#34;cコード&#34;&gt;Cコード&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;typedef struct {
   double a;
   int b;
} Foo;

# 構造体のポインタを引数にとる関数1
void print(Foo* foo) {
    printf(&amp;quot;a=%lf\n&amp;quot;, foo-&amp;gt;a);
    printf(&amp;quot;b=%d\n&amp;quot;, foo-&amp;gt;b);
}

# 構造体のポインタを引数にとる関数2
void reset(Foo* foo) {
    foo-&amp;gt;a = 0.0;
    foo-&amp;gt;b = 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;juliaコード&#34;&gt;Juliaコード&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Cの構造体 Foo に相当する型を宣言します
immutable Foo
    a::Float64
    b::Int32 # cのintはjuliaのInt32に対応します
end

foo = Foo(10.0, 2)

# Cの関数に、ポインタとしてJuliaの型を渡すことができます
ccall(:print, &amp;quot;libfoo&amp;quot;, Void, (Ptr{Foo},), &amp;amp;foo)

# ポインタで渡す場合、Cで変更した結果はJuliaにも反映されます
ccall(:reset, &amp;quot;libfoo&amp;quot;, Void, (Ptr{Foo},), &amp;amp;foo)

# foo(0.0, 0) と表示される
println(foo)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ちなみにJuliaからCへ値渡しをしてもエラーにならないので、お気をつけください（ハマりました）。&lt;/p&gt;
&lt;p&gt;公式ドキュメントは不親切と言いましたが、 プルリクエスト &lt;a href=&#34;https://github.com/JuliaLang/julia/pull/8948&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;update documentation for passing struct pointers to C #8948&lt;/a&gt;（まだマージはされていない）で改善されているので、もしかするとこの記事が読まれる頃には改善されているかもしれません。&lt;/p&gt;
&lt;p&gt;また、値渡しを可能にしようとする動きもあります（&lt;a href=&#34;https://github.com/JuliaLang/julia/pull/3466&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RFC: Make struct passing work properly #3466&lt;/a&gt;, &lt;a href=&#34;https://github.com/JuliaLang/julia/pull/2818&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WIP: types as C-structs #2818&lt;/a&gt; マージ待ち）。&lt;/p&gt;
&lt;h3 id=&#34;構造体渡しのまとめ&#34;&gt;構造体渡しのまとめ&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Cの構造体に相当するJuliaの型を定義して、ポインタで渡せばOK&lt;/li&gt;
&lt;li&gt;値渡しは現状できない&lt;/li&gt;
&lt;li&gt;ポインタを受けることはできる（Ptr{Void}として受ける）&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ポインタのポインタを引数に持つ例-double関数のラッピングは&#34;&gt;ポインタのポインタを引数に持つ（例. &lt;code&gt;double**&lt;/code&gt;）関数のラッピングは？&lt;/h2&gt;
&lt;p&gt;さて、これはドキュメントにまったく書かれておらず、かつハマりやすいと僕は思っています。例を交えつつ解説します。以下のような関数のラッピングを考えます。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;void fooo(double** input, int w, int h, double** output);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;input&lt;/code&gt;は入力の行列、&lt;code&gt;output&lt;/code&gt;は計算結果が格納される行列、行列のサイズは共に 列数&lt;code&gt;w&lt;/code&gt;、行数&lt;code&gt;h&lt;/code&gt; だと思ってください。Juliaからは &lt;code&gt;input::Array{Float64,2}&lt;/code&gt; を入力として、&lt;code&gt;output::Array{Float64,2}&lt;/code&gt; を得たいとします。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;double*&lt;/code&gt;を引数にとる場合は&lt;code&gt;Array{Float64,1}&lt;/code&gt;を渡せばよかったのに対して、&lt;code&gt;double**&lt;/code&gt;を引数に取る関数に &lt;code&gt;Array{Float64,2}&lt;/code&gt;や&lt;code&gt;Array{Array{Float64,1},1}&lt;/code&gt;を単純に渡すだけでは、残念ながらコンパイルエラーになります。はい、すでに若干面倒ですね。。さて、どうすればいいかですが、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;どんな型で渡せばいいか&lt;/li&gt;
&lt;li&gt;どのように型を変換するか&lt;/li&gt;
&lt;li&gt;変換した型をどのように元に戻すか&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;という三点に分けて説明します。&lt;/p&gt;
&lt;h3 id=&#34;1-どんな型で渡せばいいか&#34;&gt;1. どんな型で渡せばいいか&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Array{Ptr{Float64}}&lt;/code&gt; で渡せばよいです。外側のArrayは、&lt;code&gt;ccall&lt;/code&gt; がポインタに変換してくれるので、Juliaの型でいえば&lt;code&gt;Ptr{Ptr{Float64}}&lt;/code&gt;、Cの型で言えば&lt;code&gt;double**&lt;/code&gt;になるわけです。&lt;/p&gt;
&lt;h3 id=&#34;2-どのように型を変換するか&#34;&gt;2. どのように型を変換するか&lt;/h3&gt;
&lt;p&gt;ここがハマりどころです。今回の例では、&lt;code&gt;Array{Float64,2}&lt;/code&gt; を &lt;code&gt;Array{Ptr{Float64},1}&lt;/code&gt; に変換すればよいので、例えば以下のような実装が思いつきます。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Array{T,2} -&amp;gt; Array{Ptr{T}}
function ptrarray2d{T&amp;lt;:Real}(src::Array{T,2})
    dst = Array{Ptr{T}, size(src, 2))
    for i=1:size(src, 2)
        dst[i] = pointer(src[:,i], 1) # 先頭要素のポインタを取り出す
    end
    dst
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;実はこの実装はバグを含んでいます。バグがあるとしたら一行しか該当する部分はないですが、&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;dst[i] = pointer(src[:,i], 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ここが間違っています。何が間違っているかというと、&lt;code&gt;pointer(src[:,i], 1)&lt;/code&gt;は一見&lt;code&gt;src&lt;/code&gt;の&lt;code&gt;i&lt;/code&gt;列目の先頭要素のポインタを指しているような気がしますが、&lt;code&gt;src[:,1]&lt;/code&gt;で &lt;code&gt;getindex&lt;/code&gt;という関数が走って内部データのコピーを行っているので、そのコピーに対するポインタを指している（元データの&lt;code&gt;i&lt;/code&gt;列目のポインタを指していない）点が間違っています&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;。これは、JuliaのArray実装ついて多少知らないとわからないと思うので、ハマりどころと書きました。&lt;/p&gt;
&lt;p&gt;Array &lt;code&gt;A&lt;/code&gt;に対する syntax &lt;code&gt;X = A[I_1, I_2, ..., I_n]&lt;/code&gt; は &lt;code&gt;X = getindex(A, I_1, I_2, ..., I_n)&lt;/code&gt; と等価です。詳細は、&lt;a href=&#34;http://docs.julialang.org/en/latest/manual/arrays/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Multi-dimensional Arrays&lt;/a&gt;や&lt;a href=&#34;http://docs.julialang.org/en/latest/stdlib/base/?highlight=getindex#Base.getindex&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;標準ライブラリのドキュメント&lt;/a&gt; を参考にしてください&lt;/p&gt;
&lt;p&gt;さて、正解を示します。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Array{T,2} -&amp;gt; Array{Ptr{T}}
function ptrarray2d{T&amp;lt;:Real}(src::Array{T,2})
    dst = Array{Ptr{T}, size(src, 2))
    for i=1:size(src, 2)
         dst[i] = pointer(sub(src, 1:size(src,1), i), 1)
    end
    dst
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;違いは &lt;code&gt;SubArray&lt;/code&gt;を使うようになった点です。&lt;code&gt;SubArray&lt;/code&gt;は、indexingを行うときにコピーを作らないので、期待した通りに&lt;code&gt;i&lt;/code&gt;列目の先頭要素のポインタを取得することができます。&lt;code&gt;SubArray&lt;/code&gt;について、以下引用しておきます&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;SubArray is a specialization of AbstractArray that performs indexing by reference rather than by copying. A SubArray is created with the sub() function, which is called the same way as getindex() (with an array and a series of index arguments). The result of sub() looks the same as the result of getindex(), except the data is left in place. sub() stores the input index vectors in a SubArray object, which can later be used to index the original array indirectly.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;引用元: &lt;a href=&#34;http://docs.julialang.org/en/latest/manual/arrays/#implementation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Multi-dimensional Arrays&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;3-変換した型をどのように元に戻すか&#34;&gt;3. 変換した型をどのように元に戻すか&lt;/h3&gt;
&lt;p&gt;Juliaで計算結果（上の例でいう &lt;code&gt;double** output&lt;/code&gt;）を受け取りたい場合、ポインタに変換した値をJuliaのArrayに戻す必要があります（必ずしもそうではないですが、まぁほぼそうでしょう）。つまり、&lt;code&gt;Array(Ptr{Float64},1)&lt;/code&gt;を&lt;code&gt;Array{Float64,2}&lt;/code&gt;したいわけです。幸いにも、これは&lt;code&gt;pointer_to_array&lt;/code&gt;を使うと簡単にできます。コードを以下に示します。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# ccallを実行した後の計算結果が coutput に格納されているとします
coutput::Array{Ptr{Float64},1}

# Cに渡した型 Array{Ptr{Float64},1} から Array{Float64,2}に変換
for i=1:length(coutput)
    output[:,i] = pointer_to_array(coutput[i], size(output, 1))
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;pointer_to_array&lt;/code&gt; は、その名前の通りの関数ですね。pointerをArrayに変換してくれます。&lt;/p&gt;
&lt;h3 id=&#34;1-2-3-をまとめる&#34;&gt;1, 2, 3 をまとめる&lt;/h3&gt;
&lt;p&gt;最後に、1, 2, 3の内容をまとめて、ポインタのポインタを引数にもつ関数のラッパー例を書いておきます。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function fooo(input::Array{Float64,2})
    h, w = size(intput)
    output = Array(Float64, h, w)

    # C関数に渡す用の変数
    cinput::Array{Ptr{Float64}} = ptrarray2d(input)
    coutput::Array{Ptr{Float64}} = ptrarray2d(output)

    ccall(:fooo, &amp;quot;libfooo&amp;quot;, Void,
    		 (Ptr{Ptr{Float64}}, Int, Int, Ptr{Ptr{Float64}}),
    		 cinput, w, h, coutput)

    # coutputをJuliaのArrayに変換
    for i=1:length(coutput)
        output[i,:] = pointer_to_array(coutput[i], h)
    end

    output
end
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;ポインタのポインタまとめ&#34;&gt;ポインタのポインタまとめ&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Array&lt;/code&gt;のindexingはコピーを作るのである要素のポインタがほしい時は注意&lt;/li&gt;
&lt;li&gt;行/列の先頭のポインタがほしいときは &lt;code&gt;SubArray&lt;/code&gt; を使いましょう&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;
&lt;p&gt;ポインタにまつわるハマりどころとその解決法を紹介しました。今回紹介したものはすべて &lt;a href=&#34;https://github.com/r9y9/WORLD.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WORLD.jl&lt;/a&gt; という &lt;a href=&#34;http://ml.cs.yamanashi.ac.jp/world/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;音声分析変換合成システムWORLD&lt;/a&gt; のラッパーを書いていたときに得た知見です。やっと&lt;code&gt;WORLD.jl&lt;/code&gt;が安定して動くようになってきて公式パッケージにしようかなぁと考えているところですので、興味のある方はぜひ触ってみてください。&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;immutableでなければいけない理由はまだよくわかっていないのですが、少なくとも &lt;a href=&#34;https://github.com/JuliaLang/julia/pull/8948&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#8948&lt;/a&gt; にはそう書いてあります&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;たちの悪いことに、この実装でもだいたい上手く動くんですよね…。数値型がimmutableだからコピーしてもそうそうアドレスが変わらないとかそういう理由だろうかと考えていますが、ちょっとよくわかっていないです&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;ArrayとSubArrayの使い分けはどうすればいいのか、それぞれどういう目的で作られたのか等、僕も勉強中で理解が曖昧なため説明できません、すみません。&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>統計的声質変換クッソムズすぎワロタ（チュートリアル編）</title>
      <link>https://r9y9.github.io/blog/2014/11/12/statistical-voice-conversion-code/</link>
      <pubDate>Wed, 12 Nov 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/11/12/statistical-voice-conversion-code/</guid>
      <description>&lt;h2 id=&#34;はじめに&#34;&gt;はじめに&lt;/h2&gt;
&lt;p&gt;こんばんは。統計的声質変換（以降、簡単に声質変換と書きます）って面白いなーと思っているのですが、興味を持つ人が増えたらいいなと思い、今回は簡単なチュートリアルを書いてみます。間違っている箇所があれば、指摘してもらえると助かります。よろしくどうぞ。&lt;/p&gt;
&lt;p&gt;前回の記事（&lt;a href=&#34;http://r9y9.github.io/blog/2014/07/13/statistical-voice-conversion-wakaran/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;統計的声質変換クッソムズすぎワロタ（実装の話） - LESS IS MORE&lt;/a&gt;）では変換部分のコードのみを貼りましたが、今回はすべてのコードを公開します。なので、記事内で示す声質変換の結果を、この記事を読んでいる方が再現することも可能です。対象読者は、特に初学者の方で、声質変換を始めたいけれど論文からコードに落とすにはハードルが高いし、コードを動かしながら仕組みを理解していきたい、という方を想定しています。役に立てば幸いです。&lt;/p&gt;
&lt;h2 id=&#34;コード&#34;&gt;コード&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/VoiceConversion.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/VoiceConversion.jl&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://julialang.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Julia&lt;/a&gt; という言語で書かれています。Juliaがどんな言語かをさっと知るのには、以下のスライドがお勧めです。人それぞれ好きな言語で書けばいいと思いますが、個人的にJuliaで書くことになった経緯は、最後の方に簡単にまとめました。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/39141184&#34; width=&#34;425&#34; height=&#34;355&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt; &lt;div style=&#34;margin-bottom:5px&#34;&gt; &lt;strong&gt; &lt;a href=&#34;//www.slideshare.net/kentaroiizuka/julia-39141184&#34; title=&#34;プログラミング言語 Julia の紹介&#34; target=&#34;_blank&#34;&gt;プログラミング言語 Julia の紹介&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href=&#34;//www.slideshare.net/kentaroiizuka&#34; target=&#34;_blank&#34;&gt;Kentaro Iizuka&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&#34;サードパーティライブラリ&#34;&gt;サードパーティライブラリ&lt;/h2&gt;
&lt;p&gt;声質変換は多くのコンポーネントによって成り立っていますが、すべてを自分で書くのは現実的ではありません。僕は、主に以下のライブラリを活用しています。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://ml.cs.yamanashi.ac.jp/world/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WORLD&lt;/a&gt; - 音声分析合成のフレームワークとして、あるいは単にスペクトル包絡を抽出するツールとして使っています。&lt;a href=&#34;https://github.com/r9y9/WORLD.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Juliaラッパー&lt;/a&gt;を書きました。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;sp-tk.sourceforge.net&#34;&gt;SPTK&lt;/a&gt; - メル対数スペクトル近似（Mel-Log Spectrum Approximation; MLSA）フィルタを変換処理に使っています。これも&lt;a href=&#34;https://github.com/r9y9/SPTK.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Juliaラッパー&lt;/a&gt;を書きました。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://scikit-learn.org/stable/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sklearn&lt;/a&gt; - sklearn.mixture をGMMの学習に使っています。pythonのライブラリは、juliaから簡単に呼べます。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;音声分析合成に関しては、アカデミック界隈ではよく使われている&lt;a href=&#34;http://www.wakayama-u.ac.jp/~kawahara/STRAIGHTadv/index_j.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;STRAIGHT&lt;/a&gt;がありますが、WORLDの方がライセンスもゆるくソースも公開されていて、かつ性能も劣らない（正確な話は、森勢先生の論文を参照してください）ので、おすすめです。&lt;/p&gt;
&lt;h2 id=&#34;voiceconversionjlhttpsgithubcomr9y9voiceconversionjl-でできること&#34;&gt;&lt;a href=&#34;https://github.com/r9y9/VoiceConversion.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VoiceConversion.jl&lt;/a&gt; でできること&lt;/h2&gt;
&lt;h3 id=&#34;追記-20150107&#34;&gt;追記 2015/01/07&lt;/h3&gt;
&lt;p&gt;この記事を書いた段階のv0.0.1は、依存ライブラリの変更のため、現在は動きません。すみません。何のためのタグだ、という気がしてきますが、、最低限masterは動作するようにしますので、そちらをお試しください（基本的には、新しいコードの方が改善されています）。それでも動かないときは、issueを投げてください。&lt;/p&gt;
&lt;p&gt;2014/11/10現在（v0.0.1のタグを付けました）、できることは以下の通りです（外部ライブラリを叩いているものを含む）。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;音声波形からのメルケプストラムの抽出&lt;/li&gt;
&lt;li&gt;DPマッチングによるパラレルデータの作成&lt;/li&gt;
&lt;li&gt;GMMの学習&lt;/li&gt;
&lt;li&gt;GMMベースのframe-by-frame特徴量変換&lt;/li&gt;
&lt;li&gt;GMMベースのtrajectory特徴量変換&lt;/li&gt;
&lt;li&gt;GMMベースのtrajectory特徴量変換（GV考慮版）&lt;/li&gt;
&lt;li&gt;音声分析合成系WORLDを使った声質変換&lt;/li&gt;
&lt;li&gt;MLSAフィルタを使った差分スペクトルに基づく声質変換&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;これらのうち、trajectory変換以外を紹介します。&lt;/p&gt;
&lt;h2 id=&#34;チュートリアルcmu_arcticを使ったgmmベースの声質変換特徴抽出からパラレルデータの作成gmmの学習変換合成処理まで&#34;&gt;チュートリアル：CMU_ARCTICを使ったGMMベースの声質変換（特徴抽出からパラレルデータの作成、GMMの学習、変換・合成処理まで）&lt;/h2&gt;
&lt;p&gt;データセットに&lt;a href=&#34;http://festvox.org/cmu_arctic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CMU_ARCTIC&lt;/a&gt;を使って、GMMベースの声質変換（clb -&amp;gt; slt）を行う方法を説明します。なお、VoiceConversion.jl のv0.0.1を使います。ubuntuで主に動作確認をしていますが、macでも動くと思います。&lt;/p&gt;
&lt;h2 id=&#34;0-前準備&#34;&gt;0. 前準備&lt;/h2&gt;
&lt;h3 id=&#34;01-データセットのダウンロード&#34;&gt;0.1. データセットのダウンロード&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://festvox.org/cmu_arctic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Festvox: CMU_ARCTIC Databases&lt;/a&gt; を使います。コマンド一発ですべてダウンロードする&lt;a href=&#34;https://gist.github.com/r9y9/ff67c05aeb87410eae2e&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;スクリプト&lt;/a&gt;を書いたので、ご自由にどうぞ。&lt;/p&gt;
&lt;h3 id=&#34;02-juliaのインストール&#34;&gt;0.2. juliaのインストール&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://julialang.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;公式サイト&lt;/a&gt;からバイナリをダウンロードするか、&lt;a href=&#34;https://github.com/JuliaLang/julia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;githubのリポジトリ&lt;/a&gt;をクローンしてビルドしてください。バージョンは、現在の最新安定版のv0.3.2を使います。&lt;/p&gt;
&lt;p&gt;記事内では、juliaの基本的な使い方については解説しないので、前もってある程度調べておいてもらえると、スムーズに読み進められるかと思います。&lt;/p&gt;
&lt;h3 id=&#34;03-voiceconversionjl-のインストール&#34;&gt;0.3. VoiceConversion.jl のインストール&lt;/h3&gt;
&lt;p&gt;juliaを起動して、以下のコマンドを実行してください。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;julia&amp;gt; Pkg.clone(&amp;quot;https://github.com/r9y9/VoiceConversion.jl&amp;quot;)
julia&amp;gt; Pkg.build(&amp;quot;VoiceConversion&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;サードパーティライブラリは、sklearnを除いてすべて自動でインストールされます。sklearnは、例えば以下のようにしてインストールしておいてください。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo pip install sklearn
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;これで準備は完了です！&lt;/p&gt;
&lt;h2 id=&#34;1-音声波形からのメルケプストラムの抽出&#34;&gt;1. 音声波形からのメルケプストラムの抽出&lt;/h2&gt;
&lt;p&gt;まずは、音声から声質変換に用いる特徴量を抽出します。特徴量としては、声質変換や音声合成の分野で広く使われているメルケプストラムを使います。メルケプストラムの抽出は、&lt;code&gt;scripts/mcep.jl&lt;/code&gt; を使うことでできます。&lt;/p&gt;
&lt;h3 id=&#34;20141115-追記&#34;&gt;2014/11/15 追記&lt;/h3&gt;
&lt;p&gt;実行前に、&lt;code&gt;julia&amp;gt; Pkg.add(&amp;quot;WAV&amp;quot;)&lt;/code&gt; として、WAVパッケージをインストールしておいてください。(2014/11/15時点のmasterでは自動でインストールされますが、v0.0.1ではインストールされません、すいません）。また、メルケプストラムの出力先ディレクトリは事前に作成しておいてください（最新のスクリプトでは自動で作成されます）。&lt;/p&gt;
&lt;p&gt;以下のようにして、2話者分の特徴量を抽出しましょう。以下のスクリプトでは、 &lt;code&gt;~/data/cmu_arctic/&lt;/code&gt; にデータがあることを前提としています。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# clb
julia mcep.jl ~/data/cmu_arctic/cmu_us_clb_arctic/wav/ ~/data/cmu_arctic_jld/speakers/clb/
# slt
julia mcep.jl ~/data/cmu_arctic/cmu_us_slt_arctic/wav/ ~/data/cmu_arctic_jld/speakers/slt/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;基本的な使い方は、&lt;code&gt;mcep.jl &amp;lt;wavファイルがあるディレクトリ&amp;gt; &amp;lt;メルケプストラムが出力されるディレクトリ&amp;gt;&lt;/code&gt; になっています。オプションについては、 &lt;code&gt;mcep.jl -h&lt;/code&gt; としてヘルプを見るか、コードを直接見てください。&lt;/p&gt;
&lt;p&gt;抽出されたメルケプストラムは、HDF5フォーマットで保存されます。メルケプストラムの中身を見てみると、以下のような感じです。可視化には、PyPlotパッケージが必要です。Juliaを開いて、&lt;code&gt;julia&amp;gt; Pkg.add(&amp;quot;PyPlot&amp;quot;)&lt;/code&gt; とすればOKです。IJuliaを使いたい場合（僕は使っています）は、&lt;code&gt;julia&amp;gt; Pkg.add(&amp;quot;IJulia&amp;quot;)&lt;/code&gt; としてIJuliaもインストールしておきましょう。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# メルケプストラムの可視化

using HDF5, JLD, PyPlot

x = load(&amp;quot;clb/arctic_a0028.jld&amp;quot;)

figure(figsize=(16, 6), dpi=80, facecolor=&amp;quot;w&amp;quot;, edgecolor=&amp;quot;k&amp;quot;)
imshow(x[&amp;quot;feature_matrix&amp;quot;], origin=&amp;quot;lower&amp;quot;, aspect=&amp;quot;auto&amp;quot;)
colorbar()
&lt;/code&gt;&lt;/pre&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/clb_a0028_melcepstrum.png &#34;Mel-cepstrum of clb_a0028.&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;0次成分だけ取り出してみると、以下のようになります。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# メルケプストラムの0次成分のみを可視化

figure(figsize=(16, 6), dpi=80, facecolor=&amp;quot;w&amp;quot;, edgecolor=&amp;quot;k&amp;quot;)
plot(vec(x[&amp;quot;feature_matrix&amp;quot;][1,:]), linewidth=2.0, label=&amp;quot;0th order mel-cesptrum of clb_a0028&amp;quot;)
xlim(0, size(x[&amp;quot;feature_matrix&amp;quot;], 2)-10) # 末尾がsilenceだった都合上…（決め打ち）
xlabel(&amp;quot;Frame&amp;quot;)
legend(loc=&amp;quot;upper right&amp;quot;)
ylim(-10, -2) # 見やすいように適当に決めました
&lt;/code&gt;&lt;/pre&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/clb_a0028_melcepstrum_0th.png &#34;Mel-cepstrum of clb_a0028 0th.&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;こんな感じです。話者clbの&lt;code&gt;clb_a0028.wav&lt;/code&gt;を聞きながら、特徴量見てみてください。0次の成分からは、音量の大小が読み取れると思います。&lt;/p&gt;
&lt;h2 id=&#34;2-dpマッチングによるパラレルデータの作成&#34;&gt;2. DPマッチングによるパラレルデータの作成&lt;/h2&gt;
&lt;p&gt;次に、2話者分の特徴量を時間同期して連結します。基本的に声質変換では、音韻の違いによらない特徴量（非言語情報）の対応関係を学習するために、同一発話内容の特徴量を時間同期し（音韻の違いによる変動を可能な限りなくすため）、学習データとして用います。このデータのことを、パラレルデータと呼びます。&lt;/p&gt;
&lt;p&gt;パラレルデータの作成には、DPマッチングを使うのが一般的です。&lt;code&gt;scripts/align.jl&lt;/code&gt; を使うとできます。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;julia align.jl ~/data/cmu_arctic_jld/speakers/clb ~/data/cmu_arctic_jld/speakers/slt ~/data/cmu_arctic_jld/parallel/clb_and_slt/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;使い方は、&lt;code&gt;align.jl &amp;lt;話者1（clb）の特徴量のパス&amp;gt; &amp;lt;話者2（slt）の特徴量のパス&amp;gt; &amp;lt;パラレルデータの出力先&amp;gt;&lt;/code&gt; になっています。&lt;/p&gt;
&lt;p&gt;きちんと時間同期されているかどうか、0次成分を見て確認してみましょう。&lt;/p&gt;
&lt;p&gt;時間同期を取る前のメルケプストラムを以下に示します。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# 時間同期前のメルケプストラム（0次）を可視化

x = load(&amp;quot;clb/arctic_a0028.jld&amp;quot;)
y = load(&amp;quot;slt/arctic_a0028.jld&amp;quot;)

figure(figsize=(16, 6), dpi=80, facecolor=&amp;quot;w&amp;quot;, edgecolor=&amp;quot;k&amp;quot;)
plot(vec(x[&amp;quot;feature_matrix&amp;quot;][1,:]), linewidth=2.0, label=&amp;quot;0th order mel-cesptrum of clb_a0028&amp;quot;)
plot(vec(y[&amp;quot;feature_matrix&amp;quot;][1,:]), linewidth=2.0, label=&amp;quot;0th order mel-cesptrum of slt_a0028&amp;quot;)
xlim(0, min(size(x[&amp;quot;feature_matrix&amp;quot;], 2), size(y[&amp;quot;feature_matrix&amp;quot;], 2))-10) # 決め打ち
xlabel(&amp;quot;Frame&amp;quot;)
legend(loc=&amp;quot;upper right&amp;quot;)
ylim(-10, -2) # 決め打ち
&lt;/code&gt;&lt;/pre&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/clb_and_slt_a0028_melcepstrum_0th.png &#34;0th order mel-cepstrum (not aligned)&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;ちょっとずれてますね&lt;/p&gt;
&lt;p&gt;次に、時間同期後のメルケプストラムを示します。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# 時間同期後のメルケプストラム（0次）を可視化

parallel = load(&amp;quot;arctic_a0028_parallel.jld&amp;quot;)

figure(figsize=(16, 6), dpi=80, facecolor=&amp;quot;w&amp;quot;, edgecolor=&amp;quot;k&amp;quot;)
plot(vec(parallel[&amp;quot;src&amp;quot;][&amp;quot;feature_matrix&amp;quot;][1,:]), linewidth=2.0, &amp;quot;b&amp;quot;, label=&amp;quot;0th order mel-cesptrum of clb_a0028&amp;quot;)
plot(vec(parallel[&amp;quot;tgt&amp;quot;][&amp;quot;feature_matrix&amp;quot;][1,:]), linewidth=2.0, &amp;quot;g&amp;quot;, label=&amp;quot;0th order mel-cesptrum of slt_a0028&amp;quot;)
xlim(0, size(parallel[&amp;quot;tgt&amp;quot;][&amp;quot;feature_matrix&amp;quot;], 2))
xlabel(&amp;quot;Frame&amp;quot;)
legend()
&lt;/code&gt;&lt;/pre&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/clb_and_slt_a0028_melcepstrum_0th_aligned.png &#34;0th order mel-cepstrum (aligned)&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;ずれが修正されているのがわかりますね。注意として、&lt;code&gt;align.jl&lt;/code&gt; の中身を追えばわかるのですが、無音区間をしきい値判定で検出して、パラレルデータから除外しています。&lt;/p&gt;
&lt;p&gt;結果、時間同期されたパラレルデータは以下のようになります。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# パラレルデータの可視化

figure(figsize=(16, 6), dpi=80, facecolor=&amp;quot;w&amp;quot;, edgecolor=&amp;quot;k&amp;quot;)
imshow(vcat(parallel[&amp;quot;src&amp;quot;][&amp;quot;feature_matrix&amp;quot;], parallel[&amp;quot;tgt&amp;quot;][&amp;quot;feature_matrix&amp;quot;]), origin=&amp;quot;lower&amp;quot;, aspect=&amp;quot;auto&amp;quot;)
colorbar()
&lt;/code&gt;&lt;/pre&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/clb_and_slt_a0028_parallel.png &#34;example of parallel data&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;このパラレルデータを（複数の発話分さらに結合して）使って、特徴量の対応関係を学習していきます。モデルには、GMMを使います。&lt;/p&gt;
&lt;h2 id=&#34;3-gmmの学習&#34;&gt;3. GMMの学習&lt;/h2&gt;
&lt;p&gt;GMMの学習には、&lt;code&gt;sklearn.mixture.GMM&lt;/code&gt; を使います。GMMは古典的な生成モデルで、実装は探せばたくさん見つかるので、既存の有用なライブラリを使えば十分です。（余談ですが、pythonのライブラリを簡単に呼べるのはjuliaの良いところの一つですね）&lt;/p&gt;
&lt;p&gt;&lt;code&gt;scripts/train_gmm.jl&lt;/code&gt; を使うと、モデルのダンプ、julia &amp;lt;-&amp;gt; python間のデータフォーマットの変換等、もろもろやってくれます。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;julia train_gmm.jl ~/data/cmu_arctic_jld/parallel/clb_and_slt/ clb_and_slt_gmm32_order40.jld --max 200 --n_components 32 --n_iter=100 --n_init=1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;使い方は、&lt;code&gt;train_gmm.jl &amp;lt;パラレルデータのパス&amp;gt; &amp;lt;出力するモデルデータのパス&amp;gt;&lt;/code&gt; になっています。上の例では、学習に用いる発話数、GMMの混合数、反復回数等を指定しています。オプションの詳細はスクリプトをご覧ください。&lt;/p&gt;
&lt;p&gt;僕の環境では、上記のコマンドを叩くと2時間くらいかかりました。学習が終わったところで、学習済みのモデルのパラメータを可視化してみましょう。&lt;/p&gt;
&lt;p&gt;まずは平均を見てみます。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# GMMの平均ベクトルを（いくつか）可視化
gmm = load(&amp;quot;clb_and_slt_gmm32_order40.jld&amp;quot;)

figure(figsize=(16, 6), dpi=80, facecolor=&amp;quot;w&amp;quot;, edgecolor=&amp;quot;k&amp;quot;)
for k=1:3
    plot(gmm[&amp;quot;means&amp;quot;][:,k], linewidth=2.0, label=&amp;quot;mean of mixture $k&amp;quot;)
end
legend()
&lt;/code&gt;&lt;/pre&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/clb_and_slt_gmm32_order40_mean.png &#34;means of trained GMM&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;共分散の一部可視化してみると、以下のようになります。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# GMMの共分散行列を一部可視化

figure(figsize=(16, 6), dpi=80, facecolor=&amp;quot;w&amp;quot;, edgecolor=&amp;quot;k&amp;quot;)
imshow(gmm[&amp;quot;covars&amp;quot;][:,:,2])
colorbar()
clim(0.0, 0.16)
&lt;/code&gt;&lt;/pre&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/clb_and_slt_gmm32_order40_covar.png &#34;covariance of trained GMM&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;まぁこんなもんですね。&lt;/p&gt;
&lt;h2 id=&#34;4-音声分析合成worldを用いたgmmベースのframe-by-frame声質変換&#34;&gt;4. 音声分析合成WORLDを用いたGMMベースのframe-by-frame声質変換&lt;/h2&gt;
&lt;p&gt;さて、ようやく声質変換の準備が整いました。学習したモデルを使って、GMMベースのframe-by-frame声質変換（clb -&amp;gt; slt ）をやってみましょう。具体的な変換アルゴリズムは、論文（例えば&lt;a href=&#34;http://isw3.naist.jp/~tomoki/Tomoki/Journals/IEEE-Nov-2007_MLVC.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;戸田先生のこれ&lt;/a&gt;）をチェックしてみてください。音声分析合成系にはWORLDを使います。&lt;/p&gt;
&lt;p&gt;一般的な声質変換では、まず音声を以下の三つの成分に分解します。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基本周波数&lt;/li&gt;
&lt;li&gt;スペクトル包絡（今回いじりたい部分）&lt;/li&gt;
&lt;li&gt;非周期性成分&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;その後、スペクトル包絡に対して変換を行い、変換後のパラメータを使って音声波形を合成するといったプロセスを取ります。これらは、&lt;code&gt;scripts/vc.jl&lt;/code&gt; を使うと簡単にできるようになっています。本当にWORLDさまさまです。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;julia vc.jl ~/data/cmu_arctic/cmu_us_clb_arctic/wav/arctic_a0028.wav clb_and_slt_gmm32_order40.jld clb_to_slt_a0028.wav --order 40
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;使い方は、&lt;code&gt;vc.jl &amp;lt;変換対象の音声ファイル&amp;gt; &amp;lt;変換モデル&amp;gt; &amp;lt;出力wavファイル名&amp;gt;&lt;/code&gt; となっています。&lt;/p&gt;
&lt;p&gt;上記のコマンドを実行すると、GMMベースのframe-by-frame声質変換の結果が音声ファイルに出力されます。以下に結果を貼っておくので、聞いてみてください。&lt;/p&gt;
&lt;h3 id=&#34;変換元となる音声-clb_a0028&#34;&gt;変換元となる音声 clb_a0028&lt;/h3&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;166&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/176093202&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false&#34;&gt;&lt;/iframe&gt;
&lt;h3 id=&#34;変換目標となる話者-slt_a0028&#34;&gt;変換目標となる話者 slt_a0028&lt;/h3&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;166&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/176093240&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false&#34;&gt;&lt;/iframe&gt;
&lt;h3 id=&#34;変換結果-clb_to_slt_a0028&#34;&gt;変換結果 clb_to_slt_a0028&lt;/h3&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;166&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/176093403&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;話者性はなんとなく目標話者に近づいている気がしますが、音質が若干残念な感じですね。。&lt;/p&gt;
&lt;h2 id=&#34;5-差分スペクトル補正に基づく声質変換&#34;&gt;5. 差分スペクトル補正に基づく声質変換&lt;/h2&gt;
&lt;p&gt;最後に、より高品質な声質変換を達成可能な差分スペクトル補正に基づく声質変換を紹介します。差分スペクトル補正に基づく声質変換では、基本周波数や非周期性成分をいじれない代わりに音質はかなり改善します。以前書いた記事（&lt;a href=&#34;http://r9y9.github.io/blog/2014/07/05/statistical-voice-conversion-muzui/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;統計的声質変換クッソムズすぎワロタ - LESS IS MORE&lt;/a&gt;）から、着想に関連する部分を引用します。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;これまでは、音声を基本周波数、非周期性成分、スペクトル包絡に分解して、スペクトル包絡を表す特徴量を変換し、変換後の特徴量を元に波形を再合成していました。ただ、よくよく考えると、そもそも基本周波数、非周期性成分をいじる必要がない場合であれば、わざわざ分解して再合成する必要なくね？声質の部分のみ変換するようなフィルタかけてやればよくね？という考えが生まれます。実は、そういったアイデアに基づく素晴らしい手法があります。それが、差分スペクトル補正に基づく声質変換です。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;差分スペクトル補正に基づく声質変換の詳細ついては、最近inter speechに論文が出たようなので、そちらをご覧ください。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://isw3.naist.jp/~kazuhiro-k/resource/kobayashi14IS.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Kobayashi 2014] Kobayashi, Kazuhiro, et al. &amp;ldquo;Statistical Singing Voice Conversion with Direct Waveform Modification based on the Spectrum Differential.&amp;rdquo; Fifteenth Annual Conference of the International Speech Communication Association. 2014.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;こばくん、論文を宣伝しておきますね＾＾&lt;/p&gt;
&lt;h3 id=&#34;51-差分特徴量の学習&#34;&gt;5.1 差分特徴量の学習&lt;/h3&gt;
&lt;p&gt;さて、差分スペクトル補正に基づく声質変換行うには、変換元話者$X$と目標話者$Y$の特徴量の同時分布$P(X,Y)$を学習するのではなく、$P(X, Y-X)$ （日本語で書くとややこしいのですが、変換元話者の特徴量$X$と、変換元話者と目標話者の差分特徴量$Y-X$の同時分布）を学習します。これは、 &lt;code&gt;train_gmm.jl&lt;/code&gt; を使ってGMMを学習する際に、&lt;code&gt;--diff&lt;/code&gt; とオプションをつけるだけでできます。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;julia train_gmm.jl ~/data/cmu_arctic_jld/parallel/clb_and_slt/ clb_to_slt_gmm32_order40_diff.jld --max 200 --n_components 32 --n_iter=100 --n_init=1 --diff
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可視化してみます。&lt;/p&gt;
&lt;p&gt;平均&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/clb_to_slt_gmm32_order40_mean.png &#34;means of trained DIFFGMM&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;共分散&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/clb_to_slt_gmm32_order40_covar.png &#34;covar of trained DIFFGMM&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;さっき学習したGMMとは、共分散はかなり形が違いますね。高次元成分でも、分散が比較的大きな値をとっているように見えます。形が異っているのは見てすぐにわかりますが、では具体的には何が異っているのか、それはなぜなのか、きちんと考えると面白そうですね。&lt;/p&gt;
&lt;h3 id=&#34;52-mlsaフィルタによる声質変換&#34;&gt;5.2 MLSAフィルタによる声質変換&lt;/h3&gt;
&lt;p&gt;差分スペクトル補正に基づく声質変換では、WORLDを使って音声の分析合成を行うのではなく、生の音声波形を入力として、MLSAフィルタをかけるのみです。これは、 &lt;code&gt;scripts/diffvc.jl&lt;/code&gt; を使うと簡単にできます。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;julia diffvc.jl ~/data/cmu_arctic/cmu_us_clb_arctic/wav/arctic_a0028.wav clb_to_slt_gmm32_order40_diff.jld clb_to_slt_a0028_diff.wav --order 40
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;さて、結果を聞いてみましょう。&lt;/p&gt;
&lt;h3 id=&#34;53-差分スペクトル補正に基づく声質変換結果&#34;&gt;5.3 差分スペクトル補正に基づく声質変換結果&lt;/h3&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;166&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/176093513&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;アイデアはシンプル、結果は良好、最高の手法ですね（べた褒め&lt;/p&gt;
&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;
&lt;p&gt;以上、長くなりましたが、統計的声質変換についてのチュートリアルはこれで終わります。誰の役に立つのか知らないけれど、役に立てば嬉しいです。トラジェクトリ変換やGVを考慮したバージョンなど、今回紹介していないものも実装しているので、詳しくは&lt;a href=&#34;https://github.com/r9y9/VoiceConversion.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Githubのリポジトリ&lt;/a&gt;をチェックしてください。バグをレポートしてくれたりすると、僕は喜びます。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;h3 id=&#34;以前書いた声質変換に関する記事&#34;&gt;以前書いた声質変換に関する記事&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://r9y9.github.io/blog/2014/07/05/statistical-voice-conversion-muzui/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;統計的声質変換クッソムズすぎワロタ - LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://r9y9.github.io/blog/2014/07/13/statistical-voice-conversion-wakaran/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;統計的声質変換クッソムズすぎワロタ（実装の話） - LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;論文&#34;&gt;論文&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://isw3.naist.jp/~tomoki/Tomoki/Journals/IEEE-Nov-2007_MLVC.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Toda 2007] T. Toda, A. W. Black, and K. Tokuda, “Voice conversion based on maximum likelihood estimation of spectral parameter trajectory,” IEEE
Trans. Audio, Speech, Lang. Process, vol. 15, no. 8, pp. 2222–2235,
Nov. 2007.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://isw3.naist.jp/~kazuhiro-k/resource/kobayashi14IS.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Kobayashi 2014] Kobayashi, Kazuhiro, et al. &amp;ldquo;Statistical Singing Voice Conversion with Direct Waveform Modification based on the Spectrum Differential.&amp;rdquo; Fifteenth Annual Conference of the International Speech Communication Association. 2014.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;faq&#34;&gt;FAQ&lt;/h2&gt;
&lt;h3 id=&#34;前はpythonで書いてなかった&#34;&gt;前はpythonで書いてなかった？&lt;/h3&gt;
&lt;p&gt;はい、https://gist.github.com/r9y9/88bda659c97f46f42525 ですね。正確には、GMMの学習・変換処理はpythonで書いて、特徴抽出、パラレルデータの作成、波形合成はGo言語で書いていました。が、Goとpythonでデータのやりとり、Goとpythonをいったり来たりするのが面倒になってしまって、一つの言語に統一したいと思うようになりました。Goで機械学習は厳しいと感じていたので、pythonで書くかなぁと最初は思ったのですが、WORLDやSPTKなど、Cのライブラリをpythonから使うのが思いの他面倒だったので（&lt;a href=&#34;https://github.com/r9y9/SPTK&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SPTKのpythonラッパー&lt;/a&gt;は書きましたが）、Cやpythonとの連携がしやすく、スクリプト言語でありながらCに速度面で引けをとらないjuliaに興味があったので、juliaですべて完結するようにしました。かなり実験的な試みでしたが、今はかなり満足しています。juliaさいこー&lt;/p&gt;
&lt;h3 id=&#34;新規性は&#34;&gt;新規性は？&lt;/h3&gt;
&lt;p&gt;ありません&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NMFで音源分離を試してみる</title>
      <link>https://r9y9.github.io/blog/2014/10/19/nmf-music-source-separation/</link>
      <pubDate>Sun, 19 Oct 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/10/19/nmf-music-source-separation/</guid>
      <description>&lt;p&gt;ずーーっと前に、 &lt;a href=&#34;http://r9y9.github.io/blog/2013/07/27/nmf-euclid/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NMFアルゴリズムの導出（ユークリッド距離版） - LESS IS MORE&lt;/a&gt; で実際に実装してみてやってみると書いていたのに、まったくやっていなかったことに気づいたのでやりました。&lt;/p&gt;
&lt;p&gt;音楽に対してやってみたのですが、簡単な曲だったら、まぁぼちぼち期待通りに動いたかなぁという印象です。コードとノートを挙げたので、興味のある方はどうぞ。&lt;/p&gt;
&lt;h2 id=&#34;github&#34;&gt;Github&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/julia-nmf-ss-toy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/julia-nmf-ss-toy&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;ノート&#34;&gt;ノート&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/r9y9/julia-nmf-ss-toy/blob/master/NMF-based%20Music%20Source%20Separation%20Demo.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NMF-based Music Source Separation Demo.ipynb | nbviewer&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;nmfのコード-julia&#34;&gt;NMFのコード (Julia)&lt;/h2&gt;
&lt;p&gt;NMFの実装の部分だけ抜き出しておきます。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function nmf_euc(Y::AbstractMatrix, K::Int=4;
                        maxiter::Int=100)
    H = rand(size(Y, 1), K)
    U = rand(K, size(Y, 2))
    const ϵ = 1.0e-21
    for i=1:maxiter
        H = H .* (Y*U&#39;) ./ (H*U*U&#39; + ϵ)
        U = U .* (H&#39;*Y) ./ (H&#39;*H*U + ϵ)
        U = U ./ maximum(U)
    end
    return H, U
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;いやー簡単ですねー。&lt;a href=&#34;http://r9y9.github.io/blog/2013/07/27/nmf-euclid/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NMFアルゴリズムの導出（ユークリッド距離版） - LESS IS MORE&lt;/a&gt; で導出した更新式ほぼそのままになってます（異なる点としては、ゼロ除算回避をしているのと、Uをイテレーション毎に正規化していることくらい）。&lt;/p&gt;
&lt;p&gt;B3, B4くらいの人にとっては参考になるかもしれないと思ってあげてみた。&lt;/p&gt;
&lt;p&gt;おわり&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>JuliaTokyo #2でBinDeps.jl についてLTしてきた</title>
      <link>https://r9y9.github.io/blog/2014/09/30/juliatokyo2/</link>
      <pubDate>Tue, 30 Sep 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/09/30/juliatokyo2/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;juliatokyo.connpass.com/event/8010/&#34;&gt;JuliaTokyo #2 - connpass&lt;/a&gt;&lt;/p&gt;
&lt;script async class=&#34;speakerdeck-embed&#34; data-id=&#34;21106ae0285e01327810268beacd0cf3&#34; data-ratio=&#34;1.77777777777778&#34; src=&#34;//speakerdeck.com/assets/embed.js&#34;&gt;&lt;/script&gt;
&lt;h2 id=&#34;発表概要&#34;&gt;発表概要&lt;/h2&gt;
&lt;p&gt;C/C++ライブラリのラッパー（C++は現状のJuliaでは難しいけど）を作るときに、どうやってライブラリの依存関係を管理するか？という話です。結論としては、方法はいくつかありますが　BinDeps.jl というパッケージを使うのが楽で良いですよ、ということです。Githubのいろんなリポジトリをあさった僕の経験上、BinDeps.jl はバイナリの依存関係管理におけるデファクトスタンダードな気がしています。BinDeps.jl の使い方は、既存のパッケージのコードを読みまくって学ぶのがおすすめです。&lt;/p&gt;
&lt;p&gt;さて、途中で書くのに疲れてしまったのですが、&lt;a href=&#34;http://qiita.com/r9y9/items/73806e3ce7f3a372d0b3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;自作のJuliaパッケージで、Cライブラリとの依存性を記述する - Qiita&lt;/a&gt; に以前似たような内容をまとめたので、併せてどうぞ。qiitaにも書きましたが、最適化関係のプロジェクトを集めた &lt;a href=&#34;http://www.juliaopt.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JuliaOpt&lt;/a&gt; コミュニティでは、バイナリの依存関係管理にBinDeps.jlを使用することを推奨しています。&lt;/p&gt;
&lt;h2 id=&#34;雑感&#34;&gt;雑感&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;勉強会にはデータ分析界隈の人が多い印象。音声系の人はとても少なかった。&lt;/li&gt;
&lt;li&gt;R人気だった&lt;/li&gt;
&lt;li&gt;Go使ってる！って人と合わなかった（つらい）&lt;/li&gt;
&lt;li&gt;@show マクロ最高&lt;/li&gt;
&lt;li&gt;unicode最高&lt;/li&gt;
&lt;li&gt;懇親会では、なぜか途中から深層学習やベイズの話をしていた…&lt;/li&gt;
&lt;li&gt;いい忘れたけど僕もnightly build勢でした。毎日あたたかみのある手動pull &amp;amp; make をしています。&lt;/li&gt;
&lt;li&gt;Julia の話ができて楽しかったので、また参加したいなー&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;LTで &lt;a href=&#34;https://github.com/chezou/MeCab.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MeCab.jl&lt;/a&gt; について話をしてくれたchezouさんが、ちょうどBinDeps.jl に興味を持たれているようだったので、勉強会のあとに BinDeps.jl を使ってバイナリの管理を実装して、&lt;a href=&#34;https://github.com/chezou/MeCab.jl/pull/2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;プルリク&lt;/a&gt;をしてみました。参考になればうれしいなーと思います。&lt;/p&gt;
&lt;p&gt;おしまい。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SPTKのJuliaラッパーも書いた</title>
      <link>https://r9y9.github.io/blog/2014/09/15/sptk-for-julia/</link>
      <pubDate>Mon, 15 Sep 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/09/15/sptk-for-julia/</guid>
      <description>&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/IMG_0960.JPG &#34;sea&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;夏も終わったようですね。またSPTKかという感じですが、Juliaから使うためのラッパーを書きました。必要そうなのはだいたいラップしたので、よろしければどうぞ。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/SPTK.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Julia wrapper for Speech Signal Processing Toolkit (SPTK) | Github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;かれこれ、Go, Python, Juliaと、3つの言語でラッパーを書いてしまいました。どれだけSPTK好きなんだと。そしてどれだけ言語触ってるんだ絞れと。うーん、とはいえどれも良いところと悪いところがあってですね（何も言ってない）、難しい…&lt;/p&gt;
&lt;p&gt;おしまい&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gamma Process Non-negative Matrix Factorization (GaP-NMF) in Julia</title>
      <link>https://r9y9.github.io/blog/2014/08/20/gap-nmf-julia/</link>
      <pubDate>Wed, 20 Aug 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/08/20/gap-nmf-julia/</guid>
      <description>&lt;p&gt;最近 &lt;a href=&#34;julialang.org&#34;&gt;Julia&lt;/a&gt; で遊んでいて、その過程で非負値行列因子分解（NMF）のノンパラ版の一つであるGamma Process Non-negative Matrix Factorization (GaP-NMF) を書いてみました。（まぁmatlabコードの写経なんですが）&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/BNMF.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/BNMF.jl&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;元論文:
&lt;a href=&#34;http://soundlab.cs.princeton.edu/publications/2010_icml_gapnmf.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bayesian Nonparametric Matrix Factorization for Recorded Music&lt;/a&gt;
by Matthew D. Hoffman et al. in ICML 2010.&lt;/p&gt;
&lt;h2 id=&#34;デモ&#34;&gt;デモ&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/r9y9/BNMF.jl/blob/master/notebook/GaP-NMF.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://nbviewer.ipython.org/github/r9y9/BNMF.jl/blob/master/notebook/GaP-NMF.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;適当な音声（音楽じゃなくてごめんなさい）に対して、GaP-NMFをfittingしてみた結果のメモです。$K=100$ で始めて、100回ほどイテレーションを回すと適度な数（12くらい）にtruncateしているのがわかると思います。予めモデルの複雑度を指定しなくても、データから適当な数を自動決定してくれる、ノンパラベイズの良いところですね。&lt;/p&gt;
&lt;h2 id=&#34;ハマったところ&#34;&gt;ハマったところ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;GIGの期待値を求めるのに必要な第二種変形ベッセル関数は、exponentially scaled versionを使いましょう。じゃないとInf地獄を見ることになると思います（つらい）。Juliaで言うなら &lt;a href=&#34;https://julia.readthedocs.org/en/latest/stdlib/base/?highlight=besselkx#Base.besselkx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;besselkx&lt;/a&gt; で、scipyで言うなら &lt;a href=&#34;http://students.mimuw.edu.pl/~pbechler/scipy_doc/generated/scipy.special.kve.html#scipy.special.kve&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;scipy.special.kve&lt;/a&gt; です。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;雑感&#34;&gt;雑感&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;MatlabのコードをJuliaに書き直すのは簡単。ところどころ作法が違うけど（例えば配列の要素へのアクセスはmatlabはA(i,j)でJuliaはA[i,j]）、だいたい一緒&lt;/li&gt;
&lt;li&gt;というかJuliaがMatlabに似すぎ？&lt;/li&gt;
&lt;li&gt;Gamma分布に従う乱数は、&lt;a href=&#34;https://github.com/JuliaStats/Distributions.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Distributions,jl&lt;/a&gt; を使えばめっちゃ簡単に生成できた。素晴らしすぎる&lt;/li&gt;
&lt;li&gt;行列演算がシンプルにかけてホント楽。pythonでもmatlabでもそうだけど（Goだとこれができないんですよ…）&lt;/li&gt;
&lt;li&gt;第二種変形ベッセル関数とか、scipy.special にあるような特殊関数が標準である。素晴らしい。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;python版と速度比較&#34;&gt;Python版と速度比較&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/dawenl/bp_nmf/tree/master/code/gap_nmf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bp_nmf/code/gap_nmf&lt;/a&gt; と比較します。matlabはもってないので比較対象からはずします、ごめんなさい&lt;/p&gt;
&lt;p&gt;Gistにベンチマークに使ったスクリプトと実行結果のメモを置いときました
&lt;a href=&#34;https://gist.github.com/r9y9/3d0c6a90dd155801c4c1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gist.github.com/r9y9/3d0c6a90dd155801c4c1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;結果だけ書いておくと、あらゆる現実を（ry の音声にGaP-NMFをepochs=100でfittingするのにかかった時間は、&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Julia: Mean elapsed time: 21.92968243 [sec]
Python: Mean elapsed time: 18.3550617 [sec]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;という結果になりました。つまりJuliaのほうが1.2倍くらい遅かった（僕の実装が悪い可能性は十分ありますが）。どこがボトルネックになっているのか調べていないので、気が向いたら調べます。Juliaの方が速くなったらいいなー&lt;/p&gt;
&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;
&lt;p&gt;GaP-NMFの実装チャレンジは二回目でした。（たぶん）一昨年、年末に実家に帰るときに、何を思ったのか急に実装したくなって、電車の中で論文を読んで家に着くなり実装するというエクストリームわけわからんことをしていましたが、その時はNaN and Inf地獄に負けてしまいました。Pythonで書いていましたが、今見るとそのコードマジクソでした。&lt;/p&gt;
&lt;p&gt;そして二回目である今回、最初はmatlabコードを見ずに自力で書いていたんですが、またもやInf地獄に合いもうだめだと思って、matlabコードを写経しました。あんま成長していないようです（つらい）&lt;/p&gt;
&lt;p&gt;Julia歴二週間くらいですが、良い感じなので使い続けて見ようと思います。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SPTKのPythonラッパーを書いた</title>
      <link>https://r9y9.github.io/blog/2014/08/10/sptk-from-python/</link>
      <pubDate>Sun, 10 Aug 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/08/10/sptk-from-python/</guid>
      <description>&lt;h2 id=&#34;20150906-追記&#34;&gt;2015/09/06 追記&lt;/h2&gt;
&lt;p&gt;ましなpythonラッパーを新しく作りました: &lt;a href=&#34;https://r9y9.github.io/blog/2015/09/06/pysptk/&#34;&gt;Pysptk: SPTKのpythonラッパーを作った (Part 2)&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;20140810-追記&#34;&gt;2014/08/10 追記&lt;/h2&gt;
&lt;p&gt;ipython notebookによる簡単なチュートリアルを貼っておきます&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/r9y9/SPTK/blob/master/notebook/SPTK%20calling%20from%20python.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SPTK を Pythonから呼ぶ | nbviewer&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;20141109&#34;&gt;2014/11/09&lt;/h2&gt;
&lt;p&gt;タイポ修正しました…&lt;/p&gt;
&lt;p&gt;scipy.mixture -&amp;gt; sklearn.mixture&lt;/p&gt;
&lt;p&gt;SPTKの中で最も価値がある（と僕が思っている）メルケプストラム分析、メルケプストラムからの波形合成（MLSA filter）がpythonから可能になります。&lt;/p&gt;
&lt;p&gt;ご自由にどうぞ&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/SPTK&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Speech Signal Processing Toolkit (SPTK) for API use with python | Github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;注意ですが、&lt;code&gt;SPTK.h&lt;/code&gt;にある関数を全部ラップしているわけではないです。僕が必要なものしか、現状はラップしていません（例えば、GMMとかラップする必要ないですよね？sklearn.mixture使えばいいし）。ただ、大方有用なものはラップしたと思います。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://r9y9.github.io/blog/2014/02/10/sptk-go-wrapper/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goで音声信号処理をしたいのでSPTKのGoラッパーを書く - LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Goでも書いたのにPythonでも書いてしまった。&lt;/p&gt;
&lt;p&gt;一年くらい前に元指導教員の先生と「Pythonから使えたらいいですよね」と話をしていました。先生、ようやく書きました。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tokyo.Scipyに参加してきた</title>
      <link>https://r9y9.github.io/blog/2014/08/05/tokyo-scipy/</link>
      <pubDate>Tue, 05 Aug 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/08/05/tokyo-scipy/</guid>
      <description>&lt;h2 id=&#34;tokyoscipyhttpsgithubcomtokyo-scipyarchive&#34;&gt;&lt;a href=&#34;https://github.com/tokyo-scipy/archive&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tokyo.SciPy&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;ハッシュタグ: &lt;a href=&#34;https://twitter.com/search?q=%23tokyoscipy&amp;amp;src=tyah&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#tokyoscipy&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Tokyo.Scipy は科学技術計算で Python を利用するための勉強会です．&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;とのことです。最近、python/numpy/scipyによくお世話になっているので、参加してみました。雑感をメモしておきます。&lt;/p&gt;
&lt;h2 id=&#34;tokyoscipy-006httpsgithubcomtokyo-scipyarchivetreemaster006&#34;&gt;&lt;a href=&#34;https://github.com/tokyo-scipy/archive/tree/master/006&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tokyo.Scipy 006&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;第6回のようでした。プログラムだけさっとまとめると、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;そこそこ大規模Python並列/パイプライン処理入門 w/o MapReduceレジーム (柏野雄太 @yutakashino) 45分&lt;/li&gt;
&lt;li&gt;初心者が陥るN個の罠。いざ進めNumpy/Scipyの道 (@nezuq) 15分&lt;/li&gt;
&lt;li&gt;Making computations reproducible (@fuzzysphere) 30分&lt;/li&gt;
&lt;li&gt;IPython Notebookで始めるデータ分析と可視化 (杜世橋 @lucidfrontier45) 30分&lt;/li&gt;
&lt;li&gt;PyMCがあれば，ベイズ推定でもう泣いたりなんかしない (神嶌敏弘 @shima__shima) 45分&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;という感じ。僕的には、@shima__shima 先生の発表が目当てだった&lt;/p&gt;
&lt;h2 id=&#34;雑感&#34;&gt;雑感&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;今回（？）はscipyの話はほとんどなかった。pythonを使った科学技術計算に関する幅広いトピックを扱ってる印象。&lt;/li&gt;
&lt;li&gt;ipython はやっぱ便利ですね。僕も良く使います&lt;/li&gt;
&lt;li&gt;@shima__shima 先生の発表がとてもわかりやすかったので、本当に参考にしたい&lt;/li&gt;
&lt;li&gt;正直もっとコアな話もあっていいのでは、と思った&lt;/li&gt;
&lt;li&gt;懇親会で気づいたが、意外と音声信号処理やってる（た）人がいてびっくりした&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/scikit-learn/scikit-learn&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;scikit-learn&lt;/a&gt; を初期の頃に作られてた方 &lt;a href=&#34;https://twitter.com/cournape&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@cournape&lt;/a&gt; がいてびっくり。開発当初はGMMとSVMくらいしかなくて全然ユーザーがつかなかったなどなど、裏話を色々聞けた&lt;/li&gt;
&lt;li&gt;フランス人の「たぶん大丈夫」は絶対無理の意（わろた&lt;/li&gt;
&lt;li&gt;Rust, juliaがいいと教えてもらった。うちjuliaは今やってみてるがなかなかいい&lt;/li&gt;
&lt;li&gt;発表でも話題に上がったけど、Pandasがいいという話を聞いたので、試してみたい&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;運営の方々、発表された方々、ありがとうございました。僕も機会が合えば何か発表したい&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Goでニューラルネットいくつか書いたけどやっぱPythonが楽でいいですね</title>
      <link>https://r9y9.github.io/blog/2014/07/29/neural-networks-in-go-and-python/</link>
      <pubDate>Tue, 29 Jul 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/07/29/neural-networks-in-go-and-python/</guid>
      <description>&lt;p&gt;いまいち成果出ないので気分転換にブログをだらだら書いてみるテストです。&lt;/p&gt;
&lt;h2 id=&#34;まえがき&#34;&gt;まえがき&lt;/h2&gt;
&lt;p&gt;半年くらい前に、某深層学習に興味を持ってやってみようかなーと思っていた時期があって、その時にGoでいくつかニューラルネットを書きました（参考：&lt;a href=&#34;http://r9y9.github.io/blog/2014/03/06/restricted-boltzmann-machines-mnist/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Restricted Boltzmann Machines with MNIST - LESS IS MORE&lt;/a&gt;、&lt;a href=&#34;https://github.com/r9y9/nnet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;githubに上げたコード&lt;/a&gt;）。なぜGoだったかというと、僕がGoに興味を持ち始めていたからというのが大きいです。Goを知る前は、たくさん計算するようなコードを書くときはC++だったけれど、C++は色々つらいものがあるし、GoはC++には速度面で劣るもののそこそこ速く、かつスクリプト的な書きやすさもあります。C++のデバッグやメンテに費やす膨大な時間に比べれば、計算時間が1.5~2倍に増えるくらい気にしないというスタンスで、僕はC++のかわりGoを使おうとしていました（※今でも間違っているとは思いませんが、とはいえ、厳しいパフォーマンスを求められる場合や既存の資産を有効活用したい場合など、必要な場面ではC++を書いています）。&lt;/p&gt;
&lt;h2 id=&#34;goで機械学習&#34;&gt;Goで機械学習&lt;/h2&gt;
&lt;p&gt;僕は機械学習がけっこう好きなので、Goでコード書くかーと思っていたのですが、結果としてまったく捗りませんでした。ニューラルネットをてきとーに書いたくらいです。&lt;/p&gt;
&lt;p&gt;検索するとわかりますが、現状、他の主流な言語に比べて圧倒的に数値計算のライブラリが少ないです。特に、線形代数、行列演算のデファクト的なライブラリがないのはつらいです。いくつか代表的なものをあげます。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/skelterjohn/go.matrix&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;skelterjohn/go.matrix&lt;/a&gt; - もうまったくメンテされていないし、たぶんするつもりはないと思います。使い勝手は、僕にとってはそんなに悪くなかった（試しに&lt;a href=&#34;https://gist.github.com/r9y9/9030922&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NMF&lt;/a&gt;を書いてみた）ですが、実装は純粋なGoで書かれていて、GPUを使って計算するのが流行りな時代では、例えば大きなニューラルネットをパラメータを変えながら何度も学習するのにはしんどいと思いました。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gonum/matrix&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gonum/matrix&lt;/a&gt; - 比較的最近出てきたライブラリで、&lt;a href=&#34;https://code.google.com/p/biogo/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;biogo&lt;/a&gt; から行列演算に関する部分を切り出して作られたもののようです。行列演算の内部でblasを使っていて、かつ将来的にはcublasにも対応したい、みたいな投稿をGoogle Groupsで見たのもあって、半年くらい前にはgoで行列演算を行うならこのライブラリを使うべきだと判断しました（以前けっこう調べました：&lt;a href=&#34;http://qiita.com/r9y9/items/7f93a89e3a88bb4ed263&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gonum/matrix のデザインコンセプトに関するメモ - Qiita&lt;/a&gt;）。しかし、それほど頻繁にアップデートされていませんし、機能もまだ少ないです。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;自分で作るかー、という考えも生まれなかったことはないですが、端的に言えばそれを行うだけのやる気がありませんでした。まぁ本当に必要だったら多少難しくてもやるのですが、ほら、僕達にはpythonがあるじゃないですか…&lt;/p&gt;
&lt;h2 id=&#34;pythonで機械学習&#34;&gt;Pythonで機械学習&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.google.co.jp/search?q=python&amp;#43;%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92&amp;amp;oq=python&amp;#43;%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;python 機械学習 - Google 検索&lt;/a&gt; 約 119,000 件（2014/07/29現在）&lt;/p&gt;
&lt;p&gt;もうみんなやってますよね。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.google.co.jp/search?q=Golang&amp;#43;%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92&amp;amp;oq=Golang&amp;#43;%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Golang 機械学習 - Google 検索&lt;/a&gt; 約 9,130 件（2014/07/29現在）&lt;/p&gt;
&lt;p&gt;いつかpythonのように増えるんでしょうか。正直に言って、わかりません（正確には、あんま考えていませんごめんなさい）&lt;/p&gt;
&lt;p&gt;さて、僕もよくpython使います。機械学習のコードを書くときは、だいたいpythonを使うようになりました（昔はC++で書いていました）。なぜかって、numpy, scipyのおかげで、とても簡潔に、かつ上手く書けばそこそこ速く書けるからです。加えて、ライブラリがとても豊富なんですよね、機械学習にかかわらず。numpy, scipyに加えて、matplotlibという優秀な描画ライブラリがあるのが、僕がpythonを使う大きな理由になっています。&lt;/p&gt;
&lt;p&gt;pythonの機械学習ライブラリは、&lt;a href=&#34;http://scikit-learn.org/stable/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;scikit-learn&lt;/a&gt; が特に有名でしょうか。僕もちょいちょい使います。使っていて最近おどろいたのは、scipy.mixtureには通常のGMMだけでなく変分GMM、無限混合GMMも入っていることですよね。自分で実装しようとしたら、たぶんとても大変です。昔変分GMMの更新式を導出したことがありますが、何度も心が折れそうになりました。いやー、いい時代になったもんですよ…（遠い目&lt;/p&gt;
&lt;h2 id=&#34;pythonでニューラルネットpylearn2を使おう&#34;&gt;Pythonでニューラルネット（pylearn2を使おう）&lt;/h2&gt;
&lt;p&gt;Deep何とかを含め流行りのニューラルネットが使える機械学習のライブラリでは、僕は &lt;a href=&#34;https://github.com/lisa-lab/pylearn2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pylearn2&lt;/a&gt; がよさ気だなーと思っています。理由は、高速かつ拡張性が高いからです。pylearn2は、数学的な記号表現からGPUコード（GPUがなければCPU向けのコード）を生成するmathコンパイラ &lt;a href=&#34;https://github.com/Theano/Theano&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Theano&lt;/a&gt; で書かれているためpythonでありながら高速で、かつ機械学習に置いて重要なコンポーネントであるデータ、モデル、アルゴリズムが上手く分離されて設計されているのがいいところかなと思います（全部ごっちゃに書いていませんか？僕はそうですごめんなさい。データはともかくモデルと学習を上手く切り分けるの難しい）。A Machine Learning library based on Theanoとのことですが、Deep learningで有名な &lt;a href=&#34;http://lisa.iro.umontreal.ca/index_en.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lisa lab&lt;/a&gt; 発ということもあり、ニューラルネットのライブラリという印象が少し強いですね。&lt;/p&gt;
&lt;p&gt;一つ重要なこととして、このライブラリはかなり研究者向けです。ブラックボックスとして使うのではなく、中身を読んで必要に応じて自分で拡張を書きたい人に向いているかと思います。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://arxiv.org/pdf/1308.4214v1.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ian J. Goodfellow, David Warde-Farley, Pascal Lamblin, Vincent Dumoulin, Mehdi Mirza, Razvan Pascanu, James Bergstra, Frédéric Bastien, and Yoshua Bengio. “Pylearn2: a machine learning research library”. arXiv preprint arXiv:1308.4214&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;↑の論文のIntroductionの部分に、その旨は明記されています。と、論文のリンクを貼っておいてなんですが、&lt;a href=&#34;http://www-etud.iro.umontreal.ca/~goodfeli/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ian Goodfellow&lt;/a&gt; のホームページにもっと簡潔に書いてありました。以下、引用します。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I wrote most of Pylearn2, a python library designed to make machine learning research convenient. Its mission is to provide a toolbox of interchangeable parts that provide a lot of flexibility for setting up machine learning experiments, providing enough extensibility that pretty much any research idea is feasible within the context of the library. This is in contrast to other machine learning libraries such as scikits-learn that are designed to be black boxes that just work. Think of pylearn2 as user friendly for machine learning researchers and scikits-learn as user friendly for developers that want to apply machine learning.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;pylearn2では、Multi-layer Perceptron (MLP)、Deep Bolztmann Machines (DBM)、新しいものでMaxout Network等、手軽に試すことができます（まぁゆうて計算はめっちゃ時間かかるけど）。先述の通りmathコンパイラの &lt;a href=&#34;https://github.com/Theano/Theano&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Theano&lt;/a&gt; を使って実装されているので、GPUがある場合はGPUを使って計算してくれます。環境構築に関しては、今はAWSという便利なサービスがあるので、GPUを持っていなくてもウェブ上でポチポチしてるだけで簡単にGPU環境を構築できます（参考：&lt;a href=&#34;http://r9y9.github.io/blog/2014/07/20/pylearn2-on-ec2-g2-2xlarge/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pylearn2, theanoをEC2 g2.x2large で動かす方法 - LESS IS MORE&lt;/a&gt;）。本当にいい時代になったものですね（二回目&lt;/p&gt;
&lt;p&gt;pylearn2、コードやドキュメント、活発なgithubでの開発、議論を見ていて、素晴らしいなーと思いました（まだ使い始めたばかりの僕の意見にあまり信憑性はないのですが…）。僕もこれくらい汎用性、拡張性のあるコードを書きたい人生でした…（自分の書いたニューラルネットのコードを見ながら）&lt;/p&gt;
&lt;h2 id=&#34;pylearn2は遅いって&#34;&gt;Pylearn2は遅いって？&lt;/h2&gt;
&lt;p&gt;本当に速さを求めるなら &lt;a href=&#34;https://code.google.com/p/cuda-convnet2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cuda-convnet2&lt;/a&gt; や &lt;a href=&#34;http://caffe.berkeleyvision.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cafee&lt;/a&gt;、もしくは直でcudaのAPIをだな…と言いたいところですが、確かにpylearn2は他の深層学習のライブラリに比べて遅いようです。最近、Convolutional Neural Network (CNN) に関するベンチマークがGithubで公開されていました。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/soumith/convnet-benchmarks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;soumith/convnet-benchmarks&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;現時点でまだ work in progressと書いてありますが、参考になると思います。優劣の問題ではなく、必要に応じて使い分ければいいと僕は思っています。&lt;/p&gt;
&lt;p&gt;さてさて、本当はここから僕が書いたGoのニューラルネットのコードがいかにクソかという話を書こうかと思ったのですが、長くなったのでまた今度にします。&lt;/p&gt;
&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Goでニューラルネットとか機械学習をやるのは現状しんどいし（&lt;a href=&#34;https://github.com/sjwhitworth/golearn&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;golearn&lt;/a&gt;とかあるけど、まだまだearly stage）、おとなしくpython使うのが無難&lt;/li&gt;
&lt;li&gt;pythonはやっぱり楽。ライブラリ豊富だし。ニューラルネットならpylearn2がおすすめ。ただし自分で拡張まで書きたい人向けです。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;散々pythonいいよゆうてますが、どちらかといえば僕はGoの方が好きです。機械学習には現状pythonを使うのがいいんじゃないかなーと思って、Goでニューラルネットを書いていた時を思い出しながらつらつらと書いてみました。&lt;/p&gt;
&lt;p&gt;おわり。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pylearn2, theanoをEC2 g2.x2large で動かす方法</title>
      <link>https://r9y9.github.io/blog/2014/07/20/pylearn2-on-ec2-g2-2xlarge/</link>
      <pubDate>Sun, 20 Jul 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/07/20/pylearn2-on-ec2-g2-2xlarge/</guid>
      <description>&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/dbm_learned_from_mnist.png &#34;Weight visualization of Restricted bolztomann machine trained on MNIST dataset.&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;h2 id=&#34;モチベーション&#34;&gt;モチベーション&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;手元のへぼマシンでニューラルネットの学習を回わす&lt;/li&gt;
&lt;li&gt;半日たっても終わらない&lt;/li&gt;
&lt;li&gt;最近だとGPU使って計算を高速化するのが流行りだが、手元にGPUはない&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;http://www.kurtsp.com/deep-learning-in-python-with-pylearn2-and-amazon-ec2.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Learning in Python with Pylearn2 and Amazon EC2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;手元にGPUがない…？大丈夫！Amazon EC2を使えば良さそう！！！&lt;/p&gt;
&lt;p&gt;というわけで、めんどくさいと言わずにec2にお手軽計算環境を整えます。ec2でGPUが乗ったものだと、g2.2xlargeがよさそうですね。&lt;/p&gt;
&lt;p&gt;ちなみに↑の図、pylearn2のtutorialのRestricted Bolzmann MachinesをMNISTで学習した結果なんですが、手元のマシンだとだいたい6時間くらい？（忘れた）だったのがg2.2xlargeだと30分もかかってない（ごめんなさい時間図るの忘れた）。$0.65/hourと安いんだし（他のインスタンスに比べればそりゃ高いけど）、もう手元のマシンで計算するの時間の無駄だしやめようと思います。&lt;/p&gt;
&lt;p&gt;さてさて、今回環境構築に少しはまったので、もうはまらないように簡単にまとめておきます。&lt;/p&gt;
&lt;h2 id=&#34;結論&#34;&gt;結論&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://aws.amazon.com/marketplace/pp/B00FYCDDTE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Amazon Linux AMI with NVIDIA GRID GPU Driver on AWS Marketplace &lt;/a&gt;&lt;/p&gt;
&lt;p&gt;すでにNVIDIAのドライバとCUDA（5.5）が入ったインスタンスをベースに使いましょう。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://xanxys.hatenablog.jp/entry/2014/05/17/135932&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EC2(g2.2xlarge)でOpenGLを使う方法&lt;/a&gt; で挙げられているように普通のlinuxを使う方法もありますが、ハマる可能性大です。僕はubuntuが使いたかったので最初はubuntu 14.04 server でドライバ、cuda (5.5 or 6.0) のインストールを試しましたが同じように失敗しました。&lt;/p&gt;
&lt;p&gt;イケイケと噂の音声認識ライブラリKaldiの&lt;a href=&#34;https://220-135-252-130.hinet-ip.hinet.net/speechwiki/index.php/Kaldi#installing_and_testing_CUDA-6.0_in_Ubuntu_14.04&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ドキュメントらしきもの&lt;/a&gt;を見ると、Ubuntu 14.04でもcuda 6.0インストールできるっぽいんですけどね…だめでした。頑張ればできるかもしれませんが、よほど強いメリットがない場合は、おとなしくpre-installされたインスタンスを使うのが吉だと思います。&lt;/p&gt;
&lt;h2 id=&#34;セットアップ&#34;&gt;セットアップ&lt;/h2&gt;
&lt;p&gt;↑で上げたインスタンスにはGPUドライバやCUDAは入っていますが、theanoもpylearn2もnumpyもscipyも入っていません。よって、それらは手動でインストールする必要があります。&lt;/p&gt;
&lt;p&gt;というわけで、インストールするシェルをメモって置きます。試行錯誤したあとに適当にまとめたshellなので、なんか抜けてたらごめんなさい。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://gist.github.com/r9y9/50f13ba28b5b158c25ae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gist.github.com/r9y9/50f13ba28b5b158c25ae&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash

# Pylearn2 setup script for Amazon Linux AMI with NVIDIA GRID GPU Driver.
# http://goo.gl/3KeXXW

sudo yum update -y
sudo yum install -y emacs tmux python-pip
sudo yum install -y python-devel git blas-devel lapack-devel

# numpy, scipy, matplotlib, etc.
sudo pip install numpy
sudo pip install scipy
sudo pip install cython
sudo pip install ipython nose

# matplotlib
sudo yum install -y libpng-devel freetype-devel
sudo pip install matplotlib

# Scikit-learn
sudo pip install scikit-learn

# Theano
sudo pip install --upgrade git+git://github.com/Theano/Theano.git

# Enable GPU for theano
echo &#39;[global]
floatX = float32
device = gpu0

[nvcc]
fastmath = True&#39; &amp;gt; .theanorc

# pylearn2
git clone git://github.com/lisa-lab/pylearn2.git
cd pylearn2
sudo python setup.py develop
cd ..

echo &amp;quot;export PYLEARN2_DATA_PATH=/home/ec2-user/data&amp;quot; &amp;gt;&amp;gt; .bashrc

# MNIST dataset
mkdir -p data/mnist/
cd data/mnist/
wget http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
gunzip train-images-idx3-ubyte.gz
wget http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
gunzip train-labels-idx1-ubyte.gz
wget http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
gunzip t10k-images-idx3-ubyte.gz
wget http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
gunzip t10k-labels-idx1-ubyte.gz
cd ../..
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;簡単ですね&lt;/p&gt;
&lt;p&gt;また、上記のような手順を踏まなくても、Community AMIs でpylearn2で検索するとすでにpylearn2が入ったAMIが出てくるので、それを使うのもありかもです（僕は試してません）。&lt;/p&gt;
&lt;p&gt;僕がAMIを公開してもいいんですが、今のところする予定はありません&lt;/p&gt;
&lt;h1 id=&#34;まとめ&#34;&gt;まとめ&lt;/h1&gt;
&lt;p&gt;そこそこ良い計算環境がさくっとできました、まる。ラーメン食べたい&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://aws.amazon.com/jp/ec2/instance-types/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;インスタンスタイプ - Amazon EC2 (クラウド上の仮想サーバー Amazon Elastic Compute Cloud) | アマゾン ウェブ サービス（AWS 日本語）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.kurtsp.com/deep-learning-in-python-with-pylearn2-and-amazon-ec2.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Learning in Python with Pylearn2 and Amazon EC2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://xanxys.hatenablog.jp/entry/2014/05/17/135932&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EC2(g2.2xlarge)でOpenGLを使う方法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>統計的声質変換クッソムズすぎワロタ（実装の話）</title>
      <link>https://r9y9.github.io/blog/2014/07/13/statistical-voice-conversion-wakaran/</link>
      <pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/07/13/statistical-voice-conversion-wakaran/</guid>
      <description>&lt;p&gt;2014/07/28 追記：
重み行列の構築の部分を改良したのでちょいアップデート。具体的にはdense matrixとして構築してからスパース行列に変換していたのを、はじめからスパース行列として構築するようにして無駄にメモリを使わないようにしました。あとdiffが見やすいようにgistにあげました
&lt;a href=&#34;https://gist.github.com/r9y9/88bda659c97f46f42525&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gist.github.com/r9y9/88bda659c97f46f42525&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;まえがき&#34;&gt;まえがき&lt;/h2&gt;
&lt;p&gt;前回、&lt;a href=&#34;http://r9y9.github.io/blog/2014/07/05/statistical-voice-conversion-muzui/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;統計的声質変換クッソムズすぎワロタ - LESS IS MORE&lt;/a&gt; という記事を書いたら研究者の方々等ちょいちょい反応してくださって嬉しかったです。差分スペクトル補正、その道の人が聴いても音質がいいそう。これはいい情報です。&lt;/p&gt;
&lt;p&gt;Twitter引用:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; lang=&#34;en&#34;&gt;&lt;p&gt;統計的声質変換クッソムズすぎワロタ - LESS IS MORE &lt;a href=&#34;http://t.co/8RkeXIf6Ym&#34;&gt;http://t.co/8RkeXIf6Ym&lt;/a&gt; &lt;a href=&#34;https://twitter.com/r9y9&#34;&gt;@r9y9&lt;/a&gt;さんから ムズすぎと言いながら，最後の音はしっかり出ているあたり凄いなぁ．&lt;/p&gt;&amp;mdash; M. Morise (忍者系研究者) (@m_morise) &lt;a href=&#34;https://twitter.com/m_morise/statuses/485339123171852289&#34;&gt;July 5, 2014&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; lang=&#34;en&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://twitter.com/ballforest&#34;&gt;@ballforest&lt;/a&gt; 従来のパラメータ変換と比較すると、音質は従来よりもよさそうな気はしますがスペクトル包絡の性差ががっつりと影響しそうな気もするんですよね。&lt;/p&gt;&amp;mdash; 縄文人（妖精系研究者なのです） (@dicekicker) &lt;a href=&#34;https://twitter.com/dicekicker/statuses/485380534122463232&#34;&gt;July 5, 2014&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;異性間に関しては、実験が必要ですね。異性間だとF0が結構変わってくると思いますが、差分スペクトル補正の場合そもそもF0をいじらないという前提なので、F0とスペクトル包絡が完全に独立でない（ですよね？）以上、同姓間に比べて音質は劣化する気はします。簡単にやったところ、少なくとも僕の主観的には劣化しました&lt;/p&gt;
&lt;p&gt;ところで、結構いい感じにできたぜひゃっはーと思って、先輩に聞かせてみたら違いわかんねと言われて心が折れそうになりました。やはり現実はつらいです。&lt;/p&gt;
&lt;h2 id=&#34;実装の話&#34;&gt;実装の話&lt;/h2&gt;
&lt;p&gt;さて、今回は少し実装のことを書こうと思います。学習&amp;amp;変換部分はPythonで書いています。その他はGo（※Goの話は書きません）。&lt;/p&gt;
&lt;h3 id=&#34;トラジェクトリベースのパラメータ変換が遅いのは僕の実装が悪いからでした本当に申し訳ありませんでしたorz&#34;&gt;トラジェクトリベースのパラメータ変換が遅いのは僕の実装が悪いからでした本当に申し訳ありませんでしたorz&lt;/h3&gt;
&lt;p&gt;前回トラジェクトリベースは処理が激重だと書きました。なんと、4秒程度の音声（フレームシフト5msで777フレーム）に対して変換部分に600秒ほどかかっていたのですが（重すぎワロタ）、結果から言えばPythonでも12秒くらいまでに高速化されました（混合数64, メルケプの次元数40+デルタ=80次元、分散共分散はfull）。本当にごめんなさい。&lt;/p&gt;
&lt;p&gt;何ヶ月か前、ノリでトラジェクトリベースの変換を実装しようと思ってサクッと書いたのがそのままで、つまりとても効率の悪い実装になっていました。具体的には放置していた問題が二つあって、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ナイーブな逆行列の計算&lt;/li&gt;
&lt;li&gt;スパース性の無視&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;です。特に後者はかなりパフォーマンスに影響していました&lt;/p&gt;
&lt;h3 id=&#34;ナイーブな逆行列の計算&#34;&gt;ナイーブな逆行列の計算&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://d.hatena.ne.jp/sleepy_yoshi/20120513/p1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;numpy.linalg.invとnumpy.linalg.solveを用いた逆行列計算 - 睡眠不足？！ (id:sleepy_yoshi)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;numpy.linalg.inv&lt;/code&gt;を使っていましたよね。しかも&lt;code&gt;numpy.linalg.solve&lt;/code&gt;のほうが速いことを知っていながら。一ヶ月前の自分を問い詰めたい。&lt;code&gt;numpy.linalg.solve&lt;/code&gt;で置き換えたら少し速くなりました。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;600秒 -&amp;gt; 570秒&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;1.05倍の高速化&lt;/p&gt;
&lt;h3 id=&#34;スパース性の無視&#34;&gt;スパース性の無視&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://isw3.naist.jp/~tomoki/Tomoki/Journals/IEEE-Nov-2007_MLVC.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;T. Toda, A. W. Black, and K. Tokuda, “Voice conversion based on maximum likelihood estimation of spectral parameter trajectory,” IEEE Trans. Audio, Speech, Lang. Process, vol. 15, no. 8, pp. 2222–2235, Nov. 2007&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;論文を見ていただければわかるのですが、トラジェクトリベースの変換法における多くの計算は、行列を使って表すことができます。で、論文中の$W$という行列は、サイズがめちゃくちゃでかいのですがほとんどの要素は0です。この性質を使わない理由はないですよね？？&lt;/p&gt;
&lt;p&gt;…残念なことに、僕は密行列として扱って計算していました。ほら、疎行列ってちょっと扱いづらいじゃないですか…めんどくさそう…と思って放置してました。ごめんなさい&lt;/p&gt;
&lt;p&gt;pythonで疎行列を扱うなら、scipy.sparseを使えば良さそうです。結果、$W$を疎行列として扱うことで行列演算は大きく高速化されました。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;570秒 -&amp;gt; 12秒くらい&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;単純に考えると50倍の高速化ですか。本当にアホだった。最初からscipy.sparse使っておけばよかったです。&lt;/p&gt;
&lt;p&gt;scipy.sparseの使い方は以下を参考にしました。みなさんぜひ使いましょう&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://sucrose.hatenablog.com/entry/2013/04/07/130625&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Python で疎行列(SciPy) - 唯物是真 @Scaled_Wurm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://docs.scipy.org/doc/scipy/reference/sparse.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sparse matrices (scipy.sparse) — SciPy v0.14.0 Reference Guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://lucidfrontier45.wordpress.com/2011/08/02/scipysparse_matmul/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;scipy.sparseで疎行列の行列積 | frontier45&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;コード&#34;&gt;コード&lt;/h2&gt;
&lt;p&gt;メモ的な意味で主要なコードを貼っておきます。
&lt;a href=&#34;https://gist.github.com/r9y9/88bda659c97f46f42525&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gist.github.com/r9y9/88bda659c97f46f42525&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/usr/bin/python
# coding: utf-8

import numpy as np
from numpy import linalg
from sklearn.mixture import GMM
import scipy.linalg
import scipy.sparse
import scipy.sparse.linalg

class GMMMap:
    &amp;quot;&amp;quot;&amp;quot;GMM-based frame-by-frame speech parameter mapping.

    GMMMap represents a class to transform spectral features of a source
    speaker to that of a target speaker based on Gaussian Mixture Models
    of source and target joint spectral features.

    Notation
    --------
    Source speaker&#39;s feature: X = {x_t}, 0 &amp;lt;= t &amp;lt; T
    Target speaker&#39;s feature: Y = {y_t}, 0 &amp;lt;= t &amp;lt; T
    where T is the number of time frames.

    Parameters
    ----------
    gmm : scipy.mixture.GMM
        Gaussian Mixture Models of source and target joint features

    swap : bool
        True: source -&amp;gt; target
        False target -&amp;gt; source

    Attributes
    ----------
    num_mixtures : int
        the number of Gaussian mixtures

    weights : array, shape (`num_mixtures`)
        weights for each gaussian

    src_means : array, shape (`num_mixtures`, `order of spectral feature`)
        means of GMM for a source speaker

    tgt_means : array, shape (`num_mixtures`, `order of spectral feature`)
        means of GMM for a target speaker

    covarXX : array, shape (`num_mixtures`, `order of spectral feature`,
        `order of spectral feature`)
        variance matrix of source speaker&#39;s spectral feature

    covarXY : array, shape (`num_mixtures`, `order of spectral feature`,
        `order of spectral feature`)
        covariance matrix of source and target speaker&#39;s spectral feature

    covarYX : array, shape (`num_mixtures`, `order of spectral feature`,
        `order of spectral feature`)
        covariance matrix of target and source speaker&#39;s spectral feature

    covarYY : array, shape (`num_mixtures`, `order of spectral feature`,
        `order of spectral feature`)
        variance matrix of target speaker&#39;s spectral feature

    D : array, shape (`num_mixtures`, `order of spectral feature`,
        `order of spectral feature`)
        covariance matrices of target static spectral features

    px : scipy.mixture.GMM
        Gaussian Mixture Models of source speaker&#39;s features

    Reference
    ---------
      - [Toda 2007] Voice Conversion Based on Maximum Likelihood Estimation
        of Spectral Parameter Trajectory.
        http://isw3.naist.jp/~tomoki/Tomoki/Journals/IEEE-Nov-2007_MLVC.pdf

    &amp;quot;&amp;quot;&amp;quot;
    def __init__(self, gmm, swap=False):
        # D is the order of spectral feature for a speaker
        self.num_mixtures, D = gmm.means_.shape[0], gmm.means_.shape[1]/2
        self.weights = gmm.weights_

        # Split source and target parameters from joint GMM
        self.src_means = gmm.means_[:, 0:D]
        self.tgt_means = gmm.means_[:, D:]
        self.covarXX = gmm.covars_[:, :D, :D]
        self.covarXY = gmm.covars_[:, :D, D:]
        self.covarYX = gmm.covars_[:, D:, :D]
        self.covarYY = gmm.covars_[:, D:, D:]

        # swap src and target parameters
        if swap:
            self.tgt_means, self.src_means = self.src_means, self.tgt_means
            self.covarYY, self.covarXX = self.covarXX, self.covarYY
            self.covarYX, self.covarXY = self.XY, self.covarYX

        # Compute D eq.(12) in [Toda 2007]
        self.D = np.zeros(self.num_mixtures*D*D).reshape(self.num_mixtures, D, D)
        for m in range(self.num_mixtures):
            xx_inv_xy = np.linalg.solve(self.covarXX[m], self.covarXY[m])
            self.D[m] = self.covarYY[m] - np.dot(self.covarYX[m], xx_inv_xy)

        # p(x), which is used to compute posterior prob. for a given source
        # spectral feature in mapping stage.
        self.px = GMM(n_components=self.num_mixtures, covariance_type=&amp;quot;full&amp;quot;)
        self.px.means_ = self.src_means
        self.px.covars_ = self.covarXX
        self.px.weights_ = self.weights

    def convert(self, src):
        &amp;quot;&amp;quot;&amp;quot;
        Mapping source spectral feature x to target spectral feature y
        so that minimize the mean least squared error.
        More specifically, it returns the value E(p(y|x)].

        Parameters
        ----------
        src : array, shape (`order of spectral feature`)
            source speaker&#39;s spectral feature that will be transformed

        Return
        ------
        converted spectral feature
        &amp;quot;&amp;quot;&amp;quot;
        D = len(src)

        # Eq.(11)
        E = np.zeros((self.num_mixtures, D))
        for m in range(self.num_mixtures):
            xx = np.linalg.solve(self.covarXX[m], src - self.src_means[m])
            E[m] = self.tgt_means[m] + self.covarYX[m].dot(xx)

        # Eq.(9) p(m|x)
        posterior = self.px.predict_proba(np.atleast_2d(src))

        # Eq.(13) conditinal mean E[p(y|x)]
        return posterior.dot(E)

class TrajectoryGMMMap(GMMMap):
    &amp;quot;&amp;quot;&amp;quot;
    Trajectory-based speech parameter mapping for voice conversion
    based on the maximum likelihood criterion.

    Parameters
    ----------
    gmm : scipy.mixture.GMM
        Gaussian Mixture Models of source and target speaker joint features

    gv : scipy.mixture.GMM (default=None)
        Gaussian Mixture Models of target speaker&#39;s global variance of spectral
        feature

    swap : bool (default=False)
        True: source -&amp;gt; target
        False target -&amp;gt; source

    Attributes
    ----------
    TODO

    Reference
    ---------
      - [Toda 2007] Voice Conversion Based on Maximum Likelihood Estimation
        of Spectral Parameter Trajectory.
        http://isw3.naist.jp/~tomoki/Tomoki/Journals/IEEE-Nov-2007_MLVC.pdf
    &amp;quot;&amp;quot;&amp;quot;
    def __init__(self, gmm, T, gv=None, swap=False):
        GMMMap.__init__(self, gmm, swap)

        self.T = T
        # shape[1] = d(src) + d(src_delta) + d(tgt) + d(tgt_delta)
        D = gmm.means_.shape[1] / 4

        ## Setup for Trajectory-based mapping
        self.__construct_weight_matrix(T, D)

        ## Setup for GV post-filtering
        # It is assumed that GV is modeled as a single mixture GMM
        if gv != None:
            self.gv_mean = gv.means_[0]
            self.gv_covar = gv.covars_[0]
            self.Pv = np.linalg.inv(self.gv_covar)

    def __construct_weight_matrix(self, T, D):
        # Construct Weight matrix W
        # Eq.(25) ~ (28)
        for t in range(T):
            w0 = scipy.sparse.lil_matrix((D, D*T))
            w1 = scipy.sparse.lil_matrix((D, D*T))
            w0[0:,t*D:(t+1)*D] = scipy.sparse.diags(np.ones(D), 0)

            if t-1 &amp;gt;= 0:
                tmp = np.zeros(D)
                tmp.fill(-0.5)
                w1[0:,(t-1)*D:t*D] = scipy.sparse.diags(tmp, 0)
            if t+1 &amp;lt; T:
                tmp = np.zeros(D)
                tmp.fill(0.5)
                w1[0:,(t+1)*D:(t+2)*D] = scipy.sparse.diags(tmp, 0)

            W_t = scipy.sparse.vstack([w0, w1])

            # Slower
            # self.W[2*D*t:2*D*(t+1),:] = W_t

            if t == 0:
                self.W = W_t
            else:
                self.W = scipy.sparse.vstack([self.W, W_t])

        self.W = scipy.sparse.csr_matrix(self.W)

        assert self.W.shape == (2*D*T, D*T)

    def convert(self, src):
        &amp;quot;&amp;quot;&amp;quot;
        Mapping source spectral feature x to target spectral feature y
        so that maximize the likelihood of y given x.

        Parameters
        ----------
        src : array, shape (`the number of frames`, `the order of spectral feature`)
            a sequence of source speaker&#39;s spectral feature that will be
            transformed

        Return
        ------
        a sequence of transformed spectral features
        &amp;quot;&amp;quot;&amp;quot;
        T, D = src.shape[0], src.shape[1]/2

        if T != self.T:
            self.__construct_weight_matrix(T, D)

        # A suboptimum mixture sequence  (eq.37)
        optimum_mix = self.px.predict(src)

        # Compute E eq.(40)
        self.E = np.zeros((T, 2*D))
        for t in range(T):
            m = optimum_mix[t] # estimated mixture index at time t
            xx = np.linalg.solve(self.covarXX[m], src[t] - self.src_means[m])
            # Eq. (22)
            self.E[t] = self.tgt_means[m] + np.dot(self.covarYX[m], xx)
        self.E = self.E.flatten()

        # Compute D eq.(41). Note that self.D represents D^-1.
        self.D = np.zeros((T, 2*D, 2*D))
        for t in range(T):
            m = optimum_mix[t]
            xx_inv_xy = np.linalg.solve(self.covarXX[m], self.covarXY[m])
            # Eq. (23)
            self.D[t] = self.covarYY[m] - np.dot(self.covarYX[m], xx_inv_xy)
            self.D[t] = np.linalg.inv(self.D[t])
        self.D = scipy.linalg.block_diag(*self.D)

        # represent D as a sparse matrix
        self.D = scipy.sparse.csr_matrix(self.D)

        # Compute target static features
        # eq.(39)
        covar = self.W.T.dot(self.D.dot(self.W))
        y = scipy.sparse.linalg.spsolve(covar, self.W.T.dot(self.D.dot(self.E)),\
                                        use_umfpack=False)
        return y.reshape((T, D))
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;結論&#34;&gt;結論&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;疎行列の演算を考えるときは、間違ってもめんどくさいとか思わずに疎行列を積極的に使おう&lt;/li&gt;
&lt;li&gt;統計的声質変換ムズすぎ&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;おまけめも&#34;&gt;おまけめも&lt;/h2&gt;
&lt;p&gt;僕が変換精度を改善するために考えていることのめも&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;統計的な手法を使う限りover-smoothingの問題はついてくる。ならば、逆にover-smoothingされることで都合の良い特徴量を考えることはできないか&lt;/li&gt;
&lt;li&gt;メルケプとかそもそもスペクトル包絡をコンパクトにparamtricに表現するために考えられたもの（だと思ってる）ので、高品質な変換を考えるならばスペクトル包絡をそのまま使うなりした方がいいんじゃないか。とはいえスペクトル包絡をそのまま使うのはぼちぼち高次元なので、個人性に依存する部分を残した形で非線形次元削減したらどうか（例えばニューラルネットを使って統計的に個人性に依存する部分を見つけ出すとか）&lt;/li&gt;
&lt;li&gt;time-dependentな関係をモデル化しないとだめじゃないか、確率モデルとして。RNNとか普通に使えそうだし、まぁHMMでもよい&lt;/li&gt;
&lt;li&gt;音素境界を推定して、segment単位で変換するのも良いかも&lt;/li&gt;
&lt;li&gt;識別モデルもっと使ってもいいんじゃないか&lt;/li&gt;
&lt;li&gt;波形合成にSPTKのmlsadfコマンド使ってる？あれ実はフレーム間のメルケプが線形補間されてるんですよね。本当に線形補間でいいんでしょうか？他の補間法も試したらどうですかね&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;こんなかんじですか。おやすみなさい&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>統計的声質変換クッソムズすぎワロタ</title>
      <link>https://r9y9.github.io/blog/2014/07/05/statistical-voice-conversion-muzui/</link>
      <pubDate>Sat, 05 Jul 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/07/05/statistical-voice-conversion-muzui/</guid>
      <description>&lt;h2 id=&#34;20141012-追記&#34;&gt;2014/10/12 追記&lt;/h2&gt;
&lt;p&gt;少なくともGVのコードに致命的なバグがあったことがわかりました。よって、あまりあてにしないでください…（ごめんなさい&lt;/p&gt;
&lt;p&gt;こんにちは。&lt;/p&gt;
&lt;p&gt;最近、統計的声質変換の勉強をしていました。で、メジャーなGMM（混合ガウスモデル）ベースの変換を色々やってみたので、ちょろっと書きます。実は（というほどでもない?）シンプルなGMMベースの方法だと音質クッソ悪くなってしまうんですが、色々試してやっとまともに聞ける音質になったので、試行錯誤の形跡を残しておくとともに、音声サンプルを貼っておきます。ガチ勢の方はゆるりと見守ってください&lt;/p&gt;
&lt;p&gt;基本的に、以下の論文を参考にしています&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://isw3.naist.jp/~tomoki/Tomoki/Journals/IEEE-Nov-2007_MLVC.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;T. Toda, A. W. Black, and K. Tokuda, “Voice conversion based on maximum likelihood estimation of spectral parameter trajectory,” IEEE Trans. Audio, Speech, Lang. Process, vol. 15, no. 8, pp. 2222–2235, Nov. 2007&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;gmmベースの声質変換の基本&#34;&gt;GMMベースの声質変換の基本&lt;/h2&gt;
&lt;p&gt;シンプルなGMMベースの声質変換は大きく二つのフェーズに分けられます。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;参照話者と目標話者のスペクトル特徴量の結合GMM $P(x,y)$を学習する&lt;/li&gt;
&lt;li&gt;入力$x$が与えらたとき、$P(y|x)$が最大となるようにスペクトル特徴量を変換する&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;あらかじめ話者間の関係をデータから学習しておくことで、未知の入力が来た時にも変換が可能になるわけです。&lt;/p&gt;
&lt;p&gt;具体的な変換プロセスとしては、音声を&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基本周波数&lt;/li&gt;
&lt;li&gt;非周期性成分&lt;/li&gt;
&lt;li&gt;スペクトル包絡&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;の3つに分解し、スペクトル包絡の部分（≒声質を表す特徴量）に対して変換を行い、最後に波形を再合成するといった方法がよく用いられます。基本周波数や非周期性成分も変換することがありますが、ここではとりあえず扱いません&lt;/p&gt;
&lt;p&gt;シンプルな方法では、フレームごとに独立に変換を行います。&lt;/p&gt;
&lt;p&gt;GMMベースのポイントは、東大の齋藤先生の以下のツイートを引用しておきます。&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; lang=&#34;en&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://twitter.com/shurabaP&#34;&gt;@shurabaP&lt;/a&gt; GMMベースの声質変換の肝は、入力xが与えられた時の出力yの条件付き確率P(y|x) が最大になるようにyを選ぶという確率的な考えです。私のショボい自作スクリプトですが、HTKを使ったGMMの学習レシピは研究室内部用に作ってあるので、もし必要なら公開しますよ。&lt;/p&gt;&amp;mdash; Daisuke Saito (@dsk_saito) &lt;a href=&#34;https://twitter.com/dsk_saito/statuses/48442052534472706&#34;&gt;March 17, 2011&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;ちなみに僕はscipy.mixture.GMMを使いました。HTKヤダー&lt;/p&gt;
&lt;h2 id=&#34;やってみる&#34;&gt;やってみる&lt;/h2&gt;
&lt;p&gt;さて、実際にやってみます。データベースには、[CMU_ARCTIC speech synthesis databases](ht
tp://www.festvox.org/cmu_arctic/)を使います。今回は、女性話者の二人を使いました。&lt;/p&gt;
&lt;p&gt;音声の分析合成には、&lt;a href=&#34;http://ml.cs.yamanashi.ac.jp/world/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WORLD&lt;/a&gt;を使います。WORLDで求めたスペクトル包絡からメルケプストラム（今回は32次元）に変換したものを特徴量として使いました。&lt;/p&gt;
&lt;p&gt;学習では、学習サンプル10641フレーム（23フレーズ）、GMMの混合数64、full-covarianceで学習しました。&lt;/p&gt;
&lt;h3 id=&#34;変換元となる話者参照話者&#34;&gt;変換元となる話者（参照話者）&lt;/h3&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;166&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/157362625&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false&#34;&gt;&lt;/iframe&gt;
&lt;h3 id=&#34;変換対象となる話者目標話者&#34;&gt;変換対象となる話者（目標話者）&lt;/h3&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;166&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/157362613&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false&#34;&gt;&lt;/iframe&gt;
&lt;h3 id=&#34;gmmベースのframe-by-frameな声質変換の結果&#34;&gt;GMMベースのframe-by-frameな声質変換の結果&lt;/h3&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;166&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/157371966&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;はぁー、正直聞けたもんじゃないですね。声質は目標話者に近づいている感がありますが、何分音質が悪い。学習条件を色々変えて試行錯誤しましたけどダメでした&lt;/p&gt;
&lt;h2 id=&#34;gmmベースの声質変換の弱点&#34;&gt;GMMベースの声質変換の弱点&lt;/h2&gt;
&lt;p&gt;さて、なぜダメかを考えます。もう考えつくされてる感あるけど、大事なところだけ整理します&lt;/p&gt;
&lt;h3 id=&#34;フレーム毎に独立な変換処理&#34;&gt;フレーム毎に独立な変換処理&lt;/h3&gt;
&lt;p&gt;まず、音声が時間的に独立なわけないですよね。フレームごとに独立に変換すると、時間的に不連続な点が出てきてしまいます。その結果、ちょっとノイジーな音声になってしまったのではないかと考えられます。&lt;/p&gt;
&lt;p&gt;これに対する解決法としては、戸田先生の論文にあるように、動的特徴量も併せてGMMを学習して、系列全体の確率が最大となるように変換を考えるトラジェクトリベースのパラメータ生成方法があります。&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;さて、やってみます。参照音声、目標音声は↑で使ったサンプルと同じです。&lt;/p&gt;
&lt;h3 id=&#34;トラジェクトリベースの声質変換の結果&#34;&gt;トラジェクトリベースの声質変換の結果&lt;/h3&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;166&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/157371969&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;あんま変わらないですね。計算量めっちゃ食うのに、本当につらい。実装が間違ってる可能性もあるけど…&lt;/p&gt;
&lt;p&gt;他の方法を考えるとするならば、まぁいっぱいあると思うんですが、スペクトル包絡なんて時間的に不連続にコロコロ変わるようなもんでもない気がするので、確率モデルとしてそういう依存関係を考慮した声質変換があってもいいもんですけどね。あんま見てない気がします。&lt;/p&gt;
&lt;p&gt;ちょっと調べたら見つかったもの↓&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://koasas.kaist.ac.kr/bitstream/10203/17632/1/25.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kim, E.K., Lee, S., Oh, Y.-H. (1997). &amp;ldquo;Hidden Markov Model Based Voice Conversion Using Dynamic Characteristics of Speaker&amp;rdquo;, Proc. of Eurospeech’97, Rhodes, Greece, pp. 2519-2522.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;過剰な平滑化&#34;&gt;過剰な平滑化&lt;/h3&gt;
&lt;p&gt;これはGMMに限った話ではないですが、GMMベースのFrame-by-Frameな声質変換の場合でいえば、変換後の特徴量は条件付き期待値を取ることになるので、まぁ常識的に考えて平滑化されますよね。&lt;/p&gt;
&lt;p&gt;これに対する解法としては、GV（Global Variance）を考慮する方法があります。これは戸田先生が提案されたものですね。&lt;/p&gt;
&lt;p&gt;さて、やってみます。wktk&lt;/p&gt;
&lt;h3 id=&#34;gvを考慮したトラジェクトリベースの声質変換の結果&#34;&gt;GVを考慮したトラジェクトリベースの声質変換の結果&lt;/h3&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;166&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/157371971&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;多少ましになった気もしなくもないけど、やっぱり音質はいまいちですね。そして計算量は激マシします。本当につらい。学会で聞いたGVありの音声はもっと改善してた気がするんだけどなー音声合成の話だけど。僕の実装が間違ってるんですかね…&lt;/p&gt;
&lt;h2 id=&#34;ムズすぎわろた&#34;&gt;ムズすぎわろた&lt;/h2&gt;
&lt;p&gt;以上、いくつか試しましたが、統計的声質変換は激ムズだということがわかりました。え、ここで終わるの？という感じですが、最後に一つ別の手法を紹介します。&lt;/p&gt;
&lt;h2 id=&#34;差分スペクトル補正に基づく統計的声質変換&#34;&gt;差分スペクトル補正に基づく統計的声質変換&lt;/h2&gt;
&lt;p&gt;これまでは、音声を基本周波数、非周期性成分、スペクトル包絡に分解して、スペクトル包絡を表す特徴量を変換し、変換後の特徴量を元に波形を再合成していました。ただ、よくよく考えると、そもそも基本周波数、非周期性成分をいじる必要がない場合であれば、わざわざ分解して再合成する必要なくね？声質の部分のみ変換するようなフィルタかけてやればよくね？という考えが生まれます。実は、そういったアイデアに基づく素晴らしい手法があります。それが、差分スペクトル補正に基づく声質変換です。&lt;/p&gt;
&lt;p&gt;詳細は、以下の予稿をどうぞ&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.phontron.com/paper/kobayashi14asj.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;小林 和弘, 戸田 智基, Graham Neubig, Sakriani Sakti, 中村 哲. &amp;ldquo;差分スペクトル補正に基づく統計的歌声声質変換&amp;rdquo;, 日本音響学会2014年春季研究発表会(ASJ). 東京. 2014年3月.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;では、やってみます。歌声ではなく話し声ですが。他の声質変換の結果とも聴き比べてみてください。&lt;/p&gt;
&lt;h3 id=&#34;差分スペクトル補正に基づく声質変換の結果&#34;&gt;差分スペクトル補正に基づく声質変換の結果&lt;/h3&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;166&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/157362603&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;かなり音声の自然性は上がりましたね。これはヘタすると騙されるレベル。本当に素晴らしいです。しかも簡単にできるので、お勧めです。↑のは、GMMに基づくframe-by-frameな変換です。計算量も軽いので、リアルタイムでもいけますね。&lt;/p&gt;
&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;
&lt;p&gt;声質変換であれこれ試行錯誤して、ようやくスタートラインにたてた感があります。今後は新しい方法を考えようかなーと思ってます。&lt;/p&gt;
&lt;p&gt;おわり&lt;/p&gt;
&lt;h2 id=&#34;おわび&#34;&gt;おわび&lt;/h2&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; lang=&#34;en&#34;&gt;&lt;p&gt;お盆の間に学習ベースの声質変換のプログラム書く（宿題） &lt;a href=&#34;https://twitter.com/hashtag/%E5%AE%A3%E8%A8%80?src=hash&#34;&gt;#宣言&lt;/a&gt;&lt;/p&gt;&amp;mdash; 山本りゅういち (@r9y9) &lt;a href=&#34;https://twitter.com/r9y9/statuses/366928228465655808&#34;&gt;August 12, 2013&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;約1年かかりました……。本当に申し訳ありませんでした(´･_･`)&lt;/p&gt;
&lt;h2 id=&#34;追記&#34;&gt;追記&lt;/h2&gt;
&lt;p&gt;Twitterで教えてもらいました。トラジェクトリベースで学習も変換も行う研究もありました&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; lang=&#34;en&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://twitter.com/r9y9&#34;&gt;@r9y9&lt;/a&gt; つ トラジェクトリＧＭＭな特徴量変換 &lt;a href=&#34;http://t.co/kUn7bp9EUt&#34;&gt;http://t.co/kUn7bp9EUt&lt;/a&gt;&lt;/p&gt;&amp;mdash; 縄文人（妖精系研究者なのです） (@dicekicker) &lt;a href=&#34;https://twitter.com/dicekicker/statuses/485376823308455936&#34;&gt;July 5, 2014&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;ただ、これはトラジェクトリベースのパラメータ生成法の提案であって、トラジェクトリモデル自体を学習してるわけではないんだよなー。普通に考えると学習もトラジェクトリで考える方法があっていい気がするが、 &lt;del&gt;まだ見てないですね。&lt;/del&gt; ありました。追記参照&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>GOSSP - Go言語で音声信号処理</title>
      <link>https://r9y9.github.io/blog/2014/06/08/gossp-speech-signal-processing-for-go/</link>
      <pubDate>Sun, 08 Jun 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/06/08/gossp-speech-signal-processing-for-go/</guid>
      <description>&lt;h1 id=&#34;cからgoへ&#34;&gt;C++からGoへ&lt;/h1&gt;
&lt;p&gt;みなさん、C++で信号処理のアルゴリズムを書くのはつらいと思ったことはありませんか？C++で書くと速いのはいいけれど、いかんせん書くのが大変、コンパイルエラーは読みづらい、はたまたライブラリをビルドしようとしたら依存関係が上手く解決できず……そんな覚えはないでしょうか？謎のコンパイルエラーに悩みたくない、ガーベジコレクションほしい、Pythonのようにさくっと書きたい、型推論もほしい、でも動作は速い方がいい、そう思ったことはないでしょうか。&lt;/p&gt;
&lt;p&gt;そこでGoです。もちろん、そういった思いに完全に答えてくれるわけではありませんが、厳しいパフォーマンスを要求される場合でなければ、Goの方が良い場合も多いと僕は思っています。
とはいえ、まだ比較的新しい言語のため、ライブラリは少なく信号処理を始めるのも大変です。というわけで、僕がC++をやめてGoに移行したことを思い出し、Goでの信号処理の基礎と、今まで整備してきたGoでの音声信号処理ライブラリを紹介します。&lt;/p&gt;
&lt;p&gt;Goの良いところ/悪いところについては書きません。正直、本当は何の言語でもいいと思っていますが、僕はGoが好きなので、ちょっとでもGoで信号処理したいと思う人が増えるといいなーと思って書いてみます。&lt;/p&gt;
&lt;p&gt;あとで書きますが、僕が書いたコードで使えそうなものは、以下にまとめました。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/gossp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/gossp&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;基礎&#34;&gt;基礎&lt;/h1&gt;
&lt;h2 id=&#34;wavファイルの読み込み書き込み-wavhttpgodocorggithubcommjibsongo-dspwav&#34;&gt;Wavファイルの読み込み/書き込み &lt;a href=&#34;http://godoc.org/github.com/mjibson/go-dsp/wav&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[wav]&lt;/a&gt;&lt;/h2&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/speech_signal.png &#34;Speech signal example.&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;まずは音声ファイルの読み込みですね。wavファイルの読み込みさえできれば十分でしょう。&lt;/p&gt;
&lt;p&gt;これは、すでに有用なライブラリが存在します。&lt;a href=&#34;https://github.com/mjibson/go-dsp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GO-DSP&lt;/a&gt; とういデジタル信号処理のライブラリに含まれるwavパッケージを使いましょう。&lt;/p&gt;
&lt;p&gt;次のように書くことができます。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main

import (
	&amp;quot;fmt&amp;quot;
	&amp;quot;github.com/mjibson/go-dsp/wav&amp;quot;
	&amp;quot;log&amp;quot;
	&amp;quot;os&amp;quot;
)

func main() {
	// ファイルのオープン
	file, err := os.Open(&amp;quot;./test.wav&amp;quot;)
	if err != nil {
		log.Fatal(err)
	}

	// Wavファイルの読み込み
	w, werr := wav.ReadWav(file)
	if werr != nil {
		log.Fatal(werr)
	}

	// データを表示
	for i, val := range w.Data {
		fmt.Println(i, val)
	}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;簡単ですね。&lt;/p&gt;
&lt;p&gt;Goはウェブ周りの標準パッケージが充実しているので、以前&lt;a href=&#34;http://qiita.com/r9y9/items/35a1cf139332a3072fc8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;qiitaに書いた記事&lt;/a&gt;のように、wavファイルを受け取って何らかの処理をする、みたいなサーバも簡単に書くことができます&lt;/p&gt;
&lt;p&gt;wavファイルの書き込み＋ユーティリティを追加したかったので、僕は自分でカスタムしたパッケージを使っています。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/go-dsp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/go-dsp&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;高速フーリエ変換-fast-fourier-transform-fft-ffthttpgodocorggithubcommjibsongo-dspfft&#34;&gt;高速フーリエ変換 (Fast Fourier Transform; FFT) &lt;a href=&#34;http://godoc.org/github.com/mjibson/go-dsp/fft&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[fft]&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;言わずとしれたFFTです。音のスペクトルを求めるのに必須の処理です。で、Goではどうすればいいのか？ということですが、こちらもすでに有用なライブラリが存在します。&lt;a href=&#34;https://github.com/mjibson/go-dsp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GO-DSP&lt;/a&gt;に含まれる、fftパッケージを使いましょう。&lt;/p&gt;
&lt;p&gt;このfftパッケージは、go routinesを使って平行化されているため速いです。僕は、1次元のフーリエ変換以外めったに使いませんが、N次元のフーリエ変換をサポートしているのもこのライブラリのいいところです。&lt;/p&gt;
&lt;h3 id=&#34;参考&#34;&gt;参考&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://mattjibson.com/blog/2013/01/04/go-dsp-fft-performance-with-go-routines/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;go-dsp FFT performance with go routines · Matt Jibson&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;使い方は、とても簡単です。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main

import (
	&amp;quot;fmt&amp;quot;
	&amp;quot;github.com/mjibson/go-dsp/fft&amp;quot;
)

func main() {
	fmt.Println(fft.FFTReal([]float64{1, 2, 3, 4, 5, 6, 7, 8}))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;離散コサイン変換-discrete-cosine-transform-dct-dcthttpgodocorggithubcomr9y9gosspdct&#34;&gt;離散コサイン変換 (Discrete Cosine Transform; DCT) &lt;a href=&#34;http://godoc.org/github.com/r9y9/gossp/dct&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[dct]&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;DCTは、Mel-Frequency Cepstrum Coefficients (通称MFCC) 求めるのに必要な変換です。こちらは、残念ながら良さそうなライブラリがなかったので、自分で書きました。&lt;/p&gt;
&lt;p&gt;使い方はFFTとほとんど一緒です。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main

import (
	&amp;quot;fmt&amp;quot;
	&amp;quot;github.com/r9y9/gossp/dct&amp;quot;
)

func main() {
	y := dct.DCTOrthogonal([]float64{1, 2, 3, 4, 5, 6, 7, 8})
	fmt.Println(dct.IDCTOrthogonal(y)) // 直交変換では、逆変換すると元に戻る
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;さて、基本的なところは一旦ここまでです。次からは、少し音声寄りの信号処理手法の紹介です。&lt;/p&gt;
&lt;h1 id=&#34;時間周波数解析&#34;&gt;時間周波数解析&lt;/h1&gt;
&lt;h2 id=&#34;短時間フーリエ変換-short-time-fourier-transform-stft-stfthttpgodocorggithubcomr9y9gosspstft&#34;&gt;短時間フーリエ変換 (Short Time Fourier Transform; STFT) &lt;a href=&#34;http://godoc.org/github.com/r9y9/gossp/stft&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[stft]&lt;/a&gt;&lt;/h2&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/stft.png &#34;STFT spectrogram&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;STFTは、音声の時間周波数解析手法として定番の方法ですね。音声を可視化したり、何らかの認識アルゴリズムの特徴抽出に使ったり、まぁ色々です。&lt;/p&gt;
&lt;p&gt;次のようなコードを書くと、スペクトログラムが作れます&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main

import (
	&amp;quot;flag&amp;quot;
	&amp;quot;fmt&amp;quot;
	&amp;quot;github.com/r9y9/gossp&amp;quot;
	&amp;quot;github.com/r9y9/gossp/io&amp;quot;
	&amp;quot;github.com/r9y9/gossp/stft&amp;quot;
	&amp;quot;github.com/r9y9/gossp/window&amp;quot;
	&amp;quot;log&amp;quot;
	&amp;quot;math&amp;quot;
)

func main() {
	filename := flag.String(&amp;quot;i&amp;quot;, &amp;quot;input.wav&amp;quot;, &amp;quot;Input filename&amp;quot;)
	flag.Parse()

	w, werr := io.ReadWav(*filename)
	if werr != nil {
		log.Fatal(werr)
	}
	data := w.GetMonoData()

	s := &amp;amp;stft.STFT{
		FrameShift: int(float64(w.SampleRate) / 100.0), // 0.01 sec,
		FrameLen:   2048,
		Window:     window.CreateHanning(2048),
	}

	spectrogram := s.STFT(data)
	amplitudeSpectrogram, _ := gossp.SplitSpectrogram(spectrogram)
	PrintMatrixAsGnuplotFormat(amplitudeSpectrogram)
}

func PrintMatrixAsGnuplotFormat(matrix [][]float64) {
	fmt.Println(&amp;quot;#&amp;quot;, len(matrix[0]), len(matrix))
	for i, vec := range matrix {
		for j, val := range vec {
			fmt.Println(i, j, math.Log(val))
		}
		fmt.Println(&amp;quot;&amp;quot;)
	}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;上の画像は、gnuplotで表示したものです&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;set pm3d map
sp &amp;quot;spectrogram.txt&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;逆短時間フーリエ変換-inverse-short-time-fourier-transform-istft-stfthttpgodocorggithubcomr9y9gosspstft&#34;&gt;逆短時間フーリエ変換 (Inverse Short Time Fourier Transform; ISTFT) &lt;a href=&#34;http://godoc.org/github.com/r9y9/gossp/stft&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[stft]&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;ISTFTは、STFTの逆変換でスペクトログラムから時間領域の信号に戻すために使います。スペクトログラムを加工するような音源分離、ノイズ除去手法を使う場合には、必須の処理です。これはstftと同じパッケージ下にあります。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;	reconstructed := s.ISTFT(spectrogram)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;これで、スペクトログラムから音声を再構築することができます。&lt;/p&gt;
&lt;p&gt;逆変換の仕組みは、意外と難しかったりします。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.306.7858&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;D. W. Griffin and J. S. Lim, &amp;ldquo;Signal estimation from modified short-time Fourier transform,&amp;rdquo; IEEE Trans. ASSP, vol.32, no.2, pp.236–243, Apr. 1984.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://research.cs.tamu.edu/prism/lectures/sp/l6.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;L6: Short-time Fourier analysis and synthesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://yukara-13.hatenablog.com/entry/2013/11/17/210204&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pythonで短時間フーリエ変換（STFT）と逆変換 - 音楽プログラミングの超入門（仮）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;この辺を参考にしました。興味のある人は読んで見てください。&lt;/p&gt;
&lt;h2 id=&#34;連続ウェーブレット変換-continuous-wavelet-transform-cwt&#34;&gt;連続ウェーブレット変換 (Continuous Wavelet Transform; CWT)&lt;/h2&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/morlet_6_log.png &#34;Morlet Wavelet spectrogram&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;これは何回かブログで書きました。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://r9y9.github.io/blog/2013/10/20/continuous-wavelet-tranform/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FFTを使った連続ウェーブレット変換の高速化 - LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://r9y9.github.io/blog/2014/06/01/continuouos-wavelet-transform-types/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;連続ウェーブレット変換に使うマザーウェーブレット色々: Morlet, Paul, DOG - LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;コードは、テストがまだ通らないので開発中ということで…orz&lt;/p&gt;
&lt;h2 id=&#34;逆連続ウェーブレット変換-inverse-continuous-wavelet-transform-icwt&#34;&gt;逆連続ウェーブレット変換 (Inverse Continuous Wavelet Transform; ICWT)&lt;/h2&gt;
&lt;p&gt;連続ウェーブレット変換の逆変換ですね。これもけっこう難しいです。こちらもまだテストに通っていないので、開発中です。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://r9y9.github.io/blog/2013/10/21/signal-reconstruction-using-invere-cwt/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;逆連続ウェーブレット変換による信号の再構成 - LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;さて、この辺でまた一区切りです。次は、より音声に特化した信号処理手法を紹介します。&lt;/p&gt;
&lt;p&gt;※以降紹介するもののうち、多くは&lt;a href=&#34;http://sp-tk.sourceforge.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SPTK&lt;/a&gt;のGo-portになっていて、一部はcgoを使ってラップしただけです（後々はpure goにしたいけれど、特にメルケプストラム分析あたりは難しいのでできていません）&lt;/p&gt;
&lt;h1 id=&#34;音声分析系&#34;&gt;音声分析系&lt;/h1&gt;
&lt;h2 id=&#34;基本周波数推定-f0httpgodocorggithubcomr9y9gosspf0&#34;&gt;基本周波数推定 &lt;a href=&#34;http://godoc.org/github.com/r9y9/gossp/f0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[f0]&lt;/a&gt;&lt;/h2&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/arayuru_f0.png &#34;Fundamental frequency trajectory example.&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;ざっくり言えば音の高さを求める方法ですね。一応、音声に特化した方法をいくつか使えるようにしました。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://audition.ens.fr/adc/pdf/2002_JASA_YIN.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A. de Cheveigne and H. Kawahara. YIN, a fundamental frequency estimator for speech and music. J. Acoust. Soc. Am., 111(4):1917–1930, 2002.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cise.ufl.edu/~acamacho/publications/dissertation.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A. Camacho. SWIPE: A sawtooth waveform inspired pitch estimator for speech and music. PhD thesis, University of Florida, 2007.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ただしYINはもどきです。&lt;/p&gt;
&lt;p&gt;以前、&lt;a href=&#34;https://github.com/r9y9/go-world&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GO-WORLD&lt;/a&gt;という音声分析合成系WORLDのGoラッパーを書いたので、それを使えばF0推定手法Dioが使えます。&lt;/p&gt;
&lt;h3 id=&#34;参考-1&#34;&gt;参考&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://r9y9.github.io/blog/2014/03/22/go-world/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;音声分析変換合成システムWORLDのGoラッパーを書いた - LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;メルケプストラム分析-mgcephttpgodocorggithubcomr9y9gosspmgcep&#34;&gt;メルケプストラム分析 &lt;a href=&#34;http://godoc.org/github.com/r9y9/gossp/mgcep&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[mgcep]&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;音声合成界隈ではよく聞くメルケプストラム（※MFCCとは異なります）を求めるための分析手法です。メルケプストラムは、HMM（Hidden Markov Models; 隠れマルコフモデル）音声合成や統計的声質変換において、声道特徴（いわゆる、声質）のパラメータ表現としてよく使われています。メルケプストラムの前に、LPCとかPARCORとか色々あるのですが、現在のHMM音声合成で最もよく使われているのはメルケプストラムな気がするので、メルケプストラム分析があれば十分な気がします。&lt;/p&gt;
&lt;p&gt;これは、SPTKをcgoを使ってラップしました&lt;/p&gt;
&lt;h3 id=&#34;参考-2&#34;&gt;参考&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://ci.nii.ac.jp/naid/40004638236/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;徳田恵一, 小林隆夫, 深田俊明, 斎藤博徳, 今井 聖, “メルケプストラムをパラメータとする音声のスペクトル推定,” 信学論(A), vol.J74-A, no.8, pp.1240–1248, Aug. 1991.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;メル一般化ケプストラム分析-mgcephttpgodocorggithubcomr9y9gosspmgcep&#34;&gt;メル一般化ケプストラム分析 &lt;a href=&#34;http://godoc.org/github.com/r9y9/gossp/mgcep&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[mgcep]&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;メル一般化ケプストラム分析は、その名の通りメルケプストラム分析を一般化したものです。メルケプストラム分析はもちろん、LPCも包含します（詳細は、参考文献をチェックしてみてください）。論文をいくつかあさっている限り、あんまり使われていない気はしますが、これもSPTKをラップしてGoから使えるようにしました。メルケプストラム分析もメル一般化ケプストラム分析に含まれるので、mgcepという一つのパッケージにしました。&lt;/p&gt;
&lt;h3 id=&#34;参考-3&#34;&gt;参考&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.utdallas.edu/~john.hanse/nPublications/JP-55-SpeechComm-Yapanel-Hansen-PMVDR-Feb08.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tokuda, K., Masuko, T., Kobayashi, T., Imai, S., 1994. Mel-generalized Cepstral Analysis-A Uniﬁed Approach to Speech Spectral Estimation, ISCA ICSLP-94: Inter. Conf. Spoken Lang. Proc., Yokohama, Japan, pp. 1043–1046.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;音声合成系&#34;&gt;音声合成系&lt;/h1&gt;
&lt;h2 id=&#34;励起信号の生成-excitehttpgodocorggithubcomr9y9gosspexcite&#34;&gt;励起信号の生成 &lt;a href=&#34;http://godoc.org/github.com/r9y9/gossp/excite&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[excite]&lt;/a&gt;&lt;/h2&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/pulse_excite.png &#34;Exciation eignal.&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;SPTKのexciteのGo実装です。いわゆるPulseExcitationという奴ですね。非周期成分まったく考慮しない単純な励起信号です。&lt;/p&gt;
&lt;p&gt;高品質な波形合成が必要な場合は、WORLDやSTRAIGHTを使うのが良いです。&lt;/p&gt;
&lt;h2 id=&#34;mlsa-mel-log-spectrum-approximation-デジタルフィルタ-vocoderhttpgodocorggithubcomr9y9gosspvocoder&#34;&gt;MLSA (Mel Log Spectrum Approximation) デジタルフィルタ &lt;a href=&#34;http://godoc.org/github.com/r9y9/gossp/vocoder&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[vocoder]&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;MLSAフィルタは、メルケプストラムと励起信号から音声波形を合成するためのデジタルフィルタです。HMM音声合成の波形合成部で使われています（今もきっと）。Pure goで書き直しました。&lt;/p&gt;
&lt;p&gt;昔、C++でも書いたことあります。&lt;/p&gt;
&lt;h3 id=&#34;参考-4&#34;&gt;参考&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://r9y9.github.io/blog/2013/12/01/mlsa-filter-with-c-plus-plus/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MLSA digital filter のC++実装 - LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;mglsa-mel-genaralized-log-spectrum-approximation-デジタルフィルタ-vocoderhttpgodocorggithubcomr9y9gosspvocoder&#34;&gt;MGLSA (Mel Genaralized-Log Spectrum Approximation) デジタルフィルタ &lt;a href=&#34;http://godoc.org/github.com/r9y9/gossp/vocoder&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[vocoder]&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;MGLSAフィルタは、メル一般化ケプストラムから波形を合成するためのデジタルフィルタですね。これも pure goで書きました。&lt;/p&gt;
&lt;h2 id=&#34;sptkの再実装について&#34;&gt;&lt;strong&gt;※SPTKの再実装について&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;SPTKの実装をGoで書き直したものについては、SPTKの実装と結果が一致するかどうかを確認するテストを書いてあります。よって、誤った結果になるということは（計算誤差が影響する場合を除き）基本的にないので、お気になさらず。&lt;/p&gt;
&lt;h2 id=&#34;高品質な音声分析変換合成系-world-go-worldhttpgodocorggithubcomr9y9go-world&#34;&gt;高品質な音声分析変換合成系 WORLD &lt;a href=&#34;http://godoc.org/github.com/r9y9/go-world&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[go-world]&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://r9y9.github.io/blog/2014/03/22/go-world/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;音声分析変換合成システムWORLDのGoラッパーを書いた - LESS IS MORE&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;以前WORLDのGoラッパーを書いたので、色々使えると思います。統計ベースの音声合成とか、声質変換とか。僕は声質変換に使おうと思ってラップしました。&lt;/p&gt;
&lt;h1 id=&#34;おわりに&#34;&gt;おわりに&lt;/h1&gt;
&lt;p&gt;長々と書きましたが、Go言語での信号処理の基礎と、今まで整備してきた音声信号処理ライブラリを簡単に紹介しました。僕が書いたものは、まとめてGithubで公開しています。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/gossp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/gossp&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;使ってももらって、あわよくばバグとか報告してもらって、改善していければいいなーというのと、あとGithubのissue管理便利だし使おうと思ってGithubに上げました。&lt;/p&gt;
&lt;p&gt;みなさん、Goで音声信号処理始めてみませんか？&lt;/p&gt;
&lt;h1 id=&#34;余談&#34;&gt;余談&lt;/h1&gt;
&lt;h2 id=&#34;pythonではダメなのその他言語は&#34;&gt;Pythonではダメなの？その他言語は？&lt;/h2&gt;
&lt;p&gt;なんでGoなの？と思う人がいると思います。冒頭にも書いたとおり、正直好きなのにすればいいですが、適当に書いて速いのがいいならC++だし、型を意識せずさくっと書きたいならPythonだし、そこそこ速くて型があって型推論もあって、とかだったらGoがいいかなと僕は思います。&lt;/p&gt;
&lt;p&gt;Goの特徴（≒良さ）ついては、&lt;a href=&#34;http://www.slideshare.net/ymotongpoo/20130228-gobp-study-66-16830134&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;20130228 Goノススメ（BPStudy #66） | SlideShare&lt;/a&gt;
の11枚目が僕にはドンピシャです。&lt;/p&gt;
&lt;p&gt;numpy, scipy, matplotlib, scikit-learnあたりが最強すぎるので、僕はpythonも良く使います。&lt;/p&gt;
&lt;h2 id=&#34;きっかけ&#34;&gt;きっかけ&lt;/h2&gt;
&lt;p&gt;この記事を書いたきっかけは、友人にGoをおすすめしまくっていたのに全然聞いてくれなかったからでした。Goでも信号処理はできるよ&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>連続ウェーブレット変換に使うマザーウェーブレット色々: Morlet, Paul, DOG</title>
      <link>https://r9y9.github.io/blog/2014/06/01/continuous-wavelet-transform-types/</link>
      <pubDate>Sun, 01 Jun 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/06/01/continuous-wavelet-transform-types/</guid>
      <description>&lt;p&gt;「ウェーブレット変換って難しいんじゃ…マザーウェーブレット？よくわかんない…」&lt;/p&gt;
&lt;p&gt;大丈夫、そんな人には以下の文献がお勧めです&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://paos.colorado.edu/research/wavelets/bams_79_01_0061.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Torrence, C. and G.P. Compo &amp;ldquo;A Practical Guide to Wavelet Analysis&amp;rdquo;, Bull. Am. Meteorol. Soc., 79, 61–78, 1998.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;前置きはさておき、上の文献を参考にMorlet, Paul, DOG (Derivative of Gaussian) の代表的な3つのマザーウェーブレットで音声に対してウェーブレット変換をしてみたので、メモがてら結果を貼っておく&lt;/p&gt;
&lt;p&gt;図の横軸はサンプルで、縦軸は周波数Hz（対数目盛り）にした&lt;/p&gt;
&lt;p&gt;マザーウェーブレットのパラメータは、Morletは $\omega_{0} = 6.0$、Paulは $M = 4$、DOGは $M = 6$&lt;/p&gt;
&lt;p&gt;スケールは、min=55hzで、25cent毎に8オクターブ分取った※厳密には違うのでごめんなさい&lt;/p&gt;
&lt;p&gt;分析に使った音声は、&lt;a href=&#34;http://r9y9.github.io/blog/2013/10/21/signal-reconstruction-using-invere-cwt/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;逆連続ウェーブレット変換による信号の再構成 - LESS IS MORE&lt;/a&gt; で使ったのと同じ&lt;/p&gt;
&lt;h2 id=&#34;morlet&#34;&gt;Morlet&lt;/h2&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/morlet_6.png &#34;Morlet Wavelet spectrogram&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;h2 id=&#34;paul&#34;&gt;Paul&lt;/h2&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/paul_4.png &#34;Paul Wavelet spectrogram&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;h2 id=&#34;dog&#34;&gt;DOG&lt;/h2&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/dog_6.png &#34;DOG Wavelet spectrogram&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;対数を取ると、以下のような感じ&lt;/p&gt;
&lt;h2 id=&#34;morlet-1&#34;&gt;Morlet&lt;/h2&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/morlet_6_log.png &#34;Morlet Wavelet log spectrogram&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;h2 id=&#34;paul-1&#34;&gt;Paul&lt;/h2&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/paul_4_log.png &#34;Paul Wavelet log spectrogram&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;h2 id=&#34;dog-1&#34;&gt;DOG&lt;/h2&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/dog_6_log.png &#34;DOG Wavelet log spectrogram&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;Paulは時間解像度は高いけど周波数解像度はいまいちなので、音声とかには向かないのかなー。DOGはMorletとPaulの中間くらいの位置づけの様子。DOGはorderを上げればMorletっぽくなるけど、Morletの方がやっぱ使いやすいなーという印象。&lt;/p&gt;
&lt;h2 id=&#34;スケールから周波数への変換&#34;&gt;スケールから周波数への変換&lt;/h2&gt;
&lt;p&gt;実は今日まで知らなかったんだけど、マザーウェーブレットによっては時間領域でのスケールの逆数は必ずしも周波数領域での周波数に対応するとは限らないそう。というかずれる（詳細はPractical Guideの3.hを参照）。上で書いた厳密には違うというのは、これが理由。&lt;/p&gt;
&lt;p&gt;ただし、スケールから周波数への変換はマザーウェーブレットから一意に決まるようなので、正しい周波数を求めることは可能。上に貼った図は、Practical Guideにしたがってスケールから周波数に変換している。&lt;/p&gt;
&lt;p&gt;例えば、$f = \frac{1}{s}$となるようにスケールを与えていたとき、$\omega_0 = 6.0$のMorletを使ったウェーブレット変換の真の周波数は、&lt;/p&gt;
&lt;div&gt;
\begin{align}
f&#39; &amp;= \frac{\omega_0 + \sqrt{2+\omega_{0}^2}}{4\pi s} \\
&amp;= \frac{0.96801330919}{s} \\
&amp;= 0.96801330919f
\end{align}
&lt;/div&gt;
&lt;p&gt;となる。
$\omega_0 = 6.0$のMorletだとスケールの逆数にほぼ一致するので今まで気づかなかった…&lt;/p&gt;
&lt;p&gt;めんどくさい。これを知ってからちょっとウェーブレット嫌いになった。でもめげない&lt;/p&gt;
&lt;p&gt;おわり&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://paos.colorado.edu/research/wavelets/bams_79_01_0061.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Torrence, C. and G.P. Compo &amp;ldquo;A Practical Guide to Wavelet Analysis&amp;rdquo;, Bull. Am. Meteorol. Soc., 79, 61–78, 1998.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.mark-bishop.net/signals/CWTReconstructionFactors.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Continuous Wavelet Transform Reconstruction Factors for Selected Wavelets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.hulinks.co.jp/support/flexpro/v7/dataanalysis_cwt.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HULINKS | テクニカルサポート | FlexPro | 連続ウェーブレット変換 (CWT)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;前書いた記事&#34;&gt;前書いた記事&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://r9y9.github.io/blog/2013/10/20/continuous-wavelet-tranform/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FFTを使った連続ウェーブレット変換の高速化 - LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://r9y9.github.io/blog/2013/10/21/signal-reconstruction-using-invere-cwt/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;逆連続ウェーブレット変換による信号の再構成 - LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>PythonによるニューラルネットのToyコード</title>
      <link>https://r9y9.github.io/blog/2014/05/11/python-feed-forward-neural-network-toy-code/</link>
      <pubDate>Sun, 11 May 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/05/11/python-feed-forward-neural-network-toy-code/</guid>
      <description>&lt;p&gt;1000番煎じだけど、知り合いにニューラルネットを教えていて、その過程で書いたコード。わかりやすさ重視。&lt;/p&gt;
&lt;p&gt;このために、誤差伝播法をn回導出しました（意訳：何回もメモなくしました）&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/usr/bin/python
# coding: utf-8

# ニューラルネットワーク(Feed-Forward Neural Networks)の学習、認識の
# デモコードです。
# 誤差伝搬法によってニューラルネットを学習します。
# XORの学習、テストの簡単なデモコードもついています
# 2014/05/10 Ryuichi Yamamoto

import numpy as np

def sigmoid(x):
    return 1.0 / (1.0 + np.exp(-x))

def dsigmoid(y):
    return y * (1.0 - y)

class NeuralNet:
    def __init__(self, num_input, num_hidden, num_output):
        &amp;quot;&amp;quot;&amp;quot;
        パラメータの初期化
        &amp;quot;&amp;quot;&amp;quot;
        # 入力層から隠れ層への重み行列
        self.W1 = np.random.uniform(-1.0, 1.0, (num_input, num_hidden))
        self.hidden_bias = np.ones(num_hidden, dtype=float)
        # 隠れ層から出力層への重み行列
        self.W2 = np.random.uniform(-1.0, 1.0, (num_hidden, num_output))
        self.output_bias = np.ones(num_output, dtype=float)

    def forward(self, x):
        &amp;quot;&amp;quot;&amp;quot;
        前向き伝搬の計算
        &amp;quot;&amp;quot;&amp;quot;
        h = sigmoid(np.dot(self.W1.T, x) + self.hidden_bias)
        return sigmoid(np.dot(self.W2.T, h) + self.output_bias)

    def cost(self, data, target):
        &amp;quot;&amp;quot;&amp;quot;
        最小化したい誤差関数
        &amp;quot;&amp;quot;&amp;quot;
        N = data.shape[0]
        E = 0.0
        for i in range(N):
            y, t = self.forward(data[i]), target[i]
            E += np.sum((y - t) * (y - t))
        return 0.5 * E / float(N)

    def train(self, data, target, epoches=30000, learning_rate=0.1,\
              monitor_period=None):
        &amp;quot;&amp;quot;&amp;quot;
        Stochastic Gradient Decent (SGD) による学習
        &amp;quot;&amp;quot;&amp;quot;
        for epoch in range(epoches):
            # 学習データから1サンプルをランダムに選ぶ
            index = np.random.randint(0, data.shape[0])
            x, t = data[index], target[index]

            # 入力から出力まで前向きに信号を伝搬
            h = sigmoid(np.dot(self.W1.T, x) + self.hidden_bias)
            y = sigmoid(np.dot(self.W2.T, h) + self.output_bias)

            # 隠れ層-&amp;gt;出力層の重みの修正量を計算
            output_delta = (y - t) * dsigmoid(y)
            grad_W2 = np.dot(np.atleast_2d(h).T, np.atleast_2d(output_delta))

            # 隠れ層-&amp;gt;出力層の重みを更新
            self.W2 -= learning_rate * grad_W2
            self.output_bias -= learning_rate * output_delta

            # 入力層-&amp;gt;隠れ層の重みの修正量を計算
            hidden_delta = np.dot(self.W2, output_delta) * dsigmoid(h)
            grad_W1 = np.dot(np.atleast_2d(x).T, np.atleast_2d(hidden_delta))

            # 入力層-&amp;gt;隠れ層の重みを更新
            self.W1 -= learning_rate * grad_W1
            self.hidden_bias -= learning_rate * hidden_delta

            # 現在の目的関数の値を出力
            if monitor_period != None and epoch % monitor_period == 0:
                print &amp;quot;Epoch %d, Cost %f&amp;quot; % (epoch, self.cost(data, target))

        print &amp;quot;Training finished.&amp;quot;

    def predict(self, x):
        &amp;quot;&amp;quot;&amp;quot;
        出力層の最も反応するニューロンの番号を返します
        &amp;quot;&amp;quot;&amp;quot;
        return np.argmax(self.forward(x))

if __name__ == &amp;quot;__main__&amp;quot;:
    import argparse

    parser = argparse.ArgumentParser(description=&amp;quot;Specify options&amp;quot;)
    parser.add_argument(&amp;quot;--epoches&amp;quot;, dest=&amp;quot;epoches&amp;quot;, type=int, required=True)
    parser.add_argument(&amp;quot;--learning_rate&amp;quot;, dest=&amp;quot;learning_rate&amp;quot;,\
                        type=float, default=0.1)
    parser.add_argument(&amp;quot;--hidden&amp;quot;, dest=&amp;quot;hidden&amp;quot;, type=int, default=20)
    args = parser.parse_args()

    nn = NeuralNet(2, args.hidden, 1)

    data = np.array([[0, 0], [0 ,1], [1, 0], [1, 1]])
    target = np.array([0, 1, 1, 0])

    nn.train(data, target, args.epoches, args.learning_rate,\
             monitor_period=1000)

    for x in data:
        print &amp;quot;%s : predicted %s&amp;quot; % (x, nn.forward(x))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/usr/bin/python
# coding: utf-8

# MNISTを用いたニューラルネットによる手書き数字認識のデモコードです
# 学習方法やパラメータによりますが、だいたい 90 ~ 97% くらいの精度出ます。
# 使い方は、コードを読むか、
# python mnist_net.py -h
# としてください
# 参考までに、
# python mnist_net.py --epoches 50000 --learning_rate 0.1 --hidden 100
# とすると、テストセットに対して、93.2%の正解率です
# 僕の環境では、学習、認識合わせて（だいたい）5分くらいかかりました。
# 2014/05/10 Ryuichi Yamamoto

import numpy as np
from sklearn.externals import joblib
import cPickle
import gzip
import os

# 作成したニューラルネットのパッケージ
import net

def load_mnist_dataset(dataset):
    &amp;quot;&amp;quot;&amp;quot;
    MNISTのデータセットをダウンロードします
    &amp;quot;&amp;quot;&amp;quot;
    # Download the MNIST dataset if it is not present
    data_dir, data_file = os.path.split(dataset)
    if (not os.path.isfile(dataset)) and data_file == &#39;mnist.pkl.gz&#39;:
        import urllib
        origin = &#39;http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz&#39;
        print &#39;Downloading data from %s&#39; % origin
        urllib.urlretrieve(origin, dataset)

    f = gzip.open(dataset, &#39;rb&#39;)
    train_set, valid_set, test_set = cPickle.load(f)
    f.close()

    return train_set, valid_set, test_set

def augument_labels(labels, order):
    &amp;quot;&amp;quot;&amp;quot;
    1次元のラベルデータを、ラベルの種類数(order)次元に拡張します
    &amp;quot;&amp;quot;&amp;quot;
    new_labels = []
    for i in range(labels.shape[0]):
        v = np.zeros(order)
        v[labels[i]] = 1
        new_labels.append(v)

    return np.array(new_labels).reshape((labels.shape[0], order))

if __name__ == &amp;quot;__main__&amp;quot;:
    import argparse

    parser = argparse.ArgumentParser(description=&amp;quot;MNIST手書き数字認識のデモ&amp;quot;)
    parser.add_argument(&amp;quot;--epoches&amp;quot;, dest=&amp;quot;epoches&amp;quot;, type=int, required=True)
    parser.add_argument(&amp;quot;--learning_rate&amp;quot;, dest=&amp;quot;learning_rate&amp;quot;,\
                        type=float, default=0.1)
    parser.add_argument(&amp;quot;--hidden&amp;quot;, dest=&amp;quot;hidden&amp;quot;, type=int, default=100)
    args = parser.parse_args()

    train_set, valid_set, test_set = load_mnist_dataset(&amp;quot;mnist.pkl.gz&amp;quot;)
    n_labels = 10 # 0,1,2,3,4,5,6,7,9
    n_features = 28*28

    # モデルを新しく作る
    nn = net.NeuralNet(n_features, args.hidden, n_labels)

    # モデルを読み込む
    # nn = joblib.load(&amp;quot;./nn_mnist.pkl&amp;quot;)

    nn.train(train_set[0], augument_labels(train_set[1], n_labels),\
             args.epoches, args.learning_rate, monitor_period=2000)

    ## テスト
    test_data, labels = test_set
    results = np.arange(len(test_data), dtype=np.int)
    for n in range(len(test_data)):
        results[n] = nn.predict(test_data[n])
        # print &amp;quot;%d : predicted %s, expected %s&amp;quot; % (n, results[n], labels[n])
    print &amp;quot;recognition rate: &amp;quot;, (results == labels).mean()

    # モデルを保存
    model_filename = &amp;quot;nn_mnist.pkl&amp;quot;
    joblib.dump(nn, model_filename, compress=9)
    print &amp;quot;The model parameters are dumped to &amp;quot; + model_filename
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/python-neural-net-toy-codes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/python-neural-net-toy-codes&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;以下のようなコマンドを叩いて、正解率が97%くらいになるまで学習してから入力層から隠れ層への重みを可視化してみた&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# python mnist_net.py --epoches 50000 --learning_rate 0.1 --hidden 100 # epochesは適当に
&lt;/code&gt;&lt;/pre&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/nn_mnist_W1_100.png&#34; alt=&#34;Input to Hidden weight filters after trained on MNIST.&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;興味深いことに、RBMと違って重み行列の解釈はしにくい。生成モデルの尤度を最大化することと、誤差を最小化することはこんなにも違うんだなぁというこなみな感想&lt;/p&gt;
&lt;p&gt;RBMについては、以下へ&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://r9y9.github.io/blog/2014/03/06/restricted-boltzmann-machines-mnist/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Restricted Boltzmann Machines with MNIST - LESS IS MORE&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;おわり&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>cgo の基本的な使い方とポインタ周りのTips (Go v1.2)</title>
      <link>https://r9y9.github.io/blog/2014/03/22/cgo-tips/</link>
      <pubDate>Sat, 22 Mar 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/03/22/cgo-tips/</guid>
      <description>&lt;p&gt;C/C++ライブラリのGoラッパーを書くためには、cgoというパッケージを使うのだけど、特にCのポインタ周りにハマりどころが多かったので、少しまとめとく&lt;/p&gt;
&lt;p&gt;cgoの基礎については、以下の二つを読むことを推奨&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://golang.org/cmd/cgo/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://golang.org/cmd/cgo/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://code.google.com/p/go-wiki/wiki/cgo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://code.google.com/p/go-wiki/wiki/cgo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;この記事では、cgo基本的な使い方と、いくつかポインタ絡みのTipsをまとめる。Tipsのみ必要な場合は、最初の方は飛ばして下さい&lt;/p&gt;
&lt;h2 id=&#34;cgo&#34;&gt;cgo&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Cgo enables the creation of Go packages that call C code.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://golang.org/cmd/cgo/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://golang.org/cmd/cgo/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;cgoとは、GoからCの関数/型にアクセスするために用いるパッケージのこと。cgoを使えば、GoからCのコードが呼べる。つまり、&lt;strong&gt;Cで書かれたライブラリが、Goでも再利用できる&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;なお、go v1.2 から、C++もサポートされている様子
&lt;a href=&#34;http://golang.org/doc/go1.2#cgo_and_cpp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://golang.org/doc/go1.2#cgo_and_cpp&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ただし、C++ライブラリの使用方法については現時点でドキュメントはほぼ無し。僕の経験では、extern &amp;ldquo;C&amp;rdquo; を付けておくとC++用のコンパイラでコンパイルされたライブラリでも呼べる&lt;/p&gt;
&lt;h2 id=&#34;基本的な使い方&#34;&gt;基本的な使い方&lt;/h2&gt;
&lt;p&gt;まず、Cの型/関数にアクセスするために、cgoパッケージのimportを行う&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;import &amp;quot;C&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;import文のすぐ上のコメントにinclude &amp;lt;ヘッダ.h&amp;gt; と書けば、コンパイルする際に自動で読み込まれるので、必要なヘッダを書く&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// #include &amp;lt;stdio.h&amp;gt;
// #include &amp;lt;stdlib.h&amp;gt;
import &amp;quot;C&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;これで、C.int, C.float, C.double, *C.char、C.malloc, C.free などのようにして、Cの型や関数にアクセスできる&lt;/p&gt;
&lt;h2 id=&#34;外部ライブラリを呼ぶ&#34;&gt;外部ライブラリを呼ぶ&lt;/h2&gt;
&lt;p&gt;通常は、ヘッダファイルをincludeするだけでなく、何かしらのライブラリとリンクして用いることが多いので、そのような場合には、ライブラリの依存関係をgoのコードに記述する&lt;/p&gt;
&lt;p&gt;cgoでは、includeの設定と同様に、CFLAGS、CPPFLAGS、CXXFLAGS、LDFLAGS、pkg-configを記述することができる&lt;/p&gt;
&lt;p&gt;pkg-configを使うと 、&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// #cgo pkg-config: png cairo
// #include &amp;lt;png.h&amp;gt;
import &amp;quot;C&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;こんな感じ（&lt;a href=&#34;http://golang.org/cmd/cgo/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goの公式ページ&lt;/a&gt;から参照）&lt;/p&gt;
&lt;h2 id=&#34;tips&#34;&gt;Tips&lt;/h2&gt;
&lt;p&gt;さて、ここからTips。主に、&lt;a href=&#34;ml.cs.yamanashi.ac.jp/world/&#34;&gt;WORLD&lt;/a&gt;のGoラッパーを書いていたときに得た知見です。ラッパーは、&lt;a href=&#34;https://github.com/r9y9/go-world&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;にあげた&lt;/p&gt;
&lt;h2 id=&#34;1-goのスライスをcのポインタとして関数の引数に渡す&#34;&gt;1. GoのスライスをCのポインタとして関数の引数に渡す&lt;/h2&gt;
&lt;p&gt;例えば、[]float64 -&amp;gt; double* のイメージ&lt;/p&gt;
&lt;p&gt;これは比較的簡単にできる。以前qiitaにも書いた
&lt;a href=&#34;http://qiita.com/r9y9/items/e6d879c9b7d4f2697593&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://qiita.com/r9y9/items/e6d879c9b7d4f2697593&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;(*C.double)(&amp;amp;slice[0])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;のようにキャストしてやればOK&lt;/p&gt;
&lt;h2 id=&#34;2-goのスライスのスライスをcのポインタのポインタとして関数の引数に渡す&#34;&gt;2. GoのスライスのスライスをCのポインタのポインタとして関数の引数に渡す&lt;/h2&gt;
&lt;p&gt;[][]float64 -&amp;gt; double** のようなイメージ&lt;/p&gt;
&lt;p&gt;例として、worldから引っ張ってきた以下のようなCの関数を考える&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;void Star(double *x, int x_length, int fs, double *time_axis, double *f0,
  int f0_length, double **spectrogram);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;**spectrogramには処理結果が格納される。もちろん処理結果はGoの型で扱いたいんだが、では****spectrogramにどうやってGoの型を渡すか？**ということが問題になる&lt;/p&gt;
&lt;p&gt;doubleの二次元配列なので、&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;s := [][]float64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;というスライスのスライスを考えて、キャストして渡したいところだけど、結論から言うとこれはできない&lt;/p&gt;
&lt;p&gt;ではどうするかというと、苦肉の策として、&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;wspace := make([]*C.double, len(f0))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;というスライスを考えて、&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(**C.double)(&amp;amp;wspace[0])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;とすれば、double**として関数の引数に渡すことができる。他にも方法がある気がするが、これでも期待通りの動作をする（あまりハックっぽいことしたくない…&lt;/p&gt;
&lt;p&gt;まとめると、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[][]float64 -&amp;gt; double**はできないが、&lt;/li&gt;
&lt;li&gt;[]*C.double -&amp;gt; double**はできる。よって、一応Goの型をdouble**に渡すことはできる&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;です。&lt;/p&gt;
&lt;h2 id=&#34;3-ポインタのポインタからスライスのスライスへの変換&#34;&gt;3. ポインタのポインタからスライスのスライスへの変換&lt;/h2&gt;
&lt;p&gt;double** -&amp;gt; [][]float64 のようなイメージ&lt;/p&gt;
&lt;p&gt;Tipsその2の例より、Cの関数の処理が終われば**spectrogramにデータが格納される。もちろん処理結果はGoの型で扱いたいので、[][]float64 にしたい。ただし、先程の例では、Cの関数に渡した型は実際には []*C.doubleで、Cの型を含んでいる。&lt;/p&gt;
&lt;p&gt;そこで、次に問題になるのは、**[]*C.doubleにから[][]float64 に変換するにはどうするか？**ということ。そして、これも面倒です…（※節の頭でdouble** -&amp;gt; [][]float64と書いたけど、正確には []*C.double -&amp;gt; [][]float64）&lt;/p&gt;
&lt;p&gt;結論から言えば、直接の変換は難しいけど中間変数をかませばできる&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[]bytes型でtmp変数を作り、&lt;code&gt;C.GoBytes&lt;/code&gt; を使って*C.double -&amp;gt; []bytes にコピー&lt;/li&gt;
&lt;li&gt;encoding/binaryパッケージを使って、[]bytes -&amp;gt; []float64 に書き込み&lt;/li&gt;
&lt;li&gt;この処理をsliceOfSlices[0], sliceOfSlices[1], &amp;hellip; に対して繰り返す&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以上。とても面倒ですね…&lt;/p&gt;
&lt;p&gt;さて、結局上のStarのラッパーは以下のようになった&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func Star(x []float64, fs int, timeAxis, f0 []float64) [][]float64 {
	FFTSize := C.size_t(C.GetFFTSizeForStar(C.int(fs)))
	numFreqBins := FFTSize/2 + 1

	// Create workspace
	wspace := make([]*C.double, len(f0))
	for i := range wspace {
		wspace[i] = (*C.double)(C.malloc(byteSizeOfFloat64 * numFreqBins))
		defer C.free(unsafe.Pointer(wspace[i]))
	}

	// Perform star
	C.Star((*C.double)(&amp;amp;x[0]),
		C.int(len(x)),
		C.int(fs),
		(*C.double)(&amp;amp;timeAxis[0]),
		(*C.double)(&amp;amp;f0[0]),
		C.int(len(f0)),
		(**C.double)(&amp;amp;wspace[0]))

	// Copy to go slice
	spectrogram := make([][]float64, len(f0))
	for i := range spectrogram {
		spectrogram[i] = CArrayToGoSlice(wspace[i], C.int(numFreqBins))
	}

	return spectrogram
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;上で使っているutility function&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func CArrayToGoSlice(array *C.double, length C.int) []float64 {
	slice := make([]float64, int(length))
	b := C.GoBytes(unsafe.Pointer(array), C.int(byteSizeOfFloat64*length))
	err := binary.Read(bytes.NewReader(b), binary.LittleEndian, slice)
	if err != nil {
		panic(err)
	}
	return slice
}
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;[]*C.double のスライスを作り、作業領域のメモリを確保する（Tips2の内容+メモリ確保）&lt;/li&gt;
&lt;li&gt;[]&lt;em&gt;C.double のスライスをdouble&lt;/em&gt;* にキャストして、Cの関数を実行（Tips2の内容）&lt;/li&gt;
&lt;li&gt;[]*C.double から[][]float64に変換する（Tips3の内容）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;という手順になってます&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;※2013/03/27 追記&lt;/strong&gt;
:もっとシンプルかつ効率的（copyの必要がないように）に書けた。[][]float64で返り値用のスライスを作り、それを[]*double型に変換してCに渡せば、[][]float64に変更が反映されるので、そもそも[]*doubleから[][]float64に変換する必要はなかった。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func Star(x []float64, fs int, timeAxis, f0 []float64) [][]float64 {
	FFTSize := C.size_t(C.GetFFTSizeForStar(C.int(fs)))
	numFreqBins := C.size_t(FFTSize/2 + 1)

	spectrogram := make([][]float64, len(f0))
	for i := range spectrogram {
		spectrogram[i] = make([]float64, numFreqBins)
	}

	spectrogramUsedInC := Make2DCArrayAlternative(spectrogram)

	// Perform star
	C.Star((*C.double)(&amp;amp;x[0]),
		C.int(len(x)),
		C.int(fs),
		(*C.double)(&amp;amp;timeAxis[0]),
		(*C.double)(&amp;amp;f0[0]),
		C.int(len(f0)),
		(**C.double)(&amp;amp;spectrogramUsedInC[0]))

	return spectrogram
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;
func Make2DCArrayAlternative(matrix [][]float64) []*C.double {
	alternative := make([]*C.double, len(matrix))
	for i := range alternative {
		// DO NOT free because the source slice is managed by Go
		alternative[i] = (*C.double)(&amp;amp;matrix[i][0])
	}
	return alternative
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ポインタのポインタを引数に取る関数のラップはめんどくさい&lt;/li&gt;
&lt;li&gt;Goは使いやすいのに &lt;del&gt;cgoは使いにくい&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;cgoつらい&lt;/li&gt;
&lt;li&gt;よりいい方法があれば教えて下さい&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;20140810-追記&#34;&gt;2014/08/10 追記&lt;/h2&gt;
&lt;p&gt;cgo使いにくいと書いたけど、あとから考えてみればcgoさんまじ使いやすかったです（遅い&lt;/p&gt;
&lt;p&gt;Juliaのccallはもっと使いやすい。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>音声分析変換合成システムWORLDのGoラッパーを書いた</title>
      <link>https://r9y9.github.io/blog/2014/03/22/go-world/</link>
      <pubDate>Sat, 22 Mar 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/03/22/go-world/</guid>
      <description>&lt;h2 id=&#34;音声分析変換合成システムworld&#34;&gt;音声分析変換合成システムWORLD&lt;/h2&gt;
&lt;p&gt;WORLDとは、山梨大学の森勢先生が作られている高品質な音声分析変換合成システムです。非常に高品質かつ高速に動作するのが良い所です。詳細は以下のURLへ&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://ml.cs.yamanashi.ac.jp/world/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://ml.cs.yamanashi.ac.jp/world/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;オリジナルはC+＋で書かれていますが、Goからも使えるようにラッパーを書きました。非常にいいソフトウェアなので、もしよろしければどうぞ&lt;/p&gt;
&lt;h2 id=&#34;go-world&#34;&gt;GO-WORLD&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/go-world&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/go-world&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;使い方について、ほんの少し解説を書きます&lt;/p&gt;
&lt;p&gt;※ubuntu12.04でのみ動作確認してます。&lt;/p&gt;
&lt;h2 id=&#34;準備&#34;&gt;準備&lt;/h2&gt;
&lt;h3 id=&#34;1-worldのインストール&#34;&gt;1. WORLDのインストール&lt;/h3&gt;
&lt;p&gt;まずWORLDをインストールする必要があります。公式のパッケージではinstallerに相当するものがなかったので、作りました&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/world&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/world&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt; ./waf configure &amp;amp;&amp;amp; ./waf
 sudo ./waf install
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;でインストールできます。&lt;/p&gt;
&lt;p&gt;なお、WORLDは最新版ではなく0.1.2としています。最新版にすると自分の環境でビルドコケてしまったので…&lt;/p&gt;
&lt;h3 id=&#34;2-go-worldのインストール&#34;&gt;2. GO-WORLDのインストール&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;go get github.com/r9y9/go-world
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;簡単ですね！&lt;/p&gt;
&lt;h2 id=&#34;使い方&#34;&gt;使い方&lt;/h2&gt;
&lt;h3 id=&#34;1-インポートする&#34;&gt;1. インポートする&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;import &amp;quot;github.com/r9y9/go-world&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;2-worldのインスタンスを作る&#34;&gt;2. worldのインスタンスを作る&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;w := world.New(sampleRate, framePeriod) // e.g. (44100, 5)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;3-好きなworldのメソッドを呼ぶ&#34;&gt;3. 好きなworldのメソッドを呼ぶ&lt;/h3&gt;
&lt;h4 id=&#34;基本周波数の推定-dio&#34;&gt;基本周波数の推定: Dio&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;timeAxis, f0 := w.Dio(input, w.NewDioOption()) // default option is used
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;スペクトル包絡の推定-star&#34;&gt;スペクトル包絡の推定: Star&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;spectrogram := w.Star(input, timeAxis, f0)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;励起信号の推定-platinum&#34;&gt;励起信号の推定: Platinum&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;residual := w.Platinum(input, timeAxis, f0, spectrogram)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;パラメータから音声の再合成-synthesis&#34;&gt;パラメータから音声の再合成: Synthesis&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;synthesized := w.Synthesis(f0, spectrogram, residual, len(input))
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;使い方例&#34;&gt;使い方例.&lt;/h2&gt;
&lt;p&gt;音声（wavファイル）を分析して、パラメータから音声を再合成する例を紹介します。80行弱と少し長いですがはっつけておきます&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main

import (
	&amp;quot;flag&amp;quot;
	&amp;quot;fmt&amp;quot;
	&amp;quot;github.com/mjibson/go-dsp/wav&amp;quot;
	&amp;quot;github.com/r9y9/go-world&amp;quot;
	&amp;quot;log&amp;quot;
	&amp;quot;os&amp;quot;
)

var defaultDioOption = world.DioOption{
	F0Floor:          80.0,
	F0Ceil:           640.0,
	FramePeriod:      5,
	ChannelsInOctave: 4.0,
	Speed:            6,
}

// 音声を基本周波数、スペクトル包絡、励起信号の三つに分解したあと、再合成します
func worldExample(input []float64, sampleRate int) []float64 {
	w := world.New(sampleRate, defaultDioOption.FramePeriod)

	// 1. Fundamental frequency
	timeAxis, f0 := w.Dio(input, defaultDioOption)

	// 2. Spectral envelope
	spectrogram := w.Star(input, timeAxis, f0)

	// 3. Excitation spectrum
	residual := w.Platinum(input, timeAxis, f0, spectrogram)

	// 4. Synthesis
	return w.Synthesis(f0, spectrogram, residual, len(input))
}

// 音声を基本周波数、スペクトル包絡、非周期成分の三つに分解したあと、再合成します
func worldExampleAp(input []float64, sampleRate int) []float64 {
	w := world.New(sampleRate, defaultDioOption.FramePeriod)

	// 1. Fundamental frequency
	timeAxis, f0 := w.Dio(input, defaultDioOption)

	// 2. Spectral envelope
	spectrogram := w.Star(input, timeAxis, f0)

	// 3. Apiriodiciy
	apiriodicity, targetF0 := w.AperiodicityRatio(input, f0)

	// 4. Synthesis
	return w.SynthesisFromAperiodicity(f0, spectrogram, apiriodicity, targetF0, len(input))
}

func GetMonoDataFromWavData(data [][]int) []float64 {
	y := make([]float64, len(data))
	for i, val := range data {
		y[i] = float64(val[0])
	}
	return y
}

func main() {
	ifilename := flag.String(&amp;quot;i&amp;quot;, &amp;quot;default.wav&amp;quot;, &amp;quot;Input filename&amp;quot;)
	flag.Parse()

	// Read wav data
	file, err := os.Open(*ifilename)
	if err != nil {
		log.Fatal(err)
	}
	defer file.Close()

	w, werr := wav.ReadWav(file)
	if werr != nil {
		log.Fatal(werr)
	}
	input := GetMonoDataFromWavData(w.Data)
	sampleRate := int(w.SampleRate)

	synthesized := worldExample(input, sampleRate)
	// synthesized := worldExampleAp(input, sampleRate)

	for i, val := range synthesized {
		fmt.Println(i, val)
	}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Goだとメモリ管理きにしなくていいしそこそこ速いし読みやすいし書きやすいし楽でいいですね（信者&lt;/p&gt;
&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;GoはC++ほど速くはないですが、C++の何倍も書きやすいし読みやすい（メンテしやすい）ので、個人的にオススメです（パフォーマンスが厳しく要求される場合には、C++の方がいいかもしれません）&lt;/li&gt;
&lt;li&gt;WORLD良いソフトウェアなので使いましょう&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ちなみに&#34;&gt;ちなみに&lt;/h2&gt;
&lt;p&gt;元はと言えば、オレオレ基本周波数推定（YINもどき）が微妙に精度悪くて代替を探していたとき、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SPTKのRAPTかSWIPE使おうかな…&lt;/li&gt;
&lt;li&gt;RAPTもSWIPEもSPTK.hにインタフェースがない…&lt;/li&gt;
&lt;li&gt;うわRAPTのコード意味わからん&lt;/li&gt;
&lt;li&gt;SWIPEのコードまじ謎&lt;/li&gt;
&lt;li&gt;後藤さんのPreFest実装しよう&lt;/li&gt;
&lt;li&gt;あれ上手くいかない…orz&lt;/li&gt;
&lt;li&gt;どうしようかな…&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;となっていたときに、森勢先生が書いたと思われる以下の文献を見つけて、&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://crestmuse.jp/handbookMI/pdf/2_2_PitchExtraction_Morise.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2-2 基本周波数推定（歌声研究に関する視点から）&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本方法は，低域に雑音が存在する音声に対する推定は困難であるが，低域の雑音が存在しない音声の場合，SWIPE′ や NDF と実質的に同等の性能を達成しつつ，計算時間を SWIPE′の 1/42, NDF の 1/80 にまで低減可能である．&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;あぁworld使おう（白目&lt;/p&gt;
&lt;p&gt;となり、ラッパーを書くにいたりましたとさ、おしまい&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Restricted Boltzmann Machines with MNIST</title>
      <link>https://r9y9.github.io/blog/2014/03/06/restricted-boltzmann-machines-mnist/</link>
      <pubDate>Thu, 06 Mar 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/03/06/restricted-boltzmann-machines-mnist/</guid>
      <description>&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/RBM_mnist_Hidden_500_layers.png &#34;RBM training result on MNIST handwritten digit dataset. Each image represents a filter learned by RBM.&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;ディープ某を使った研究を再現してみたくて、最近某ニューラルネットに手を出し始めた。で、手始めにRestricted Boltzmann Machinesを実装してみたので、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MNISTを使って学習した結果の重み（22*22=484個）を貼っとく（↑）&lt;/li&gt;
&lt;li&gt;得た知見をまとめとく&lt;/li&gt;
&lt;li&gt;Goのコード貼っとく&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ってな感じで書いておく&lt;/p&gt;
&lt;p&gt;(本当はRBMについて自分なりの解釈を書こうと思ったのだけど、それはまた今度)&lt;/p&gt;
&lt;h2 id=&#34;実験条件&#34;&gt;実験条件&lt;/h2&gt;
&lt;p&gt;データベースはmnist。手書き数字認識で有名なアレ。学習の条件は、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;隠れ層のユニット数: 500&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;mini-batch size: 20&lt;/li&gt;
&lt;li&gt;iterationの回数: 15&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;対数尤度の変化&#34;&gt;対数尤度の変化&lt;/h2&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/RBM_mnist_Hidden_500_log_likelihood.png &#34;Pseudo log-likelihood on mnist databae.&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;以下グラフに表示している生データ&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0 -196.59046099622128
1 -70.31708616742365
2 -65.29499371647965
3 -62.37983267378022
4 -61.5359019358253
5 -60.917772257650164
6 -59.64207778426757
7 -59.42201674307857
8 -59.18497336138633
9 -58.277168243126305
10 -58.36279288392401
11 -58.57805165724595
12 -57.71043215987184
13 -58.17783142034138
14 -57.53629129936344
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;尤度上がると安心する。厳密に対数尤度を計算することは難しいので、&lt;a href=&#34;http://deeplearning.net/tutorial/rbm.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Restricted Boltzmann Machines (RBM) | DeepLearning Tutorial&lt;/a&gt; にある擬似尤度を参考にした&lt;/p&gt;
&lt;h2 id=&#34;学習時間&#34;&gt;学習時間&lt;/h2&gt;
&lt;p&gt;うちのcore2duoのPCで4時間弱だった気がする（うろ覚え&lt;/p&gt;
&lt;p&gt;隠れ層のユニット数100だと、40分ほどだった&lt;/p&gt;
&lt;h2 id=&#34;知見&#34;&gt;知見&lt;/h2&gt;
&lt;p&gt;今の所、試行錯誤して自分が得た知見は、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sample by sampleのSGDよりmini-batch SGDの方が安定して尤度上がる&lt;/li&gt;
&lt;li&gt;mini-batch sizeを大きくしすぎると学習が進まない。20くらいがちょうど良かった&lt;/li&gt;
&lt;li&gt;k-CD のkを大きくしてもさほど学習結果変わらない（計算コストはけっこう増すけど）&lt;/li&gt;
&lt;li&gt;persistent CDを使ってもあまりよくならない（計算コストはけっこう増すけど）&lt;/li&gt;
&lt;li&gt;やっぱ1-CDで十分だった&lt;/li&gt;
&lt;li&gt;データの正規化方法によって結構結果も変わる。ノイズを足すかどうか、とか&lt;/li&gt;
&lt;li&gt;学習率超重要すぎわろた。今回の場合は0.1くらいかちょうど良かった&lt;/li&gt;
&lt;li&gt;隠れ層のユニット数が大きいほど学習が上手く行けばと尤度は上がる(?)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;まぁだいたい &lt;a href=&#34;http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Practical Guide to Training Restricted Boltzmann Machines (PDF)&lt;/a&gt; に書いてあるけど、実際に肌で感じて理解した。persistent CDはもうちょっと成果出て欲しい。データ変えると成果出るんかな？&lt;/p&gt;
&lt;h2 id=&#34;コード&#34;&gt;コード&lt;/h2&gt;
&lt;p&gt;コアの部分だけ、一応&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package rbm

import (
	&amp;quot;encoding/json&amp;quot;
	&amp;quot;errors&amp;quot;
	&amp;quot;fmt&amp;quot;
	&amp;quot;github.com/r9y9/nn&amp;quot; // sigmoid, matrix
	&amp;quot;math&amp;quot;
	&amp;quot;math/rand&amp;quot;
	&amp;quot;os&amp;quot;
	&amp;quot;time&amp;quot;
)

// References:
// [1] G. Hinton, &amp;quot;A Practical Guide to Training Restricted Boltzmann Machines&amp;quot;,
// UTML TR 2010-003.
// url: http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf
//
// [2] A. Fischer and C. Igel. &amp;quot;An introduction to restricted Boltzmann machines&amp;quot;,
// Proc. of the 17th Iberoamerican Congress on Pattern Recognition (CIARP),
// Volume 7441 of LNCS, pages 14–36. Springer, 2012
// url: http://image.diku.dk/igel/paper/AItRBM-proof.pdf
//
// [3] Restricted Boltzmann Machines (RBM),  DeepLearning tutorial
// url: http://deeplearning.net/tutorial/rbm.html

// Notes about implementation:
// Notation used in this code basically follows [2].
// e.g. W for weight, B for bias of visible layer, C for bias of hidden layer.

// Graphical representation of Restricted Boltzmann Machines (RBM).
//
//     ○ ○ .... ○  h(hidden layer), c(bias)
//     /\ /\ /    /\
//    ○ ○ ○ ... ○ v(visible layer), b(bias)
type RBM struct {
	W               [][]float64 // Weight
	B               []float64   // Bias of visible layer
	C               []float64   // Bias of hidden layer
	NumHiddenUnits  int
	NumVisibleUnits int
	Option          TrainingOption
}

type TrainingOption struct {
	LearningRate        float64
	OrderOfGibbsSamping int // It is known that 1 is enough for many cases.
	Epoches             int
	MiniBatchSize       int
	L2Regularization    bool
	RegularizationRate  float64
}

// NewRBM creates new RBM instance. It requires input data and number of
// hidden units to initialize RBM.
func NewRBM(numVisibleUnits, numHiddenUnits int) *RBM {
	rbm := new(RBM)
	rand.Seed(time.Now().UnixNano())

	rbm.W = nn.MakeMatrix(numHiddenUnits, numVisibleUnits)
	rbm.B = make([]float64, numVisibleUnits)
	rbm.C = make([]float64, numHiddenUnits)
	rbm.NumVisibleUnits = numVisibleUnits
	rbm.NumHiddenUnits = numHiddenUnits

	rbm.InitRBM()
	return rbm
}

// NewRBMWithParameters returns RBM instance given RBM parameters.
// This func will be used in Deep Belief Networks.
func NewRBMWithParameters(W [][]float64, B, C []float64) (*RBM, error) {
	rbm := new(RBM)

	rbm.NumVisibleUnits = len(B)
	rbm.NumHiddenUnits = len(C)

	if len(W) != rbm.NumHiddenUnits || len(W[0]) != rbm.NumVisibleUnits {
		return nil, errors.New(&amp;quot;Shape of weight matrix is wrong.&amp;quot;)
	}

	rand.Seed(time.Now().UnixNano())
	rbm.W = W
	rbm.B = B
	rbm.C = C

	return rbm, nil
}

// LoadRBM loads RBM from a dump file and return its instatnce.
func LoadRBM(filename string) (*RBM, error) {
	file, err := os.Open(filename)
	if err != nil {
		return nil, err
	}
	defer file.Close()

	decoder := json.NewDecoder(file)
	rbm := &amp;amp;RBM{}
	err = decoder.Decode(rbm)

	if err != nil {
		return nil, err
	}

	return rbm, nil
}

// Dump writes RBM parameters to file in json format.
func (rbm *RBM) Dump(filename string) error {
	file, err := os.Create(filename)
	if err != nil {
		return err
	}
	defer file.Close()

	encoder := json.NewEncoder(file)
	err = encoder.Encode(rbm)
	if err != nil {
		return err
	}

	return nil
}

// Heuristic initialization of visible bias.
func (rbm *RBM) InitVisibleBiasUsingTrainingData(data [][]float64) {
	// Init B (bias of visible layer)
	activeRateInVisibleLayer := rbm.getActiveRateInVisibleLayer(data)
	for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
		rbm.B[j] = math.Log(activeRateInVisibleLayer[j] / (1.0 - activeRateInVisibleLayer[j]))
	}
}

func (rbm *RBM) getActiveRateInVisibleLayer(data [][]float64) []float64 {
	rate := make([]float64, rbm.NumVisibleUnits)
	for _, sample := range data {
		for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
			rate[j] += sample[j]
		}
	}
	for j := range rate {
		rate[j] /= float64(len(data))
	}
	return rate
}

// InitRBM performes a heuristic parameter initialization.
func (rbm *RBM) InitRBM() {
	// Init W
	for i := 0; i &amp;lt; rbm.NumHiddenUnits; i++ {
		for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
			rbm.W[i][j] = 0.01 * rand.NormFloat64()
		}
	}

	for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
		rbm.B[j] = 0.0
	}

	// Init C (bias of hidden layer)
	for i := 0; i &amp;lt; rbm.NumHiddenUnits; i++ {
		rbm.C[i] = 0.0
	}
}

// P_H_Given_V returns the conditinal probability of a hidden unit given a set of visible units.
func (rbm *RBM) P_H_Given_V(hiddenIndex int, v []float64) float64 {
	sum := 0.0
	for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
		sum += rbm.W[hiddenIndex][j] * v[j]
	}
	return nn.Sigmoid(sum + rbm.C[hiddenIndex])
}

// P_V_Given_H returns the conditinal probability of a visible unit given a set of hidden units.
func (rbm *RBM) P_V_Given_H(visibleIndex int, h []float64) float64 {
	sum := 0.0
	for i := 0; i &amp;lt; rbm.NumHiddenUnits; i++ {
		sum += rbm.W[i][visibleIndex] * h[i]
	}
	return nn.Sigmoid(sum + rbm.B[visibleIndex])
}

// GibbsSampling performs k-Gibbs sampling algorithm,
// where k is the number of iterations in gibbs sampling.
func (rbm *RBM) GibbsSampling(v []float64, k int) []float64 {
	// Initial value is set to input
	vUsedInSamping := make([]float64, len(v))
	copy(vUsedInSamping, v)

	for t := 0; t &amp;lt; k; t++ {
		sampledH := make([]float64, rbm.NumHiddenUnits)
		for i := 0; i &amp;lt; rbm.NumHiddenUnits; i++ {
			p := rbm.P_H_Given_V(i, vUsedInSamping)
			if p &amp;gt; rand.Float64() {
				sampledH[i] = 1.0
			} else {
				sampledH[i] = 0.0
			}
		}
		for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
			p := rbm.P_V_Given_H(j, sampledH)
			if p &amp;gt; rand.Float64() {
				vUsedInSamping[j] = 1.0
			} else {
				vUsedInSamping[j] = 0.0
			}
		}
	}

	return vUsedInSamping
}

func flip(x []float64, bit int) []float64 {
	y := make([]float64, len(x))
	copy(y, x)
	y[bit] = 1.0 - x[bit]
	return y
}

// FreeEnergy returns F(v), the free energy of RBM given a visible vector v.
// refs: eq. (25) in [1].
func (rbm *RBM) FreeEnergy(v []float64) float64 {
	energy := 0.0

	for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
		energy -= rbm.B[j] * v[j]
	}

	for i := 0; i &amp;lt; rbm.NumHiddenUnits; i++ {
		sum := rbm.C[i]
		for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
			sum += rbm.W[i][j] * v[j]
		}
		energy -= math.Log(1 + math.Exp(sum))
	}

	return energy
}

// PseudoLogLikelihood returns pseudo log-likelihood for a given input data.
func (rbm *RBM) PseudoLogLikelihood(v []float64) float64 {
	bitIndex := rand.Int() % len(v)
	fe := rbm.FreeEnergy(v)
	feFlip := rbm.FreeEnergy(flip(v, bitIndex))
	cost := float64(rbm.NumVisibleUnits) * math.Log(nn.Sigmoid(feFlip-fe))
	return cost
}

// PseudoLogLikelihood returns pseudo log-likelihood for a given dataset (or mini-batch).
func (rbm *RBM) PseudoLogLikelihoodForAllData(data [][]float64) float64 {
	sum := 0.0
	for i := range data {
		sum += rbm.PseudoLogLikelihood(data[i])
	}
	cost := sum / float64(len(data))
	return cost
}

// ComputeGradient returns gradients of RBM parameters for a given (mini-batch) dataset.
func (rbm *RBM) ComputeGradient(data [][]float64) ([][]float64, []float64, []float64) {
	gradW := nn.MakeMatrix(rbm.NumHiddenUnits, rbm.NumVisibleUnits)
	gradB := make([]float64, rbm.NumVisibleUnits)
	gradC := make([]float64, rbm.NumHiddenUnits)

	for _, v := range data {
		// Gibbs Sampling
		gibbsStart := v
		vAfterSamping := rbm.GibbsSampling(gibbsStart, rbm.Option.OrderOfGibbsSamping)

		// pre-computation that is used in gradient computation
		p_h_given_v1 := make([]float64, rbm.NumHiddenUnits)
		p_h_given_v2 := make([]float64, rbm.NumHiddenUnits)
		for i := 0; i &amp;lt; rbm.NumHiddenUnits; i++ {
			p_h_given_v1[i] = rbm.P_H_Given_V(i, v)
			p_h_given_v2[i] = rbm.P_H_Given_V(i, vAfterSamping)
		}

		// Gompute gradient of W
		for i := 0; i &amp;lt; rbm.NumHiddenUnits; i++ {
			for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
				gradW[i][j] += p_h_given_v1[i]*v[j] - p_h_given_v2[i]*vAfterSamping[j]
			}
		}

		// Gompute gradient of B
		for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
			gradB[j] += v[j] - vAfterSamping[j]
		}

		// Gompute gradient of C
		for i := 0; i &amp;lt; rbm.NumHiddenUnits; i++ {
			gradC[i] += p_h_given_v1[i] - p_h_given_v2[i]
		}
	}

	return gradW, gradB, gradC
}

func (rbm *RBM) ParseTrainingOption(option TrainingOption) error {
	rbm.Option = option

	if rbm.Option.MiniBatchSize &amp;lt;= 0 {
		return errors.New(&amp;quot;Number of mini-batchs must be larger than zero.&amp;quot;)
	}
	if rbm.Option.Epoches &amp;lt;= 0 {
		return errors.New(&amp;quot;Epoches must be larger than zero.&amp;quot;)
	}
	if rbm.Option.OrderOfGibbsSamping &amp;lt;= 0 {
		return errors.New(&amp;quot;Order of Gibbs sampling must be larger than zero.&amp;quot;)
	}
	if rbm.Option.LearningRate == 0 {
		return errors.New(&amp;quot;Learning rate must be specified to train RBMs.&amp;quot;)
	}

	return nil
}

// Train performs Contrastive divergense learning algorithm to train RBM.
// The alrogithm is basedd on (mini-batch) Stochastic Gradient Ascent.
func (rbm *RBM) Train(data [][]float64, option TrainingOption) error {
	err := rbm.ParseTrainingOption(option)
	if err != nil {
		return err
	}

	numMiniBatches := len(data) / rbm.Option.MiniBatchSize

	for epoch := 0; epoch &amp;lt; option.Epoches; epoch++ {
		// Monitoring
		fmt.Println(epoch, rbm.PseudoLogLikelihoodForAllData(data))

		for m := 0; m &amp;lt; numMiniBatches; m++ {
			// Compute Gradient
			batch := data[m*rbm.Option.MiniBatchSize : (m+1)*rbm.Option.MiniBatchSize]
			gradW, gradB, gradC := rbm.ComputeGradient(batch)

			// Update W
			for i := 0; i &amp;lt; rbm.NumHiddenUnits; i++ {
				for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
					rbm.W[i][j] += rbm.Option.LearningRate * gradW[i][j] / float64(rbm.Option.MiniBatchSize)
					if rbm.Option.L2Regularization {
						rbm.W[i][j] *= (1.0 - rbm.Option.RegularizationRate)
					}
				}
			}

			// Update B
			for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
				rbm.B[j] += rbm.Option.LearningRate * gradB[j] / float64(rbm.Option.MiniBatchSize)
			}

			// Update C
			for i := 0; i &amp;lt; rbm.NumHiddenUnits; i++ {
				rbm.C[i] += rbm.Option.LearningRate * gradC[i] / float64(rbm.Option.MiniBatchSize)
			}
		}
	}

	return nil
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;使い方とかは察して（どうせ誰も使わないはず&lt;/p&gt;
&lt;p&gt;今は、通常のRBMのvisible layerを連続値に拡張した Gaussian Bernoulli RBMを学習しようとしてるんだけど、これがムズイ。実装ミスもあるかもだけど、局所解に落ちまくってる気がする。&lt;/p&gt;
&lt;p&gt;Gaussian Bernoulli RBM、Deep Belief Networks, Deep Neural Networksについてはまた今度&lt;/p&gt;
&lt;p&gt;2014/05/11
要望があったので、もろもろコードあげました
&lt;a href=&#34;https://github.com/r9y9/nnet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/nnet&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;参考資料&#34;&gt;参考資料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://image.diku.dk/igel/paper/AItRBM-proof.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Introduction to Restricted Boltzmann Machines (PDF)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Practical Guide to Training Restricted Boltzmann Machines (PDF)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://mglab.blogspot.jp/2012/08/restricted-boltzmann-machine.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Restricted Boltzmann Machineの学習手法についての簡単なまとめ | 映像奮闘記&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://d.hatena.ne.jp/saket/20121212&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ゆるふわ Restricted Boltzmann Machine | Risky Dune&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://deeplearning.net/tutorial/rbm.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Restricted Boltzmann Machines (RBM) | DeepLearning Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://imonad.com/rbm/restricted-boltzmann-machine/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Restricted Boltzmann Machine - Short Tutorial | iMonad&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://scikit-learn.org/stable/auto_examples/plot_rbm_logistic_classification.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Restricted Boltzmann Machine features for digit classification | scikit-learn&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>マルコフ確率場 (MRF) と条件付き確率場 (CRF) の違い</title>
      <link>https://r9y9.github.io/blog/2014/03/01/difference-between-mrf-and-crf/</link>
      <pubDate>Sat, 01 Mar 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/03/01/difference-between-mrf-and-crf/</guid>
      <description>&lt;p&gt;一番の違いは、生成モデルか識別モデルか、ということ。それぞれ、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Markov Random Fields (MRF) は生成モデル&lt;/li&gt;
&lt;li&gt;Conditional Random Fields (CRF) は識別モデル&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;です。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://metaoptimize.com/qa/questions/4021/what-is-exactly-the-difference-between-mrf-and-crf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;What is exactly the difference between MRF and CRF&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ここを見ると割とすっきりする。&lt;/p&gt;
&lt;p&gt;ただ、少しスムーズに納得できないことがありまして…それは、MRFもCRFもグラフィカルモデルで書くと無向グラフとなること。識別モデルは無向グラフで生成モデルは有向グラフなんじゃ…？と思ってしまう人もいるんじゃないかと思う（いなかったらごめんなさい）。&lt;/p&gt;
&lt;h2 id=&#34;グラフィカルモデルとしての表現&#34;&gt;グラフィカルモデルとしての表現&lt;/h2&gt;
&lt;p&gt;一般に、生成モデルは有向グラフの形で記述され、識別モデルは無向グラフとして記述される。例えば、隠れマルコフモデル (HMM) は有向グラフで、条件付き確率場 (CRF) は無向グラフで表される。図を貼っておく&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/HMM_and_CRF.png&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;その道の人には、馴染みのある図だと思う（ｼｭｳﾛﾝから引っ張ってきた）。グレーの○が観測変数、白い○が隠れ変数です&lt;/p&gt;
&lt;p&gt;ここで重要なのは、例外もあるということ。具体的には、タイトルにあるMRFは生成モデルだけど無向グラフで書かれる。MRFというと、例えばRestricted Boltzmann Machine とかね！&lt;/p&gt;
&lt;p&gt;単純なことだけど、これを知らないとMRFについて学習するときにつっかかってしまうので注意&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Introduction to Conditional Random Fields&lt;/a&gt; の2.2 Generative versus Discriminative Models から引用すると、&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Because a generative model takes the form p(y,x) = p(y)p(x|y), it is often natural to represent a generative model by a directed graph in which in outputs y topologically precede the inputs. Similarly, we will see that it is often natural to represent a discriminative model by a undirected graph. However, this need not always be the case, and both undirected generative models, such as the Markov random ﬁeld (2.32), and directed discriminative models, such as the MEMM (6.2), are sometimes used. It can also be useful to depict discriminative models by directed graphs in which the x precede the y.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;らしいです&lt;/p&gt;
&lt;h2 id=&#34;結論&#34;&gt;結論&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;生成モデル＝有向グラフ、識別モデル＝無向グラフで&lt;strong&gt;表されるとは限らない&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;ことMRFに関して言えば生成モデルだけど無向グラフで表されるよ&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ということです&lt;/p&gt;
&lt;p&gt;さらに言えば、MRFとCRFはグラフィカルモデルでは同じように書けてしまうけれど、両者には明確な違いがあることに気をつけましょう、ということです（ちょっと自信ない）&lt;/p&gt;
&lt;p&gt;間違っていたら教えて下さい&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://metaoptimize.com/qa/questions/4021/what-is-exactly-the-difference-between-mrf-and-crf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;What is exactly the difference between MRF and CRF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Introduction to Conditional Random Fields (PDF)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cs.helsinki.fi/group/cosco/Teaching/Probability/2010/lecture5_MRF2.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;More about Undirected Graphical Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Goで音声信号処理をしたいのでSPTKのGoラッパーを書く</title>
      <link>https://r9y9.github.io/blog/2014/02/10/sptk-go-wrapper/</link>
      <pubDate>Mon, 10 Feb 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/02/10/sptk-go-wrapper/</guid>
      <description>&lt;p&gt;2014/07/22 追記：
パッケージの一部として書きました（&lt;a href=&#34;http://r9y9.github.io/blog/2014/06/08/gossp-speech-signal-processing-for-go/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GOSSP - Go言語で音声信号処理 - LESS IS MORE&lt;/a&gt;を参照）。
SPTKのラップも含め、いくつかGoで信号処理アルゴリズムを実装したので、お求めの方はどうぞ&lt;/p&gt;
&lt;p&gt;&amp;ndash;&lt;/p&gt;
&lt;p&gt;Goが最近オススメです（n度目&lt;/p&gt;
&lt;p&gt;Goで音声信号処理をしたいけど、全部一から書くのは大変だし、既存の資産は出来るだけ再利用したい。というわけで、C言語製の&lt;a href=&#34;http://sp-tk.sourceforge.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SPTK&lt;/a&gt; をGoから使えるようにする&lt;/p&gt;
&lt;h2 id=&#34;cgo&#34;&gt;cgo&lt;/h2&gt;
&lt;p&gt;GoにはC言語のライブラリを使うには、cgoというパッケージを使えばできる。使い方は、公式のページ等を見るといいと思う &lt;a href=&#34;https://golang.org/cmd/cgo/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://golang.org/cmd/cgo/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Cの関数や変数などには、 &lt;code&gt;C.&lt;/code&gt; でアクセスできる&lt;/p&gt;
&lt;h2 id=&#34;ラッパー&#34;&gt;ラッパー&lt;/h2&gt;
&lt;p&gt;例えば以下のように書く。MFCCの計算を例に上げる。必要に応じで&lt;code&gt;SPTK.h&lt;/code&gt;に定義されている関数をラップする&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package sptk

// #cgo pkg-config: SPTK
// #include &amp;lt;stdio.h&amp;gt;
// #include &amp;lt;SPTK/SPTK.h&amp;gt;
import &amp;quot;C&amp;quot;

func MFCC(audioBuffer []float64, sampleRate int, alpha, eps float64, wlng, flng, m, n, ceplift int, dftmode, usehamming bool) []float64 {
	// Convert go bool to C.Boolean (so annoying..
	var dftmodeInGo, usehammingInGo C.Boolean
	if dftmode {
		dftmodeInGo = 1
	} else {
		dftmodeInGo = 0
	}
	if usehamming {
		usehammingInGo = 1
	} else {
		usehammingInGo = 0
	}

	resultBuffer := make([]float64, m)
	C.mfcc((*_Ctype_double)(&amp;amp;audioBuffer[0]), (*_Ctype_double)(&amp;amp;resultBuffer[0]), C.double(sampleRate), C.double(alpha), C.double(eps), C.int(wlng), C.int(flng), C.int(m), C.int(n), C.int(ceplift), dftmodeInGo, usehammingInGo)
	return resultBuffer
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;このパッケージを使う前に、 &lt;a href=&#34;https://github.com/r9y9/SPTK&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/SPTK&lt;/a&gt; を使ってSPTKをインストールする。本家のを使ってもいいですが、その場合は #cgo の設定が変わると思います。公式のSPTK、pkg-configに対応してくれんかな…&lt;/p&gt;
&lt;p&gt;最初は、LDFLAGS つけ忘れてて、symbol not foundってなってつらまった。次回から気をつけよう&lt;/p&gt;
&lt;p&gt;SPTKの、特に（メル）ケプストラム分析当たりは本当に難しいので、論文読んで実装するのも大変だし中身がわからなくてもラップする方が合理的、という結論に至りました。簡単なもの（例えば、メルケプからMLSA filterの係数への変換とか）は、依存関係を少なくするためにもGo nativeで書きなおした方がいいです&lt;/p&gt;
&lt;p&gt;コードは気が向いたら上げる&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Goでクロマベクトルを求める</title>
      <link>https://r9y9.github.io/blog/2014/01/28/go-chroma-vector/</link>
      <pubDate>Tue, 28 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/01/28/go-chroma-vector/</guid>
      <description>&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/pcp_result.png &#34;Chromagram&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;Chromagram。ドレミの歌の冒頭を分析した結果です&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main

import (
	&amp;quot;fmt&amp;quot;
	&amp;quot;github.com/mjibson/go-dsp/wav&amp;quot;
	&amp;quot;github.com/r9y9/go-msptools/pcp&amp;quot;
	&amp;quot;log&amp;quot;
	&amp;quot;os&amp;quot;
)

func main() {
	// reading data
	file, err := os.Open(&amp;quot;/path/to/file.wav&amp;quot;)
	if err != nil {
		log.Fatal(err)
	}
	defer file.Close()

	wav, werr := wav.ReadWav(file)
	if werr != nil {
		log.Fatal(werr)
	}

	// convert to []float64 from []int
	data := make([]float64, len(wav.Data[0]))
	for i := range data {
		data[i] = float64(wav.Data[0][i])
	}

	// settings for analysis
	frameShift := int(float64(wav.SampleRate) / 100.0) // 0.01 sec
	sampleRate := int(wav.SampleRate)

	// create PCP extrator
	p := pcp.NewPCP(sampleRate, frameShift)

	// analysis roop
	result := make([][]float64, p.NumFrames(data))
	for i := 0; i &amp;lt; p.NumFrames(data); i++ {
		pcp := p.PCP(data, i*frameShift)
		//pcp := p.PCPNormalized(data, i*frameShift)
		result[i] = pcp
	}

	// print as a gnuplot 3D plotting format
	fmt.Println(&amp;quot;#&amp;quot;, len(result[0]), len(result))
	for i, spec := range result {
		for j, val := range spec {
			fmt.Println(i, j, val)
		}
		fmt.Println(&amp;quot;&amp;quot;)
	}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;こんな感じでOK。Chromagramをgnuplot形式で標準出力に出力します&lt;/p&gt;
&lt;h2 id=&#34;pitch-class-profile-pcp-in-go-codehttpsgithubcomr9y9go-msptoolstreemasterpcp&#34;&gt;Pitch Class Profile (PCP) in Go &lt;a href=&#34;https://github.com/r9y9/go-msptools/tree/master/pcp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Code]&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;どうやってクロマベクトルを計算しているかざっくり説明すると、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;入力信号をガボールウェーブレット変換&lt;/li&gt;
&lt;li&gt;オクターブ無視して12次元に圧縮（例えば55Hz, 110Hz, 220Hz, 440Hz はすべてAとする）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;という手順で求めてます&lt;/p&gt;
&lt;p&gt;Goかどうかなんてどうでもいいんだけど、まぁC++に比べて書きやすすぎて泣けるよね&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Goで信号処理</title>
      <link>https://r9y9.github.io/blog/2014/01/27/start-coding-go-msptools/</link>
      <pubDate>Mon, 27 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/01/27/start-coding-go-msptools/</guid>
      <description>&lt;p&gt;最近Go言語を触っていて、これがなかなかいい感じ。そこそこ速いので、信号処理や機械学習も行けると思う&lt;/p&gt;
&lt;h2 id=&#34;goの良い所&#34;&gt;Goの良い所&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;デフォでたくさん便利なパッケージがある。http, json, os, &amp;hellip;&lt;/li&gt;
&lt;li&gt;パッケージのインストールはとても簡単。go getするだけ&lt;/li&gt;
&lt;li&gt;デフォでテストの枠組みがある&lt;/li&gt;
&lt;li&gt;gofmtでコードのformattingしてくれるので書き方で迷わなくて良い&lt;/li&gt;
&lt;li&gt;使わないパッケージをimportするとコンパイルエラーになるし自然と疎結合なコードを書くようになる&lt;/li&gt;
&lt;li&gt;並列処理を言語レベルでサポート&lt;/li&gt;
&lt;li&gt;GCあるのでメモリ管理なんてしなくていい&lt;/li&gt;
&lt;li&gt;全般的にC++より書きやすい（ここ重要）&lt;/li&gt;
&lt;li&gt;そこそこ速い（C++よりは遅いけど）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ホントはPythonでさくっと書きたいけどパフォーマンスもほしいからC++で書くかー（嫌だけど）。と思ってた自分にはちょうどいい&lt;/p&gt;
&lt;h2 id=&#34;goの悪い所主にcと比べて&#34;&gt;Goの悪い所（主にC++と比べて）&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ちょっと遅い。さっと試したウェーブレット変換は、1.5倍くらい遅かった気がする（うろ覚え）&lt;/li&gt;
&lt;li&gt;C++やpythonに比べるとライブラリは少ない&lt;/li&gt;
&lt;li&gt;言語仕様とかそのへんが優れてるかどうかは判断つきませんごめんなさい&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;go-msptools&#34;&gt;Go-msptools&lt;/h1&gt;
&lt;p&gt;2014/07/22 追記：
Go-msptoolsはGOSSPに吸収されました。（&lt;a href=&#34;https://r9y9.github.io/blog/2014/06/08/gossp-speech-signal-processing-for-go/&#34;&gt;GOSSP - Go言語で音声信号処理 - LESS IS MORE&lt;/a&gt;を参照）&lt;/p&gt;
&lt;h2 id=&#34;おまけ音の信号処理に役立ちそうなライブラリ&#34;&gt;おまけ：音の信号処理に役立ちそうなライブラリ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mjibson/go-dsp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;go-dsp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://code.google.com/p/portaudio-go/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;portaudio-go&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>MLSA digital filter のC&#43;&#43;実装</title>
      <link>https://r9y9.github.io/blog/2013/12/01/mlsa-filter-with-c-plus-plus/</link>
      <pubDate>Sun, 01 Dec 2013 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2013/12/01/mlsa-filter-with-c-plus-plus/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://r9y9.github.io/blog/2013/09/23/mlsa-filter-wakaran/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MLSAフィルタわからん&lt;/a&gt;という記事を書いて早2ヶ月、ようやく出来た。&lt;/p&gt;
&lt;p&gt;Mel-log spectrum approximate (MLSA) filterというのは、対数振幅スペクトルを近似するようにメルケプストラムから直接音声を合成するデジタルフィルタです。&lt;a href=&#34;http://sp-tk.sourceforge.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SPTK&lt;/a&gt;のmlsa filterと比較して完全に計算結果が一致したので、間違ってはないはず。MLSAフィルタを使ってメルケプから音声合成するプログラムをC++で自分で書きたいという稀有な人であれば、役に立つと思います。基本的に、SPTKのmlsa filterの再実装です。&lt;/p&gt;
&lt;h1 id=&#34;mlsa_filterh&#34;&gt;mlsa_filter.h&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://gist.github.com/r9y9/7735120&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gist.github.com/r9y9/7735120&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;#pragma once

#include &amp;lt;cmath&amp;gt;
#include &amp;lt;memory&amp;gt;
#include &amp;lt;vector&amp;gt;
#include &amp;lt;cassert&amp;gt;

namespace sp {

/**
 * MLSA BASE digital filter (Mel-log Spectrum Approximate digital filter)
 */
class mlsa_base_filter {
public:
  mlsa_base_filter(const int order, const double alpha);

  template &amp;lt;class Vector&amp;gt;
  double filter(const double x, const Vector&amp;amp; b);

 private:
  mlsa_base_filter();

  double alpha_;
  std::vector&amp;lt;double&amp;gt; delay_;
};

mlsa_base_filter::mlsa_base_filter(const int order, const double alpha)
: alpha_(alpha),
  delay_(order+1)
{
}

template &amp;lt;class Vector&amp;gt;
double mlsa_base_filter::filter(const double x, const Vector&amp;amp; b)
{
  double result = 0.0;

  delay_[0] = x;
  delay_[1] = (1.0-alpha_*alpha_)*delay_[0] + alpha_*delay_[1];

  for (size_t i = 2; i &amp;lt; b.size(); ++i) {
    delay_[i] = delay_[i] + alpha_*(delay_[i+1]-delay_[i-1]);
    result += delay_[i] * b[i];
  }

  // special case
  // TODO: other solution?
  if (b.size() == 2) {
    result += delay_[1] * b[1];
  }

  // t &amp;lt;- t+1 in time
  for (size_t i = delay_.size()-1; i &amp;gt; 1; --i) {
    delay_[i] = delay_[i-1];
  }

  return result;
}

/**
 * MLSA digital filter cascaded
 */
class mlsa_base_cascaded_filter {
 public:
  mlsa_base_cascaded_filter(const int order,
			    const double alpha,
			    const int n_pade);

  template &amp;lt;class Vector&amp;gt;
  double filter(const double x, const Vector&amp;amp; b);

 private:
  mlsa_base_cascaded_filter();

  std::vector&amp;lt;std::unique_ptr&amp;lt;mlsa_base_filter&amp;gt;&amp;gt; base_f_; // cascadad filters
  std::vector&amp;lt;double&amp;gt; delay_;
  std::vector&amp;lt;double&amp;gt; pade_coef_;
};

mlsa_base_cascaded_filter::mlsa_base_cascaded_filter(const int order,
						     const double alpha,
						     const int n_pade)
  : delay_(n_pade + 1),
  pade_coef_(n_pade + 1)
{
  using std::unique_ptr;

  if (n_pade != 4 &amp;amp;&amp;amp; n_pade != 5) {
    std::cerr &amp;lt;&amp;lt; &amp;quot;The number of pade approximations must be 4 or 5.&amp;quot;
	      &amp;lt;&amp;lt; std::endl;
  }
  assert(n_pade == 4 || n_pade == 5);

  for (int i = 0; i &amp;lt;= n_pade; ++i) {
    mlsa_base_filter* p = new mlsa_base_filter(order, alpha);
    base_f_.push_back(unique_ptr&amp;lt;mlsa_base_filter&amp;gt;(p));
  }

  if (n_pade == 4) {
    pade_coef_[0] = 1.0;
    pade_coef_[1] = 4.999273e-1;
    pade_coef_[2] = 1.067005e-1;
    pade_coef_[3] = 1.170221e-2;
    pade_coef_[4] = 5.656279e-4;
  }

  if (n_pade == 5) {
    pade_coef_[0] = 1.0;
    pade_coef_[1] = 4.999391e-1;
    pade_coef_[2] = 1.107098e-1;
    pade_coef_[3] = 1.369984e-2;
    pade_coef_[4] = 9.564853e-4;
    pade_coef_[5] = 3.041721e-5;
  }
}

template &amp;lt;class Vector&amp;gt;
double mlsa_base_cascaded_filter::filter(const double x, const Vector&amp;amp; b)
{
  double result = 0.0;
  double feed_back = 0.0;

  for (size_t i = pade_coef_.size()-1; i &amp;gt;= 1; --i) {
    delay_[i] = base_f_[i]-&amp;gt;filter(delay_[i-1], b);
    double v = delay_[i] * pade_coef_[i];
    if (i % 2 == 1) {
      feed_back += v;
    } else {
      feed_back -= v;
    }
    result += v;
  }

  delay_[0] = feed_back + x;
  result += delay_[0];

  return result;
}

/**
 * MLSA digital filter (Mel-log Spectrum Approximate digital filter)
 * The filter consists of two stage cascade filters
 */
class mlsa_filter {
 public:
  mlsa_filter(const int order, const double alpha, const int n_pade);
 ~mlsa_filter();

 template &amp;lt;class Vector&amp;gt;
 double filter(const double x, const Vector&amp;amp; b);

 private:
 mlsa_filter();

  double alpha_;
  std::unique_ptr&amp;lt;mlsa_base_cascaded_filter&amp;gt; f1_; // first stage
  std::unique_ptr&amp;lt;mlsa_base_cascaded_filter&amp;gt; f2_; // second stage
};

mlsa_filter::mlsa_filter(const int order,
			 const double alpha,
			 const int n_pade)
  : alpha_(alpha),
  f1_(new mlsa_base_cascaded_filter(2, alpha, n_pade)),
  f2_(new mlsa_base_cascaded_filter(order, alpha, n_pade))
{
}

mlsa_filter::~mlsa_filter()
{
}

template &amp;lt;class Vector&amp;gt;
double mlsa_filter::filter(const double x, const Vector&amp;amp; b)
{
  // 1. First stage filtering
  Vector b1 = {0, b[1]};
  double y = f1_-&amp;gt;filter(x, b1);

  // 2. Second stage filtering
  double result = f2_-&amp;gt;filter(y, b);

  return result;
}

} // end namespace sp
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;使い方&#34;&gt;使い方&lt;/h1&gt;
&lt;p&gt;mlsa_filter.hをインクルードすればおｋ&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#include &amp;quot;mlsa_filter.h&amp;quot;

// セットアップ
const double alpha = 0.42;
const int order = 30;
const int n_pade = 5;
sp::mlsa_filter mlsa_f(order, alpha, n_pade);

...
// MLSA フィルタリング
出力一サンプル = mlsa_f.filter(入力一サンプル, フィルタ係数);
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;何で再実装したのか&#34;&gt;何で再実装したのか&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;mlsa filterをC++的なインタフェースで使いたかった&lt;/li&gt;
&lt;li&gt;コード見たらまったく意味がわからなくて、意地でも理解してやろうと思った&lt;/li&gt;
&lt;li&gt;反省はしている&lt;/li&gt;
&lt;li&gt;知り合いの声質変換やってる方がMLSAフィルタを波形合成に使ってるっていうし、ちょっとやってみようかなって&lt;/li&gt;
&lt;li&gt;あと最近音声合成の低レベルに手をつけようとと思ってたし勉強にもなるかなって&lt;/li&gt;
&lt;li&gt;思ったんだ……んだ…だ…&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;車輪の再開発はあんま良くないと思ってるけど許して。
誰かがリファクタせないかんのだ&lt;/p&gt;
&lt;h1 id=&#34;感想&#34;&gt;感想&lt;/h1&gt;
&lt;p&gt;SPTKのmlsa filterは、正直に言うとこれまで読んできたコードの中で一二を争うほど難解でした（いうてC言語はあまり読んできてないので、Cだとこれが普通なのかもしれないけど）。特に、元コードの d: delayという変数の使われ方が複雑過ぎて、とても読みにくくございました。MLSAフィルタは複数のbase filterのcascade接続で表されるわけだけど、それぞれの遅延が一つのdという変数で管理されていたのです。つまり、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;d[1] ~ d[5] までは、あるフィルタの遅延&lt;/li&gt;
&lt;li&gt;d[6] ~ d[11] までは、別のフィルタの遅延&lt;/li&gt;
&lt;li&gt;d[12] ~ にはまた別のフィルタの遅延&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;という感じです。&lt;/p&gt;
&lt;p&gt;改善しようと思って、base filterというクラスを作ってそのクラスの状態として各フィルタの遅延を持たせて、見通しを良くしました&lt;/p&gt;
&lt;h2 id=&#34;さいごに&#34;&gt;さいごに&lt;/h2&gt;
&lt;p&gt;MLSAフィルタ、難しいですね（小並感&lt;/p&gt;
&lt;p&gt;いつかリアルタイム声質変換がやってみたいので、それに使う予定（worldを使うことになるかもしれんけど）。戸田先生当たりがやってる声質変換を一回真似してみたいと思ってる&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SPTKをC&#43;&#43;から使えるようにする</title>
      <link>https://r9y9.github.io/blog/2013/12/01/sptk-with-waf/</link>
      <pubDate>Sun, 01 Dec 2013 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2013/12/01/sptk-with-waf/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://sp-tk.sourceforge.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;音声信号処理ツールキットSPTK&lt;/a&gt;をC++から使おうと思ったら意外とハマってしまったので、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C++から使えるようにC++コンパイラでコンパイルできるようにした&lt;/li&gt;
&lt;li&gt;使いやすいようにwafを組み込みんだ&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;リポジトリ: &lt;a href=&#34;https://github.com/r9y9/SPTK&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/SPTK&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;というわけで、使い方について簡単に書いておく&lt;/p&gt;
&lt;h1 id=&#34;sptk-について&#34;&gt;SPTK について&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;SPTKを使うと何ができるか: &lt;a href=&#34;http://aidiary.hatenablog.com/entry/20120701/1341126474&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SPTKの使い方 (1) インストール・波形描画・音声再生 | 人工知能に関する断創録&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SPTKとは: &lt;a href=&#34;[http://sp-tk.sourceforge.net/]&#34;&gt;Speech Signal Processing Toolkit (SPTK)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;sptk-with-waf&#34;&gt;SPTK with waf&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/SPTK&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SPTK with waf&lt;/a&gt;は、SPTKをwafでビルド管理できるようにしたものです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SPTKを共有ライブラリとしてインストールできます。&lt;/li&gt;
&lt;li&gt;C、C++の好きな方でコンパイルできます。&lt;/li&gt;
&lt;li&gt;wafが使えます（速い、出力がキレイ）&lt;/li&gt;
&lt;li&gt;自分のC、C++コードからSPTKのメソッドを呼べます。&lt;/li&gt;
&lt;li&gt;コマンドラインツールはインストールされません。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;コマンドラインツールを使いたい人は、元のconfigure scriptを使えば十分です。&lt;/p&gt;
&lt;h1 id=&#34;環境&#34;&gt;環境&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Unix系&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ubuntu 12.04 LTS 64 bitとMac OS X 10.9では確認済み&lt;/p&gt;
&lt;h1 id=&#34;sptkのインストール&#34;&gt;SPTKのインストール&lt;/h1&gt;
&lt;p&gt;リポジトリをクローンしたあと、&lt;/p&gt;
&lt;h2 id=&#34;build&#34;&gt;Build&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt; ./waf configure &amp;amp;&amp;amp; ./waf
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;build-with-clang&#34;&gt;Build with clang++&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt; CXX=clang++ ./waf configure &amp;amp;&amp;amp; ./waf
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;build-with-gcc&#34;&gt;Build with gcc&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt; git checkout c
 ./waf configure &amp;amp;&amp;amp; ./waf
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;build-with-clang-1&#34;&gt;Build with clang&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt; git checkout c
 CC=clang ./waf configure &amp;amp;&amp;amp; ./waf
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;install&#34;&gt;Install&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt; sudo ./waf install
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Include files: &lt;code&gt;/usr/local/include/SPTK&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Library: &lt;code&gt;/usr/local/lib/SPTK&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Pkg-config: &lt;code&gt;/usr/local/lib/pkgconfig&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;オリジナルのSPTKとはインストール場所が異なります（オリジナルは、&lt;code&gt;/usr/local/SPTK&lt;/code&gt;）&lt;/p&gt;
&lt;h1 id=&#34;sptkを使ってコードを書く&#34;&gt;SPTKを使ってコードを書く&lt;/h1&gt;
&lt;p&gt;&lt;code&gt;&amp;lt;SPTK/SPTK.h&amp;gt;&lt;/code&gt; をインクルードして、好きな関数を呼ぶ&lt;/p&gt;
&lt;p&gt;コンパイルは、例えば以下のようにする&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; g++ test.cpp `pkg-config SPTK --cflags --libs`
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;面倒なので、example/ 内のコードを修正して使う（wafを使おう）のがおすすめです。&lt;/p&gt;
&lt;br/&gt;
&lt;h1 id=&#34;きっかけ&#34;&gt;きっかけ&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;SPTKはコマンドラインツールだと思ってたけど、どうやらSPTK.hをインクルードすれば一通りのツールを使えるらしい&lt;/li&gt;
&lt;li&gt;SPTK.hをインクルードして使う方法のマニュアルが見つからない…&lt;/li&gt;
&lt;li&gt;SPTKはC言語で書かれてるし、C++から使うの地味にめんどくさい&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;cから簡単に使いたかった&#34;&gt;C++から簡単に使いたかった&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;gccやclangだけじゃなくg++やclang++でコンパイルできるようにしよう&lt;/li&gt;
&lt;li&gt;自分のコードのビルド管理にはwafを使ってるし、wafで管理できるようにしてしまおう&lt;/li&gt;
&lt;li&gt;waf素晴らしいしな （参考: &lt;a href=&#34;http://d.hatena.ne.jp/tanakh/20100212&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;waf チュートリアル | 純粋関数型雑記帳 &lt;/a&gt;）&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;最後に&#34;&gt;最後に&lt;/h1&gt;
&lt;p&gt;SPTKもwafも素晴らしいので積極的に使おう＾＾&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MFCCの計算方法についてメモ</title>
      <link>https://r9y9.github.io/blog/2013/11/24/mfcc-calculation-memo/</link>
      <pubDate>Sun, 24 Nov 2013 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2013/11/24/mfcc-calculation-memo/</guid>
      <description>&lt;h2 id=&#34;mfcc-とは&#34;&gt;MFCC とは&lt;/h2&gt;
&lt;p&gt;Mel-Frequency Cepstral Coefficients (MFCCs) のこと。音声認識でよく使われる、音声の特徴表現の代表的なもの。&lt;/p&gt;
&lt;h3 id=&#34;算出手順&#34;&gt;算出手順&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;音声信号を適当な長さのフレームで切り出し&lt;/li&gt;
&lt;li&gt;窓がけ&lt;/li&gt;
&lt;li&gt;フーリエ変換して対数振幅スペクトルを求める&lt;/li&gt;
&lt;li&gt;メルフィルタバンクを掛けて、メル周波数スペクトルを求める&lt;/li&gt;
&lt;li&gt;離散コサイン変換により、MFCCを求める&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以上。SPTKのmfccコマンドのソースもだいたいそうなってた。&lt;/p&gt;
&lt;h3 id=&#34;さて&#34;&gt;さて&lt;/h3&gt;
&lt;h4 id=&#34;ここに音声波形があるじゃろ&#34;&gt;ここに音声波形があるじゃろ？？&lt;/h4&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/speech-signal.png &#34;音声信号を適当な長さのフレームで切り出し&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;h4 id=&#34;音声波形を窓がけして&#34;&gt;音声波形を窓がけして…&lt;/h4&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/windowed-signal.png &#34;窓がけ&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;h4 id=&#34;さらにフーリエ変換して対数取って&#34;&gt;さらにフーリエ変換して対数取って…&lt;/h4&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/log-amplitude.png &#34;フーリエ変換して振幅スペクトルを求める&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;h4 id=&#34;ここでメルフィルタバンクの出番じゃ&#34;&gt;ここでメルフィルタバンクの出番じゃ&lt;/h4&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/after-mel-filterbank.png &#34;メルフィルタバンクを掛けて、メル周波数スペクトルを求める&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;h4 id=&#34;最後に離散コサイン変換で完成じゃ&#34;&gt;最後に離散コサイン変換で完成じゃ&lt;/h4&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/MFCC.png &#34;離散コサイン変換により、MFCCを求める&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;MFCC求めたかったら、普通はHTKかSPTK使えばいいんじゃないですかね。自分で書くと面倒くさいです&lt;/li&gt;
&lt;li&gt;正規化はどうするのがいいのか、まだよくわかってない。単純にDCT（IIを使った）を最後に掛けると、かなり大きい値になって使いにくい。ので、 &lt;a href=&#34;http://research.cs.tamu.edu/prism/lectures/sp/l9.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://research.cs.tamu.edu/prism/lectures/sp/l9.pdf&lt;/a&gt; にもあるとおり、mel-filterbankの数（今回の場合は64）で割った。&lt;/li&gt;
&lt;li&gt;間違ってるかもしれないけどご愛嬌&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://research.cs.tamu.edu/prism/lectures/sp/l9.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;L9: Cepstral analysis [PDF]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://shower.human.waseda.ac.jp/~m-kouki/pukiwiki_public/66.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;メル周波数ケプストラム（MFCC） | Miyazawa’s Pukiwiki 公開版&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://aidiary.hatenablog.com/entry/20120225/1330179868&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;メル周波数ケプストラム係数（MFCC） | 人工知能に関する断創録&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>MIT Media Lab 特別フォーラムに参加してきた</title>
      <link>https://r9y9.github.io/blog/2013/11/03/mit-media-lab-talk-participated/</link>
      <pubDate>Sun, 03 Nov 2013 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2013/11/03/mit-media-lab-talk-participated/</guid>
      <description>&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/mit_media_lab_talk_participated.jpg &#34;MIT Media Lab Talk&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&#34;https://www.tdwa.com/tdw/special/forum/mitmedialab.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TOKYO DESIGNES WEEK MIT Media Lab 特別フォーラム&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;すごい刺激になった。悔しかった。このままで終わりたくない。世の中にはすごい人がいる中で、自分が大したことも為せないでいること、ビジョンを持てていないこと、自分がとても小さな人間だと感じた。石井先生の話の一つに「ビジョンを持て」っていうのがあったけど、僕にはビジョンが圧倒的に欠けてると思った。&lt;/p&gt;
&lt;p&gt;メディアラボの学生の発表は、内容に興味を感じたのはもちろん、プレゼンテーションの魅せ方が上手かった。一番面白かったのはsuper shoes。プレゼンの魅せ方も一際良かったし話の構成もわかりやすくて、super shoes自体も面白かった。自分の行きたい所、食べたいもの、聴きたい音楽、それらに合わせて道案内してくれる靴。&lt;/p&gt;
&lt;p&gt;石井先生の話では、今の僕に一番響いたのは「屈辱力」という言葉だった。理由は単純で、今の僕が一番頻繁に感じている感情だから。石井先生の言葉で「出杭力」とか有名だけど、それはもうすでに身に染みてるので。打たれても打たれてもやるしかない。何度叩かれようが、出すぎるまでやる。&lt;/p&gt;
&lt;p&gt;とはいえ、なかなか思ったようにできない。努力は最低条件と思っているが、十分にできていない。理想と現実のギャップが大きくて、毎日悔しいと感じる。恥ずかしい話ですね、こういうの書いてしまうあたり。僕は弱音はかずに頑張れるほど強くない・・・。&lt;/p&gt;
&lt;p&gt;話がそれてしまったけど、それはさておき、パーティーで石井先生と話したとき、悔しいのは誰でも感じること、そこから何が出来るかが大事、的なことを言われた（はず…緊張していてうろ覚え…）。当たり前なんですけどね。最近、どうも悔しさの数だけ強くなれると思っていた節があるけど、正確にはそれは違って、強くなるチャンスが得られる、が正しいと思う。悔しさを感じただけでは、変われない。行動に移さないと行けない。変わるために、自分で一歩踏み出さねければならない。&lt;/p&gt;
&lt;p&gt;最近、いつも通り毎日悔しいけど、その分楽しいよね。Mかもしれない。成長するって、人間の本質的な欲求の一つなんじゃないか（適当&lt;/p&gt;
&lt;p&gt;っと、すごい恥ずかしいことを書いてしまった。ただせっかくなので、思ったことを残しておこうと思った。&lt;/p&gt;
&lt;p&gt;Xiao Xiaoのピアノ、良かったよね。プレゼンもとても良かった。日本語しゃべってるXiao Xiaoに惚れた&lt;/p&gt;
&lt;p&gt;趣向は違うけれど、自動伴奏紹介すればよかったよ。お前何やってんの？って言われて何も語れないとか、いやだなぁって思う。せっかく修士でやったんだし、もっとブラッシュアップして、使えるレベルにまで持っていきたい。僕だって、何のビジョンもなく自動伴奏やってたわけじゃないんだし。&lt;/p&gt;
&lt;p&gt;当面やりたいことが、また増えた。&lt;/p&gt;
&lt;p&gt;感じたこと一行まとめ&lt;/p&gt;
&lt;p&gt;「なぜを突き詰めて、自分の本当にやりたいことを理解して、ビジョンを持ってひたすら進む」&lt;/p&gt;
&lt;p&gt;ところでどうでもいいけど、最近、さくら荘7.5巻の生徒会長のはうはうな彼女、って短編読みましたが、はうはうかわいいですね&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>逆連続ウェーブレット変換による信号の再構成</title>
      <link>https://r9y9.github.io/blog/2013/10/21/signal-reconstruction-using-invere-cwt/</link>
      <pubDate>Mon, 21 Oct 2013 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2013/10/21/signal-reconstruction-using-invere-cwt/</guid>
      <description>&lt;p&gt;やったのでメモ。おそらく正しくできたと思う。結果貼っとく。ウェーブレットの参考は以下の文献&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://paos.colorado.edu/research/wavelets/bams_79_01_0061.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Torrence, C. and G.P. Compo &amp;ldquo;A Practical Guide to Wavelet Analysis&amp;rdquo;, Bull. Am. Meteorol. Soc., 79, 61–78, 1998.&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;ウェーブレットの条件&#34;&gt;ウェーブレットの条件&lt;/h2&gt;
&lt;p&gt;マザーウェーブレットはmorletを使う&lt;/p&gt;
&lt;div&gt;
\begin{align}
\psi_{0}(\eta) = \pi^{-1/4}e^{i\omega_{0}\eta}e^{-\eta^{2}/2}
\end{align}
&lt;/div&gt;
&lt;p&gt;文献に従って$\omega_{0} = 6.0$とした。&lt;/p&gt;
&lt;p&gt;以下にいっぱい図を張る。軸は適当&lt;/p&gt;
&lt;h2 id=&#34;元の信号&#34;&gt;元の信号&lt;/h2&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/wavelet/original_signal.png &#34;The original signal&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;h2 id=&#34;ウェーブレットスペクトログラム&#34;&gt;ウェーブレットスペクトログラム&lt;/h2&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/wavelet/morlet_wavelet_spectrogram.png &#34;Morlet wavelet spectrogram&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;Gaborではなく、Morletで求めたもの。スケールは、min=55hzで、25cent毎に8オクターブ分取った。一サンプル毎にウェーブレット変換を求めてるので、前回の記事でガボールウェーブレットで求めた奴よりよっぽど解像度高いっすね（前のは10ms毎だった、書いてなかったけど）。見てて綺麗（こなみ&lt;/p&gt;
&lt;p&gt;計算はFFT使ってるので速い&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://hp.vector.co.jp/authors/VA046927/gabor_wavelet/gabor_wavelet.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://hp.vector.co.jp/authors/VA046927/gabor_wavelet/gabor_wavelet.html&lt;/a&gt;
スケールのとり方はここを参考にするといい&lt;/p&gt;
&lt;h2 id=&#34;再構成した信号&#34;&gt;再構成した信号&lt;/h2&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/wavelet/recostructed_signal.png &#34;The recostructed signal&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;連続ウェーブレットの逆変換は、フーリエ変換と違ってそんなシンプルじゃないんだけど、結果から言えばウェーブレット変換の実数部を足しあわせて適当にスケールすれば元の信号が再構成できるみたい。ほんまかと思って実際にやってみたけど、できた&lt;/p&gt;
&lt;p&gt;が、実は少し誤差がある&lt;/p&gt;
&lt;h2 id=&#34;重ねてプロット&#34;&gt;重ねてプロット&lt;/h2&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/wavelet/double_0.png &#34;The original signal and recostructed signal&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;h2 id=&#34;あっぷ&#34;&gt;あっぷ&lt;/h2&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/wavelet/double_1.png &#34;The original signal and recostructed signal with zoom 1&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/wavelet/double_2.png &#34;The original signal and recostructed signal with zoom 2&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/wavelet/double_3.png &#34;The original signal and recostructed signal with zoom 3&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/wavelet/double_4.png &#34;The original signal and recostructed signal with zoom 4&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/wavelet/double_5.png &#34;The original signal and recostructed signal with zoom 5&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;んー、まぁだいたいあってんじゃないですかね&lt;/p&gt;
&lt;h2 id=&#34;誤差&#34;&gt;誤差&lt;/h2&gt;
&lt;p&gt;平均誤差を計算すると、図の縦軸の量で考えて55.3994だった。16bitのwavが-32768〜32767なので、どうだろう、大きいのか小さいのかわからん&lt;/p&gt;
&lt;p&gt;ただ、再合成した音声を聞いた所それほど違和感はなかった。これはつまり、スペクトルいじる系の分析にSTFTがではなくウェーブレット使ってもいいんではないか？という考えが生まれますね。果たして、ウェーブレットが音声/音楽の分析にフーリエ変換ほど使われないのはなぜなのか、突き詰めたい&lt;/p&gt;
&lt;h2 id=&#34;メモ&#34;&gt;メモ&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://paos.colorado.edu/research/wavelets/wavelet3.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://paos.colorado.edu/research/wavelets/wavelet3.html&lt;/a&gt;
ここの最後に書かれている以下の文章、&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;One problem with performing the wavelet transform in Fourier space is that this assumes the time series is periodic. The result is that signals in the wavelet transform at one end of the time series will get wrapped around to the other end.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;FFT使うウェーブレット変換の問題は、信号を周期関数として仮定してしまうことにある、と。まあ、ですよねー。信号がめちゃくちゃ長くてこの仮定が破綻してしまう場合、どうするのがいいんだろう。&lt;/p&gt;
&lt;p&gt;あと、FFT使うウェーブレットの問題として、メモリ食うってのがあるんよな。ウェーブレット変換を計算する前に、マザーウェーブレットのフーリエ変換を持っとかないといけないし、サンプル毎に計算しないといけないし。44.1kの数分の音声とかなると、もう無理っすね。&lt;/p&gt;
&lt;p&gt;それぞれ、解決方法は思いつかないでもないけど、まだまとまってないので、解決したらまとめる、かもしれない。&lt;/p&gt;
&lt;h2 id=&#34;さらにめも&#34;&gt;さらにめも&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;practicalなんちゃらの、マザーウェーブレットを正規化する部分のmatlabコード、文献中の数式と若干違ってトリッキー。展開すれば一緒なんだけど、文献中の数式をそのまま書いたようになってないので、注意。ちょっと戸惑った&lt;/li&gt;
&lt;li&gt;逆ウェーブレットを行う際のスケールにかかる係数（文献中でいう$C_{\delta}$）は、マザーウェーブレットが決まれば値が定まる（らしい）。例えばMorletの$\omega_0 = 6$なら0.776とわかってるので、積分して計算する必要はない&lt;/li&gt;
&lt;li&gt;ウェーブレット変換と戯れてたら週末終わった&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>FFTを使った連続ウェーブレット変換の高速化</title>
      <link>https://r9y9.github.io/blog/2013/10/20/continuous-wavelet-tranform/</link>
      <pubDate>Sun, 20 Oct 2013 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2013/10/20/continuous-wavelet-tranform/</guid>
      <description>&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/gabor_wavelet_nnmnkwii.png &#34;An example of Gabor Wavelet spectrogram (the original wav file is generated using Open Jalk)&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;h2 id=&#34;そもそもウェーブレット変換って何&#34;&gt;そもそもウェーブレット変換って何&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://ja.wikipedia.org/wiki/%E3%82%A6%E3%82%A7%E3%83%BC%E3%83%96%E3%83%AC%E3%83%83%E3%83%88%E5%A4%89%E6%8F%9B&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jump to wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;いわゆる時間周波数解析の手法の一つで、音声、音楽、画像の解析に使われる。直感的には、STFTでいう窓関数の幅を周波数に応じて拡大・伸縮させて、時間変化する信号の特徴を上手く捉えようとする手法のこと&lt;/p&gt;
&lt;h2 id=&#34;高速化の仕組み&#34;&gt;高速化の仕組み&lt;/h2&gt;
&lt;p&gt;さて、本題。ウェーブレット変換は、(スケールパラメータを固定すれば)入力信号とマザーウェーブレットのたたみ込みで表されるので、たたみ込み定理よりフーリエ変換を使った計算方法が存在する。&lt;/p&gt;
&lt;p&gt;つまり、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;入力信号とマザーウェーブレットをそれぞれフーリエ変換する&lt;/li&gt;
&lt;li&gt;掛け算する&lt;/li&gt;
&lt;li&gt;逆フーリエ変換する&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;というプロセスでウェーブレット変換を求めることができて、かつフーリエ変換にはFFTという高速なアルゴリズムが存在するので、計算を高速化できるという仕組み。まぁ原理としてはシンプルなんだけど以外と面倒くさい（気のせい？）。&lt;/p&gt;
&lt;p&gt;色々調べたので、メモ代わりにまとめておく。解説ではなくリンク集です&lt;/p&gt;
&lt;h2 id=&#34;torrence-c-and-gp-compo-a-practical-guide-to-wavelet-analysis-bull-am-meteorol-soc-79-6178-1998httpspaoscoloradoeduresearchwaveletsbams_79_01_0061pdf&#34;&gt;&lt;a href=&#34;https://paos.colorado.edu/research/wavelets/bams_79_01_0061.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Torrence, C. and G.P. Compo &amp;ldquo;A Practical Guide to Wavelet Analysis&amp;rdquo;, Bull. Am. Meteorol. Soc., 79, 61–78, 1998.&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;結論から言えばここが一番わかりやすかった。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;実装よりで理論の解説がある&lt;/li&gt;
&lt;li&gt;matlab/fortran のコードがある&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;がいいところ&lt;/p&gt;
&lt;p&gt;基本的にはこれ読めばわかる。数学全然わからん俺でも読めた。特に、離散表現でのウェーブレットについても書かれているのは良い。連続ウェーブレットといっても、デジタル信号処理で扱う上では離散化しないといけないわけなので&lt;/p&gt;
&lt;p&gt;さて、僕が参考にしたmatlabコードへの直リンクは以下&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://paos.colorado.edu/research/wavelets/wave_matlab/wave_bases.m&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;マザーウェーブレットの周波数応答の計算部分&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://paos.colorado.edu/research/wavelets/wave_matlab/wavelet.m&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;連続ウェーブレット変換の本体&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://paos.colorado.edu/research/wavelets/wave_matlab/wavetest.m&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;連続ウェーブレット変換のテストコード&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;その他、fortanコードなどいくつかあるので、それらはウェブサイトからどうぞ&lt;/p&gt;
&lt;h2 id=&#34;matlab&#34;&gt;Matlab&lt;/h2&gt;
&lt;p&gt;mathworksさんのwavelet toolboxのドキュメントもよかった。ここから上記のpracticalなんちゃらのリンクもある&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.mathworks.co.jp/jp/help/wavelet/gs/continuous-wavelet-transform.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Continuous Wavelet Transform&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.mathworks.co.jp/jp/help/wavelet/ref/cwtft.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Continuous wavelet transform using FFT algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.mathworks.co.jp/jp/help/wavelet/ref/icwtft.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Inverse CWT&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;コードは転がってないですね。まぁ有料なので&lt;/p&gt;
&lt;h2 id=&#34;日本語でわかりやすいもの&#34;&gt;日本語でわかりやすいもの&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://hp.vector.co.jp/authors/VA046927/gabor_wavelet/gabor_wavelet.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;C/C++言語でガボールウェーブレット変換により時間周波数解析を行うサンプルプログラム&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;ここは本当に素晴らしい。何年か前にも参考にさせて頂きました。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.hulinks.co.jp/support/flexpro/v7/dataanalysis_cwt.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;連続ウェーブレット変換 (CWT) - FlexPro 7 日本語版サポート情報&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;日本語で丁寧に書かれてる。内容自体は、practicalなんちゃらと似ている&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.makino.ecei.tohoku.ac.jp/~aito/wavelet/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;東北大学 伊藤先生の講義資料&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;数少ない日本語でのウェーブレットに関する資料。ただし連続ウェーブレットについてはあんまり解説はない。C言語のサンプル付き&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;書籍&#34;&gt;書籍&lt;/h2&gt;
&lt;p&gt;今回は調べてない。数年前にちょいちょい調べたことがあるけど忘れた&lt;/p&gt;
&lt;h2 id=&#34;その他&#34;&gt;その他&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://code.google.com/p/tspl/source/browse/trunk/include/cwt-impl.h?spec=svn2&amp;amp;r=2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tspl Signal Processing Library in C++&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;連続ウェーブレット変換/逆変換のC++実装。細部までコードは追えてないけど、それっぽいコードがある（俺が読んだ記事とはマザーウェーブレットのnormalizationが違う気もする…&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://dsp.stackexchange.com/questions/10979/inverse-continuous-wavelet-transform-and-matlab&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Inverse Continuous Wavelet Transform and matlab - dsp StackExchange&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;逆連続ウェーブレット変換教えてーっていう質問。ここでpracticalなんちゃらを知った&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://staff.aist.go.jp/h.fujihara/voice_conversion/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;混合音中の歌声の声質変換手法&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;ガチ技術。元産総研の藤原さんが研究開発したもの。&lt;a href=&#34;http://staff.aist.go.jp/m.goto/PAPER/SIGMUS201007fujihara.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;論文(PDF)&lt;/a&gt;の方に少し説明がある。&lt;/li&gt;
&lt;li&gt;声質変換でウェーブレット使うのは僕が知る限りではこれくらい&lt;/li&gt;
&lt;li&gt;ちなみに結果めっちゃすごい&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;さいごに&#34;&gt;さいごに&lt;/h2&gt;
&lt;p&gt;以上。ウェーブレット変換は難しいことがわかった（こなみ）。ウェーブレットの利点欠点については書かなかったけれど、音声や音楽を解析したい場合に、時間周波数解析によく用いられる短時間フーリエ解析よりもウェーブレット解析の方が望ましい場合は非常によくあると思っているので、ぜひもっと使われてほしいですね。作ってるライブラリには必ず入れます。&lt;/p&gt;
&lt;h2 id=&#34;ちなみに&#34;&gt;ちなみに&lt;/h2&gt;
&lt;p&gt;計算コストがそこまでボトルネックにならないなら、畳み込みでウェーブレット計算してもいいんじゃないかと思ってる。FFTを使う方法の場合、あるスケールパラメータに対する時間方向のウェーブレット変換係数を一気に求められても、あるシフトパラメータに対する周波数方向のウェーブレット変換係数（つまりある時間でのスペクトルのようなもの）は一気に求められない気がしている。つまり、STFTみたいな形でインクリメンタルにスペクトルは求めにくいんじゃないかってこと（少なくとも自明には思えない）。畳み込み計算するなら、間違いなくできるけど。このあたり理解がまだあやふやなので、間違ってる可能性大&lt;/p&gt;
&lt;p&gt;さらにちなみに、僕が作ってたリアルタイムで動く自動伴奏システムは畳み込みでウェーブレット変換してたよ。ウェーブレットよりもアルゴリズムのほうがボトルネックになっていたので全然気にならなかった。参考まで&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MLSA フィルタの実装</title>
      <link>https://r9y9.github.io/blog/2013/09/23/mlsa-filter-wakaran/</link>
      <pubDate>Mon, 23 Sep 2013 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2013/09/23/mlsa-filter-wakaran/</guid>
      <description>&lt;p&gt;音声合成に使われるMLSA（Mel-Log Spectrum Approximatation）フィルタを実装したいんだが、なにぶんわからん。SPTKにコードはあるけれど、正直理解できない。デジタル信号処理を小学一年生から勉強しなおしたいレベルだ&lt;/p&gt;
&lt;p&gt;と、前置きはさておき、MLSAフィルタの実装を見つけたのでメモ。ここ最近ちょくちょく調べているが、SPTK以外で初めて見つけた。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://simple4all.org/wp-content/uploads/2013/05/Jiunn.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Realisation and Simulation of the Mel Log Spectrum Approximation Filter | Simple4All Internship Report&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Simple4Allという音声技術系のコミュニティの、学生さんのインターンの成果らしい。ちらっと調べてたら山岸先生も参加してる（た？）っぽい。&lt;/p&gt;
&lt;p&gt;上のreportで引用されているように、MLSA filterの実現方法については、益子さんのD論に詳しく書いてあることがわかった。今井先生の論文と併せて読んでみようと思う。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.109.3623&amp;amp;rep=rep1&amp;amp;type=pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;T. Masuko, &amp;ldquo;HMM-Based Speech Synthesis and Its Applications&amp;rdquo;, Ph.D Thesis, 2002.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;もう正直わからんしブラックボックスでもいいから既存のツール使うかーと諦めかけていたところで割りと丁寧な実装付き解説を見つけたので、もう一度勉強して実装してみようと思い直した。&lt;/p&gt;
&lt;p&gt;機械学習にかまけて信号処理をちゃんと勉強していなかったつけがきている。LMA filterもMLSA filterも、本当にわからなくてツライ……&lt;/p&gt;
&lt;p&gt;(実装だけであれば、実はそんなに難しくなかった 2013/09後半)&lt;/p&gt;
&lt;h3 id=&#34;追記-20150225&#34;&gt;追記 2015/02/25&lt;/h3&gt;
&lt;p&gt;誤解を生む表現があったので、直しました&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>調波打楽器音分離（HPSS）を試す</title>
      <link>https://r9y9.github.io/blog/2013/09/14/hpss/</link>
      <pubDate>Sat, 14 Sep 2013 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2013/09/14/hpss/</guid>
      <description>&lt;h2 id=&#34;hpssとは一行説明&#34;&gt;HPSSとは（一行説明）&lt;/h2&gt;
&lt;p&gt;HPSS（Harmonic/Percussive Sound Separation）というのは、音源中の調波音/打楽器音が、それぞれ時間方向に滑らか/周波数方向に滑らかという異った性質を持つことを利用して、両者を分離する方法のこと。わからんければ論文へ&lt;/p&gt;
&lt;p&gt;アイデアはシンプル、実装は簡単、効果は素晴らしい。specmurtに似たものを感じる。ということで少し感動したので結果を載せる&lt;/p&gt;
&lt;h2 id=&#34;実装&#34;&gt;実装&lt;/h2&gt;
&lt;p&gt;調波音のスペクトログラムを$H$、打楽器音のスペクトログラムを$P$、時間indexをt、周波数indexをkとして、以下の数式をそのまま実装して、適当に反復計算すればおｋ&lt;/p&gt;
&lt;div&gt;
\begin{align}
|H_{t, k}| = \frac{w_{H}^2 (|H_{t+1,k}| + |H_{t-1,k}|)^2 |W_{t,k}|}{w_{H}^2 (|H_{t+1,k}| + |H_{t-1,k}|)^2 + w_{P}^2(|P_{t,k+1}| + |P_{t,k-1}|)^2}
\end{align}
&lt;/div&gt;
&lt;div&gt;
\begin{align}
|P_{t, k}| = \frac{w_{P}^2 (|P_{t,k+1}| + |P_{t,k-1}|)^2 |W_{t,k}|}{w_{H}^2 (|H_{t+1,k}| + |H_{t-1,k}|)^2 + w_{P}^2(|P_{t,k+1}| + |P_{t,k-1}|)^2}
\end{align}
&lt;/div&gt;
&lt;p&gt;ただし&lt;/p&gt;
&lt;div&gt;
\begin{align}
|W_{t,k}| = |H_{t,k}| + |P_{t,k}|
\end{align}
&lt;/div&gt;
&lt;p&gt;絶対値はパワースペクトル。論文中の表記とはけっこう違うので注意。厳密ではないです。$w_{H}, w_{P}$は重み係数で、両方共1.0くらいにしとく。&lt;/p&gt;
&lt;p&gt;HPSSの論文はたくさんあるけど、日本語でかつ丁寧な &lt;a href=&#34;http://ci.nii.ac.jp/naid/110007997346&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;スペクトルの時間変化に基づく音楽音響信号からの歌声成分の強調と抑圧&amp;rdquo;&lt;/a&gt; を参考にした。&lt;/p&gt;
&lt;p&gt;H/Pから音源を再合成するときは、位相は元の信号のものを使えばおｋ&lt;/p&gt;
&lt;p&gt;一点だけ、HとPの初期値どうすればいいんかなぁと思って悩んだ。まぁ普通に元音源のスペクトログラムを両方の初期値としてやったけど、うまく動いてるっぽい。&lt;/p&gt;
&lt;h2 id=&#34;結果&#34;&gt;結果&lt;/h2&gt;
&lt;p&gt;フリー音源でテストしてみたので、結果を貼っとく。$w_{H}=1.0, w_{P}=1.0$、サンプリング周波数44.1kHz、モノラル、フレーム長512、窓関数はhanning。反復推定の回数は30。音源は、&lt;a href=&#34;http://maoudamashii.jokersounds.com/archives/song_kyoko_feels_happiness.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;歌もの音楽素材：歌入り素材系のフリー音楽素材一覧&lt;/a&gt; から使わせてもらいました。ありがとうございまっす。元音源だけステレオです。
18秒目くらいからを比較すると効果がわかりやすいです&lt;/p&gt;
&lt;h3 id=&#34;元音源&#34;&gt;元音源&lt;/h3&gt;
&lt;iframe frameborder=&#34;no&#34; height=&#34;166&#34; scrolling=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=http%3A%2F%2Fapi.soundcloud.com%2Ftracks%2F110367442&#34; width=&#34;100%&#34;&gt;&lt;/iframe&gt;
&lt;h3 id=&#34;hのみ取り出して再合成した音源&#34;&gt;Hのみ取り出して再合成した音源&lt;/h3&gt;
&lt;iframe frameborder=&#34;no&#34; height=&#34;166&#34; scrolling=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=http%3A%2F%2Fapi.soundcloud.com%2Ftracks%2F110367534&#34; width=&#34;100%&#34;&gt;&lt;/iframe&gt;
&lt;h3 id=&#34;pのみ取り出して再合成した音源&#34;&gt;Pのみ取り出して再合成した音源&lt;/h3&gt;
&lt;iframe frameborder=&#34;no&#34; height=&#34;166&#34; scrolling=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=http%3A%2F%2Fapi.soundcloud.com%2Ftracks%2F110367599&#34; width=&#34;100%&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;それにしても特に泥臭い努力をせずに、このクオリティーが出せるのはすごい。音源に対する事前知識も何もないし。あと、ちょっとノイズが載ってるのはたぶんプログラムミス。つらたーん&lt;/p&gt;
&lt;p&gt;コレ以外にも多重HPSSとかもやったけど、いやーおもしろい手法だなーと思いました（こなみ&lt;/p&gt;
&lt;p&gt;詳しくは論文へ（僕のじゃないけど&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ci.nii.ac.jp/naid/110007997346&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;橘 秀幸, 小野 順貴, 嵯峨山 茂樹, &amp;ldquo;スペクトルの時間変化に基づく音楽音響信号からの歌声成分の強調と抑圧&amp;rdquo;, 情報処理学会研究報告, vol. 2009-MUS-81(12), pp. 1-6, 2009.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Naive Bayesの復習（実装編）: MNISTを使って手書き数字認識</title>
      <link>https://r9y9.github.io/blog/2013/08/06/naive-bayes-mnist/</link>
      <pubDate>Tue, 06 Aug 2013 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2013/08/06/naive-bayes-mnist/</guid>
      <description>&lt;p&gt;前回は学習アルゴリズムを導出したので、今回はそれを実装する。Gaussian Naive Bayesのみやった。例によって、アルゴリズムを書く時間よりも言語の使い方等を調べてる時間などの方が圧倒的に多いという残念感だったけど、とりあえずメモる。python, numpy, scipy, matplotlibすべて忘れてた。どれも便利だから覚えよう…&lt;/p&gt;
&lt;p&gt;そもそもナイーブベイズやろうとしてたのも、MNISTのdigit recognitionがやりたかったからなので、実際にやってみた。&lt;/p&gt;
&lt;p&gt;コードはgithubに置いた &lt;a href=&#34;https://github.com/r9y9/naive_bayes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/naive_bayes&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;結果だけ知りたい人へ：正解率  76 %くらいでした。まぁこんなもんですね&lt;/p&gt;
&lt;h2 id=&#34;手書き数字認識&#34;&gt;手書き数字認識&lt;/h2&gt;
&lt;p&gt;手書き数字の画像データから、何が書かれているのか当てる。こういうタスクを手書き数字認識と言う。郵便番号の自動認識が有名ですね。&lt;/p&gt;
&lt;p&gt;今回は、MNISTという手書き数字のデータセットを使って、0〜9の数字認識をやる。MNISTについて詳しくは本家へ→&lt;a href=&#34;http://yann.lecun.com/exdb/mnist/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;THE MNIST DATABASE of handwritten digits&lt;/a&gt;
ただし、MNISTのデータセットは直接使わず、Deep Learningのチュートリアルで紹介されていた（&lt;a href=&#34;http://deeplearning.net/tutorial/gettingstarted.html#gettingstarted&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ここ&lt;/a&gt;）、pythonのcPickleから読める形式に変換されているデータを使った。感謝&lt;/p&gt;
&lt;h2 id=&#34;とりあえずやってみる&#34;&gt;とりあえずやってみる&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone https://github.com/r9y9/naive_bayes
$ cd naive_bayes
$ python mnist_digit_recognition.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;プログラムの中身は以下のようになってる。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MNISTデータセットのダウンロード&lt;/li&gt;
&lt;li&gt;モデルの学習&lt;/li&gt;
&lt;li&gt;テスト&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;実行すると、学習されたGaussianの平均が表示されて、最後に認識結果が表示される。今回は、単純に画像のピクセル毎に独立なGaussianを作ってるので、尤度の計算にめちゃくちゃ時間かかる。実装のせいもあるけど。なので、デフォでは50サンプルのみテストするようにした。&lt;/p&gt;
&lt;h2 id=&#34;学習されたgaussianの平均&#34;&gt;学習されたGaussianの平均&lt;/h2&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/mnist_mean_of_gaussian.png &#34;gaussian means&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;学習されたGaussianの平均をプロットしたもの。上のコードを実行すると表示される。&lt;/p&gt;
&lt;p&gt;それっぽい。学習データは50000サンプル&lt;/p&gt;
&lt;h2 id=&#34;認識結果&#34;&gt;認識結果&lt;/h2&gt;
&lt;p&gt;時間がかかるけど、テストデータ10000個に対してやってみると、結果は以下のようになった。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;0.7634 (7634/10000)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;まぁナイーブベイズなんてこんなもん。もちろん、改善のしようはいくらでもあるけれども。ちなみにDeep learningのチュートリアルで使われてたDBN.pyだと0.987くらいだった。&lt;/p&gt;
&lt;h2 id=&#34;感想&#34;&gt;感想&lt;/h2&gt;
&lt;p&gt;相関が強い特徴だと上手くいかんのは当たり前で、ピクセル毎にGaussianなんて作らずに（ピクセル間の相関を無視せずに）、少しまともな特徴抽出をかませば、8割りは超えるんじゃないかなぁと思う。&lt;/p&gt;
&lt;p&gt;あとこれ、実装してても機械学習的な面白さがまったくない（上がれ目的関数ｩｩーー！的な）ので、あまりおすすめしません。おわり。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://r9y9.github.io/blog/2013/07/28/naive-bayes-formulation/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;導出編→Naive Bayesの復習（導出編）&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.slideshare.net/shima__shima/python-13349162&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;機械学習のPythonとの出会い（１）：単純ベイズ基礎編 - slideshare&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Multinomial distributionとCategorical distributionの違い</title>
      <link>https://r9y9.github.io/blog/2013/07/31/multinomial-categorical-diff/</link>
      <pubDate>Wed, 31 Jul 2013 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2013/07/31/multinomial-categorical-diff/</guid>
      <description>&lt;p&gt;些細な違いなんだけど調べたのでメモ。Multinomial distributionは多項分布のこと。Categorical distributionは、一般的な日本語表現が見つからなかった（なのでタイトルは英語）。打つのが大変なので、以下カテゴリカル分布と書く。&lt;/p&gt;
&lt;p&gt;結論としては、多項分布のn=1の特殊な場合がカテゴリカル分布ですよってこと。以下少しまとめる。&lt;/p&gt;
&lt;p&gt;分布を仮定する離散変数をカテゴリと呼ぶとして、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;多項分布は、n回試行したときに各カテゴリが何回出るかを表す確率分布&lt;/li&gt;
&lt;li&gt;多項分布は、二項分布を多カテゴリに一般化したもの&lt;/li&gt;
&lt;li&gt;カテゴリカル分布は、多項分布のn=1の場合に相当する&lt;/li&gt;
&lt;li&gt;カテゴリカル分布は、ベルヌーイ分布を多カテゴリに一般化したもの&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以上&lt;/p&gt;
&lt;p&gt;nokunoさんによるこの記事→ &lt;a href=&#34;http://d.hatena.ne.jp/nokuno/20111006/1317853653&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;多項分布の最尤推定&lt;/a&gt; は、多項分布というよりカテゴリカル分布の話。本文には書いてあるけどね。あと最尤推定の結果はどちらにしろ同じなんだけどね&lt;/p&gt;
&lt;h2 id=&#34;導出メモ&#34;&gt;導出メモ&lt;/h2&gt;
&lt;p&gt;一応最尤推定をやってみる。前回のナイーブベイズのメモの時は省略したので。入力の変数を $ Y = {y_n}_{n=1}^{N} $ とする。&lt;/p&gt;
&lt;h3 id=&#34;カテゴリカル分布&#34;&gt;カテゴリカル分布&lt;/h3&gt;
&lt;div&gt;
\begin{align}
p(l) = \pi_{l}, \hspace{2mm} \sum_{l=1}^{L}\pi_{l} = 1
\end{align}
&lt;/div&gt;
&lt;p&gt;ここで、$\pi_{l}$がパラメータ、lはカテゴリの番号&lt;/p&gt;
&lt;h3 id=&#34;最尤推定&#34;&gt;最尤推定&lt;/h3&gt;
&lt;p&gt;尤度関数を立てて、最大化することでパラメータを求める。各データは独立に生起すると仮定すると、尤度関数は以下のようになる。&lt;/p&gt;
&lt;div&gt;
\begin{align}
L(Y; \theta) = \prod_{n=1}^{N} \pi_{y_{n}}
\end{align}
&lt;/div&gt;
&lt;p&gt;$\theta$はパラメータの集合ということで。&lt;/p&gt;
&lt;p&gt;ラベルlの出現回数を$N_{l} = \sum_{n=1}^{N} \delta (y_{n} = l)$とすると、次のように書き直せる。&lt;/p&gt;
&lt;div&gt;
\begin{align}
L(Y; \theta) = \prod_{l=1}^{L}\pi_{l}^{N_{l}}
\end{align}
&lt;/div&gt;
&lt;p&gt;よって、対数尤度は以下のようになる。&lt;/p&gt;
&lt;div&gt;
\begin{align}
\log L(Y; \theta) = \sum_{l=1}^{L} N_{l}\log \pi_{l}
\end{align}
&lt;/div&gt;
&lt;h3 id=&#34;ラグランジュの未定乗数法で解く&#34;&gt;ラグランジュの未定乗数法で解く&lt;/h3&gt;
&lt;p&gt;nokunoさんの記事の通りだけど、一応手でも解いたのでメモ&lt;/p&gt;
&lt;div&gt;
\begin{align}
G = \sum_{l=1}^{L} N_{l}\log \pi_{l} + \lambda \Bigl[ \sum_{l=1}^{L} \pi_{l} -1) \Bigr]
\end{align}
&lt;/div&gt;
として、
&lt;div&gt;
\begin{align}
\frac{\partial G}{\partial \pi_{l}} = \frac{N_{l}}{\pi_{l}} + \lambda  =0
\end{align}
&lt;/div&gt;
&lt;p&gt;よって、&lt;/p&gt;
&lt;div&gt;
\begin{align}
\pi_{l} = -\frac{N_{l}}{\lambda}
\end{align}
&lt;/div&gt;
&lt;p&gt;ここで、以下の制約条件に代入すると、&lt;/p&gt;
&lt;div&gt;
\begin{align}
\sum_{l=1}^{L} \pi_{l} = 1
\end{align}
&lt;/div&gt;
&lt;p&gt;$\lambda = -N$となることがわかるので、求めたかったパラメータは以下のようになる&lt;/p&gt;
&lt;div&gt;
\begin{align}
\pi_{l} = \frac{N_{l}}{N}
\end{align}
&lt;/div&gt;
&lt;p&gt;カテゴリの頻度を計算するだけ、カンタン！！&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Categorical_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Categorical distribution - Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Multinomial_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Multinomial distribution - Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://d.hatena.ne.jp/nokuno/20111006/1317853653&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;多項分布の最尤推定 - nokunoの日記&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://d.hatena.ne.jp/sleepy_yoshi/20111107/p1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;多項分布の最尤推定とMAP推定 - 睡眠時間？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://lef-t.blogspot.jp/2013/02/categorical-distribution-wikipedia-free.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Categorical distribution - Researcher&amp;rsquo;s Eye&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Naive Bayesの復習（導出編）</title>
      <link>https://r9y9.github.io/blog/2013/07/28/naive-bayes-formulation/</link>
      <pubDate>Sun, 28 Jul 2013 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2013/07/28/naive-bayes-formulation/</guid>
      <description>&lt;p&gt;すぐ忘れるのでメモ。ナイーブベイズの学習アルゴリズムの導出とか、そもそもナイーブベイズが定番過ぎて意外とやったことなかった気もするので、復習がてらやってみた。&lt;/p&gt;
&lt;p&gt;ちょっと修正 2013/07/30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ナイーブベイズについて整理&lt;/li&gt;
&lt;li&gt;学習アルゴリズムの導出&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;naive-bayes-ナイーブベイズ&#34;&gt;Naive bayes （ナイーブベイズ）&lt;/h2&gt;
&lt;p&gt;スパムフィルタで使われたことで有名な確率モデルで、シンプルだけどそこそこ実用的なのが良い所。Naive bayesという名前は、特徴ベクトル間に条件付き独立性を仮定してることにある（実際は相関あることが多いけど、まぁ簡単のためって感じ）。具体的に例を挙げて言うと、例えば文書分類タスクの場合、各単語は独立に生起するという仮定を置くことに相当する。&lt;/p&gt;
&lt;p&gt;まずはモデルを書き下す。入力データを$\mathbf{x}$（D次元）、ラベルを$y$（離散値）とすると、ナイーブベイズでは以下のように同時確率をモデル化する。&lt;/p&gt;
&lt;div&gt;
\begin{align}
p(\mathbf{x}, y) &amp;= p(y)p(\mathbf{x}|y)\\
&amp;= p(y)p(x_{1}, x_{2}, \dots, x_{D}|y)\\
&amp;= p(y)\prod_{d=1}^{D} p(x_{d}|y)
\end{align}
&lt;/div&gt;
&lt;p&gt;カンタン。基本的にdは次元に対するインデックス、nはデータに対するインデックスとして書く。&lt;/p&gt;
&lt;p&gt;ポイントは特徴ベクトル間に条件付き独立性の仮定を置いていること（二度目）で、それによってパラメータの数が少なくて済む。&lt;/p&gt;
&lt;h2 id=&#34;分類&#34;&gt;分類&lt;/h2&gt;
&lt;p&gt;一番確率の高いラベルを選べばいい。数式で書くと以下のようになる。&lt;/p&gt;
&lt;div&gt;
\begin{align}
\hat{y} &amp;= \argmax_{y} [p(y|\mathbf{x})]\\
 &amp;= \argmax_{y} [p(\mathbf{x}, y)]\\
 &amp;= \argmax_{y} \Bigl[ p(y)\prod_{d=1}^{D} p(x_{d}|y)\Bigr]
\end{align}
&lt;/div&gt;
&lt;p&gt;argmaxを取る上では、$y$に依存しない項は無視していいので、事後確率の最大化は、同時確率の最大化に等しくなる。&lt;/p&gt;
&lt;h2 id=&#34;学習アルゴリズムの導出&#34;&gt;学習アルゴリズムの導出&lt;/h2&gt;
&lt;p&gt;ここからが本番。学習データを$X = {\mathbf{x}_{n}}_{n=1}^{N}$、対応する正解ラベルを$Y = {y_n}_{n=1}^{N} $として、最尤推定により学習アルゴリズムを導出する。実際はMAP推定をすることが多いけど、今回は省略。拡張は簡単。&lt;/p&gt;
&lt;h3 id=&#34;尤度関数&#34;&gt;尤度関数&lt;/h3&gt;
&lt;p&gt;各サンプルが独立に生起したと仮定すると、尤度関数は以下のように書ける。&lt;/p&gt;
&lt;div&gt;
\begin{align}
L(X,Y; \mathbf{\theta}) &amp;= \prod_{n=1}^{N}p(y_{n})p(\mathbf{x_{n}}|y_{n})\\
&amp;= \prod_{n=1}^{N} \Bigl[ p(y_{n})\prod_{d=1}^{D}p(x_{nd}|y_{n})\Bigr]
\end{align}
&lt;/div&gt;
&lt;p&gt;対数を取って、&lt;/p&gt;
&lt;div&gt;
\begin{align}
\log L(X,Y; \mathbf{\theta}) =  \sum_{n=1}^{N}\Bigl[\log p(y_{n}) + \sum_{d=1}^{D}\log p(x_{nd}|y_{n})\Bigr]
\end{align}
&lt;/div&gt;
&lt;p&gt;学習アルゴリズムは、この関数の最大化として導くことができる。&lt;/p&gt;
&lt;h3 id=&#34;ところで&#34;&gt;ところで&lt;/h3&gt;
&lt;p&gt;特徴ベクトルにどのような分布を仮定するかでアルゴリズムが少し変わるので、今回は以下の二つをやってみる。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ベルヌーイ分布&lt;/li&gt;
&lt;li&gt;正規分布&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;前者は、binary featureを使う場合で、後者は、continuous featureを使う場合を想定してる。画像のピクセル値とか連続値を扱いたい場合は、正規分布が無難。その他、多項分布を使うこともあるけど、ベルヌーイ分布の場合とほとんど一緒なので今回は省略&lt;/p&gt;
&lt;p&gt;ラベルに対する事前分布は、ラベルが離散値なので多項分布（間違ってた）categorical distributionとする。日本語でなんて言えばいいのか…&lt;a href=&#34;http://en.wikipedia.org/wiki/Categorical_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;wikipedia&lt;/a&gt; 参考&lt;/p&gt;
&lt;h2 id=&#34;bernoulli-naive-bayes&#34;&gt;Bernoulli naive bayes&lt;/h2&gt;
&lt;p&gt;特徴ベクトルにベルヌーイ分布を仮定する場合。0 or 1のbinary featureを使う場合にはこれでおｋ．ベルヌーイ分布は以下&lt;/p&gt;
&lt;div&gt;
\begin{align}
p(x;q) = q^{x}(1-q)^{1-x}
\end{align}
&lt;/div&gt;
&lt;p&gt;特徴ベクトルに対するパラメータは、ラベル数×特徴ベクトルの次元数（L×D）個ある。対数尤度関数（Gとする）は、以下のように書ける。&lt;/p&gt;
&lt;div&gt;
\begin{align}
G &amp;=  \sum_{n=1}^{N}\Bigl[ \log \pi_{y_{n}} \notag \\
 &amp;+ \sum_{d=1}^{D} \bigl[ x_{nd} \log q_{y_{n}d} + (1-x_{nd}) \log (1-q_{y_{n}d}) \bigr] \Bigr]
\end{align}
&lt;/div&gt;
&lt;p&gt;ここで、$\pi_{y_{n}}$ はcategorical distributionのパラメータ。&lt;/p&gt;
&lt;h3 id=&#34;微分方程式を解く&#34;&gt;微分方程式を解く&lt;/h3&gt;
&lt;p&gt;あとは微分してゼロ。ラベルに対するインデックスをl 、学習データ中のラベルlが出現する回数を$N_{l} = \sum_{n=1}^{N} \delta(y_{n}= l)$、さらにその中で$x_{nd}=1 $となる回数を$N_{ld} = \sum_{n=1}^{N} \delta(y_{n}= l) \cdot x_{nd} $とすると、&lt;/p&gt;
&lt;div&gt;
\begin{align}
\frac{\partial G}{\partial q_{ld}} &amp;= \frac{N_{ld}}{q_{ld}} - \frac{N_{l} - N_{ld}}{1-q_{ld}}  = 0
\end{align}
&lt;/div&gt;
&lt;p&gt;よって、&lt;/p&gt;
&lt;div&gt;
\begin{align}
q_{ld} = \frac{N_{ld}}{N_{l}} \label{eq:naive1}
\end{align}
&lt;/div&gt;
&lt;p&gt;できました。厳密に数式で書こうとするとめんどくさい。日本語で書くと、&lt;/p&gt;
&lt;div&gt;
\begin{align}
パラメータ = \frac{特徴ベクトルの出現回数}{ラベルの出現回数}
\end{align}
&lt;/div&gt;
&lt;p&gt;って感じでしょうか。&lt;/p&gt;
&lt;p&gt;categoricalのパラメータについては、めんどくさくなってきたのでやらないけど、もう直感的に以下。ラグランジュの未定定数法でおｋ&lt;/p&gt;
&lt;div&gt;
\begin{align}
\pi_{l} = \frac{N_{l}}{N} \label{eq:naive2}
\end{align}
&lt;/div&gt;
&lt;p&gt;学習は、式 ($\ref{eq:naive1}$)、($\ref{eq:naive2}$) を計算すればおｋ．やっと終わった。。。長かった。&lt;/p&gt;
&lt;h2 id=&#34;gaussian-naive-bayes&#34;&gt;Gaussian naive bayes&lt;/h2&gt;
&lt;p&gt;次。$x$が連続変数で、その分布に正規分布（Gaussian）を仮定する場合。まず、正規分布は以下のとおり。&lt;/p&gt;
&lt;div&gt;
\begin{align}
p(x; \mu, \sigma^{2}) = \frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\Bigl\{-\frac{(x-\mu)^{2}}{2\sigma^{2}}\Bigr\}
\end{align}
&lt;/div&gt;
&lt;p&gt;正規分布を使う場合、特徴ベクトルに対するパラメータは、ラベル数×特徴ベクトルの次元数×2個ある。×2となっているのは、平均と分散の分。対数尤度関数は、以下のようになる&lt;/p&gt;
&lt;div&gt;
\begin{align}
G &amp;=  \sum_{n=1}^{N}\Bigl[ \log \pi_{y_{n}} \notag \\
 &amp;+ \sum_{d=1}^{D} \bigl[ -\frac{1}{2}\log 2\pi - \log\sigma_{y_{n}d} -  \frac{(x_{nd}-\mu_{y_{n}d})^2}{2\sigma_{y_{n}d}} \bigr] \Bigr]
\end{align}
&lt;/div&gt;
&lt;h3 id=&#34;微分方程式を解く-1&#34;&gt;微分方程式を解く&lt;/h3&gt;
&lt;p&gt;計算は省略するけど、偏微分してゼロと置けば、結果は以下のようになる。式が若干煩雑だけど、基本的には正規分布の最尤推定をしてるだけ。&lt;/p&gt;
&lt;div&gt;
\begin{align}
\mu_{ld} = \frac{1}{N_{l}} \sum_{n=1}^{N} x_{nd} \cdot \delta(y_{n} =l) = \frac{N_{ld}}{N_{l}} \label{eq:naive3}
\end{align}
&lt;/div&gt;
&lt;div&gt;
\begin{align}
\sigma_{ld} = \frac{1}{N_{l}} \sum_{n=1}^{N} (x_{nd}-\mu_{ld})^{2} \cdot \delta (y_{n}= l) \label{eq:naive4}
\end{align}
&lt;/div&gt;
&lt;p&gt;学習では、式 ($\ref{eq:naive2}$)、($\ref{eq:naive3}$)、($\ref{eq:naive4}$)を計算すればおｋ．式 ($\ref{eq:naive3}$)は式 ($\ref{eq:naive1}$)と一緒なんだけど、正規分布の場合はxが連続値なので注意。分散が特徴ベクトルの次元によらず一定とすれば、パラメータの数をぐっと減らすこともできる。&lt;/p&gt;
&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;
&lt;p&gt;これで終わり。予想以上に書くのに時間かかった…。今日logistic regressionを見直してて、ふとnaive bayesやったことないなーと思って、まぁ試すだけならscipy使えば一瞬なんだろうけどちょっと導出までやってみようと思った。&lt;/p&gt;
&lt;p&gt;実装編→&lt;a href=&#34;http://r9y9.github.io/blog/2013/08/06/naive-bayes-mnist/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Naive Bayesの復習（実装編）: MNISTを使って手書き数字認識&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://d.hatena.ne.jp/saket/20130212/1360678478&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;scikit.learn手法徹底比較！ ナイーブベイズ編Add Star - Risky Dune&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cs.cmu.edu/~epxing/Class/10701-10s/Lecture/lecture5.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gaussian Naïve Bayes, andLogistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://aidiary.hatenablog.com/entry/20100613/1276389337&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ナイーブベイズを用いたテキスト分類 - 人工知能に関する断創録&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>NMFアルゴリズムの導出（ユークリッド距離版）</title>
      <link>https://r9y9.github.io/blog/2013/07/27/nmf-euclid/</link>
      <pubDate>Sat, 27 Jul 2013 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2013/07/27/nmf-euclid/</guid>
      <description>&lt;h2 id=&#34;はじめに&#34;&gt;はじめに&lt;/h2&gt;
&lt;p&gt;シングルトラックにミックスダウンされた音楽から、その構成する要素（例えば、楽器とか）を分離したいと思うことがある。
音源分離と言えば、最近はNon-negative Matrix Factorization (非負値行列因子分解; NMF) が有名。
実装は非常に簡単だけど、実際にやってみるとどの程度の音源分離性能が出るのか気になったので、やってみる。&lt;/p&gt;
&lt;p&gt;と思ったけど、まずNMFについて整理してたら長くなったので、実装は今度にして、まずアルゴリズムを導出してみる。&lt;/p&gt;
&lt;h2 id=&#34;20141019-追記&#34;&gt;2014/10/19 追記&lt;/h2&gt;
&lt;p&gt;実装しました&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/julia-nmf-ss-toy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/julia-nmf-ss-toy&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;nmfの問題設定&#34;&gt;NMFの問題設定&lt;/h2&gt;
&lt;p&gt;NMFとは、与えられた行列を非負という制約の元で因子分解する方法のこと。
音楽の場合、対象はスペクトログラムで、式で書くとわかりやすい。
スペクトログラムを $\mathbf{Y}: [\Omega \times T] $
とすると、&lt;/p&gt;
&lt;div&gt;
\begin{align}
\mathbf{Y} \simeq \mathbf{H} \mathbf{U}
\end{align}
&lt;/div&gt;
&lt;p&gt;となる、$\mathbf{H}: [\Omega \times K]、\mathbf{U}: [K \times T]$ を求めるのがNMFの問題。
ここで、Hが基底、Uがアクティビティ行列に相当する。
NMFは、元の行列Yと分解後の行列の距離の最小化問題として定式化できる。&lt;/p&gt;
&lt;div&gt;
\begin{align}
\mathbf{H}, \mathbf{U} = \mathop{\rm arg~min}\limits_{\mathbf{H}, \mathbf{U}} D (\mathbf{Y}|\mathbf{H}\mathbf{U}), \hspace{3mm} {\rm subect\ to} \hspace{3mm} H_{\omega,k}, U_{k, t} &gt; 0
\end{align}
&lt;/div&gt;
&lt;p&gt;すごくシンプル。Dは距離関数で色んなものがある。ユークリッド距離、KLダイバージェンス、板倉斎藤距離、βダイバージェンスとか。&lt;/p&gt;
&lt;h2 id=&#34;ユークリッド距離の最小化&#34;&gt;ユークリッド距離の最小化&lt;/h2&gt;
&lt;p&gt;ここではユークリッド距離（Frobeniusノルムともいう）として、二乗誤差最小化問題を解くことにする。
一番簡単なので。最小化すべき目的関数は次のようになる。&lt;/p&gt;
&lt;div&gt;
\begin{align}
D (\mathbf{Y}|\mathbf{H}\mathbf{U}) =&amp; || \mathbf{Y}-\mathbf{HU}||_{F} \\
=&amp; \sum_{\omega, k}|Y_{\omega,t} - \sum_{k}H_{\omega, k}U_{k, t}|^{2}
\end{align}
&lt;/div&gt;
&lt;p&gt;行列同士の二乗誤差の最小化は、要素毎の二乗誤差の和の最小化ということですね。展開すると、次のようになる。&lt;/p&gt;
&lt;div&gt;
\begin{align}
\sum_{\omega, k}|Y_{\omega,t} - \sum_{k}H_{\omega, k}U_{k, t}|^{2}
= \sum_{\omega, t}(|Y_{\omega, t}|^2 -2Y_{\omega, t} \sum_{k}H_{\omega, k}U_{k, t} + |\sum_{k}H_{\omega, k}U_{k, t}|^2)
\end{align}
&lt;/div&gt;
&lt;p&gt;微分してゼロ！としたいところだけど、3つ目の項を見ると、絶対値の中に和が入っているので、そうはいかない。
なので、補助関数法を使う。
基本的なアイデアは、目的関数の直接の最適化が難しい場合には、上界関数を立てることで間接的に最小化するということ。&lt;/p&gt;
&lt;p&gt;3項目に対してイェンセンの不等式を適応すると、&lt;/p&gt;
&lt;div&gt;
\begin{align}
|\sum_{k}H_{\omega,k}U_{k,t}|^{2} \le \sum_{k} \frac{H_{\omega,k}^{2}U_{k, t}^{2}}{\lambda_{k, \omega, t}}
\end{align}
&lt;/div&gt;
&lt;p&gt;これで、右辺は $ H_{\omega,k}, U_{k, t} $ について二次関数になったので、微分できてはっぴー。
上の不等式を使えば、実際に最小化する目的関数は、次のようになる。&lt;/p&gt;
&lt;div&gt;
\begin{align}
G := \sum_{\omega, t}(|Y_{\omega, t}|^2 -2Y_{\omega, t} \sum_{k}H_{\omega, k}U_{k, t} + \sum_{k} \frac{H_{\omega,k}^{2}U_{k, t}^{2}}{\lambda_{k, \omega, t}})
\end{align}
&lt;/div&gt;
&lt;p&gt;Gを最小化すれば、間接的に元の目的関数も小さくなる。&lt;/p&gt;
&lt;h2 id=&#34;更新式の導出&#34;&gt;更新式の導出&lt;/h2&gt;
&lt;p&gt;あとは更新式を導出するだけ。
まず、目的関数を上から押さえるイメージで、イェンセンの不等式の等号条件から補助変数の更新式を求める。
この場合、kに関して和が1になることに注意して、&lt;/p&gt;
&lt;div&gt;
\begin{align}
\lambda_{k,\omega,t} = \frac{H_{\omega, k}U_{k, t}}{\sum_{k&#39;}H_{\omega, k&#39;}U_{k&#39;, t}}
\end{align}
&lt;/div&gt;
&lt;p&gt;次に、目的関数Gを $H_{\omega,k}, U_{k,t} $で偏微分する。&lt;/p&gt;
&lt;div&gt;
\begin{align}
\frac{\partial G}{\partial H_{\omega,k}} &amp;= \sum_{t} (-2 Y_{\omega,t}U_{k,t} + 2 \frac{H_{\omega, k}U_{k, t}^2}{\lambda_{k,\omega,t}}) &amp;= 0\\
\frac{\partial G}{\partial U_{k, t}} &amp;= \sum_{\omega} (-2 Y_{\omega,t}H_{\omega,k} + 2 \frac{H_{\omega, k}^2U_{k, t}}{\lambda_{k,\omega,t}}) &amp;= 0
\end{align}
&lt;/div&gt;
&lt;p&gt;少し変形すれば、以下の式を得る。&lt;/p&gt;
&lt;div&gt;
\begin{align}
H_{\omega,k} = \frac{\sum_{t}Y_{\omega,t}U_{k,t}}{\sum_{t}\frac{U_{k, t}^2}{\lambda_{k,\omega,t}}}, \hspace{3mm}
U_{k,t} = \frac{\sum_{\omega}Y_{\omega,t}H_{\omega,k}}{\sum_{\omega}\frac{H_{\omega, k}^2}{\lambda_{k,\omega,t}}}
\end{align}
&lt;/div&gt;
&lt;p&gt;補助変数を代入すれば、出来上がり。&lt;/p&gt;
&lt;div&gt;
\begin{align}
H_{\omega,k} = H_{\omega,k} \frac{\sum_{t}Y_{\omega,t}U_{k,t}}{\sum_{t}U_{k, t}\sum_{k&#39;}H_{\omega, k&#39;}U_{k&#39;, t}}, \hspace{3mm}
U_{k,t} = U_{k,t}\frac{\sum_{\omega}Y_{\omega,t}H_{\omega,k}}{\sum_{\omega}H_{\omega, k}\sum_{k&#39;}H_{\omega, k&#39;}U_{k&#39;, t}}
\end{align}
&lt;/div&gt;
&lt;h2 id=&#34;行列表記で&#34;&gt;行列表記で&lt;/h2&gt;
&lt;p&gt;これで終わり…ではなく、もう少しスマートに書きたい。
ここで、少し実装を意識して行列表記を使って書きなおす。
行列の積は、AB（A: [m x n] 行列、B: [n x l] 行列）のようにAの列数とBの行数が等しくなることに注意して、
ほんの少し変形すれば最終的には次のように書ける。&lt;/p&gt;
&lt;div&gt;
\begin{align}
H_{\omega,k} &amp;= H_{\omega,k} \frac{[\mathbf{Y}\mathbf{U}^{\mathrm{T}}]_{\omega,k}}{[\mathbf{H}\mathbf{U}\mathbf{U}^{\mathrm{T}}]_{\omega,k}}, \\
U_{k,t} &amp;= U_{k,t}\frac{[\mathbf{H}^{\mathrm{T}}\mathbf{Y}]_{k, t}}{[\mathbf{H}^{\mathrm{T}}\mathbf{H}\mathbf{U}]_{k,t}}
\end{align}
&lt;/div&gt;
&lt;p&gt;乗法更新式というやつですね。
元々の行列の要素が非負なら、掛けても非負のままですよってこと。
NMFのアルゴリズムは、この更新式を目的関数が収束するまで計算するだけ、簡単。Pythonなら数行で書ける。&lt;/p&gt;
&lt;h2 id=&#34;メモ&#34;&gt;メモ&lt;/h2&gt;
&lt;p&gt;自分で導出していて思ったことをメモっておこうと思う。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;更新式は、行列の要素毎に独立して求められるんだなぁということ。
&lt;ul&gt;
&lt;li&gt;まぁ要素毎に偏微分して等式立ててるからそうなんだけど。更新の順番によって、収束する値、速度が変わるといったことはないんだろうか。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;行列演算とスカラー演算が同じ式に同時に含まれていることがあるので注意。例えば、最終的な更新式の割り算は、要素毎のスカラー演算で、行列演算ではない。&lt;/li&gt;
&lt;li&gt;何かいっぱいシグマがあるけど、めげない。計算ミスしやすい、つらい。&lt;/li&gt;
&lt;li&gt;NMFという名前から行列操作を意識してしまうけど、更新式の導出の過程に行列の微分とか出てこない。更新式の導出は、行列の要素個々に対して行うイメージ。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NMFなんて簡単、と言われますが（要出典）、実際にやってみると結構めんどくさいなー、と思いました（小並感&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://r9y9.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
