<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning | LESS IS MORE</title>
    <link>https://r9y9.github.io/tag/deep-learning/</link>
      <atom:link href="https://r9y9.github.io/tag/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Deep Learning</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Copyright © Ryuichi YAMAMOTO All rights reserved.</copyright><lastBuildDate>Sun, 16 Oct 2022 14:50:26 +0900</lastBuildDate>
    <image>
      <url>https://r9y9.github.io/media/icon_hu71488a41e9448d472219f1cc71ecc0ad_259818_512x512_fill_lanczos_center_3.png</url>
      <title>Deep Learning</title>
      <link>https://r9y9.github.io/tag/deep-learning/</link>
    </image>
    
    <item>
      <title>NNSVS: Neural Network Based Singing Voice Synthesis Toolkit</title>
      <link>https://r9y9.github.io/projects/nnsvs/</link>
      <pubDate>Sun, 16 Oct 2022 14:50:26 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/nnsvs/</guid>
      <description>&lt;p&gt;Submitted to &lt;a href=&#34;https://2023.ieeeicassp.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ICASSP 2023&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#authors&#34;&gt;Authors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#abstract&#34;&gt;Abstract&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#systems&#34;&gt;Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#samples&#34;&gt;Samples&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#svs&#34;&gt;SVS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#as&#34;&gt;A/S&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;authors&#34;&gt;Authors&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Ryuichi Yamamoto (LINE Corp., Nagoya University)&lt;/li&gt;
&lt;li&gt;Reo Yoneyama (Nagoya University)&lt;/li&gt;
&lt;li&gt;Tomoki Toda (Nagoya University)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;This paper describes the design of NNSVS, open-source software for neural network-based singing voice synthesis research.
NNSVS is inspired by Sinsy, one of the open-source pioneers in singing voice synthesis research, and provides many new features such as multi-stream autoregressive models, autoregressive fundamental frequency models, and neural vocoders.
Furthermore, NNSVS provides extensive documentation and scripts to build complete singing voice synthesis systems.
Experimental results demonstrate that our best system significantly outperforms our reproduction of Sinsy and other baseline systems.
The toolkit is available on GitHub at &lt;a href=&#34;https://github.com/nnsvs/nnsvs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/nnsvs/nnsvs&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;systems&#34;&gt;Systems&lt;/h2&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h2 id=&#34;samples&#34;&gt;Samples&lt;/h2&gt;
&lt;h3 id=&#34;svs&#34;&gt;SVS&lt;/h3&gt;
&lt;p&gt;Sample 1: 1st color&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;NNSVS pre-trained&lt;/th&gt;&lt;th&gt;NNSVS-WORLD v1&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test01]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test01]-NNSVS pre-trained.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test01]-NNSVS-WORLD v1.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;NNSVS-WORLD v2&lt;/th&gt;&lt;th&gt;NNSVS-WORLD v3&lt;/th&gt;&lt;th&gt;NNSVS-WORLD v4&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test01]-NNSVS-WORLD v2.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test01]-NNSVS-WORLD v3.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test01]-NNSVS-WORLD v4.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;NNSVS-Mel v1&lt;/th&gt;&lt;th&gt;NNSVS-Mel v2&lt;/th&gt;&lt;th&gt;NNSVS-Mel v3&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test01]-NNSVS-Mel v1.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test01]-NNSVS-Mel v2.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test01]-NNSVS-Mel v3.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Sinsy&lt;/th&gt;&lt;th&gt;Sinsy (pitch corr)&lt;/th&gt;&lt;th&gt;Sinsy (vibrato modeling)&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test01]-Sinsy.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test01]-Sinsy (pitch corr).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test01]-Sinsy (vibrato modeling).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Muskits RNN&lt;/th&gt;&lt;th&gt;DiffSinger&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test01]-Muskits RNN.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test01]-DiffSinger.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2: ARROW&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;NNSVS pre-trained&lt;/th&gt;&lt;th&gt;NNSVS-WORLD v1&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test02]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test02]-NNSVS pre-trained.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test02]-NNSVS-WORLD v1.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;NNSVS-WORLD v2&lt;/th&gt;&lt;th&gt;NNSVS-WORLD v3&lt;/th&gt;&lt;th&gt;NNSVS-WORLD v4&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test02]-NNSVS-WORLD v2.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test02]-NNSVS-WORLD v3.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test02]-NNSVS-WORLD v4.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;NNSVS-Mel v1&lt;/th&gt;&lt;th&gt;NNSVS-Mel v2&lt;/th&gt;&lt;th&gt;NNSVS-Mel v3&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test02]-NNSVS-Mel v1.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test02]-NNSVS-Mel v2.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test02]-NNSVS-Mel v3.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Sinsy&lt;/th&gt;&lt;th&gt;Sinsy (pitch corr)&lt;/th&gt;&lt;th&gt;Sinsy (vibrato modeling)&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test02]-Sinsy (vibrato modeling).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test02]-Sinsy (pitch corr).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test02]-Sinsy (vibrato modeling).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Muskits RNN&lt;/th&gt;&lt;th&gt;DiffSinger&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test02]-Muskits RNN.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test02]-DiffSinger.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3: BC&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;NNSVS pre-trained&lt;/th&gt;&lt;th&gt;NNSVS-WORLD v1&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test03]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test03]-NNSVS pre-trained.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test03]-NNSVS-WORLD v1.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;NNSVS-WORLD v2&lt;/th&gt;&lt;th&gt;NNSVS-WORLD v3&lt;/th&gt;&lt;th&gt;NNSVS-WORLD v4&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test03]-NNSVS-WORLD v2.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test03]-NNSVS-WORLD v3.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test03]-NNSVS-WORLD v4.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;NNSVS-Mel v1&lt;/th&gt;&lt;th&gt;NNSVS-Mel v2&lt;/th&gt;&lt;th&gt;NNSVS-Mel v3&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test03]-NNSVS-Mel v1.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test03]-NNSVS-Mel v2.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test03]-NNSVS-Mel v3.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Sinsy&lt;/th&gt;&lt;th&gt;Sinsy (pitch corr)&lt;/th&gt;&lt;th&gt;Sinsy (vibrato modeling)&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test03]-Sinsy.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test03]-Sinsy (pitch corr).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test03]-Sinsy (vibrato modeling).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Muskits RNN&lt;/th&gt;&lt;th&gt;DiffSinger&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test03]-Muskits RNN.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test03]-DiffSinger.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 4: Close to you&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;NNSVS pre-trained&lt;/th&gt;&lt;th&gt;NNSVS-WORLD v1&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test04]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test04]-NNSVS pre-trained.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test04]-NNSVS-WORLD v1.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;NNSVS-WORLD v2&lt;/th&gt;&lt;th&gt;NNSVS-WORLD v3&lt;/th&gt;&lt;th&gt;NNSVS-WORLD v4&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test04]-NNSVS-WORLD v2.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test04]-NNSVS-WORLD v3.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test04]-NNSVS-WORLD v4.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;NNSVS-Mel v1&lt;/th&gt;&lt;th&gt;NNSVS-Mel v2&lt;/th&gt;&lt;th&gt;NNSVS-Mel v3&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test04]-NNSVS-Mel v1.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test04]-NNSVS-Mel v2.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test04]-NNSVS-Mel v3.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Sinsy&lt;/th&gt;&lt;th&gt;Sinsy (pitch corr)&lt;/th&gt;&lt;th&gt;Sinsy (vibrato modeling)&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test04]-Sinsy (vibrato modeling).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test04]-Sinsy (pitch corr).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test04]-Sinsy (vibrato modeling).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Muskits RNN&lt;/th&gt;&lt;th&gt;DiffSinger&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test04]-Muskits RNN.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test04]-DiffSinger.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 5: ERROR&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;NNSVS pre-trained&lt;/th&gt;&lt;th&gt;NNSVS-WORLD v1&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test05]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test05]-NNSVS pre-trained.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test05]-NNSVS-WORLD v1.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;NNSVS-WORLD v2&lt;/th&gt;&lt;th&gt;NNSVS-WORLD v3&lt;/th&gt;&lt;th&gt;NNSVS-WORLD v4&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test05]-NNSVS-WORLD v2.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test05]-NNSVS-WORLD v3.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test05]-NNSVS-WORLD v4.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;NNSVS-Mel v1&lt;/th&gt;&lt;th&gt;NNSVS-Mel v2&lt;/th&gt;&lt;th&gt;NNSVS-Mel v3&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test05]-NNSVS-Mel v1.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test05]-NNSVS-Mel v2.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test05]-NNSVS-Mel v3.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Sinsy&lt;/th&gt;&lt;th&gt;Sinsy (pitch corr)&lt;/th&gt;&lt;th&gt;Sinsy (vibrato modeling)&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test05]-Sinsy (vibrato modeling).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test05]-Sinsy (pitch corr).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test05]-Sinsy (vibrato modeling).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Muskits RNN&lt;/th&gt;&lt;th&gt;DiffSinger&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test05]-Muskits RNN.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/SVS/[Test05]-DiffSinger.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&#34;as&#34;&gt;A/S&lt;/h3&gt;
&lt;p&gt;Sample 1: 1st color&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;hn-HiFi-GAN&lt;/th&gt;&lt;th&gt;hn-USFGAN-Mel&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/AnaSyn/[Test01]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/AnaSyn/[Test01]-hn-HiFi-GAN.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/AnaSyn/[Test01]-hn-USFGAN-Mel.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;hn-USFGAN-WORLD&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/AnaSyn/[Test01]-hn-USFGAN-WORLD.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2: ARROW&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;hn-HiFi-GAN&lt;/th&gt;&lt;th&gt;hn-USFGAN-Mel&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/AnaSyn/[Test02]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/AnaSyn/[Test02]-hn-HiFi-GAN.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/AnaSyn/[Test02]-hn-USFGAN-Mel.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;hn-USFGAN-WORLD&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/AnaSyn/[Test02]-hn-USFGAN-WORLD.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3: BC&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;hn-HiFi-GAN&lt;/th&gt;&lt;th&gt;hn-USFGAN-Mel&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/AnaSyn/[Test03]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/AnaSyn/[Test03]-hn-HiFi-GAN.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/AnaSyn/[Test03]-hn-USFGAN-Mel.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;hn-USFGAN-WORLD&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/AnaSyn/[Test03]-hn-USFGAN-WORLD.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 4: Close to you&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;hn-HiFi-GAN&lt;/th&gt;&lt;th&gt;hn-USFGAN-Mel&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/AnaSyn/[Test04]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/AnaSyn/[Test04]-hn-HiFi-GAN.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/AnaSyn/[Test04]-hn-USFGAN-Mel.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;hn-USFGAN-WORLD&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/AnaSyn/[Test04]-hn-USFGAN-WORLD.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 5: ERROR&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;hn-HiFi-GAN&lt;/th&gt;&lt;th&gt;hn-USFGAN-Mel&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/AnaSyn/[Test05]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/AnaSyn/[Test05]-hn-HiFi-GAN.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/AnaSyn/[Test05]-hn-USFGAN-Mel.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;hn-USFGAN-WORLD&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/202210_nnsvs/AnaSyn/[Test05]-hn-USFGAN-WORLD.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Period VITS: Variational Inference With Explicit Pitch Modeling For End-to-End Emotional Speech Synthesis</title>
      <link>https://r9y9.github.io/projects/period-vits/</link>
      <pubDate>Sun, 16 Oct 2022 14:47:59 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/period-vits/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DRSpeech: Degradation-Robust Text-to-Speech Synthesis with Frame-Level and Utterance-Level Acoustic Representation Learning</title>
      <link>https://r9y9.github.io/projects/drspeech/</link>
      <pubDate>Mon, 04 Apr 2022 12:11:07 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/drspeech/</guid>
      <description></description>
    </item>
    
    <item>
      <title>TTS-by-TTS 2: Data-selective Augmentation for Neural Speech Synthesis Using Ranking Support Vector Machine with Variational Autoencoder</title>
      <link>https://r9y9.github.io/projects/tts-by-tts2/</link>
      <pubDate>Mon, 04 Apr 2022 12:11:05 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/tts-by-tts2/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Cross-Speaker Emotion Transfer for Low-Resource Text-to-Speech Using Non-Parallel Voice Conversion with Pitch-Shift Data Augmentation</title>
      <link>https://r9y9.github.io/projects/vc-tts-ps/</link>
      <pubDate>Mon, 04 Apr 2022 12:11:04 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/vc-tts-ps/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Unified Accent Estimation Method Based on Multi-Task Learning for Japanese Text-to-Speech</title>
      <link>https://r9y9.github.io/projects/mtl_accent/</link>
      <pubDate>Mon, 04 Apr 2022 12:11:03 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/mtl_accent/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Language Model-Based Emotion Prediction Methods for Emotional Speech Synthesis Systems</title>
      <link>https://r9y9.github.io/projects/lmemotiontts/</link>
      <pubDate>Mon, 04 Apr 2022 12:11:01 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/lmemotiontts/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ESPnet2-TTS: Extending the Edge of TTS Research</title>
      <link>https://r9y9.github.io/projects/espnet2-tts/</link>
      <pubDate>Wed, 06 Oct 2021 20:42:37 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/espnet2-tts/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ttslearn: Library for Pythonで学ぶ音声合成 (Text-to-speech with Python)</title>
      <link>https://r9y9.github.io/projects/ttslearn/</link>
      <pubDate>Wed, 11 Aug 2021 16:27:22 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/ttslearn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Voicing-Aware Parallel WaveGAN for High-Quality Speech Synthesis</title>
      <link>https://r9y9.github.io/projects/va-pwg/</link>
      <pubDate>Fri, 30 Jul 2021 12:11:03 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/va-pwg/</guid>
      <description>&lt;p&gt;Submitted to &lt;a href=&#34;https://signalprocessingsociety.org/publications-resources/ieee-signal-processing-letters&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IEEE signal processing letters&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#authors&#34;&gt;Authors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#abstract&#34;&gt;Abstract&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tts-samples&#34;&gt;TTS samples&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#m1-male&#34;&gt;M1 (male)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#m2-male&#34;&gt;M2 (male)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#f1-female&#34;&gt;F1 (female)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#f2-female&#34;&gt;F2 (female)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#acknowledgements&#34;&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;authors&#34;&gt;Authors&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Ryuichi Yamamoto (LINE Corp.)&lt;/li&gt;
&lt;li&gt;Min-Jae Hwang (Search Solutions Inc.)&lt;/li&gt;
&lt;li&gt;Eunwoo Song (NAVER Corp.)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;This letter proposes a voicing-aware Parallel Wave- GAN (VA-PWG) vocoder for a neural text-to-speech (TTS) system. To generate a high-quality speech waveform, it is important to reflect the distinct characteristics of voiced and unvoiced speech signals well. However, it is difficult for the conventional PWG model to accurately represent this condition, since the single unified architectures of the generator and discriminator are insufficient to capture those characteristics. In the proposed method, both the generator and discriminator are divided into their subnetworks to individually model the voicing state-dependent characteristics of a speech signal. In particular, a VA-generator consisting of two sub-WaveNets generates the harmonic and noise components of a speech signal by inputting pitch-dependent sine wave and Gaussian noise sources, respectively. Likewise, a VA-discriminator consisting of two sub-discriminators learns the distinct characteristics of harmonic and noise components by feeding the voiced and unvoiced waveforms, respectively. Subjective evaluation results verified the effectiveness of the proposed VA-PWG vocoder by achieving a 4.25 mean opinion score from a speaker-independent training scenario that was 11% higher than that of a conventional PWG vocoder.&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/va-pwg.png&#34; width=&#34;80%&#34; /&gt;&lt;/div&gt;
&lt;h2 id=&#34;tts-samples&#34;&gt;TTS samples&lt;/h2&gt;
&lt;h3 id=&#34;m1-male&#34;&gt;M1 (male)&lt;/h3&gt;
&lt;p&gt;Sample 1: “鹿児島県で最大震度三を観測しています。”&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test01]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test01]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test01]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;VA-PWG-G&lt;/th&gt;&lt;th&gt;VA-PWG-D&lt;/th&gt;&lt;th&gt;VA-PWG-GD (proposed)&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test01]-S3-VA-PWG-G.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test01]-S4-VA-PWG-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test01]-S5-VA-PWG-GD (proposed).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2: “葉加瀬太郎の情熱大陸です。”&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test02]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test02]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test02]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;VA-PWG-G&lt;/th&gt;&lt;th&gt;VA-PWG-D&lt;/th&gt;&lt;th&gt;VA-PWG-GD (proposed)&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test02]-S3-VA-PWG-G.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test02]-S4-VA-PWG-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test02]-S5-VA-PWG-GD (proposed).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3: “それでうちの部は半分に減らされる。”&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test03]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test03]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test03]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;VA-PWG-G&lt;/th&gt;&lt;th&gt;VA-PWG-D&lt;/th&gt;&lt;th&gt;VA-PWG-GD (proposed)&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test03]-S3-VA-PWG-G.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test03]-S4-VA-PWG-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M1/TTS/[Test03]-S5-VA-PWG-GD (proposed).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&#34;m2-male&#34;&gt;M2 (male)&lt;/h3&gt;
&lt;p&gt;Sample 1: “ヨメの、レオンティーンさんですね。”&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test01]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test01]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test01]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;VA-PWG-G&lt;/th&gt;&lt;th&gt;VA-PWG-D&lt;/th&gt;&lt;th&gt;VA-PWG-GD (proposed)&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test01]-S3-VA-PWG-G.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test01]-S4-VA-PWG-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test01]-S5-VA-PWG-GD (proposed).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2: “御予約は、二泊三日ですね。”&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test02]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test02]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test02]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;VA-PWG-G&lt;/th&gt;&lt;th&gt;VA-PWG-D&lt;/th&gt;&lt;th&gt;VA-PWG-GD (proposed)&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test02]-S3-VA-PWG-G.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test02]-S4-VA-PWG-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test02]-S5-VA-PWG-GD (proposed).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3: “わたさちの、ローリーさんですね。”&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test03]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test03]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test03]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;VA-PWG-G&lt;/th&gt;&lt;th&gt;VA-PWG-D&lt;/th&gt;&lt;th&gt;VA-PWG-GD (proposed)&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test03]-S3-VA-PWG-G.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test03]-S4-VA-PWG-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/M2/TTS/[Test03]-S5-VA-PWG-GD (proposed).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&#34;f1-female&#34;&gt;F1 (female)&lt;/h3&gt;
&lt;p&gt;Sample 1: “かわいそうに、助けてやらなくてはと、家に連れて帰りましたとさ。”&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test01]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test01]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test01]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;VA-PWG-G&lt;/th&gt;&lt;th&gt;VA-PWG-D&lt;/th&gt;&lt;th&gt;VA-PWG-GD (proposed)&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test01]-S3-VA-PWG-G.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test01]-S4-VA-PWG-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test01]-S5-VA-PWG-GD (proposed).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2: “失礼のないよう、笑顔で挨拶して。”&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test02]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test02]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test02]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;VA-PWG-G&lt;/th&gt;&lt;th&gt;VA-PWG-D&lt;/th&gt;&lt;th&gt;VA-PWG-GD (proposed)&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test02]-S3-VA-PWG-G.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test02]-S4-VA-PWG-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test02]-S5-VA-PWG-GD (proposed).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3: “照れていたので、ちょっと意外な気がしましたー。”&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test03]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test03]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test03]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;VA-PWG-G&lt;/th&gt;&lt;th&gt;VA-PWG-D&lt;/th&gt;&lt;th&gt;VA-PWG-GD (proposed)&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test03]-S3-VA-PWG-G.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test03]-S4-VA-PWG-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F1/TTS/[Test03]-S5-VA-PWG-GD (proposed).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&#34;f2-female&#34;&gt;F2 (female)&lt;/h3&gt;
&lt;p&gt;Sample 1: “そして目に留まったのは、お気に入りの居酒屋の前にあるゴミの山。”&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test01]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test01]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test01]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;VA-PWG-G&lt;/th&gt;&lt;th&gt;VA-PWG-D&lt;/th&gt;&lt;th&gt;VA-PWG-GD (proposed)&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test01]-S3-VA-PWG-G.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test01]-S4-VA-PWG-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test01]-S5-VA-PWG-GD (proposed).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2: “今ひとつ、時間が足りず。”&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test02]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test02]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test02]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;VA-PWG-G&lt;/th&gt;&lt;th&gt;VA-PWG-D&lt;/th&gt;&lt;th&gt;VA-PWG-GD (proposed)&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test02]-S3-VA-PWG-G.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test02]-S4-VA-PWG-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test02]-S5-VA-PWG-GD (proposed).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3: “実はこの道の先に、高い山があってね。”&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test03]-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test03]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test03]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;VA-PWG-G&lt;/th&gt;&lt;th&gt;VA-PWG-D&lt;/th&gt;&lt;th&gt;VA-PWG-GD (proposed)&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test03]-S3-VA-PWG-G.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test03]-S4-VA-PWG-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/va-pwg/F2/TTS/[Test03]-S5-VA-PWG-GD (proposed).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;Work performed with nVoice, Clova Voice, Naver Corp.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>High-fidelity Parallel WaveGAN with Multi-band Harmonic-plus-Noise Model</title>
      <link>https://r9y9.github.io/projects/mbhnpwg/</link>
      <pubDate>Fri, 02 Apr 2021 20:34:36 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/mbhnpwg/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Phrase break prediction with bidirectional encoder representations in Japanese text-to-speech synthesis</title>
      <link>https://r9y9.github.io/projects/pbp_bert/</link>
      <pubDate>Fri, 02 Apr 2021 20:34:36 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/pbp_bert/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ここまで来た音声技術・今後の展望 / Current progress on speech technologies and its future prospects @ LINE DEV DAY 2020</title>
      <link>https://r9y9.github.io/talk/linedevday2020panel/</link>
      <pubDate>Wed, 25 Nov 2020 16:40:00 +0900</pubDate>
      <guid>https://r9y9.github.io/talk/linedevday2020panel/</guid>
      <description>&lt;h3 id=&#34;abstract-ja&#34;&gt;Abstract (ja)&lt;/h3&gt;
&lt;p&gt;人の音声をテキストに変換する音声認識技術、テキストから人の音声を生成する音声合成技術をはじめとした音声処理技術が目覚ましい速度で進歩を続けている。さらに、音声に限らないドアの開け閉めの音など一般の音を識別する音響シーン・イベント検出技術などの新しい技術分野が拓けつつある。本セッションでは、LINEから2名のエンジニア（木田祐介・山本龍一）がパネリストとして登壇し、音声認識・音声合成の現状を語る。さらに、同志社大学の井本桂右准教授に登壇いただき、今年国際会議(DCASE)を日本に誘致するなど、日本の研究者の活躍が目覚ましい音響シーン・イベント検出技術の分野の現状を語っていただく。これらの技術分野の進歩には深層学習の進歩が強い影響を与えているが、音声処理特有の要素がどのようにして深層学習と絡み合い技術進化につながっているか掘り下げていきたい。また、様々な音声処理分野で、分野間で共通要素として進展が進む技術要素と特有の要素の分析を通し、各技術分野の特性を明らかにしていきたい。そして、今後どのような方向性で技術が進化していくか、将来の展望について議論していきたい。&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/iSPBCot6n7g&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Parallel WaveGAN: GPUを利用した高速かつ高品質な音声合成 / Parallel WaveGAN: Fast and High-Quality GPU Text-to-Speech @ LINE DEV DAY 2020</title>
      <link>https://r9y9.github.io/talk/linedevday2020pwg/</link>
      <pubDate>Wed, 25 Nov 2020 14:20:00 +0900</pubDate>
      <guid>https://r9y9.github.io/talk/linedevday2020pwg/</guid>
      <description>&lt;h3 id=&#34;abstract-ja&#34;&gt;Abstract (ja)&lt;/h3&gt;
&lt;p&gt;コンピュータによってテキストから人間の声を合成する技術は、テキスト音声合成と呼ばれます。LINE CLOVAのスマートスピーカーを初めとするユーザとのリアルタイムのインタラクションが必要なサービスでは、音声合成システムには合成品質が高いことだけでなく、高速に音声を生成できることが求められます。本セッションでは、高速かつ高品質な音声合成を実現するために、NAVERとLINEで共同で開発したGPUベースの音声合成の研究成果について発表します。従来の方法では、品質が良くても合成速度が遅い、合成速度は速い一方でモデルの学習に多大な時間がかかるなどの問題がありました。我々はそのような問題に対してどのようにアプローチしたのか、音声信号処理のトップカンファレンスICASSP 2020に採択された論文の内容を元に、近年の関連分野の発展を交えて紹介します。&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/BZxqf-Wkhig&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;/br&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/knzT7M6qsl0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Improved Parallel WaveGAN with perceptually weighted spectrogram loss</title>
      <link>https://r9y9.github.io/projects/pwg-pwsl/</link>
      <pubDate>Fri, 06 Nov 2020 16:43:44 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/pwg-pwsl/</guid>
      <description></description>
    </item>
    
    <item>
      <title>TTS-by-TTS: TTS-driven Data Augmentation for Fast and High-Quality Speech Synthesis</title>
      <link>https://r9y9.github.io/projects/tts-by-tts/</link>
      <pubDate>Mon, 26 Oct 2020 16:37:12 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/tts-by-tts/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Parallel waveform synthesis based on generative adversarial networks with voicing-aware conditional discriminators</title>
      <link>https://r9y9.github.io/projects/vuvd-pwg/</link>
      <pubDate>Wed, 21 Oct 2020 23:38:48 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/vuvd-pwg/</guid>
      <description>&lt;p&gt;Preprint: &lt;a href=&#34;https://arxiv.org/abs/2010.14151&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:2010.14151&lt;/a&gt; (accepted to &lt;a href=&#34;https://2021.ieeeicassp.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ICASSP 2021&lt;/a&gt;)&lt;/p&gt;
&lt;h2 id=&#34;table-of-contents&#34;&gt;Table of contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#analysis-synthesis&#34;&gt;Analysis/synthesis samples (Japanese)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#text-to-speech&#34;&gt;Text-to-speech samples (Japanese)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bonus-analysis-synthesis-english&#34;&gt;Bonus: analysis/synthesis samples for CMU ARCTIC (English)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;authors&#34;&gt;Authors&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Ryuichi Yamamoto (LINE Corp.)&lt;/li&gt;
&lt;li&gt;Eunwoo Song (NAVER Corp.)&lt;/li&gt;
&lt;li&gt;Min-Jae Hwang (Search Solutions Inc.)&lt;/li&gt;
&lt;li&gt;Jae-Min Kim (NAVER Corp.)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;This paper proposes voicing-aware conditional discriminators for Parallel WaveGAN-based waveform synthesis systems. In this framework, we adopt a projection-based conditioning method that can significantly improve the discriminator’s performance. Furthermore, the conventional discriminator is separated into two waveform discriminators for modeling voiced and unvoiced speech. As each discriminator learns the distinctive characteristics of the harmonic and noise components, respectively, the adversarial training process becomes more efficient, allowing the generator to produce more realistic speech waveforms. Subjective test results demonstrate the superiority of the proposed method over the conventional Parallel WaveGAN and WaveNet systems. In particular, our speaker-independently trained model within a FastSpeech 2 based text-to-speech framework achieves the mean opinion scores of 4.20, 4.18, 4.21, and 4.31 for four Japanese speakers, respectively.&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/icassp2021_fig.png&#34; width=&#34;50%&#34; /&gt;&lt;/div&gt;
&lt;h2 id=&#34;systems-for-comparision&#34;&gt;Systems for comparision&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;System&lt;/th&gt;
&lt;th&gt;Voiced segments&lt;/th&gt;
&lt;th&gt;Unvoiced segments&lt;/th&gt;
&lt;th&gt;Discriminator conditioning&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;S1-WaveNet &lt;a href=&#34;https://arxiv.org/abs/1807.07281&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[1]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;S2-PWG &lt;a href=&#34;https://arxiv.org/abs/1910.11480&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;S3-PWG-cGAN-D&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;S4-PWG-V/UV-D&lt;/td&gt;
&lt;td&gt;$D^{\mathrm{{v}}}$&lt;/td&gt;
&lt;td&gt;$D^{\mathrm{{v}}}$&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;S5-PWG-V/UV-D&lt;/td&gt;
&lt;td&gt;$D^{\mathrm{{uv}}}$&lt;/td&gt;
&lt;td&gt;$D^{\mathrm{{v}}}$&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;S6-PWG-V/UV-D&lt;/td&gt;
&lt;td&gt;$D^{\mathrm{{uv}}}$&lt;/td&gt;
&lt;td&gt;$D^{\mathrm{{uv}}}$&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;S7-PWG-V/UV-D (proposed)&lt;/td&gt;
&lt;td&gt;$D^{\mathrm{{v}}}$&lt;/td&gt;
&lt;td&gt;$D^{\mathrm{{uv}}}$&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Recordings&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;$D^{\mathrm{{v}}}$: 1-D dilated CNN discrimiantor with the reseptive field size of 127.&lt;/li&gt;
&lt;li&gt;$D^{\mathrm{{uv}}}$: 1-D CNN discrimiantor with the reseptive field size of 13.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;PWG denotes Parallel WaveGAN for short. Systems S2-PWG and S3-PWG-cGAN-D used $D^{\mathrm{{v}}}$  as the primary discriminator. &lt;strong&gt;Note that all the Parallel WaveGAN systems used the same generator architecture and training configurations; they only differed in the discriminator settings.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;analysissynthesis&#34;&gt;Analysis/synthesis&lt;/h2&gt;
&lt;h3 id=&#34;f1-female&#34;&gt;F1 (female)&lt;/h3&gt;
&lt;p&gt;Sample 1&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;S1-WaveNet&lt;/th&gt;&lt;th&gt;S2-PWG&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;S7-PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/AnaSyn/[Test01]-R1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/AnaSyn/[Test01]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/AnaSyn/[Test01]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/AnaSyn/[Test01]-S7-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;S3-PWG-cGAN-D&lt;/th&gt;&lt;th&gt;S4-PWG-V/UV-D&lt;/th&gt;&lt;th&gt;S5-PWG-V/UV-D&lt;/th&gt;&lt;th&gt;S6-PWG-V/UV-D&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/AnaSyn/[Test01]-S3-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/AnaSyn/[Test01]-S4-PWG-VUV-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/AnaSyn/[Test01]-S5-PWG-VUV-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/AnaSyn/[Test01]-S6-PWG-VUV-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&#34;f2-female&#34;&gt;F2 (female)&lt;/h3&gt;
&lt;p&gt;Sample 1&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;S1-WaveNet&lt;/th&gt;&lt;th&gt;S2-PWG&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;S7-PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/AnaSyn/[Test01]-R1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/AnaSyn/[Test01]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/AnaSyn/[Test01]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/AnaSyn/[Test01]-S7-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;S3-PWG-cGAN-D&lt;/th&gt;&lt;th&gt;S4-PWG-V/UV-D&lt;/th&gt;&lt;th&gt;S5-PWG-V/UV-D&lt;/th&gt;&lt;th&gt;S6-PWG-V/UV-D&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/AnaSyn/[Test01]-S3-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/AnaSyn/[Test01]-S4-PWG-VUV-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/AnaSyn/[Test01]-S5-PWG-VUV-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/AnaSyn/[Test01]-S6-PWG-VUV-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&#34;m1-male&#34;&gt;M1 (male)&lt;/h3&gt;
&lt;p&gt;Sample 1&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;S1-WaveNet&lt;/th&gt;&lt;th&gt;S2-PWG&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;S7-PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/AnaSyn/[Test01]-R1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/AnaSyn/[Test01]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/AnaSyn/[Test01]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/AnaSyn/[Test01]-S7-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;S3-PWG-cGAN-D&lt;/th&gt;&lt;th&gt;S4-PWG-V/UV-D&lt;/th&gt;&lt;th&gt;S5-PWG-V/UV-D&lt;/th&gt;&lt;th&gt;S6-PWG-V/UV-D&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/AnaSyn/[Test01]-S3-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/AnaSyn/[Test01]-S4-PWG-VUV-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/AnaSyn/[Test01]-S5-PWG-VUV-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/AnaSyn/[Test01]-S6-PWG-VUV-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&#34;m2-male&#34;&gt;M2 (male)&lt;/h3&gt;
&lt;p&gt;Sample 1&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;S1-WaveNet&lt;/th&gt;&lt;th&gt;S2-PWG&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;S7-PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/AnaSyn/[Test01]-R1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/AnaSyn/[Test01]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/AnaSyn/[Test01]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/AnaSyn/[Test01]-S7-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;S3-PWG-cGAN-D&lt;/th&gt;&lt;th&gt;S4-PWG-V/UV-D&lt;/th&gt;&lt;th&gt;S5-PWG-V/UV-D&lt;/th&gt;&lt;th&gt;S6-PWG-V/UV-D&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/AnaSyn/[Test01]-S3-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/AnaSyn/[Test01]-S4-PWG-VUV-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/AnaSyn/[Test01]-S5-PWG-VUV-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/AnaSyn/[Test01]-S6-PWG-VUV-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h2 id=&#34;text-to-speech&#34;&gt;Text-to-speech&lt;/h2&gt;
&lt;p&gt;FastSpeech 2 (&lt;a href=&#34;https://arxiv.org/abs/2006.04558&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[3]&lt;/a&gt;) based acoustic models were used for text-to-speech experiments.&lt;/p&gt;
&lt;h3 id=&#34;f1-female-1&#34;&gt;F1 (female)&lt;/h3&gt;
&lt;p&gt;Sample 1&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;FastSpeech 2 + WaveNet&lt;/th&gt;&lt;th&gt;FastSpeech 2 + PWG&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;FastSpeech 2 + PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/TTS/[Test01]-R1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/TTS/[Test01]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/TTS/[Test01]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/TTS/[Test01]-S7-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;FastSpeech 2 + PWG-cGAN-D&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/TTS/[Test01]-S3-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;FastSpeech 2 + WaveNet&lt;/th&gt;&lt;th&gt;FastSpeech 2 + PWG&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;FastSpeech 2 + PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/TTS/[Test02]-R1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/TTS/[Test02]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/TTS/[Test02]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/TTS/[Test02]-S7-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;FastSpeech 2 + PWG-cGAN-D&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/TTS/[Test02]-S3-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;FastSpeech 2 + WaveNet&lt;/th&gt;&lt;th&gt;FastSpeech 2 + PWG&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;FastSpeech 2 + PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/TTS/[Test03]-R1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/TTS/[Test03]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/TTS/[Test03]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/TTS/[Test03]-S7-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;FastSpeech 2 + PWG-cGAN-D&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F1/TTS/[Test03]-S3-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&#34;f2-female-1&#34;&gt;F2 (female)&lt;/h3&gt;
&lt;p&gt;Sample 1&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;FastSpeech 2 + WaveNet&lt;/th&gt;&lt;th&gt;FastSpeech 2 + PWG&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;FastSpeech 2 + PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/TTS/[Test01]-R1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/TTS/[Test01]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/TTS/[Test01]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/TTS/[Test01]-S7-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;FastSpeech 2 + PWG-cGAN-D&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/TTS/[Test01]-S3-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;FastSpeech 2 + WaveNet&lt;/th&gt;&lt;th&gt;FastSpeech 2 + PWG&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;FastSpeech 2 + PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/TTS/[Test02]-R1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/TTS/[Test02]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/TTS/[Test02]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/TTS/[Test02]-S7-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;FastSpeech 2 + PWG-cGAN-D&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/TTS/[Test02]-S3-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;FastSpeech 2 + WaveNet&lt;/th&gt;&lt;th&gt;FastSpeech 2 + PWG&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;FastSpeech 2 + PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/TTS/[Test03]-R1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/TTS/[Test03]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/TTS/[Test03]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/TTS/[Test03]-S7-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;FastSpeech 2 + PWG-cGAN-D&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/F2/TTS/[Test03]-S3-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&#34;m1-male-1&#34;&gt;M1 (male)&lt;/h3&gt;
&lt;p&gt;Sample 1&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;FastSpeech 2 + WaveNet&lt;/th&gt;&lt;th&gt;FastSpeech 2 + PWG&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;FastSpeech 2 + PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/TTS/[Test01]-R1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/TTS/[Test01]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/TTS/[Test01]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/TTS/[Test01]-S7-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;FastSpeech 2 + PWG-cGAN-D&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/TTS/[Test01]-S3-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;FastSpeech 2 + WaveNet&lt;/th&gt;&lt;th&gt;FastSpeech 2 + PWG&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;FastSpeech 2 + PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/TTS/[Test02]-R1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/TTS/[Test02]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/TTS/[Test02]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/TTS/[Test02]-S7-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;FastSpeech 2 + PWG-cGAN-D&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/TTS/[Test02]-S3-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;FastSpeech 2 + WaveNet&lt;/th&gt;&lt;th&gt;FastSpeech 2 + PWG&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;FastSpeech 2 + PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/TTS/[Test03]-R1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/TTS/[Test03]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/TTS/[Test03]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/TTS/[Test03]-S7-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;FastSpeech 2 + PWG-cGAN-D&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M1/TTS/[Test03]-S3-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&#34;m2-male-1&#34;&gt;M2 (male)&lt;/h3&gt;
&lt;p&gt;Sample 1&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;FastSpeech 2 + WaveNet&lt;/th&gt;&lt;th&gt;FastSpeech 2 + PWG&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;FastSpeech 2 + PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/TTS/[Test01]-R1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/TTS/[Test01]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/TTS/[Test01]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/TTS/[Test01]-S7-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;FastSpeech 2 + PWG-cGAN-D&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/TTS/[Test01]-S3-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;FastSpeech 2 + WaveNet&lt;/th&gt;&lt;th&gt;FastSpeech 2 + PWG&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;FastSpeech 2 + PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/TTS/[Test02]-R1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/TTS/[Test02]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/TTS/[Test02]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/TTS/[Test02]-S7-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;FastSpeech 2 + PWG-cGAN-D&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/TTS/[Test02]-S3-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;FastSpeech 2 + WaveNet&lt;/th&gt;&lt;th&gt;FastSpeech 2 + PWG&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;FastSpeech 2 + PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/TTS/[Test03]-R1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/TTS/[Test03]-S1-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/TTS/[Test03]-S2-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/TTS/[Test03]-S7-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;FastSpeech 2 + PWG-cGAN-D&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/M2/TTS/[Test03]-S3-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h2 id=&#34;bonus-analysissynthesis-english&#34;&gt;Bonus: Analysis/synthesis (English)&lt;/h2&gt;
&lt;p&gt;Samples for &lt;a href=&#34;http://www.festvox.org/cmu_arctic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CMU ARCTIC database&lt;/a&gt; are provided as follows. The models were trained using total six speakers (clb, slt, bdl, rms, jmk, and ksp) in a speaker-independent way.
The models were similary configured as the above experiments.&lt;/p&gt;
&lt;h3 id=&#34;clb-female&#34;&gt;clb (female)&lt;/h3&gt;
&lt;p&gt;Sample 1&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/clb/AnaSyn/[Test01]-clb_arctic_b0490_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/clb/AnaSyn/[Test01]-clb_arctic_b0490_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/clb/AnaSyn/[Test01]-clb_arctic_b0490_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/clb/AnaSyn/[Test01]-clb_arctic_b0490_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/clb/AnaSyn/[Test02]-clb_arctic_b0491_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/clb/AnaSyn/[Test02]-clb_arctic_b0491_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/clb/AnaSyn/[Test02]-clb_arctic_b0491_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/clb/AnaSyn/[Test02]-clb_arctic_b0491_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/clb/AnaSyn/[Test03]-clb_arctic_b0492_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/clb/AnaSyn/[Test03]-clb_arctic_b0492_gen-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/clb/AnaSyn/[Test03]-clb_arctic_b0492_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/clb/AnaSyn/[Test03]-clb_arctic_b0492_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&#34;slt-female&#34;&gt;slt (female)&lt;/h3&gt;
&lt;p&gt;Sample 1&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/slt/AnaSyn/[Test01]-slt_arctic_b0490_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/slt/AnaSyn/[Test01]-slt_arctic_b0490_gen-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/slt/AnaSyn/[Test01]-slt_arctic_b0490_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/slt/AnaSyn/[Test01]-slt_arctic_b0490_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/slt/AnaSyn/[Test02]-slt_arctic_b0491_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/slt/AnaSyn/[Test02]-slt_arctic_b0491_gen-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/slt/AnaSyn/[Test02]-slt_arctic_b0491_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/slt/AnaSyn/[Test02]-slt_arctic_b0491_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/slt/AnaSyn/[Test03]-slt_arctic_b0492_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/slt/AnaSyn/[Test03]-slt_arctic_b0492_gen-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/slt/AnaSyn/[Test03]-slt_arctic_b0492_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/slt/AnaSyn/[Test03]-slt_arctic_b0492_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&#34;bdl-male&#34;&gt;bdl (male)&lt;/h3&gt;
&lt;p&gt;Sample 1&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/bdl/AnaSyn/[Test01]-bdl_arctic_b0490_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/bdl/AnaSyn/[Test01]-bdl_arctic_b0490_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/bdl/AnaSyn/[Test01]-bdl_arctic_b0490_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/bdl/AnaSyn/[Test01]-bdl_arctic_b0490_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/bdl/AnaSyn/[Test02]-bdl_arctic_b0491_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/bdl/AnaSyn/[Test02]-bdl_arctic_b0491_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/bdl/AnaSyn/[Test02]-bdl_arctic_b0491_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/bdl/AnaSyn/[Test02]-bdl_arctic_b0491_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/bdl/AnaSyn/[Test03]-bdl_arctic_b0492_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/bdl/AnaSyn/[Test03]-bdl_arctic_b0492_gen-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/bdl/AnaSyn/[Test03]-bdl_arctic_b0492_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/bdl/AnaSyn/[Test03]-bdl_arctic_b0492_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&#34;rms-male&#34;&gt;rms (male)&lt;/h3&gt;
&lt;p&gt;Sample 1&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/rms/AnaSyn/[Test01]-rms_arctic_b0490_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/rms/AnaSyn/[Test01]-rms_arctic_b0490_gen-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/rms/AnaSyn/[Test01]-rms_arctic_b0490_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/rms/AnaSyn/[Test01]-rms_arctic_b0490_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/rms/AnaSyn/[Test02]-rms_arctic_b0491_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/rms/AnaSyn/[Test02]-rms_arctic_b0491_gen-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/rms/AnaSyn/[Test02]-rms_arctic_b0491_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/rms/AnaSyn/[Test02]-rms_arctic_b0491_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/rms/AnaSyn/[Test03]-rms_arctic_b0492_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/rms/AnaSyn/[Test03]-rms_arctic_b0492_gen-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/rms/AnaSyn/[Test03]-rms_arctic_b0492_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/rms/AnaSyn/[Test03]-rms_arctic_b0492_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&#34;jmk-male&#34;&gt;jmk (male)&lt;/h3&gt;
&lt;p&gt;Sample 1&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/jmk/AnaSyn/[Test01]-jmk_arctic_b0490_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/jmk/AnaSyn/[Test01]-jmk_arctic_b0490_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/jmk/AnaSyn/[Test01]-jmk_arctic_b0490_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/jmk/AnaSyn/[Test01]-jmk_arctic_b0490_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/jmk/AnaSyn/[Test02]-jmk_arctic_b0491_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/jmk/AnaSyn/[Test02]-jmk_arctic_b0491_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/jmk/AnaSyn/[Test02]-jmk_arctic_b0491_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/jmk/AnaSyn/[Test02]-jmk_arctic_b0491_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/jmk/AnaSyn/[Test03]-jmk_arctic_b0492_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/jmk/AnaSyn/[Test03]-jmk_arctic_b0492_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/jmk/AnaSyn/[Test03]-jmk_arctic_b0492_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/jmk/AnaSyn/[Test03]-jmk_arctic_b0492_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&#34;ksp-male&#34;&gt;ksp (male)&lt;/h3&gt;
&lt;p&gt;Sample 1&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/ksp/AnaSyn/[Test01]-ksp_arctic_b0490_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/ksp/AnaSyn/[Test01]-ksp_arctic_b0490_gen-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/ksp/AnaSyn/[Test01]-ksp_arctic_b0490_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/ksp/AnaSyn/[Test01]-ksp_arctic_b0490_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/ksp/AnaSyn/[Test02]-ksp_arctic_b0491_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/ksp/AnaSyn/[Test02]-ksp_arctic_b0491_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/ksp/AnaSyn/[Test02]-ksp_arctic_b0491_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/ksp/AnaSyn/[Test02]-ksp_arctic_b0491_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3&lt;/p&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Recording&lt;/th&gt;&lt;th&gt;PWG&lt;/th&gt;&lt;th&gt;PWG-cGAN-D&lt;/th&gt;&lt;th&gt;&lt;font color=&#34;#0000cd&#34;&gt;PWG-V/UV-D (ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/ksp/AnaSyn/[Test03]-ksp_arctic_b0492_ref-Recording.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/ksp/AnaSyn/[Test03]-ksp_arctic_b0492_gen-PWG.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/ksp/AnaSyn/[Test03]-ksp_arctic_b0492_gen-PWG-cGAN-D.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2021-pwg-vuvd/bonus/ksp/AnaSyn/[Test03]-ksp_arctic_b0492_gen-PWG-VUV-D (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;[1] W. Ping, K. Peng, and J. Chen, “ClariNet: Parallel wave generation in end-to-end text-to-speech,” in Proc. ICLR, 2019 &lt;a href=&#34;https://arxiv.org/abs/1807.07281&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;[2] R Yamamoto, E Song, and J.-M Kim, “Parallel WaveGAN:A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram,”  in Proc. ICASSP, 2020, pp. 6199–6203. &lt;a href=&#34;https://arxiv.org/abs/1910.11480&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;[3] Y Ren, C Hu, T Qin, S Zhao, Z Zhao, and T.-Y Liu, “Fast-speech 2: Fast and high-quality end-to-end text-to-speech,”arXiv preprint arXiv:2006.04558, 2020. &lt;a href=&#34;https://arxiv.org/abs/2006.04558&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;Work performed with nVoice, Clova Voice, Naver Corp.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;@inproceedings{yamamoto2020parallel,
  title={Parallel waveform synthesis based on generative adversarial networks with voicing-aware conditional discriminators},
  author={Ryuichi Yamamoto and Eunwoo Song and Min-Jae Hwang and Jae-Min Kim},
  booktitle=&amp;quot;Proc. of ICASSP (in press)&amp;quot;,
  year={2021},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>NNSVS: Pytorchベースの研究用歌声合成ライブラリ</title>
      <link>https://r9y9.github.io/blog/2020/05/10/nnsvs/</link>
      <pubDate>Sun, 10 May 2020 14:42:25 +0900</pubDate>
      <guid>https://r9y9.github.io/blog/2020/05/10/nnsvs/</guid>
      <description>&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;コード: &lt;a href=&#34;https://github.com/r9y9/nnsvs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/nnsvs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Discussion: &lt;a href=&#34;https://github.com/r9y9/nnsvs/issues/1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/nnsvs/issues/1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/r9y9/Colaboratory/blob/master/Neural_network_based_singing_voice_synthesis_demo_using_kiritan_singing_database_%28Japanese%29.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Demo on Google colab&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;春が来た　春が来た　どこに来た。　山に来た　里に来た、野にも来た。花がさく　花がさく　どこにさく。山にさく　里にさく、野にもさく。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;audio controls=&#34;controls&#34; &gt;&lt;source src=&#34;https://r9y9.github.io/audio/nnsvs/20200510_haru.wav&#34; autoplay/&gt;Your browser does not support the audio element.&lt;/audio&gt;&lt;/p&gt;
&lt;h2 id=&#34;nnsvs-はなに&#34;&gt;NNSVS はなに？&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Neural network-based singing voice synthesis library for research&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;研究用途を目的とした、歌声合成エンジンを作るためのオープンソースのライブラリを作ることを目指したプロジェクトです。このプロジェクトについて、考えていることをまとめておこうと思います。&lt;/p&gt;
&lt;h3 id=&#34;なぜやるか&#34;&gt;なぜやるか？&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://n3utrino.work/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NEUTRINO&lt;/a&gt; レベルの品質の歌声合成エンジンが作れるのかやってみたかった&lt;/li&gt;
&lt;li&gt;オープンソースのツールがほぼない分野なので、ツールを作ると誰かの役にも立っていいかなと思った。研究分野が盛り上がると良いですね&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;というのが理由です。前者の割合が大きく、後者は建前の要素が強いです。要は、できるかどうかがどうしても気になって、気がづいたら熱中していた、という感じです。&lt;/p&gt;
&lt;h3 id=&#34;研究用途&#34;&gt;研究用途&lt;/h3&gt;
&lt;p&gt;機械学習や信号処理にある程度明るい人を想定しています。歌声合成技術を使って創作したい人ではなく、どのようにすればより良い歌声合成を作ることができるのか？といった興味を持つ人が主な対象です。&lt;/p&gt;
&lt;p&gt;創作活動のために歌声合成の技術を使う場合には、すでに優れたツールがあると思いますので、そちらを使っていただくのがよいと思います&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;。&lt;a href=&#34;https://n3utrino.work/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NEUTRINO&lt;/a&gt;、&lt;a href=&#34;https://synthesizerv.com/jp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Synthesizer V&lt;/a&gt;、&lt;a href=&#34;http://cevio.jp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CeVIO&lt;/a&gt; など&lt;/p&gt;
&lt;h3 id=&#34;オープンソース&#34;&gt;オープンソース&lt;/h3&gt;
&lt;p&gt;オープンソースであることを重視します。歌声合成ソフトウェアは多くありますが、オープンソースのものは多くありません&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;。このプロジェクトは僕が趣味として始めたもので、ビジネスにする気はまったくないので&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;、誰でも自由に使えるようにしたいと思っています。オープンなソフトウェアが、研究分野の一助になることを期待しています。&lt;/p&gt;
&lt;h3 id=&#34;pytorchベース&#34;&gt;Pytorchベース&lt;/h3&gt;
&lt;p&gt;過去に &lt;a href=&#34;https://github.com/r9y9/nnmnkwii&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nnmnkwii&lt;/a&gt;という音声合成のためのライブラリを作りました。その際には、任意の数値微分ライブラリと使えるようにと考えて設計しましたが、nnsvsはあえてpytorchに依存した形で作ります。&lt;/p&gt;
&lt;p&gt;Pytorchと切り離して設計すると汎用的にしやすい一方で、&lt;a href=&#34;https://github.com/kaldi-asr/kaldi&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kaldi&lt;/a&gt; や&lt;a href=&#34;https://github.com/espnet/espnet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ESPnet&lt;/a&gt; のようなプロジェクトで成功している&lt;strong&gt;レシピ&lt;/strong&gt;というものが作りずらいです。ESPnetに多少関わって、再現性の担保の重要性を身にしみて感じつつあるので、Pytorchベースの学習、推論など、歌声合成のモデルを構築するために必要なすべてをひっくるめたソフトウェアを目指したいと思います。&lt;/p&gt;
&lt;h3 id=&#34;レシピの提供&#34;&gt;レシピの提供&lt;/h3&gt;
&lt;p&gt;再現性を重視します。そのために、KaldiやESPnetの成功に習って、レシピという実験を再現するのに必要なすべてのステップが含まれたスクリプトを提供します。レシピは、データの前処理、特徴量抽出、モデル学習、推論、波形の合成などを含みます。&lt;/p&gt;
&lt;p&gt;例えば、このブログのトップに貼った音声サンプルを合成するのに使われたモデルは、公開されているレシピで再現することが可能です。歌声合成エンジンを作るためのありとあらゆるものを透明な形で提供します。&lt;/p&gt;
&lt;h2 id=&#34;プロジェクトの進め方について&#34;&gt;プロジェクトの進め方について&lt;/h2&gt;
&lt;p&gt;完全に完成してから公開する、というアプローチとは正反対で、構想のみで実態はまったくできていない状態から始めて、進捗を含めてすべてオープンで確認できるような状態で進めます。進捗は &lt;a href=&#34;https://github.com/r9y9/nnsvs/issues/1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/nnsvs/issues/1&lt;/a&gt; から確認できます。&lt;/p&gt;
&lt;p&gt;過去に&lt;a href=&#34;https://github.com/r9y9/wavenet_vocoder&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;wavenet vocoder&lt;/a&gt;をつくったときにも同じような方法ではじめました。突然知らない人がコメントをくれたりするのがオープンソースの面白いところの一つだと思っているので、この方式で進めます。&lt;/p&gt;
&lt;h2 id=&#34;現時点の状況&#34;&gt;現時点の状況&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://zunko.jp/kiridev/login.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;きりたんデータベース&lt;/a&gt;を使って、parametric SVS（Sinsyの中身に近いもの）が一通り作れるところまでできました。MusicXMLを入力として、音声波形を出力します。作った歌声合成システムは、time-lagモデル、音素継続長モデル、音響モデルの3つのtrainableなモデルで成り立っています。音楽/言語的特徴量は&lt;a href=&#34;https://github.com/r9y9/sinsy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sinsy&lt;/a&gt;で抽出して、音声分析合成には&lt;a href=&#34;https://github.com/mmorise/World&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WORLD&lt;/a&gt;を使います。仕組みは、以下の論文の内容に近いです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Y. Hono et al, &amp;ldquo;Recent Development of the DNN-based Singing Voice Synthesis System — Sinsy,&amp;rdquo; Proc. of APSIPA, 2017. (&lt;a href=&#34;http://www.apsipa.org/proceedings/2018/pdfs/0001003.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Mixture density networkは使っていない、ビブラートパラメータを推定していない等、違いはたくさんあります。現時点では劣化sinsyといったところですね T.T&lt;/p&gt;
&lt;h2 id=&#34;開発履歴&#34;&gt;開発履歴&lt;/h2&gt;
&lt;h3 id=&#34;20200408-初期版&#34;&gt;2020/04/08 (初期版)&lt;/h3&gt;
&lt;p&gt;一番最初につくったものです。見事な音痴歌声合成になりました。TTSの仕組みを使うだけでは当然だめでした、というオチです。音響モデルでは対数lf0を予測するようにしました。このころはtime-lagモデルを作っていなくて、phonetic timeingはアノテーションされたデータのものを使っています。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;iframe width=&#34;90%&#34; height=&#34;200&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; allow=&#34;autoplay&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/792271372&amp;color=%23ff5500&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false&amp;show_teaser=true&amp;visual=true&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;h3 id=&#34;20200426-本ブログ執筆時点での最新版&#34;&gt;2020/04/26 (本ブログ執筆時点での最新版)&lt;/h3&gt;
&lt;p&gt;Time-lag, duration, acoustic modelのすべてを一旦実装し終わったバージョンです。lf0の絶対値を予測するのではなく、relativeなlf0を予測するように変えました。phonetic timing はすべて予測されたものを使っています。ひととおりできたにはいいですが、完成度はいまいちというのが正直なところですね&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;iframe width=&#34;90%&#34; height=&#34;200&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; allow=&#34;autoplay&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/806654083&amp;color=%23ff5500&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false&amp;show_teaser=true&amp;visual=true&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;h2 id=&#34;今後の予定&#34;&gt;今後の予定&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/nnsvs/issues/1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/nnsvs/issues/1&lt;/a&gt; を随時更新しますが、重要なものをいくつかピップアップします。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;音響モデルの強化&lt;/strong&gt;：特にF0のモデル化が難しい印象で、改善を考えています。いまは本当に適当なCNNをつかっていますが、autoreggresive modelに変えたいと思っています。いくつか選択肢がありますが、WaveNetのようなモデルにする予定です。https://mtg.github.io/singing-synthesis-demos/ 彼らの論文を大いに参考にする予定です。NIIのWangさんのshallow ARモデルを使うもよし。最重要課題で、目下やることリストに入っています&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;離散F0モデリング&lt;/strong&gt;: NIIのWangさんの論文が大変参考になりました。音声合成では広く連続F0が使われている印象ですが、離散F0モデリングを試したいと思っています。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transformerなどの強力なモデル&lt;/strong&gt;: 今年の &lt;a href=&#34;https://2020.ieeeicassp.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ICASSP 2020&lt;/a&gt; で &lt;a href=&#34;https://mtg.github.io/singing-synthesis-demos/transformer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Feed-forward Transformerを使った歌声合成の研究発表&lt;/a&gt;がありましたが、近年のnon-autoregressiveモデルの発展はすごいので、同様のアプローチを試してみたいと思っています。製品化は考えないし、どんなにデカくて遅いモデルを使ってもよし&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ニューラルボコーダ&lt;/strong&gt;: 音響モデルの改善がある程度できれば、ニューラルボコーダを入れて高品質にできるといいですね。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;音楽/言語特徴量の簡略化&lt;/strong&gt;: 今は450次元くらいの特徴量を使っていますが、https://mtg.github.io/singing-synthesis-demos/ 彼らのグループの研究を見ると、もっとシンプルにできそうに思えてきています。音楽/言語特徴量の抽出は今はsinsyに頼りっきりですが、どこかのタイミングでシンプルにしたいと思っています。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Time-lag/duration modelの改善&lt;/strong&gt;: 現時点ではめっちゃ雑なつくりなので、https://mtg.github.io/singing-synthesis-demos/ 彼らの研究を見習って細部まで詰めたい&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;音素アライメントツール&lt;/strong&gt;: きりたんDBの音素アライメントが微妙に不正確なのがあったりします。今のところある程度手修正していますが、自動でやったほうがいいのではと思えてきました。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;その他データセット&lt;/strong&gt;: JVSなど。きりたんDBである程度できてからですかね&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;これまで歌声合成をやってみての所感&#34;&gt;これまで歌声合成をやってみての所感&lt;/h2&gt;
&lt;p&gt;歌声合成クッソムズすぎワロタ&lt;/p&gt;
&lt;p&gt;新しいことにチャレンジするのはとても楽しいですが、やっぱり難しいですね。離散化F0、autoregressive modelの導入でそれなりの品質に持っていけるという淡い期待をしていますが、さてどうなることやら。地道に頑張って改善していきます。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;きりたんデータベース: &lt;a href=&#34;https://zunko.jp/kiridev/login.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://zunko.jp/kiridev/login.php&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;NEUTRINO: &lt;a href=&#34;https://n3utrino.work/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://n3utrino.work/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;NNSVS: &lt;a href=&#34;https://github.com/r9y9/nnsvs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/nnsvs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;NNSVS 進捗: &lt;a href=&#34;https://github.com/r9y9/nnsvs/issues/1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/nnsvs/issues/1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;sinsy: &lt;a href=&#34;http://sinsy.sourceforge.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://sinsy.sourceforge.net/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;My fork of sinsy: &lt;a href=&#34;https://github.com/r9y9/sinsy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/sinsy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;nnmnkwii: &lt;a href=&#34;https://github.com/r9y9/nnmnkwii&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/nnmnkwii&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;WORLD: &lt;a href=&#34;https://github.com/mmorise/World&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/mmorise/World&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Y. Hono et al, &amp;ldquo;Recent Development of the DNN-based Singing Voice Synthesis System — Sinsy,&amp;rdquo; Proc. of APSIPA, 2017. (&lt;a href=&#34;http://www.apsipa.org/proceedings/2018/pdfs/0001003.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;NEUTRINO並の品質の歌声合成エンジンが作れたらいいなとは思っていますが、まだまだ道のりは長そうです。&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://sinsy.sourceforge.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://sinsy.sourceforge.net/&lt;/a&gt; 有名なものにsinsyがありますが、DNNモデルの学習など、すべてがオープンソースなわけではありません&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;万が一の場合は、察してください…&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Neural text-to-speech with a modeling-by-generation excitation vocoder</title>
      <link>https://r9y9.github.io/projects/mbg_excitnet/</link>
      <pubDate>Wed, 22 Apr 2020 16:51:09 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/mbg_excitnet/</guid>
      <description></description>
    </item>
    
    <item>
      <title>End-to-End 音声合成の研究を加速させるツールキット ESPnet-TTS / ESPnet-TTS: A toolkit to accelerate research on end-to-end speech synthesis @ ASJ 2020s</title>
      <link>https://r9y9.github.io/talk/asj-espnet2-tutorial/</link>
      <pubDate>Mon, 16 Mar 2020 13:00:00 +0900</pubDate>
      <guid>https://r9y9.github.io/talk/asj-espnet2-tutorial/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ESPnet-TTS: Unified, Reproducible, and Integratable Open Source End-to-End Text-to-Speech Toolkit</title>
      <link>https://r9y9.github.io/projects/espnet-tts/</link>
      <pubDate>Thu, 24 Oct 2019 16:21:27 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/espnet-tts/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Parallel WaveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram</title>
      <link>https://r9y9.github.io/projects/pwg/</link>
      <pubDate>Mon, 21 Oct 2019 20:18:27 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/pwg/</guid>
      <description>&lt;p&gt;Preprint: &lt;a href=&#34;https://arxiv.org/abs/1910.11480&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1910.11480&lt;/a&gt; (accepted to &lt;a href=&#34;https://2020.ieeeicassp.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ICASSP 2020&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#audio-samples-japanese&#34;&gt;Audio samples (Japanese)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#audio-samples-english&#34;&gt;Audio samples (English)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Japanese samples were used in the subjective evaluations reported in our paper.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;authors&#34;&gt;Authors&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Ryuichi Yamamoto (LINE Corp.)&lt;/li&gt;
&lt;li&gt;Eunwoo Song (NAVER Corp.)&lt;/li&gt;
&lt;li&gt;Jae-Min Kim (NAVER Corp.)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;We propose Parallel WaveGAN&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, a distillation-free, fast, and small-footprint waveform generation method using a generative adversarial network. In the proposed method, a non-autoregressive WaveNet is trained by jointly optimizing multi-resolution spectrogram and adversarial loss functions, which can effectively capture the time-frequency distribution of the realistic speech waveform. As our method does not require density distillation used in the conventional teacher-student framework, the entire model can be easily trained even with a small number of parameters. In particular, the proposed Parallel WaveGAN has only 1.44 M parameters and can generate 24 kHz speech waveform 28.68 times faster than real-time on a single GPU environment. Perceptual listening test results verify that our proposed method achieves 4.16 mean opinion score within a Transformer-based text-to-speech framework, which is comparative to the best distillation-based Parallel WaveNet system.&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/icassp2020_fig.png&#34; width=&#34;60%&#34; /&gt;&lt;/div&gt;
&lt;h3 id=&#34;systems-used-for-comparision&#34;&gt;Systems used for comparision&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Ground truth&lt;/strong&gt;: Recorded speech.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;WaveNet&lt;/strong&gt;: Gaussian WaveNet &lt;a href=&#34;https://arxiv.org/abs/1807.07281&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[1]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ClariNet-$L^{(1)}$&lt;/strong&gt;: ClariNet &lt;a href=&#34;https://arxiv.org/abs/1807.07281&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[1]&lt;/a&gt; with the single STFT auxiliary loss&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ClariNet-$L^{(1,2,3)}$&lt;/strong&gt;: ClariNet with the multi-resolution STFT loss&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ClariNet-GAN-$L^{(1,2,3)}$&lt;/strong&gt;: ClariNet with the multi-resolution STFT and adversarial losses &lt;a href=&#34;https://arxiv.org/abs/1807.07281&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Parallel WaveGAN-$L^{(1)}$&lt;/strong&gt;: Parallel WaveGAN with th single STFT loss&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Parallel WaveGAN-$L^{(1,2,3)}$&lt;/strong&gt;: Parallel WaveGAN with the multi-resolution STFT loss&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;audio-samples-japanese&#34;&gt;Audio samples (Japanese)&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Analysis/synthesis&lt;/li&gt;
&lt;li&gt;Text-to-speech (Transformer TTS + vocoder models)&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;analysissynthesis&#34;&gt;Analysis/synthesis&lt;/h3&gt;
&lt;p&gt;Sample 1&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;ClariNet-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample01]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample01]-2-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample01]-3-ClariNet (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;ClariNet-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;ClariNet-GAN-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample01]-4-ClariNet (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample01]-5-ClariNet-GAN (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample01]-6-Parallel WaveGAN (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1,2,3)}$ &lt;font color=&#34;#0000cd&#34;&gt;(ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample01]-7-Parallel WaveGAN (3spec) (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;ClariNet-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample02]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample02]-2-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample02]-3-ClariNet (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;ClariNet-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;ClariNet-GAN-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample02]-4-ClariNet (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample02]-5-ClariNet-GAN (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample02]-6-Parallel WaveGAN (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1,2,3)}$ &lt;font color=&#34;#0000cd&#34;&gt;(ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample02]-7-Parallel WaveGAN (3spec) (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;ClariNet-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample03]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample03]-2-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample03]-3-ClariNet (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;ClariNet-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;ClariNet-GAN-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample03]-4-ClariNet (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample03]-5-ClariNet-GAN (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample03]-6-Parallel WaveGAN (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1,2,3)}$ &lt;font color=&#34;#0000cd&#34;&gt;(ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample03]-7-Parallel WaveGAN (3spec) (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 4&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;ClariNet-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample04]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample04]-2-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample04]-3-ClariNet (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;ClariNet-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;ClariNet-GAN-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample04]-4-ClariNet (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample04]-5-ClariNet-GAN (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample04]-6-Parallel WaveGAN (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1,2,3)}$ &lt;font color=&#34;#0000cd&#34;&gt;(ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample04]-7-Parallel WaveGAN (3spec) (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 5&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;ClariNet-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample05]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample05]-2-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample05]-3-ClariNet (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;ClariNet-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;ClariNet-GAN-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample05]-4-ClariNet (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample05]-5-ClariNet-GAN (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample05]-6-Parallel WaveGAN (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1,2,3)}$ &lt;font color=&#34;#0000cd&#34;&gt;(ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample05]-7-Parallel WaveGAN (3spec) (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&#34;text-to-speech&#34;&gt;Text-to-speech&lt;/h3&gt;
&lt;p&gt;Sample 1&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;Transformer + WaveNet&lt;/th&gt;&lt;th&gt;Transformer + ClariNet-$L^{(1,2,3)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample01]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample01]-2-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample01]-4-ClariNet (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Transformer + ClariNet-GAN-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;Transformer + Parallel WaveGAN--$L^{(1,2,3)}$ &lt;font color=&#34;#0000cd&#34;&gt;(ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample01]-5-ClariNet-GAN (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample01]-7-Parallel WaveGAN (3spec) (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;Transformer + WaveNet&lt;/th&gt;&lt;th&gt;Transformer + ClariNet-$L^{(1,2,3)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample02]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample02]-2-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample02]-4-ClariNet (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Transformer + ClariNet-GAN-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;Transformer + Parallel WaveGAN--$L^{(1,2,3)}$ &lt;font color=&#34;#0000cd&#34;&gt;(ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample02]-5-ClariNet-GAN (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample02]-7-Parallel WaveGAN (3spec) (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;Transformer + WaveNet&lt;/th&gt;&lt;th&gt;Transformer + ClariNet-$L^{(1,2,3)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample03]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample03]-2-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample03]-4-ClariNet (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Transformer + ClariNet-GAN-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;Transformer + Parallel WaveGAN--$L^{(1,2,3)}$ &lt;font color=&#34;#0000cd&#34;&gt;(ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample03]-5-ClariNet-GAN (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample03]-7-Parallel WaveGAN (3spec) (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 4&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;Transformer + WaveNet&lt;/th&gt;&lt;th&gt;Transformer + ClariNet-$L^{(1,2,3)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample04]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample04]-2-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample04]-4-ClariNet (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Transformer + ClariNet-GAN-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;Transformer + Parallel WaveGAN--$L^{(1,2,3)}$ &lt;font color=&#34;#0000cd&#34;&gt;(ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample04]-5-ClariNet-GAN (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample04]-7-Parallel WaveGAN (3spec) (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 5&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;Transformer + WaveNet&lt;/th&gt;&lt;th&gt;Transformer + ClariNet-$L^{(1,2,3)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/AnaSyn/[Sample05]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample05]-2-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample05]-4-ClariNet (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Transformer + ClariNet-GAN-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;Transformer + Parallel WaveGAN--$L^{(1,2,3)}$ &lt;font color=&#34;#0000cd&#34;&gt;(ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample05]-5-ClariNet-GAN (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/TTS/[Sample05]-7-Parallel WaveGAN (3spec) (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h2 id=&#34;audio-samples-english&#34;&gt;Audio samples (English)&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Analysis/synthesis&lt;/li&gt;
&lt;li&gt;Text-to-speech (&lt;a href=&#34;https://arxiv.org/abs/1910.10909&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ESPnet-TTS&lt;/a&gt; + Our Parallel WaveGAN)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;https://keithito.com/LJ-Speech-Dataset/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LJSpeech dataset&lt;/a&gt; is used for the test. Mel-spectrograms (with the range of 70 - 7600 Hz&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;) were used for local conditioning.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Please note that the English samples were not used in the subjective evaluations reported in our paper.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;analysissynthesis-1&#34;&gt;Analysis/synthesis&lt;/h3&gt;
&lt;p&gt;That is reflected in definite and comprehensive operating procedures.&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;ClariNet-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample01]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample01]-2-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample01]-3-ClariNet (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;ClariNet-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;ClariNet-GAN-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample01]-4-ClariNet (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample01]-5-ClariNet-GAN (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample01]-6-Parallel WaveGAN (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1,2,3)}$ &lt;font color=&#34;#0000cd&#34;&gt;(ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample01]-7-Parallel WaveGAN (3spec) (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;The commission also recommends.&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;ClariNet-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample02]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample02]-2-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample02]-3-ClariNet (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;ClariNet-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;ClariNet-GAN-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample02]-4-ClariNet (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample02]-5-ClariNet-GAN (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample02]-6-Parallel WaveGAN (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1,2,3)}$ &lt;font color=&#34;#0000cd&#34;&gt;(ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample02]-7-Parallel WaveGAN (3spec) (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;That the secret service consciously set about the task of inculcating and maintaining the highest standard of excellence and esprit, for all of its personnel.&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;ClariNet-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample03]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample03]-2-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample03]-3-ClariNet (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;ClariNet-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;ClariNet-GAN-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample03]-4-ClariNet (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample03]-5-ClariNet-GAN (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample03]-6-Parallel WaveGAN (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1,2,3)}$ &lt;font color=&#34;#0000cd&#34;&gt;(ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample03]-7-Parallel WaveGAN (3spec) (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;This involves tight and unswerving discipline as well as the promotion of an outstanding degree of dedication and loyalty to duty.&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;ClariNet-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample04]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample04]-2-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample04]-3-ClariNet (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;ClariNet-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;ClariNet-GAN-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample04]-4-ClariNet (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample04]-5-ClariNet-GAN (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample04]-6-Parallel WaveGAN (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1,2,3)}$ &lt;font color=&#34;#0000cd&#34;&gt;(ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample04]-7-Parallel WaveGAN (3spec) (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;The commission emphasizes that it finds no causal connection between the assassination.&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;WaveNet&lt;/th&gt;&lt;th&gt;ClariNet-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample05]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample05]-2-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample05]-3-ClariNet (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;ClariNet-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;ClariNet-GAN-$L^{(1,2,3)}$&lt;/th&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1)}$&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample05]-4-ClariNet (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample05]-5-ClariNet-GAN (3spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample05]-6-Parallel WaveGAN (1spec).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Parallel WaveGAN-$L^{(1,2,3)}$ &lt;font color=&#34;#0000cd&#34;&gt;(ours)&lt;/font&gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample05]-7-Parallel WaveGAN (3spec) (ours).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&#34;text-to-speech-1&#34;&gt;Text-to-speech&lt;/h3&gt;
&lt;p&gt;We combined our Parallel WaveGAN with &lt;a href=&#34;https://arxiv.org/abs/1910.10909&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ESPnet-TTS&lt;/a&gt;. The systems used here are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Transformer.v3&lt;/strong&gt;: Transformer.v3 presented in &lt;a href=&#34;https://arxiv.org/abs/1910.10909&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ESPnet-TTS&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MoL WaveNet&lt;/strong&gt;: WaveNet with mixture of logistics output distribution (shipped with ESPnet). Pre-emphasis/de-emphasis were applied to reduce perceptual noise (similar to &lt;a href=&#34;https://arxiv.org/abs/1810.11846&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LPCNet&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Parallel WaveGAN&lt;/strong&gt;: Our Parallel WaveGAN with multi-resolution spectrogram loss.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that &lt;strong&gt;Transformer.v3 + MoL WaveNet&lt;/strong&gt; is the same as used in &lt;a href=&#34;https://espnet.github.io/icassp2020-tts/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://espnet.github.io/icassp2020-tts/&lt;/a&gt;.
Mel-spectrograms (with the range of 70 - 11025 Hz) were used for local conditioning.&lt;/p&gt;
&lt;p&gt;That is reflected in definite and comprehensive operating procedures.&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;Transformer.v3 + MoL WaveNet&lt;/th&gt;&lt;th&gt;Transformer.v3 + Parallel WaveGAN&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample01]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/TTS/[LJ-Sample01]-1-MoL-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/TTS/[LJ-Sample01]-2-Parallel WaveGAN.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;The commission also recommends.&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;Transformer.v3 + MoL WaveNet&lt;/th&gt;&lt;th&gt;Transformer.v3 + Parallel WaveGAN&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample02]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/TTS/[LJ-Sample02]-1-MoL-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/TTS/[LJ-Sample02]-2-Parallel WaveGAN.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;That the secret service consciously set about the task of inculcating and maintaining the highest standard of excellence and esprit, for all of its personnel.&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;Transformer.v3 + MoL WaveNet&lt;/th&gt;&lt;th&gt;Transformer.v3 + Parallel WaveGAN&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample03]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/TTS/[LJ-Sample03]-1-MoL-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/TTS/[LJ-Sample03]-2-Parallel WaveGAN.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;This involves tight and unswerving discipline as well as the promotion of an outstanding degree of dedication and loyalty to duty.&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;Transformer.v3 + MoL WaveNet&lt;/th&gt;&lt;th&gt;Transformer.v3 + Parallel WaveGAN&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample04]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/TTS/[LJ-Sample04]-1-MoL-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/TTS/[LJ-Sample04]-2-Parallel WaveGAN.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;The commission emphasizes that it finds no causal connection between the assassination.&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;Transformer.v3 + MoL WaveNet&lt;/th&gt;&lt;th&gt;Transformer.v3 + Parallel WaveGAN&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/AnaSyn/[LJ-Sample05]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/TTS/[LJ-Sample05]-1-MoL-WaveNet.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/icassp2020/LJSpeech/TTS/[LJ-Sample05]-2-Parallel WaveGAN.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;W. Ping, K. Peng, and J. Chen, “ClariNet: Parallel wave generation in end-to-end text-to-speech,” in Proc. ICLR, 2019 (&lt;a href=&#34;https://arxiv.org/abs/1807.07281&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;R. Yamamoto, E. Song, and J.-M. Kim, “Probability density distillation with generative adversarial networks for high-quality parallel waveform generation,” in Proc. INTERSPEECH, 2019, pp. 699–703. (&lt;a href=&#34;https://www.isca-speech.org/archive/Interspeech_2019/abstracts/1965.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ISCA archive&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;Work performed with nVoice, Clova Voice, Naver Corp.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;@inproceedings{yamamoto2020parallel,
  title={Parallel {WaveGAN}: {A} fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram},
  author={Yamamoto, Ryuichi and Song, Eunwoo and Kim, Jae-Min},
  booktitle	= &amp;quot;Proc. of ICASSP&amp;quot;,
  pages={6199--6203},
  year={2020}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Note that our work is not closely related to an unsupervised waveform synthesis model, &lt;a href=&#34;https://arxiv.org/abs/1802.04208&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WaveGAN&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Audio quaility can be improved by using the full-band frequency range, but it may suffer from the over-smoothing problem in TTS.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Probability Density Distillation with Generative Adversarial Networks for High-Quality Parallel Waveform Generation</title>
      <link>https://r9y9.github.io/projects/gan-pwn/</link>
      <pubDate>Tue, 25 Jun 2019 17:20:29 +0900</pubDate>
      <guid>https://r9y9.github.io/projects/gan-pwn/</guid>
      <description>&lt;p&gt;Preprint: &lt;a href=&#34;https://arxiv.org/abs/1904.04472&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1904.04472&lt;/a&gt;, Published version: &lt;a href=&#34;https://www.isca-speech.org/archive_v0/Interspeech_2019/abstracts/1965.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ISCA Archive Interspeech 2019&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;authors&#34;&gt;Authors&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Ryuichi Yamamoto (LINE Corp.)&lt;/li&gt;
&lt;li&gt;Eunwoo Song (NAVER Corp.)&lt;/li&gt;
&lt;li&gt;Jae-Min Kim (NAVER Corp.)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;This paper proposes an effective probability density distillation (PDD) algorithm for WaveNet-based parallel waveform generation (PWG) systems. Recently proposed teacher-student frameworks in the PWG system have successfully achieved a real-time generation of speech signals. However, the difficulties optimizing the PDD criteria without auxiliary losses result in quality degradation of synthesized speech. To generate more natural speech signals within the teacher-student framework, we propose a novel optimization criterion based on generative adversarial networks (GANs). In the proposed method, the inverse autoregressive flow-based student model is incorporated as a generator in the GAN framework, and jointly optimized by the PDD mechanism with the proposed adversarial learning method. As this process encourages the student to model the distribution of realistic speech waveform, the perceptual quality of the synthesized speech becomes much more natural. Our experimental results verify that the PWG systems with the proposed method outperform both those using conventional approaches, and also autoregressive generation systems with a well-trained teacher WaveNet.&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/interspeech2019_fig.png&#34; width=&#34;90%&#34; /&gt;&lt;/div&gt;
&lt;h2 id=&#34;audio-samples&#34;&gt;Audio samples&lt;/h2&gt;
&lt;p&gt;There are 8 different systems, that include 6 parallel waveform generation systems (Student-*) trained by different optimization criteria as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Ground truth&lt;/strong&gt;: Recorded speech.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Teacher&lt;/strong&gt;: Teacher Gaussian WaveNet &lt;a href=&#34;https://arxiv.org/abs/1807.07281&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[1]&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Student-AX&lt;/strong&gt;: STFT auxiliary loss.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Student-AXAD&lt;/strong&gt;: STFT and adversarial losses.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Student-KL&lt;/strong&gt;: KLD loss (Ablation study; not used for subjective evaluations).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Student-KLAX&lt;/strong&gt;: KLD and STFT auxiliary losses.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Student-KLAXAD&lt;/strong&gt;: KLD, STFT, and adversarial losses (proposed).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Student-KLAXAD&lt;/strong&gt;*: Weights optimized version of the above (proposed).&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;copy-synthesis&#34;&gt;Copy-synthesis&lt;/h3&gt;
&lt;h4 id=&#34;japanese-female-speaker&#34;&gt;Japanese female speaker&lt;/h4&gt;
&lt;p&gt;Sample 1&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;Teacher&lt;/th&gt;&lt;th&gt;Student-AX&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample01]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample01]-2-Teacher.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample01]-3-Student-AX (AuxLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Student-AXAV&lt;/th&gt;&lt;th&gt;Student-KL&lt;/th&gt;&lt;th&gt;Student-KLAX&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample01]-4-Student-AXAV (AuxLoss + AdvLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample01]-5-Student-KL (KLD only; ablation study).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample01]-6-Student-KLAX (KLD + AuxLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Student-KLAXAD&lt;/th&gt;&lt;th&gt;Student-KLAXAD*&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample01]-7-Student-KLAXAD (Proposed; KLD + AuxLoss + AdvLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample01]-8-Student-KLAXAD (Proposed; weights optimized version).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 2&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;Teacher&lt;/th&gt;&lt;th&gt;Student-AX&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample02]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample02]-2-Teacher.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample02]-3-Student-AX (AuxLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Student-AXAV&lt;/th&gt;&lt;th&gt;Student-KL&lt;/th&gt;&lt;th&gt;Student-KLAX&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample02]-4-Student-AXAV (AuxLoss + AdvLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample02]-5-Student-KL (KLD only; ablation study).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample02]-6-Student-KLAX (KLD + AuxLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Student-KLAXAD&lt;/th&gt;&lt;th&gt;Student-KLAXAD*&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample02]-7-Student-KLAXAD (Proposed; KLD + AuxLoss + AdvLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample02]-8-Student-KLAXAD (Proposed; weights optimized version).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 3&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;Teacher&lt;/th&gt;&lt;th&gt;Student-AX&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample03]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample03]-2-Teacher.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample03]-3-Student-AX (AuxLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Student-AXAV&lt;/th&gt;&lt;th&gt;Student-KL&lt;/th&gt;&lt;th&gt;Student-KLAX&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample03]-4-Student-AXAV (AuxLoss + AdvLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample03]-5-Student-KL (KLD only; ablation study).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample03]-6-Student-KLAX (KLD + AuxLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Student-KLAXAD&lt;/th&gt;&lt;th&gt;Student-KLAXAD*&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample03]-7-Student-KLAXAD (Proposed; KLD + AuxLoss + AdvLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample03]-8-Student-KLAXAD (Proposed; weights optimized version).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 4&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;Teacher&lt;/th&gt;&lt;th&gt;Student-AX&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample04]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample04]-2-Teacher.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample04]-3-Student-AX (AuxLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Student-AXAV&lt;/th&gt;&lt;th&gt;Student-KL&lt;/th&gt;&lt;th&gt;Student-KLAX&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample04]-4-Student-AXAV (AuxLoss + AdvLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample04]-5-Student-KL (KLD only; ablation study).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample04]-6-Student-KLAX (KLD + AuxLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Student-KLAXAD&lt;/th&gt;&lt;th&gt;Student-KLAXAD*&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample04]-7-Student-KLAXAD (Proposed; KLD + AuxLoss + AdvLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample04]-8-Student-KLAXAD (Proposed; weights optimized version).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Sample 5&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Ground truth&lt;/th&gt;&lt;th&gt;Teacher&lt;/th&gt;&lt;th&gt;Student-AX&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample05]-1-Ground truth.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample05]-2-Teacher.wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample05]-3-Student-AX (AuxLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Student-AXAV&lt;/th&gt;&lt;th&gt;Student-KL&lt;/th&gt;&lt;th&gt;Student-KLAX&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample05]-4-Student-AXAV (AuxLoss + AdvLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample05]-5-Student-KL (KLD only; ablation study).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample05]-6-Student-KLAX (KLD + AuxLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Student-KLAXAD&lt;/th&gt;&lt;th&gt;Student-KLAXAD*&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample05]-7-Student-KLAXAD (Proposed; KLD + AuxLoss + AdvLoss).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;
&lt;td&gt;&lt;audio controls=&#34;&#34;&gt;&lt;source src=&#34;https://r9y9.github.io/audio/interspeech2019/[Sample05]-8-Student-KLAXAD (Proposed; weights optimized version).wav&#34; type=&#34;audio/wav&#34;&gt;&lt;/audio&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;[1]: W. Ping, K. Peng, and J. Chen, “ClariNet: Parallel wave generation in end-to-end text-to-speech,” in Proc. ICLR, 2019 (&lt;a href=&#34;https://arxiv.org/abs/1807.07281&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;Work performed with nVoice, Clova Voice, Naver Corp.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;@inproceedings{Yamamoto2019,
  author={Ryuichi Yamamoto and Eunwoo Song and Jae-Min Kim},
  title={{Probability Density Distillation with Generative Adversarial Networks for High-Quality Parallel Waveform Generation}},
  year=2019,
  booktitle={Proc. Interspeech 2019},
  pages={699--703},
  doi={10.21437/Interspeech.2019-1965},
  url={http://dx.doi.org/10.21437/Interspeech.2019-1965}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title> WN-based TTSやりました / Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions [arXiv:1712.05884]</title>
      <link>https://r9y9.github.io/blog/2018/05/20/tacotron2/</link>
      <pubDate>Sun, 20 May 2018 14:21:30 +0900</pubDate>
      <guid>https://r9y9.github.io/blog/2018/05/20/tacotron2/</guid>
      <description>&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;Thank you for coming to see my blog post about WaveNet text-to-speech.&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron2/intro.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;ul&gt;
&lt;li&gt;論文リンク: &lt;a href=&#34;https://arxiv.org/abs/1712.05884&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/1712.05884&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;オンラインデモ: &lt;a href=&#34;https://colab.research.google.com/github/r9y9/Colaboratory/blob/master/Tacotron2_and_WaveNet_text_to_speech_demo.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tacotron2: WaveNet-based text-to-speech demo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;コード &lt;a href=&#34;https://github.com/r9y9/wavenet_vocoder&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/wavenet_vocoder&lt;/a&gt;, &lt;a href=&#34;https://github.com/Rayhane-mamah/Tacotron-2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rayhane-mamah/Tacotron-2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;音声サンプル: &lt;a href=&#34;https://r9y9.github.io/wavenet_vocoder/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://r9y9.github.io/wavenet_vocoder/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;三行まとめ&#34;&gt;三行まとめ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;自作WaveNet (&lt;strong&gt;WN&lt;/strong&gt;) と既存実装Tacotron 2 (WNを除く) を組み合わせて、英語TTSを作りました&lt;/li&gt;
&lt;li&gt;LJSpeechを学習データとした場合、自分史上 &lt;strong&gt;最高品質&lt;/strong&gt; のTTSができたと思います&lt;/li&gt;
&lt;li&gt;Tacotron 2と Deep Voice 3 のabstractを読ませた音声サンプルを貼っておきますので、興味のある方はどうぞ&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;なお、Tacotron 2 の解説はしません。申し訳ありません（なぜなら僕がまだ十分に読み込んでいないため）&lt;/p&gt;
&lt;h2 id=&#34;背景&#34;&gt;背景&lt;/h2&gt;
&lt;p&gt;過去に、WaveNetを実装しました（参考: &lt;a href=&#34;https://r9y9.github.io/blog/2018/01/28/wavenet_vocoder/&#34;&gt;WaveNet vocoder をやってみましたので、その記録です / WaveNet: A Generative Model for Raw Audio [arXiv:1609.03499]&lt;/a&gt;）。過去記事から引用します。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Tacotron2 は、あとはやればほぼできる感じですが、直近では僕の中で優先度が低めのため、しばらく実験をする予定はありません。興味のある方はやってみてください。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;やりたいことの一つとしてあったとはいえ、当初の予定通り、スクラッチでTacotron 2を実装する時間は取れなかったのですが、既存実装を使ってみたところ十分に上手く動いているように思えたので、ありがたく使わせていただき、WaveNet TTSを実現することができました。というわけで、結果をここにカジュアルに残しておこうという趣旨の記事になります。&lt;/p&gt;
&lt;p&gt;オープンなデータセット、コードを使って、実際どの程度の品質が得られるのか？学習/推論にどのくらい時間がかかるのか？いうのが気になる方には、参考になるかもしれませんので、よろしければ続きをどうぞ。&lt;/p&gt;
&lt;h2 id=&#34;実験条件&#34;&gt;実験条件&lt;/h2&gt;
&lt;p&gt;細かい内容はコードに譲るとして、重要な点だけリストアップします&lt;/p&gt;
&lt;h3 id=&#34;pre-trained-modelshyper-parameters-へのリンク&#34;&gt;Pre-trained models、hyper parameters へのリンク&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Tacotron2 (mel-spectrogram prediction part): trained 189k steps on LJSpeech dataset (&lt;a href=&#34;https://www.dropbox.com/s/vx7y4qqs732sqgg/pretrained.tar.gz?dl=0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pre-trained model&lt;/a&gt;, &lt;a href=&#34;https://github.com/r9y9/Tacotron-2/blob/9ce1a0e65b9217cdc19599c192c5cd68b4cece5b/hparams.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hyper params&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;WaveNet: trained over 1000k steps on LJSpeech dataset (&lt;a href=&#34;https://www.dropbox.com/s/zdbfprugbagfp2w/20180510_mixture_lj_checkpoint_step000320000_ema.pth?dl=0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pre-trained model&lt;/a&gt;, &lt;a href=&#34;https://www.dropbox.com/s/0vsd7973w20eskz/20180510_mixture_lj_checkpoint_step000320000_ema.json?dl=0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hyper params&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;wavenet&#34;&gt;WaveNet&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;1000k step以上訓練されたモデル (2018/1/27に作ったもの、10日くらい&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;学習した）をベースに、さらに 320k step学習（約3日）しました。再学習したのは、以前のコードには &lt;a href=&#34;https://github.com/r9y9/wavenet_vocoder/issues/33&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;wavenet_vocoder/issues/33&lt;/a&gt; こんなバグがあったからです。&lt;/li&gt;
&lt;li&gt;評価には、exponential moving averagingされたパラメータを使いました。decay パラメータはTaco2論文と同じ 0.9999&lt;/li&gt;
&lt;li&gt;学習には、Mel-spectrogram prediction networkにより出力される Ground-truth-aligned (GTA) なメルスペクトログラムではなく、生音声から計算されるメルスペクトログラムを使いました。時間の都合上そうしましたが、GTAを使うとより品質が向上すると考えられます&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tacotron-2-mel-spectrogram-prediction&#34;&gt;Tacotron 2 (mel-spectrogram prediction)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Rayhane-mamah/Tacotron-2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/Rayhane-mamah/Tacotron-2&lt;/a&gt; にはWaveNet実装も含まれていますが、mel-spectrogram prediction の部分だけ使用しました&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Rayhane-mamah/Tacotron-2/issues/30#issue-317360759&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/Rayhane-mamah/Tacotron-2/issues/30#issue-317360759&lt;/a&gt; で公開されている 182k step学習されたモデルを、さらに7k stepほど（数時間くらい）学習させました。再学習させた理由は、自分の実装とRayhane氏の実装で想定するメルスペクトログラムのレンジが異なっていたためです（僕: &lt;code&gt;[0, 1]&lt;/code&gt;, Rayhane: &lt;code&gt;[-4, 4]&lt;/code&gt;）。そういう経緯から、&lt;code&gt;[-4, 4]&lt;/code&gt; のレンジであったところ，&lt;code&gt;[0, 4]&lt;/code&gt; にして学習しなおしました。直接 &lt;code&gt;[0, 1]&lt;/code&gt; にして学習しなかったのは（それでも動く、と僕は思っているのですが）、mel-spectrogram のレンジを大きく取った方が良い、という報告がいくつかあったからです（例えば &lt;a href=&#34;https://github.com/Rayhane-mamah/Tacotron-2/issues/4#issuecomment-377728945&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/Rayhane-mamah/Tacotron-2/issues/4#issuecomment-377728945&lt;/a&gt; )。Attention seq2seq は経験上学習が難しいので、僕の直感よりも先人の知恵を優先することにした次第です。WNに入力するときには、 Taco2が出力するメルスペクトログラムを &lt;code&gt;c = np.interp(c, (0, 4), (0, 1))&lt;/code&gt; とレンジを変換して与えました&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;デモ音声&#34;&gt;デモ音声&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://r9y9.github.io/wavenet_vocoder/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://r9y9.github.io/wavenet_vocoder/&lt;/a&gt; にサンプルはたくさんあります。が、ここでは違うサンプルをと思い、Tacotron 2 と Deep Voice 3の abstract を読ませてみました。
学習データに若干残響が乗っているので（ノイズっぽい）それが反映されてしまっているのですが、個人的にはまぁまぁよい結果が得られたと思っています。興味がある方は、DeepVoice3など僕の過去記事で触れているTTS結果と比べてみてください。&lt;/p&gt;
&lt;p&gt;なお、推論の計算速度は,、僕のローカル環境（GTX 1080Ti, i7-7700K）でざっと 170 timesteps / second といった感じでした。これは、Parallel WaveNet の論文で触れられている数字とおおまかに一致します。&lt;/p&gt;
&lt;p&gt;This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text.&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron2/20180510_mixture_lj_checkpoint_step000320000_ema_speech-mel-00001.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize timedomain waveforms from those spectrograms.&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron2/20180510_mixture_lj_checkpoint_step000320000_ema_speech-mel-00002.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;Our model achieves a mean opinion score of 4.53 comparable to a MOS of 4.58 for professionally recorded speech.&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron2/20180510_mixture_lj_checkpoint_step000320000_ema_speech-mel-00003.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the input to WaveNet instead of linguistic, duration, and F0 features.&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron2/20180510_mixture_lj_checkpoint_step000320000_ema_speech-mel-00004.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;We further demonstrate that using a compact acoustic intermediate representation enables significant simplification of the WaveNet architecture.&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron2/20180510_mixture_lj_checkpoint_step000320000_ema_speech-mel-00005.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;We present Deep Voice 3, a fully-convolutional attention-based neural text-to-speech system.&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron2/20180510_mixture_lj_checkpoint_step000320000_ema_speech-mel-00006.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;Deep Voice 3 matches state-of-the-art neural speech synthesis systems in naturalness while training ten times faster.&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron2/20180510_mixture_lj_checkpoint_step000320000_ema_speech-mel-00007.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;We scale Deep Voice 3 to data set sizes unprecedented for TTS, training on more than eight hundred hours of audio from over two thousand speakers.&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron2/20180510_mixture_lj_checkpoint_step000320000_ema_speech-mel-00008.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;In addition, we identify common error modes of attention-based speech synthesis networks, demonstrate how to mitigate them, and compare several different waveform synthesis methods.&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron2/20180510_mixture_lj_checkpoint_step000320000_ema_speech-mel-00009.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;We also describe how to scale inference to ten million queries per day on one single-GPU server.&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron2/20180510_mixture_lj_checkpoint_step000320000_ema_speech-mel-00010.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;h2 id=&#34;オンラインデモ&#34;&gt;オンラインデモ&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/r9y9/Colaboratory/blob/master/Tacotron2_and_WaveNet_text_to_speech_demo.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tacotron2: WaveNet-based text-to-speech demo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Google Colabで動かせるようにデモノートブックを作りました。環境構築が不要なので、手軽にお試しできるかと思います。&lt;/p&gt;
&lt;h2 id=&#34;雑記&#34;&gt;雑記&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;WaveNetを学習するときに、Mel-spectrogram precition networkのGTAな出力でなく、生メルスペクトログラムをそのまま使っても品質の良い音声合成ができるのは個人的に驚きでした。これはつまり、Taco2が　(non teacher-forcingな条件で) 十分良いメルスペクトログラムを予測できている、ということなのだと思います。&lt;/li&gt;
&lt;li&gt;収束性を向上させるために、出力を127.5 倍するとよい、という件ですが、僕はやっていません。なぜなら、僕がまだこの方法の妥当性を理解できていないからです。&lt;a href=&#34;https://twitter.com/__dhgrs__/status/995962302896599040&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@__dhgrs__さんの報告&lt;/a&gt; によると、やはり有効に働くようですね…&lt;/li&gt;
&lt;li&gt;これまた &lt;a href=&#34;http://www.monthly-hack.com/entry/2018/02/23/203208&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@__dhgrs__さんのブログ記事&lt;/a&gt; にも書かれていますが、Mixture of Logistic distributions (MoLとします) を使った場合は、categoricalを考えてsoftmaxを使う場合に比べると十分な品質を得るのに大幅に計算時間が必要になりますね、、体験的には10倍程度です。計算にあまりに時間がかかるので、スクラッチで何度も学習するのは厳しく、学習済みモデルを何度も繰り返しfine turningしていくという、秘伝のタレ方式で学習を行いました（再現性なしです、懺悔）&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Rayhane-mamah/Tacotron-2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/Rayhane-mamah/Tacotron-2&lt;/a&gt; 今回使わせてもらったTaco2実装は、僕の実装も一部使われているようでした。これとは別の NVIDIA から出た &lt;a href=&#34;https://github.com/NVIDIA/tacotron2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/NVIDIA/tacotron2&lt;/a&gt; の謝辞には僕の名前を入れていただいていたり、他にもそういうケースがそれなりにあって、端的にいって光栄であり、うれしいお思いです。&lt;/li&gt;
&lt;li&gt;非公開のデータセットを使って学習/生成したWaveNet TTS のサンプルもあります。公開できないのでここにはあげていませんが、とても高品質な音声合成（主観ですが）ができることを確認しています&lt;/li&gt;
&lt;li&gt;このプロジェクトをはじめたことで、なんと光栄にも&lt;a href=&#34;http://www.nict.go.jp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NICT&lt;/a&gt;でのトークの機会をもらうことができました。オープソースについて是非はあると思いますが、個人的には良いことがとても多いなと思います。プレゼン資料は、https://github.com/r9y9/wavenet_vocoder/issues/57 に置いてあります（が、スライドだけで読み物として成立するものではないと思います、すみません）&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;
&lt;p&gt;WaveNet TTSをようやく作ることができました。Sample-levelでautoregressive modelを考えるというアプローチが本当に動かくのか疑問だったのですが、実際に作ってみて、上手く行くということを体感することができました。めでたし。&lt;/p&gt;
&lt;p&gt;Googleの研究者さま、素晴らしい研究をありがとうございます。WaveNetは本当にすごかった&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1609.03499&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Aaron van den Oord, Sander Dieleman, Heiga Zen, et al, &amp;ldquo;WaveNet: A Generative Model for Raw Audio&amp;rdquo;, 	arXiv:1609.03499, Sep 2016.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1711.10433&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Aaron van den Oord, Yazhe Li, Igor Babuschkin, et al, &amp;ldquo;Parallel WaveNet: Fast High-Fidelity Speech Synthesis&amp;rdquo;, 	arXiv:1711.10433, Nov 2017.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.isca-speech.org/archive/Interspeech_2017/pdfs/0314.PDF&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tamamori, Akira, et al. &amp;ldquo;Speaker-dependent WaveNet vocoder.&amp;rdquo; Proceedings of Interspeech. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1712.05884&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jonathan Shen, Ruoming Pang, Ron J. Weiss, et al, &amp;ldquo;Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions&amp;rdquo;, arXiv:1712.05884, Dec 2017.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1710.07654&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wei Ping, Kainan Peng, Andrew Gibiansky, et al, &amp;ldquo;Deep Voice 3: 2000-Speaker Neural Text-to-Speech&amp;rdquo;, arXiv:1710.07654, Oct. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1611.09482&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tom Le Paine, Pooya Khorrami, Shiyu Chang, et al, &amp;ldquo;Fast Wavenet Generation Algorithm&amp;rdquo;, arXiv:1611.09482, Nov. 2016&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.monthly-hack.com/entry/2018/02/23/203208&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VQ-VAEの追試で得たWaveNetのノウハウをまとめてみた。 - Monthly Hacker&amp;rsquo;s Blog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;曖昧な表現で申し訳ございません&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;僕が使った当時は、WNの部分は十分にテストされていなかったのと、WNのコードは僕のコードをtfにtranslateした感じな（著者がそういってます）ので、WNは自分の実装を使った次第です&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;</description>
    </item>
    
    <item>
      <title>【108 話者編】Deep Voice 3: 2000-Speaker Neural Text-to-Speech / arXiv:1710.07654 [cs.SD]</title>
      <link>https://r9y9.github.io/blog/2017/12/22/deepvoice3_multispeaker/</link>
      <pubDate>Fri, 22 Dec 2017 15:30:00 +0900</pubDate>
      <guid>https://r9y9.github.io/blog/2017/12/22/deepvoice3_multispeaker/</guid>
      <description>&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;論文リンク: &lt;a href=&#34;https://arxiv.org/abs/1710.07654&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1710.07654&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;コード: &lt;a href=&#34;https://github.com/r9y9/deepvoice3_pytorch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/deepvoice3_pytorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VCTK: &lt;a href=&#34;https://datashare.ed.ac.uk/handle/10283/2950&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://datashare.ed.ac.uk/handle/10283/2950&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;音声サンプルまとめ: &lt;a href=&#34;https://r9y9.github.io/deepvoice3_pytorch/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://r9y9.github.io/deepvoice3_pytorch/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;三行まとめ&#34;&gt;三行まとめ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1710.07654&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1710.07654: Deep Voice 3: 2000-Speaker Neural Text-to-Speech&lt;/a&gt; を読んで、複数話者の場合のモデルを実装しました&lt;/li&gt;
&lt;li&gt;論文のタイトル通りの2000話者とはいきませんが、&lt;a href=&#34;https://datashare.ed.ac.uk/handle/10283/2950&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VCTK&lt;/a&gt; を使って、108 話者対応の英語TTSモデルを作りました（学習時間1日くらい）&lt;/li&gt;
&lt;li&gt;入力する話者IDを変えることで、一つのモデルでバリエーションに富んだ音声サンプルを生成できることを確認しました&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;概要&#34;&gt;概要&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/12/13/deepvoice3/&#34;&gt;【単一話者編】Deep Voice 3: 2000-Speaker Neural Text-to-Speech / arXiv:1710.07654 [cs.SD]&lt;/a&gt; の続編です。&lt;/p&gt;
&lt;p&gt;論文概要は前回紹介したものと同じなので、話者の条件付けの部分についてのみ簡単に述べます。なお、話者の条件付けに関しては、DeepVoice2の論文 (&lt;a href=&#34;https://arxiv.org/abs/1705.08947&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1705.08947 [cs.CL]&lt;/a&gt;) の方が詳しいです。&lt;/p&gt;
&lt;p&gt;まず基本的に、話者の情報は trainable embedding としてモデルに組み込みます。text embeddingのうようにネットワークの入力の一箇所に入れるような設計では学習が上手くかない（話者情報を無視するようになってしまうのだと思います）ため、ネットワークのあらゆるところに入れるのがポイントのようです。具体的には、Encoder, Decoder (+ Attention), Converterのすべてに入れます。さらに具体的には、ネットワークの基本要素である Gated linear unit + Conv1d のすべてに入れます。詳細は論文に記載のarchitectureの図を参照してください。&lt;/p&gt;
&lt;p&gt;話者の条件付けに関して、一つ注意を加えるとすれば、本論文には明示的に書かれていませんが、 speaker embeddingは各時間stepすべてにexpandして用いるのだと思います（でないと実装するときに困る）。DeepVoice2の論文にはその旨が明示的に書かれています。&lt;/p&gt;
&lt;h2 id=&#34;vctk-の前処理&#34;&gt;VCTK の前処理&lt;/h2&gt;
&lt;p&gt;実験に入る前に、VCTKの前処理について、簡単にまとめたいと思います。VCTKの音声データには、数秒に渡る無音区間がそれなりに入っているので、それを取り除く必要があります。以前、&lt;a href=&#34;https://r9y9.github.io/blog/2017/11/12/jsut_ver1/&#34;&gt;日本語 End-to-end 音声合成に使えるコーパス JSUT の前処理&lt;/a&gt; で書いた内容と同じように、音素アライメントを取って無音区間を除去します。僕は以下の二つの方法をためしました。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/lowerquality/gentle&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gentle&lt;/a&gt; (&lt;a href=&#34;https://github.com/kaldi-asr/kaldi&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kaldi&lt;/a&gt;ベース)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/CSTR-Edinburgh/merlin&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Merlin&lt;/a&gt; 付属のアライメントツール (&lt;a href=&#34;http://festvox.org/cmu_arctic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;festvox&lt;/a&gt;ベース) (&lt;a href=&#34;https://gist.github.com/kastnerkyle/cc0ac48d34860c5bb3f9112f4d9a0300&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;便利スクリプト&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;論文中には、（無音除去のため、という文脈ではないのですが&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;）Gentleを使った旨が書かれています。しかし、試したところアライメントが失敗するケースがそれなりにあり、&lt;a href=&#34;https://github.com/facebookresearch/loop&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;loop&lt;/a&gt; は後者の方法を用いており良い結果も出ていることから、結論としては僕は後者を採用しました。なお、両方のコードは残してあるので、気になる方は両方ためしてみてください。&lt;/p&gt;
&lt;h2 id=&#34;実験&#34;&gt;実験&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://datashare.ed.ac.uk/handle/10283/2950&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VCTK&lt;/a&gt; の108話者分のすべて&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;を使用して、20時間くらい（30万ステップ x 2）学習しました。30万ステップ学習した後できたモデルをベースに、さらに30万ステップ学習しました&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;。モデルは、単一話者の場合とほとんど同じですが、変更を加えた点を以下にまとめます。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;共通&lt;/strong&gt;: Speaker embedding を追加しました。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;共通&lt;/strong&gt;: Speaker embeddingをすべての時間ステップにexpandしたあと、Dropoutを適用するようにしました（論文には書いていませんが、結論から言えば重要でした…）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Decoder&lt;/strong&gt;: アテンションのレイヤー数を2から1に減らしました&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;計算速度は、バッチサイズ16で、8.6 step/sec くらいでした。GPUメモリの使用量は9GB程度でした。Convolution BlockごとにLinearレイヤーが追加されるので、それなりにメモリ使用量が増えます。PyTorch v0.3.0を使いました。&lt;/p&gt;
&lt;p&gt;学習に使用したコマンドは以下です。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python train.py --data-root=./data/vctk --checkpoint-dir=checkpoints_vctk \
   --hparams=&amp;quot;preset=deepvoice3_vctk,builder=deepvoice3_multispeaker&amp;quot; \
   --log-event-path=log/deepvoice3_multispeaker_vctk_preset \
   --load-embedding=20171221_deepvoice3_checkpoint_step000300000.pth
 # &amp;lt;&amp;lt; 30万ステップで一旦打ち切り &amp;gt;&amp;gt;
 # もう一度0から30万ステップまで学習しなおし
 python train.py --data-root=./data/vctk --checkpoint-dir=checkpoints_vctk_fineturn \
   --hparams=&amp;quot;preset=deepvoice3_vctk,builder=deepvoice3_multispeaker&amp;quot; \
   --log-event-path=log/deepvoice3_multispeaker_vctk_preset_fine \
   --restore-parts=./checkpoints_vctk/checkpont_step000300000.pth
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;学習を高速化するため、LJSpeechで30万ステップ学習したモデルのembeddingの部分を再利用しました。また、cyclic annealingのような効果が得られることを期待して、一度学習を打ち切って、さらに0stepからファインチューニングしてみました。&lt;/p&gt;
&lt;p&gt;コードのコミットハッシュは &lt;a href=&#34;https://github.com/r9y9/deepvoice3_pytorch/tree/0421749af908905d181f089f06956fddd0982d47&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;0421749&lt;/a&gt; です。正確なハイパーパラメータが知りたい場合は、ここから辿れると思います。&lt;/p&gt;
&lt;h3 id=&#34;アライメントの学習過程-30万ステップ&#34;&gt;アライメントの学習過程 (~30万ステップ)&lt;/h3&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/deepvoice3_multispeaker/alignments.gif&#34; /&gt;&lt;/div&gt;
&lt;h3 id=&#34;学習された-speaker-embedding-の可視化&#34;&gt;学習された Speaker embedding の可視化&lt;/h3&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/deepvoice3_multispeaker/speaker_embedding.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;論文のappendixに書かれているのと同じように、学習されたEmbeddingに対してPCAをかけて可視化しました。論文の図とは少々異なりますが、期待通り、男女はほぼ線形分離できるようになっていることは確認できました。&lt;/p&gt;
&lt;h3 id=&#34;音声サンプル&#34;&gt;音声サンプル&lt;/h3&gt;
&lt;p&gt;最初に僕の感想を述べておくと、LJSpeechで単一話者モデルを学習した場合と比べると、汎化しにくい印象がありました。文字がスキップされるといったエラーケースも比較して多いように思いました。
たくさんサンプルを貼るのは大変なので、興味のある方は自分で適当な未知テキストを与えて合成してみてください。学習済みモデルは &lt;a href=&#34;https://github.com/r9y9/deepvoice3_pytorch#pretrained-models&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;deepvoice3_pytorch#pretrained-models&lt;/a&gt; からダウンロードできるようにしてあります。&lt;/p&gt;
&lt;h3 id=&#34;loophttpsytaigmangithubioloopnetwork-3-multiple-speakers-from-vctk-と同じ文章&#34;&gt;&lt;a href=&#34;https://ytaigman.github.io/loop/#network-3-multiple-speakers-from-vctk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Loop&lt;/a&gt; と同じ文章&lt;/h3&gt;
&lt;p&gt;Some have accepted this as a miracle without any physical explanation&lt;/p&gt;
&lt;p&gt;(69 chars, 11 words)&lt;/p&gt;
&lt;p&gt;speaker IDが若い順に12サンプルの話者ID を与えて、合成した結果を貼っておきます。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;225, 23,  F,    English,    Southern,  England&lt;/strong&gt; (ID, AGE,  GENDER,  ACCENTS,  REGION)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/loop/0_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker0.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;226,  22,  M,    English,    Surrey&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/loop/0_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker1.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;227,  38,  M,    English,    Cumbria&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/loop/0_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker2.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;228,  22,  F,    English,    Southern  England&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/loop/0_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker3.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;229,  23,  F,    English,    Southern  England&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/loop/0_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker4.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;230,  22,  F,    English,    Stockton-on-tees&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/loop/0_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker5.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;231,  23,  F,    English,    Southern  England&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/loop/0_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker6.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;232,  23,  M,    English,    Southern  England&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/loop/0_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker7.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;233,  23,  F,    English,    Staffordshire&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/loop/0_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker8.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;234,  22,  F,    Scottish,  West  Dumfries&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/loop/0_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker9.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;236,  23,  F,    English,    Manchester&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/loop/0_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker10.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;237,  22,  M,    Scottish,  Fife&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/loop/0_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker11.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;声質だけでなく、話速にもバリエーションが出ているのがわかります。&lt;code&gt;231&lt;/code&gt; の最初で一部音が消えています（こういったエラーケースはよくあります）。&lt;/p&gt;
&lt;h4 id=&#34;keithitotacotron-のサンプルhttpskeithitogithubioaudio-samples-と同じ文章&#34;&gt;&lt;a href=&#34;https://keithito.github.io/audio-samples/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron のサンプル&lt;/a&gt; と同じ文章&lt;/h4&gt;
&lt;p&gt;簡単に汎化性能をチェックするために、未知文章でテストします。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;男性 (292,  23,  M,    NorthernIrish,  Belfast)&lt;/li&gt;
&lt;li&gt;女性 (288,  22,  F,    Irish,  Dublin)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;の二つのサンプルを貼っておきます。&lt;/p&gt;
&lt;p&gt;Scientists at the CERN laboratory say they have discovered a new particle.&lt;/p&gt;
&lt;p&gt;(74 chars, 13 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/0_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker62.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/0_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker62_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/0_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker61.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/0_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker61_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;There&amp;rsquo;s a way to measure the acute emotional intelligence that has never gone out of style.&lt;/p&gt;
&lt;p&gt;(91 chars, 18 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/1_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker62.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/1_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker62_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/1_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker61.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/1_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker61_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;President Trump met with other leaders at the Group of 20 conference.&lt;/p&gt;
&lt;p&gt;(69 chars, 13 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/2_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker62.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/2_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker62_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/2_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker61.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/2_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker61_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;The Senate&amp;rsquo;s bill to repeal and replace the Affordable Care Act is now imperiled.&lt;/p&gt;
&lt;p&gt;(81 chars, 16 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/3_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker62.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/3_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker62_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/3_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker61.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/3_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker61_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;Generative adversarial network or variational auto-encoder.&lt;/p&gt;
&lt;p&gt;(59 chars, 7 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/4_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker62.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/4_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker62_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/4_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker61.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/4_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker61_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;The buses aren&amp;rsquo;t the problem, they actually provide a solution.&lt;/p&gt;
&lt;p&gt;(63 chars, 13 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/5_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker62.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/5_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker62_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/5_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker61.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3_multispeaker/3_keithito/5_20171222_deepvoice3_vctk108_checkpoint_step000300000_speaker61_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;ところどころ音が抜けているのが目立ちます。色々実験しましたが、やはり単一話者 24hのデータで学習したモデルに比べると、一話者あたり30分~1h程度のデータでは、汎化させるのが難しい印象を持ちました。&lt;/p&gt;
&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;複数話者版のDeepVoice3を実装して、実際に108話者のデータセットで学習し、それなりに動くことを確認できました&lt;/li&gt;
&lt;li&gt;複数話者版のDeepVoice3では、アテンションの学習が単一話者の場合と比べて難しい印象でした。アテンションレイヤーの数を2から1に減らすと、アライメントがくっきりする傾向にあることを確認しました。&lt;/li&gt;
&lt;li&gt;VCTKの前処理大事、きちんとしましょう&lt;/li&gt;
&lt;li&gt;Speaker embedding にDropoutをかけるのは、論文には記載されていませんが、結果から言って重要でした。ないと、音声の品質以前の問題として、文字が正しく発音されない、といった現象に遭遇しました。&lt;/li&gt;
&lt;li&gt;Speaker embedding をすべての時刻に同一の値をexpandしてしまうと過学習しやすいのではないかいう予測を元に、各時刻でランダム性をいれることでその問題を緩和できないかと考え、Dropoutを足してみました。上手く言ったように思います&lt;/li&gt;
&lt;li&gt;論文の内容について詳しく触れていませんが、実はけっこう雑というか、文章と図に不一致があったりします（例えば図1にあるEncoder PreNet/PostNet は文章中で説明がない）。著者に連絡して確認するのが一番良いのですが、どういうモデルなら上手くいくか考えて試行錯誤するのも楽しいので、今回は雰囲気で実装しました。それなりに上手く動いているように思います&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;次は、DeepVoice3、Tacotron 2 (&lt;a href=&#34;https://arxiv.org/abs/1712.0588&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1712.05884 [cs.CL]&lt;/a&gt;) で有効性が示されている WaveNet Vocoder を実装して、品質を改善してみようと思っています。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1710.07654&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wei Ping, Kainan Peng, Andrew Gibiansky, et al, &amp;ldquo;Deep Voice 3: 2000-Speaker Neural Text-to-Speech&amp;rdquo;, arXiv:1710.07654, Oct. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1705.08947&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sercan Arik, Gregory Diamos, Andrew Gibiansky,, et al, &amp;ldquo;Deep Voice 2: Multi-Speaker Neural Text-to-Speech&amp;rdquo;, arXiv:1705.08947, May 2017.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1712.05884&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jonathan Shen, Ruoming Pang, Ron J. Weiss, et al, &amp;ldquo;Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions&amp;rdquo;, arXiv:1712.05884, Dec 2017.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;関連記事&#34;&gt;関連記事&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/12/13/deepvoice3/&#34;&gt;【単一話者編】Deep Voice 3: 2000-Speaker Neural Text-to-Speech / arXiv:1710.07654 [cs.SD] | LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/11/23/dctts/&#34;&gt;Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention. [arXiv:1710.08969] | LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/11/12/jsut_ver1/&#34;&gt;日本語 End-to-end 音声合成に使えるコーパス JSUT の前処理 | LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;VCTKの無音区間除去のためという文脈ではなく、テキストにshort pause / long pause を挿入するためです&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;transcriptionがない1話者 (p315) のデータは除いています&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Dropoutをきつくするとロスが下がりにくく、一方でゆるくすると汎化しにくい印象がありました。ので、Dropoutきつめである程度汎化させたあと、Dropoutをゆるめにしてfine turningする、といった戦略を取ってみました。&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>【単一話者編】Deep Voice 3: 2000-Speaker Neural Text-to-Speech / arXiv:1710.07654 [cs.SD]</title>
      <link>https://r9y9.github.io/blog/2017/12/13/deepvoice3/</link>
      <pubDate>Wed, 13 Dec 2017 12:15:00 +0900</pubDate>
      <guid>https://r9y9.github.io/blog/2017/12/13/deepvoice3/</guid>
      <description>&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;論文リンク: &lt;a href=&#34;https://arxiv.org/abs/1710.07654&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1710.07654&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;コード: &lt;a href=&#34;https://github.com/r9y9/deepvoice3_pytorch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/deepvoice3_pytorch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;三行まとめ&#34;&gt;三行まとめ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1710.07654&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1710.07654: Deep Voice 3: 2000-Speaker Neural Text-to-Speech&lt;/a&gt; を読んで、単一話者の場合のモデルを実装しました（複数話者の場合は、今実験中です (&lt;a href=&#34;https://github.com/r9y9/deepvoice3_pytorch/pull/6&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;deepvoice3_pytorch/#6&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1710.08969&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1710.08969&lt;/a&gt; と同じく、RNNではなくCNNを使うのが肝です&lt;/li&gt;
&lt;li&gt;例によって &lt;a href=&#34;https://keithito.com/LJ-Speech-Dataset/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LJSpeech Dataset&lt;/a&gt; を使って、英語TTSモデルを作りました（学習時間半日くらい）。論文に記載のハイパーパラメータでは良い結果が得られなかったのですが、&lt;a href=&#34;https://arxiv.org/abs/1710.08969&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1710.08969&lt;/a&gt; のアイデアをいくつか借りることで、良い結果を得ることができました。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;概要&#34;&gt;概要&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/11/23/dctts/&#34;&gt;Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention. [arXiv:1710.08969]&lt;/a&gt; で紹介した方法と、モチベーション、基本的な方法論はまったく同じのため省略します。モデルのアーキテクチャが異なりますが、その点についても前回述べたので、そちらを参照ください。
今回の記事では、DeepVoice3のアーキテクチャをベースにした方法での実験結果をまとめます。&lt;/p&gt;
&lt;h2 id=&#34;予備実験&#34;&gt;予備実験&lt;/h2&gt;
&lt;p&gt;はじめに、可能な限り論文に忠実に、論文に記載のモデルアーキテクチャ、ハイパーパラメータで、レイヤー数やConvレイヤーのカーネル数を若干増やしたモデルで試しました。（増やさないと、LJSpeechではイントネーションが怪しい音声が生成されてしまいました）。しかし、どうもビブラートがかかったような音声が生成される傾向にありました。色々試行錯誤して改良したのですが、詳細は後述するとして、改良前/改良後の音声サンプルを以下に示します。&lt;/p&gt;
&lt;p&gt;Generative adversarial network or variational auto-encoder.&lt;/p&gt;
&lt;p&gt;(59 chars, 7 words)&lt;/p&gt;
&lt;p&gt;改良前：&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3/yobi/3_checkpoint_step000530000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;改良後：&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3/yobi/4_checkpoint_step000210000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;いかがでしょうか。結構違いますよね。なお、改良前のモデルは53万イテレーション、改良後は21万イテレーション学習しました。回数を増やせばいいというものではないようです（当たり前ですが）。結論からいうと、モデルの自由度が足りなかったのが品質が向上しにくかった原因ではないかと考えています。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2017/12/21 追記&lt;/strong&gt;：すいません、21万イテレーションのモデルは、何かしら別の事前学習したモデルから、さらに学習したような気がしてきました…。ただ、合計で53万もイテレーションしていないのは間違いないと思います申し訳ございません&lt;/p&gt;
&lt;h2 id=&#34;実験&#34;&gt;実験&lt;/h2&gt;
&lt;p&gt;前回と同じく &lt;a href=&#34;https://keithito.com/LJ-Speech-Dataset/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LJSpeech Dataset&lt;/a&gt; を使って、11時間くらい（21万ステップ）学習しました。モデルは、DeepVoice3で提案されているものを少しいじりました。どのような変更をしたのか、以下にまとめます。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Encoder&lt;/strong&gt;: レイヤー数を増やし、チャンネル数を大きくしました。代わりにカーネル数は7から3に減らしました&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Decoder&lt;/strong&gt;: メル周波数スペクトログラムの複数フレームをDecoderの1-stepで予測するのではなく、&lt;a href=&#34;https://arxiv.org/abs/1710.08969&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1710.08969&lt;/a&gt; で述べられているように、1-stepで（粗い）1フレームを予測して、ConvTransposed1d により元の時間解像度までアップサンプリングする（要は時間方向のアップサンプリングをモデルで学習する）ようにしました&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Decoder&lt;/strong&gt;: アテンションの前に、いくつかConv1d + ReLUを足しました&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Converter&lt;/strong&gt;: ConvTransposed1dを二つ入れて、時間解像度を4倍にアップサンプリングするようにしました&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Converter&lt;/strong&gt;: チャンネル数を大きくしました&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Decoder/Converter&lt;/strong&gt;: レイヤーの最後にSigmoidを追加しました&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Loss&lt;/strong&gt;: Guided attention lossを加えました&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Loss&lt;/strong&gt;: Binary divergenceを加えました&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;共通&lt;/strong&gt;: Linearを1x1 convolutionに変えました。Dilationを大きくとりました&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上記変更点について、本来ならば、Extensiveに実験して、どれがどの程度有効か調べるのが一番良いのですが、計算資源の都合により、部分的にしかやっていません（すいません）。部分的とはいえ、わかったことは最後にまとめておきます。&lt;/p&gt;
&lt;p&gt;計算速度は、バッチサイズ16で、5.3 step/sec くらいの計算速度でした。&lt;a href=&#34;https://arxiv.org/abs/1710.08969&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1710.08969&lt;/a&gt; よりは若干速いくらいです。GPUメモリの使用量は5 ~ 6GB程度でした。PyTorch v0.3.0を使いました。&lt;/p&gt;
&lt;p&gt;学習に使用したコマンドは以下です。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python train.py --checkpoint-dir=checkpoints_deepvoice3 \
    --hparams=&amp;quot;use_preset=True,builder=deepvoice3&amp;quot; \
    --log-event-path=log/deepvoice3_preset
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;コードのコミットハッシュは &lt;a href=&#34;https://github.com/r9y9/deepvoice3_pytorch/tree/7bcf1d070448b4127b41bdf3a1e34c9fea382054&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;7bcf1d0704&lt;/a&gt; です。正確なハイパーパラメータが知りたい場合は、ここから辿れると思います。&lt;/p&gt;
&lt;h3 id=&#34;アライメントの学習過程&#34;&gt;アライメントの学習過程&lt;/h3&gt;
&lt;p&gt;今回の実験ではアテンションレイヤーは二つ（最初と最後）ありますが、以下に平均を取ったものを示します。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/deepvoice3/alignment.gif&#34; /&gt;&lt;/div&gt;
&lt;h3 id=&#34;各種ロスの遷移&#34;&gt;各種ロスの遷移&lt;/h3&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/deepvoice3/deepvoice3_tensorboard.png&#34; /&gt;&lt;/div&gt;
&lt;h3 id=&#34;音声サンプル&#34;&gt;音声サンプル&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/11/23/dctts/&#34;&gt;前回の記事&lt;/a&gt; で貼ったサンプルとまったく同じ文章を用いました。興味のある方は聴き比べてみてください。&lt;/p&gt;
&lt;h4 id=&#34;httpstachi-higithubiotts_samples-より&#34;&gt;&lt;a href=&#34;https://tachi-hi.github.io/tts_samples/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://tachi-hi.github.io/tts_samples/&lt;/a&gt; より&lt;/h4&gt;
&lt;p&gt;icassp stands for the international conference on acoustics, speech and signal processing.&lt;/p&gt;
&lt;p&gt;(90 chars, 14 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3/nyanko/0_checkpoint_step000210000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3/nyanko/0_checkpoint_step000210000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;a matrix is positive definite, if all eigenvalues are positive.&lt;/p&gt;
&lt;p&gt;(63 chars, 12 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3/nyanko/2_checkpoint_step000210000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3/nyanko/2_checkpoint_step000210000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;a spectrogram is obtained by applying es-tee-ef-tee to a signal.&lt;/p&gt;
&lt;p&gt;(64 chars, 11 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3/nyanko/6_checkpoint_step000210000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3/nyanko/6_checkpoint_step000210000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;h4 id=&#34;keithitotacotron-のサンプルhttpskeithitogithubioaudio-samples-と同じ文章&#34;&gt;&lt;a href=&#34;https://keithito.github.io/audio-samples/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron のサンプル&lt;/a&gt; と同じ文章&lt;/h4&gt;
&lt;p&gt;Scientists at the CERN laboratory say they have discovered a new particle.&lt;/p&gt;
&lt;p&gt;(74 chars, 13 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3/3_keithito/0_checkpoint_step000210000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3/3_keithito/0_checkpoint_step000210000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;There&amp;rsquo;s a way to measure the acute emotional intelligence that has never gone out of style.&lt;/p&gt;
&lt;p&gt;(91 chars, 18 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3/3_keithito/1_checkpoint_step000210000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3/3_keithito/1_checkpoint_step000210000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;President Trump met with other leaders at the Group of 20 conference.&lt;/p&gt;
&lt;p&gt;(69 chars, 13 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3/3_keithito/2_checkpoint_step000210000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3/3_keithito/2_checkpoint_step000210000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;The Senate&amp;rsquo;s bill to repeal and replace the Affordable Care Act is now imperiled.&lt;/p&gt;
&lt;p&gt;(81 chars, 16 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3/3_keithito/3_checkpoint_step000210000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3/3_keithito/3_checkpoint_step000210000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;Generative adversarial network or variational auto-encoder.&lt;/p&gt;
&lt;p&gt;(59 chars, 7 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3/3_keithito/4_checkpoint_step000210000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3/3_keithito/4_checkpoint_step000210000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;The buses aren&amp;rsquo;t the problem, they actually provide a solution.&lt;/p&gt;
&lt;p&gt;(63 chars, 13 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/deepvoice3/3_keithito/5_checkpoint_step000210000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/deepvoice3/3_keithito/5_checkpoint_step000210000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;
&lt;p&gt;以下、知見をまとめますが、あくまでその傾向がある、という程度に受け止めてください。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tacotron, DeepVoice3で述べられているようにメル周波数スペクトログラムの複数フレームをDecoderの1-stepで予測するよりも、&lt;a href=&#34;https://arxiv.org/abs/1710.08969&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1710.08969&lt;/a&gt; で述べられているように、1-stepで（粗い）1フレームを予測して、ConvTransposed1d により元の時間解像度までアップサンプリングする方が良い。生成された音声のビブラートのような現象が緩和されるように感じた&lt;/li&gt;
&lt;li&gt;Dilationを大きくしても、大きな品質の変化はないように感じた&lt;/li&gt;
&lt;li&gt;Guided-attentionは、アテンションが早くmonotonicになるという意味で良い。ただし、品質に大きな影響はなさそうに感じた&lt;/li&gt;
&lt;li&gt;Encoderのレイヤー数を大きくするのは効果あり&lt;/li&gt;
&lt;li&gt;Converterのチャンネル数を大きくするのは効果あり&lt;/li&gt;
&lt;li&gt;Binary divergence lossは、学習を安定させるために、DeepVoice3風のアーキテクチャでも有効だった&lt;/li&gt;
&lt;li&gt;Encoder/Converterは &lt;a href=&#34;https://arxiv.org/abs/1710.08969&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1710.08969&lt;/a&gt; のものを、DecoderはDeepVoice3のものを、というパターンで試したことがありますが、&lt;a href=&#34;https://arxiv.org/abs/1710.08969&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1710.08969&lt;/a&gt;に比べて若干品質が落ちたように感じたものの、ほぼ同等と言えるような品質が得られました。&lt;a href=&#34;https://arxiv.org/abs/1710.08969&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1710.08969&lt;/a&gt; ではDecoderに20レイヤー以上使っていますが、10未満でもそれなりの品質になったように思います（上で貼った音声サンプルがまさにその例です）&lt;/li&gt;
&lt;li&gt;品質を改良するために、&lt;a href=&#34;https://arxiv.org/abs/1710.08969&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1710.08969&lt;/a&gt; から色々アイデアを借りましたが、逆にDeepVoice3のアイデアで良かったと思えるものに、Decoderの入力に、(メル周波数の次元まで小さくして、Sigmoidを通して得られる）メル周波数スペクトログラムを使うのではなくその前のhidden stateを使う、といったことがありました。勾配がサチりやすいSigmoidをかまないからか、スペクトログラムに対するL1 Lossの減少が確実に速くなりました (&lt;a href=&#34;https://github.com/r9y9/deepvoice3_pytorch/commit/22a674803f2994af2b818635a0501e4417834936&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;22a6748&lt;/a&gt;)。&lt;/li&gt;
&lt;li&gt;この記事に貼った音声サンプルにおいて、先頭のaが抜けている例が目立ちますが、過去にやった実験ではこういう例は稀だったので、何かハイパーパラメータを誤っていじったんだと思います（闇&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1710.07654&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wei Ping, Kainan Peng, Andrew Gibiansky, et al, &amp;ldquo;Deep Voice 3: 2000-Speaker Neural Text-to-Speech&amp;rdquo;, arXiv:1710.07654, Oct. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1705.03122&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jonas Gehring, Michael Auli, David Grangier, et al, &amp;ldquo;Convolutional Sequence to Sequence Learning&amp;rdquo;, arXiv:1705.03122, May 2017.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/11/23/dctts/&#34;&gt;Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention. [arXiv:1710.08969] | LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;関連記事&#34;&gt;関連記事&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/11/23/dctts/&#34;&gt;Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention. [arXiv:1710.08969] | LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention. [arXiv:1710.08969]</title>
      <link>https://r9y9.github.io/blog/2017/11/23/dctts/</link>
      <pubDate>Thu, 23 Nov 2017 19:30:00 +0900</pubDate>
      <guid>https://r9y9.github.io/blog/2017/11/23/dctts/</guid>
      <description>&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;論文リンク: &lt;a href=&#34;https://arxiv.org/abs/1710.08969&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1710.08969&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;コード: &lt;a href=&#34;https://github.com/r9y9/deepvoice3_pytorch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/deepvoice3_pytorch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;三行まとめ&#34;&gt;三行まとめ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1710.08969&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1710.08969: Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention.&lt;/a&gt; を読んで、実装しました&lt;/li&gt;
&lt;li&gt;RNNではなくCNNを使うのが肝で、オープンソースTacotronと同等以上の品質でありながら、&lt;strong&gt;高速に (一日程度で) 学習できる&lt;/strong&gt; のが売りのようです。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://keithito.com/LJ-Speech-Dataset/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LJSpeech Dataset&lt;/a&gt; を使って、英語TTSモデルを作りました（学習時間一日くらい）。完全再現とまではいきませんが、大まかに論文の主張を確認できました。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;前置き&#34;&gt;前置き&lt;/h2&gt;
&lt;p&gt;本当は &lt;a href=&#34;https://arxiv.org/abs/1710.07654&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepVoice3&lt;/a&gt; の実装をしていたのですが、どうも上手くいかなかったので気分を変えてやってみました。
以前 Tacotronに関する長いブログ記事 (&lt;a href=&#34;https://r9y9.github.io/blog/2017/10/15/tacotron/&#34;&gt;リンク&lt;/a&gt;) を書いてしまったのですが、読む方も書く方もつらいので、簡潔にまとめることにしました。興味のある人は続きもどうぞ。&lt;/p&gt;
&lt;h2 id=&#34;概要&#34;&gt;概要&lt;/h2&gt;
&lt;p&gt;End-to-endテキスト音声合成 (Text-to-speech synthesis; TTS) のための &lt;strong&gt;Attention付き畳み込みニューラルネット (CNN)&lt;/strong&gt; が提案されています。SampleRNN, Char2Wav, Tacotronなどの従来提案されてきたRNNをベースとする方法では、モデルの構造上計算が並列化しにくく、
学習/推論に時間がかかることが問題としてありました。本論文では、主に以下の二つのアイデアによって、従来法より速く学習できるモデルを提案しています。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;RNNではなくCNNを使うこと (参考論文: &lt;a href=&#34;https://arxiv.org/abs/1705.03122&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1705.03122&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Attentionがmotonicになりやすくする効果を持つLossを考えること (&lt;strong&gt;Guided attention&lt;/strong&gt;)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;実験では、オープンソースTacotron (&lt;a href=&#34;https://github.com/keithito/tacotron&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron&lt;/a&gt;) の12日学習されたモデルと比較し、主観評価により同等以上の品質が得られたことが示されています。&lt;/p&gt;
&lt;h3 id=&#34;deepvoice3httpsarxivorgabs171007654-との違い&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/1710.07654&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepVoice3&lt;/a&gt; との違い&lt;/h3&gt;
&lt;p&gt;ほぼ同時期に発表されたDeepVoice3も同じく、CNNをベースとするものです。論文を読みましたが、モチベーションとアプローチの基本は DeepVoice3 と同じに思いました。しかし、ネットワーク構造は DeepVoice3とは大きく異なります。いくつか提案法の特徴を挙げると、以下のとおりです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ネットワークが深い（DeepVoice3だとEncoder, Decoder, Converter それぞれ10未満ですが、この論文ではDecoderだけで20以上）。すべてにおいて深いです。カーネルサイズは3と小さいです&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;Fully-connected layer ではなく1x1 convolutionを使っています&lt;/li&gt;
&lt;li&gt;チャンネル数が大きい（256とか512とか、さらにネットワーク内で二倍になっていたりする）。DeepVoice3だとEncoderは64です&lt;/li&gt;
&lt;li&gt;レイヤーの深さに対して指数上に大きくなるDilationを使っています（DeepVoiceではすべてdilation=1）&lt;/li&gt;
&lt;li&gt;アテンションレイヤーは一つ（DeepVoice3は複数&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;DeepVoice3は、&lt;a href=&#34;https://arxiv.org/abs/1705.03122&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1705.03122&lt;/a&gt; のモデル構造とかなり似通っている一方で、本論文では（参考文献としてあげられていますが）影も形もないくらい変わっている、という印象を受けます。&lt;/p&gt;
&lt;p&gt;ロスに関しては、Guided attentionに関するロスが加わるのに加えて、TacotronやDeepVoice3とは異なり、スペクトログラム/メルスペクトログラムに関して binary divergence (定義は論文参照) をロスに加えているという違いがあります。&lt;/p&gt;
&lt;h2 id=&#34;実験&#34;&gt;実験&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://keithito.com/LJ-Speech-Dataset/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LJSpeech Dataset&lt;/a&gt; を使って、17時間くらい（26.5万ステップ）学習しました。計算資源の都合上、SSRNのチャンネル数は512ではなくその半分の256にしました。&lt;/p&gt;
&lt;p&gt;なお、実装するにあたっては、厳密に再現しようとはせず、色々雰囲気でごまかしました。もともとDeepVoice3の実装をしていたのもあり、アイデアをいくつか借りています。例えば、デコーダの出力をいつ止めるか、というdone flag predictionをネットワークに入れています。Dropoutについて言及がありませんが、ないと汎化しにくい印象があったので&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;、足しました。&lt;/p&gt;
&lt;p&gt;計算速度は、バッチサイズ16で、4.3 step/sec くらいの計算速度でした。僕のマシンのGPUはGTX 1080Ti です。使用したハイパーパラメータは&lt;a href=&#34;https://github.com/r9y9/deepvoice3_pytorch/blob/70dc880fae185d96effaee97f0ce55b5c0d13b61/hparams.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;こちら&lt;/a&gt;です。学習に使用したコマンドは以下です（メモ）。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python train.py --data-root=./data/ljspeech --checkpoint-dir=checkpoints_nyanko \
    --hparams=&amp;quot;use_preset=True,builder=nyanko&amp;quot; \
    --log-event-path=log/nyanko_preset
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;アライメントの学習過程&#34;&gt;アライメントの学習過程&lt;/h3&gt;
&lt;p&gt;数万ステップで、綺麗にmonotonicになりました。GIFは、同じ音声に対するアライメントではなく、毎度違う（ランダムな）音声サンプルに対するアライメントを計算して、くっつけたものです（わかりずらくすいません&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/dctts/alignment.gif&#34; /&gt;&lt;/div&gt;
&lt;h3 id=&#34;各種ロスの遷移&#34;&gt;各種ロスの遷移&lt;/h3&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/dctts/dctts_tensorboard.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;見づらくて申し訳ありませんという感じですが、僕のための簡易ログということで貼っておきます。binary divergenceは、すぐに収束したようでした。&lt;/p&gt;
&lt;h3 id=&#34;音声サンプル&#34;&gt;音声サンプル&lt;/h3&gt;
&lt;h4 id=&#34;公式音声サンプルhttpstachi-higithubiotts_samples-と同じ文章抜粋&#34;&gt;&lt;a href=&#34;https://tachi-hi.github.io/tts_samples/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;公式音声サンプル&lt;/a&gt; と同じ文章（抜粋）&lt;/h4&gt;
&lt;p&gt;公式サンプルとの比較です。11/23時点で、公式のサンプル数が15個と多いので、適当に3つ選びました。公式と比べると少し異なっている印象を受けますが、まぁまぁ良いかなと思いました（曖昧ですが&lt;/p&gt;
&lt;p&gt;icassp stands for the international conference on acoustics, speech and signal processing.&lt;/p&gt;
&lt;p&gt;(90 chars, 14 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nyanko/6_nyanko/0_checkpoint_step000265000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/nyanko/6_nyanko/0_checkpoint_step000265000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;a matrix is positive definite, if all eigenvalues are positive.&lt;/p&gt;
&lt;p&gt;(63 chars, 12 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nyanko/6_nyanko/2_checkpoint_step000265000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/nyanko/6_nyanko/2_checkpoint_step000265000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;a spectrogram is obtained by applying es-tee-ef-tee to a signal.&lt;/p&gt;
&lt;p&gt;(64 chars, 11 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nyanko/6_nyanko/6_checkpoint_step000265000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/nyanko/6_nyanko/6_checkpoint_step000265000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;h4 id=&#34;keithitotacotron-のサンプルhttpskeithitogithubioaudio-samples-と同じ文章&#34;&gt;&lt;a href=&#34;https://keithito.github.io/audio-samples/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron のサンプル&lt;/a&gt; と同じ文章&lt;/h4&gt;
&lt;p&gt;Scientists at the CERN laboratory say they have discovered a new particle.&lt;/p&gt;
&lt;p&gt;(74 chars, 13 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nyanko/3_keithito/0_checkpoint_step000265000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/nyanko/3_keithito/0_checkpoint_step000265000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;There&amp;rsquo;s a way to measure the acute emotional intelligence that has never gone out of style.&lt;/p&gt;
&lt;p&gt;(91 chars, 18 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nyanko/3_keithito/1_checkpoint_step000265000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/nyanko/3_keithito/1_checkpoint_step000265000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;President Trump met with other leaders at the Group of 20 conference.&lt;/p&gt;
&lt;p&gt;(69 chars, 13 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nyanko/3_keithito/2_checkpoint_step000265000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/nyanko/3_keithito/2_checkpoint_step000265000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;The Senate&amp;rsquo;s bill to repeal and replace the Affordable Care Act is now imperiled.&lt;/p&gt;
&lt;p&gt;(81 chars, 16 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nyanko/3_keithito/3_checkpoint_step000265000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/nyanko/3_keithito/3_checkpoint_step000265000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;Generative adversarial network or variational auto-encoder.&lt;/p&gt;
&lt;p&gt;(59 chars, 7 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nyanko/3_keithito/4_checkpoint_step000265000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/nyanko/3_keithito/4_checkpoint_step000265000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;The buses aren&amp;rsquo;t the problem, they actually provide a solution.&lt;/p&gt;
&lt;p&gt;(63 chars, 13 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nyanko/3_keithito/5_checkpoint_step000265000.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/nyanko/3_keithito/5_checkpoint_step000265000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;h2 id=&#34;まとめ--わかったことなど&#34;&gt;まとめ &amp;amp; わかったことなど&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Tacotronでは学習に何日もかかっていましたが（計算も遅く1日で10万step程度）、1日でそれなりの品質になりました。&lt;/li&gt;
&lt;li&gt;Guided atetntionがあると、確かに速くattentionがmonotonicになりました。&lt;/li&gt;
&lt;li&gt;2時間程度の学習では &lt;a href=&#34;https://tachi-hi.github.io/tts_samples/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ここ&lt;/a&gt; にあるのと同程度の品質にはなりませんでした…&lt;/li&gt;
&lt;li&gt;DeepVoice3のモデルアーキテクチャで学習した場合と比べると、品質は向上しました&lt;/li&gt;
&lt;li&gt;DeepVoice3と比べると、深いせいなのか学習が難しいように思いました。重みの初期化のパラメータをちょっといじると、sigmoidの出力が0 or 1になって学習が止まる、といったことがありました。重みの初期化はとても重要でした&lt;/li&gt;
&lt;li&gt;上記にも関連して、勾配のノルムが爆発的に大きくなることがしばしばあり、クリッピングを入れました（重要でした）&lt;/li&gt;
&lt;li&gt;Binary divergenceをロスにいれても品質には影響がないように感じました。ただしないと学習初期に勾配が爆発しやすかったです&lt;/li&gt;
&lt;li&gt;提案法は色々なアイデアが盛り込まれているのですが、実際のところどれが重要な要素なのか、といった点に関しては、論文では明らかにされていなかったように思います。今後その辺りを明らかにする論文があってもいいのではないかと思いました。&lt;/li&gt;
&lt;li&gt;学習に使うGPUメモリ量、Tacotronより多い（SSRNのチャンネル数512, バッチサイズ16で &lt;del&gt;8GBくらい&lt;/del&gt; 5~6GB くらいでした）……厳しい……&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;2017/12/19追記: Dropoutなしだと、入力テキストとは無縁の英語らしき何かが生成されるようになってしまいました。Dropoutはやはり重要でした&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一番の学びは、ネットワークの重みの初期化方法は重要、ということでした。おしまい&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1710.08969&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hideyuki Tachibana, Katsuya Uenoyama, Shunsuke Aihara, &amp;ldquo;Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention&amp;rdquo;. arXiv:1710.08969, Oct 2017.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1710.07654&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wei Ping, Kainan Peng, Andrew Gibiansky, et al, &amp;ldquo;Deep Voice 3: 2000-Speaker Neural Text-to-Speech&amp;rdquo;, arXiv:1710.07654, Oct. 2017.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1705.03122&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jonas Gehring, Michael Auli, David Grangier, et al, &amp;ldquo;Convolutional Sequence to Sequence Learning&amp;rdquo;, arXiv:1705.03122, May 2017.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;He, Kaiming, et al. &amp;ldquo;Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.&amp;rdquo; Proceedings of the IEEE international conference on computer vision. 2015.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;DeepVoice3でカーネルサイズ3で試すと、全然うまくいきませんでした&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;推論時にアテンションの制約をいれても、「ふぁふぁふぁふぁふぁ」みたいな繰り返しが起きてしまいました&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;論文ではエンコーダデコーダの学習とSRNNの学習を別々でおこなっていますが、僕は一緒にやりました。そのせいもあります&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Tacotron: Towards End-to-End Speech Synthesis / arXiv:1703.10135 [cs.CL]</title>
      <link>https://r9y9.github.io/blog/2017/10/15/tacotron/</link>
      <pubDate>Sun, 15 Oct 2017 14:00:00 +0900</pubDate>
      <guid>https://r9y9.github.io/blog/2017/10/15/tacotron/</guid>
      <description>&lt;p&gt;Googleが2017年4月に発表したEnd-to-Endの音声合成モデル &lt;a href=&#34;https://arxiv.org/abs/1703.10135&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tacotron: Towards End-to-End Speech Synthesis / arXiv:1703.10135 [cs.CL]&lt;/a&gt; に興味があったので、自分でも同様のモデルを実装して実験してみました。結果わかったことなどをまとめておこうと思います。&lt;/p&gt;
&lt;p&gt;GoogleによるTacotronの音声サンプルは、 &lt;a href=&#34;https://google.github.io/tacotron/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://google.github.io/tacotron/&lt;/a&gt; から聴けます。僕の実装による音声サンプルはこの記事の真ん中くらいから、あるいは  &lt;a href=&#34;http://nbviewer.jupyter.org/github/r9y9/tacotron_pytorch/blob/f98eda7336726cdfe4ab97ae867cc7f71353de50/notebooks/Test%20Tacotron.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Test Tacotron.ipynb | nbviewer&lt;/a&gt;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; から聴くことができます。&lt;/p&gt;
&lt;p&gt;とても長い記事になってしまったので、結論のみ知りたい方は、一番最後まで飛ばしてください。最後の方のまとめセクションに、実験した上で僕が得た知見がまとまっています。&lt;/p&gt;
&lt;h2 id=&#34;概要&#34;&gt;概要&lt;/h2&gt;
&lt;p&gt;論文のタイトルにもある通り、End-to-Endを目指しています。典型的な（複雑にあなりがちな）音声合成システムの構成要素である、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;言語依存のテキスト処理フロントエンド&lt;/li&gt;
&lt;li&gt;言語特徴量と音響特徴量のマッピング (HMMなりDNNなり)&lt;/li&gt;
&lt;li&gt;波形合成のバックエンド&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;を一つのモデルで達成しようとする、&lt;strong&gt;attention付きseq2seqモデル&lt;/strong&gt; を提案しています。ただし、&lt;strong&gt;Toward&lt;/strong&gt; とあるように、完全にEnd-to-Endではなく、ネットワークは波形ではなく &lt;strong&gt;振幅スペクトログラム&lt;/strong&gt; を出力し、Griffin limの方法によって位相を復元し、逆短時間フーリエ変換をすることによって、最終的な波形を得ます。根本にあるアイデア自体はシンプルですが、そのようなEnd-to-Endに近いモデルで高品質な音声合成を実現するのは困難であるため、論文では学習を上手くいくようするためのいくつかのテクニックを提案する、といった主張です。以下にいくつかピックアップします。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;エンコーダに &lt;strong&gt;CBFG&lt;/strong&gt; (1-D convolution bank + highway network + bidirectional GRU) というモジュールを使う&lt;/li&gt;
&lt;li&gt;デコーダの出力をスペクトログラムではなく（より低次元の）&lt;strong&gt;メル周波数スペクトログラム&lt;/strong&gt; にする。スペクトログラムはアライメントを学習するには冗長なため。&lt;/li&gt;
&lt;li&gt;スペクトログラムは、メル周波数スペクトログラムに対して &lt;strong&gt;CBFG&lt;/strong&gt; を通して得る&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;その他、BatchNormalizationを入れたり、Dropoutを入れたり、GRUをスタックしたり、と色々ありますが、正直なところ、どれがどのくらい効果があるのかはわかっていません（調べるには、途方もない時間がかかります）が、論文の主張によると、これらが有効なようです。&lt;/p&gt;
&lt;h2 id=&#34;既存実装&#34;&gt;既存実装&lt;/h2&gt;
&lt;p&gt;Googleは実装を公開していませんが、オープンソース実装がいくつかあります。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Kyubyong/tacotron&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/Kyubyong/tacotron&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/barronalex/Tacotron&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/barronalex/Tacotron&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/keithito/tacotron&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/keithito/tacotron&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;自分で実装する前に、上記をすべてを簡単に試したり、生成される音声サンプルを比較した上で、僕は &lt;a href=&#34;https://github.com/keithito/tacotron&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron&lt;/a&gt; が一番良いように思いました。最も良いと思った点は、keithito さんは、&lt;a href=&#34;https://keithito.com/LJ-Speech-Dataset/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LJ Speech Dataset&lt;/a&gt; という単一話者の英語読み上げ音声 &lt;strong&gt;約24時間のデータセットを構築&lt;/strong&gt; し、それを &lt;strong&gt;public domainで公開&lt;/strong&gt; していることです。このデータセットは貴重です。&lt;a href=&#34;https://keithito.github.io/audio-samples/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;デモ音声サンプル&lt;/a&gt;は、そのデータセットを使った結果でもあり、他と比べてとても高品質に感じました。自分でも試してみて、1時間程度で英語らしき音声が生成できるようになったのと、さらに数時間でアライメントも学習されることを確認しました。&lt;/p&gt;
&lt;p&gt;なお、上記3つすべてで学習スクリプトを回して音声サンプルを得る、程度のことは試しましたが、僕がコードレベルで読んだのは &lt;a href=&#34;https://github.com/keithito/tacotron&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron&lt;/a&gt; のみです。読んだコードは、TensorFlowに詳しくない僕でも読めるもので、とても構造化されていて読みやすかったです。&lt;/p&gt;
&lt;h2 id=&#34;自前実装&#34;&gt;自前実装&lt;/h2&gt;
&lt;p&gt;勉強も兼ねて、PyTorchでスクラッチから書きました。その結果が &lt;a href=&#34;https://github.com/r9y9/tacotron_pytorch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/tacotron_pytorch&lt;/a&gt; です。&lt;/p&gt;
&lt;p&gt;先にいくつか結論を書いておくと、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;音の品質は、&lt;a href=&#34;https://github.com/keithito/tacotron&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron&lt;/a&gt; の方が良く感じました（同じモデルの実装を心がけたのに…つらい…）。ただ、データセットの音声には残響が乗っていて、生成された音声が元音声に近いのかというのは、僕には判断がつきにくいです。記事の後半に比較できるようにサンプルを貼っておきますので、気になる方はチェックしてみてください&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/keithito/tacotron&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron&lt;/a&gt; では長い入力だと合成に失敗する一方で&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;、僕の実装では比較的長くてもある程度合成できるようです。なぜのかを突き詰めるには、TensorFlowのseq2seq APIの &lt;strong&gt;コード&lt;/strong&gt; (APIは抽象化されすぎていてdocstringからではよくわからないので…) を読みとく必要があるかなと思っています（やっていませんすいません&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;実験&#34;&gt;実験&lt;/h2&gt;
&lt;p&gt;基本的には &lt;a href=&#34;https://github.com/keithito/tacotron&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron&lt;/a&gt; の学習スクリプトと同じで、&lt;a href=&#34;https://keithito.com/LJ-Speech-Dataset/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LJ Speech Dataset&lt;/a&gt; を使って学習させました。テキスト処理、音声処理 (Griffin lim等) には既存のコードをそのまま使用し、モデル部分のみ自分で置き換えました。実験では、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;attention付きseq2seqの肝である、アライメントがどのように学習されていくのか&lt;/li&gt;
&lt;li&gt;学習が進むにつれて、生成される音声はどのように変わっていくのか&lt;/li&gt;
&lt;li&gt;学習されたモデルは、汎化性能はどの程度なのか（未知文章、長い文章、スペルミスに対してパフォーマンスはどう変わるのか、等）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;を探っていきました。&lt;/p&gt;
&lt;h3 id=&#34;アライメントの学習過程の可視化&#34;&gt;アライメントの学習過程の可視化&lt;/h3&gt;
&lt;p&gt;通常のseq2seqは、エンコーダRNNによって得た最後のタイムステップにおける隠れ層の状態を、デコーダのRNNの初期状態として渡します。一方attentiont付きのseq2seqモデルでは、デコーダRNNは各タイムステップで、エンコーダRNNの各タイムステップにおける隠れ層の状態を重みづけて使用し、その重みも学習します。attention付きのseq2seqでは、アライメントがきちんと（曖昧な表現ですが）学習されているかを可視化してチェックするのが、学習がきちんと進んでいるのか確認するのに便利です。&lt;/p&gt;
&lt;p&gt;以下に、47000 step (epochではありません。僕の計算環境 GTX 1080Ti で半日かからないくらい) iterationしたときのアライメント結果と、47000 stepの時点での予測された音声サンプルを示します。なお、gifにおける各画像は、データセットをランダムにサンプルした際のアライメントであり、ある同じ音声に対するアライメントではありません。Tacotron論文には、Bahdanau Attentionを使用したとありますが、&lt;a href=&#34;https://github.com/keithito/tacotron/issues/24&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron #24 Try Monotonic Attention&lt;/a&gt; によると、Tacotron論文の第一著者は新しいバージョンのTacotronでは Monotonic attentionを使用しているらしいということから、Monotonic Attentionでも試してみました。あとでわかったのですが、長文（200文字、数文とか）を合成しようとすると途中でアライメントがスキップすることが多々見受けられたので、そういった場合に、monotonicという制約が上手く働くのだと思います。&lt;/p&gt;
&lt;p&gt;以下の順でgifを貼ります。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/keithito/tacotron&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron&lt;/a&gt;, Bahdanau attention&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/keithito/tacotron&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron&lt;/a&gt;, Bahdanau-style monotonic attention&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/tacotron_pytorch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/tacotron_pytorch&lt;/a&gt;, Bahdanau attention&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;keithito: Bahdanau Attention&lt;/strong&gt;&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/tacotron-tf-alignment_47000steps.gif&#34; /&gt;&lt;/div&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/step-47000-audio-tf.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;keithito: (Bahdanau-style) Monotonic Attention&lt;/strong&gt;&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/tacotron-tf-monotonic-alignment_47000steps.gif&#34; /&gt;&lt;/div&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/step-47000-audio-tf-monotonic.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;自前実装: Bahdanau Attention&lt;/strong&gt;&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/tacotron-alignment_47000steps.gif&#34; /&gt;&lt;/div&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/step-47000-audio-pt.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;Monotonicかどうかで比較すると、Monotonic attentionの方がアライメントがかなり安定しているように見えます。しかし、Githubのスレッドにあった音声サンプルを聴くと、音質的な意味では大きな違いがないように思ったのと、収束速度（簡単に試したところ、アライメントがまともになりだすstepは20000くらいで、ほぼ同じでした）も同じに思えました。一方で自前実装は、アライメントがまともになるstepが10000くらいとやや早く、またシャープに見えます。&lt;/p&gt;
&lt;p&gt;音声サンプルの方ですが、既存実装は両者ともそれなりにまともです。一方自前実装では、まだかなりノイジーです。できるだけtf実装と同じようにつくり、実験条件も同じにしたつもりですが、何か間違っているかもしれません。が、イテレーションを十分に回すと、一応音声はそれなりに出るようになります。&lt;/p&gt;
&lt;p&gt;音声サンプルに関する注意点としては、これはデコードの際に教師データを使っているので、この時点でのモデル使って、同等の音質の音声を生成できるとは限りません。学習時には、デコーダの各タイムステップで教師データのスペクトログラム（正確には、デコーダの出力はメル周波数スペクトログラム）を入力とする一方で、評価時には、デコーダ自身が出力したスペクトログラムを次のタイムステップの入力に用います。評価時には、一度変なスペクトログラムを出力してしまったら、エラーが蓄積していってどんどん変な出力をするようになってしまうことは想像に難しくないと思います。seq2seqモデルのデコードにはビームサーチが代表的なものとしてありますが、Tacotronでは単純にgreedy decodingをします。&lt;/p&gt;
&lt;h3 id=&#34;学習が進むにつれて生成される音声はどのように変わっていくのか&#34;&gt;学習が進むにつれて、生成される音声はどのように変わっていくのか&lt;/h3&gt;
&lt;p&gt;さて、ここからは自前実装のみでの実験結果です。約10日、70万step程度学習させましたので、5000, 10000, 50000, そのあとは10万から10万ステップごとに70万ステップまでそれぞれで音声を生成して、どのようになっているのかを見ていきます。&lt;/p&gt;
&lt;h4 id=&#34;例文1&#34;&gt;例文1&lt;/h4&gt;
&lt;p&gt;Hi, my name is Tacotron. I&amp;rsquo;m still learning a lot from data.&lt;/p&gt;
&lt;p&gt;(56 chars, 14 words)&lt;/p&gt;
&lt;p&gt;step 5000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/0_step5000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 10000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/0_step10000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 50000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/0_step50000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 100000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/0_step100000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 200000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/0_step200000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 300000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/0_step300000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 400000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/0_step400000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 500000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/0_step500000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 600000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/0_step600000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 700000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/0_step700000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;だいたい20万ステップ（学習二日くらい）から、まともな音声になっているように感じます。細かいところでは、&lt;code&gt;Hi,&lt;/code&gt; &lt;code&gt;Tacotron&lt;/code&gt; という部分が少し発音しにくそうです。データセットにはこのような話し言葉のようなものが少ないのと、&lt;code&gt;Tacotron&lt;/code&gt; という単語が英語らしさ的な意味で怪しいから（造語ですよね、たぶん）と考えられます。&lt;/p&gt;
&lt;h4 id=&#34;例文2&#34;&gt;例文2&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Python_%28programming_language%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://en.wikipedia.org/wiki/Python_(programming_language)&lt;/a&gt; より引用：&lt;/p&gt;
&lt;p&gt;Python is a widely used high-level programming language for general-purpose programming, created by Guido van Rossum and first released in 1991.&lt;/p&gt;
&lt;p&gt;(144 chars, 23 words)&lt;/p&gt;
&lt;p&gt;step 5000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/1_step5000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 10000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/1_step10000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 50000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/1_step50000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 100000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/1_step100000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 200000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/1_step200000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 300000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/1_step300000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 400000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/1_step400000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 500000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/1_step500000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 600000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/1_step600000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;step 700000&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/progress/1_step700000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;だいたい20万ステップから、まともな音声になっているように思います。&lt;/p&gt;
&lt;h3 id=&#34;モデルの汎化性能について調査&#34;&gt;モデルの汎化性能について調査&lt;/h3&gt;
&lt;p&gt;以下、72万ステップ（一週間くらい）学習させたモデルを使って、いろんな入力でテストした結果です。音声と合わせてアライメントも貼っておきます。&lt;/p&gt;
&lt;h4 id=&#34;適当な未知入力&#34;&gt;適当な未知入力&lt;/h4&gt;
&lt;p&gt;データセットには存在しない文章を使ってテストしてみました。ところどころ（非ネイティブの僕にでも）不自然だと感じるところが見られますが、とはいえまぁまぁいい感じではないでしょうか。(google translateで同じ文章を合成してみて比べても、そんなに悪くない気がしました)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/PyPy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://en.wikipedia.org/wiki/PyPy&lt;/a&gt; より：&lt;/p&gt;
&lt;p&gt;PyPy is an alternate implementation of the Python programming language written in Python.&lt;/p&gt;
&lt;p&gt;(89 chars, 14 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/0_unknown/0_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/0_unknown/0_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/NumPy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://en.wikipedia.org/wiki/NumPy&lt;/a&gt; より：&lt;/p&gt;
&lt;p&gt;NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.&lt;/p&gt;
&lt;p&gt;(215 chars, 35 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/0_unknown/1_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/0_unknown/1_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&#34;https://numba.pydata.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://numba.pydata.org/&lt;/a&gt; より：&lt;/p&gt;
&lt;p&gt;Numba gives you the power to speed up your applications with high performance functions written directly in Python.&lt;/p&gt;
&lt;p&gt;(115 chars, 19 words)&lt;/p&gt;
&lt;p&gt;&lt;audio controls=&#34;controls&#34; &gt;は&lt;/p&gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/0_unknown/2_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/0_unknown/2_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;h4 id=&#34;スペルミス&#34;&gt;スペルミス&lt;/h4&gt;
&lt;p&gt;スペルミスがある場合に、合成結果はどうなるのか、といったテストです。&lt;a href=&#34;https://google.github.io/tacotron/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Googleのデモ&lt;/a&gt;にあるように、ある程度ロバスト（少なくとも全体が破綻するといったことはない）のように思いました。&lt;/p&gt;
&lt;p&gt;Thisss isrealy awhsome.&lt;/p&gt;
&lt;p&gt;(23 chars, 4 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/1_spell/0_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/1_spell/0_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;This is really awesome.&lt;/p&gt;
&lt;p&gt;(23 chars, 5 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/1_spell/1_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/1_spell/1_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;I cannnnnot believe it.&lt;/p&gt;
&lt;p&gt;(23 chars, 5 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/1_spell/2_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/1_spell/2_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;I cannot believe it.&lt;/p&gt;
&lt;p&gt;(20 chars, 6 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/1_spell/3_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/1_spell/3_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;h4 id=&#34;中少し長めの文章&#34;&gt;中〜少し長めの文章&lt;/h4&gt;
&lt;p&gt;だいたい250文字を越えたくらいで、単語がスキップされるなどの現象が多く確認されました。データセットは基本的に短い文章の集まりなのが理由に思います。前述の通り、monotonic attentionを使えば、原理的にはスキップされにくくなると思います。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1703.10135&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/1703.10135&lt;/a&gt; より引用：&lt;/p&gt;
&lt;p&gt;A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module.&lt;/p&gt;
&lt;p&gt;(155 chars, 26 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/2_long/1_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/2_long/1_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&#34;https://americanliterature.com/childrens-stories/little-red-riding-hood&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://americanliterature.com/childrens-stories/little-red-riding-hood&lt;/a&gt; より引用：&lt;/p&gt;
&lt;p&gt;Once upon a time there was a dear little girl who was loved by every one who looked at her, but most of all by her grandmother, and there was nothing that she would not have given to the child.&lt;/p&gt;
&lt;p&gt;(193 chars, 43 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/2_long/0_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/2_long/0_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1703.10135&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/1703.10135&lt;/a&gt; より引用：&lt;/p&gt;
&lt;p&gt;A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices.&lt;/p&gt;
&lt;p&gt;(263 chars, 41 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/2_long/2_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/2_long/2_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&#34;https://americanliterature.com/childrens-stories/little-red-riding-hood&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://americanliterature.com/childrens-stories/little-red-riding-hood&lt;/a&gt; より引用：&lt;/p&gt;
&lt;p&gt;Once upon a time there was a dear little girl who was loved by every one who looked at her, but most of all by her grandmother, and there was nothing that she would not have given to the child. Once she gave her a little cap of red velvet, which suited her so well
that she would never wear anything else. So she was always called Little Red Riding Hood.&lt;/p&gt;
&lt;p&gt;(354 chars, 77 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/2_long/3_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/2_long/3_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;h3 id=&#34;googleのデモと比較&#34;&gt;Googleのデモと比較&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://google.github.io/tacotron/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://google.github.io/tacotron/&lt;/a&gt; の音声サンプルと同じ文章で試します。大文字小文字の区別は今回学習したモデルでは区別しないので、一部例文は除いています。いくつか気づいたことを挙げておくと、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;He has read the whole thing. / He reads book. のように、readの読みが動詞の活用形によって変わるような場合なのですが、上手く行くときといかないときがありました。イテレーションを進めていく上で、ロスは下がり続ける一方で、きちんと区別して発音できるようになったりできなくなってしまったり、というのを繰り返していました。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;?&lt;/code&gt; が文末につくことで、イントネーションが変わってくれることを期待しましたが、データセット中に &lt;code&gt;?&lt;/code&gt; が少なすぎたのか、あまりうまくいかなかったように思います。&lt;/li&gt;
&lt;li&gt;out-of-domainの文章にもロバストのように思いましたが、二個目の例文のような、（複雑な？）専門用語の発音は、厳しい感じがしました。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Generative adversarial network or variational auto-encoder.&lt;/p&gt;
&lt;p&gt;(59 chars, 7 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/0_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/0_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;Basilar membrane and otolaryngology are not auto-correlations.&lt;/p&gt;
&lt;p&gt;(62 chars, 8 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/1_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/1_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;He has read the whole thing.&lt;/p&gt;
&lt;p&gt;(28 chars, 7 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/2_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/2_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;He reads books.&lt;/p&gt;
&lt;p&gt;(15 chars, 4 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/3_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/3_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;Thisss isrealy awhsome.&lt;/p&gt;
&lt;p&gt;(23 chars, 4 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/4_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/4_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;This is your personal assistant, Google Home.&lt;/p&gt;
&lt;p&gt;(45 chars, 9 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/5_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/5_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;This is your personal assistant Google Home.&lt;/p&gt;
&lt;p&gt;(44 chars, 8 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/6_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/6_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;The quick brown fox jumps over the lazy dog.&lt;/p&gt;
&lt;p&gt;(44 chars, 10 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/7_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/7_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;Does the quick brown fox jump over the lazy dog?&lt;/p&gt;
&lt;p&gt;(51 chars, 11 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/8_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/4_google_demo/8_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;h3 id=&#34;keithitotacotron-との比較&#34;&gt;keithito/tacotron との比較&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://keithito.github.io/audio-samples/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://keithito.github.io/audio-samples/&lt;/a&gt; の audio samples で使われている文章に対するテストです。比較しやすいように、比較対象の音声も合わせて貼っておきます。自前実装で生成したもの、&lt;a href=&#34;https://github.com/keithito/tacotron&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron&lt;/a&gt; で生成したもの、の順です。&lt;/p&gt;
&lt;p&gt;Scientists at the CERN laboratory say they have discovered a new particle.&lt;/p&gt;
&lt;p&gt;(74 chars, 13 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/3_keithito/0_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/keithito/eval-877000-0.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/3_keithito/0_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;There&amp;rsquo;s a way to measure the acute emotional intelligence that has never gone out of style.&lt;/p&gt;
&lt;p&gt;(91 chars, 18 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/3_keithito/1_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/keithito/eval-877000-1.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/3_keithito/1_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;President Trump met with other leaders at the Group of 20 conference.&lt;/p&gt;
&lt;p&gt;(69 chars, 13 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/3_keithito/2_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/keithito/eval-877000-2.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/3_keithito/2_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;The Senate&amp;rsquo;s bill to repeal and replace the Affordable Care Act is now imperiled.&lt;/p&gt;
&lt;p&gt;(81 chars, 16 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/3_keithito/3_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/keithito/eval-877000-3.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/3_keithito/3_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;Generative adversarial network or variational auto-encoder.&lt;/p&gt;
&lt;p&gt;(59 chars, 7 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/3_keithito/4_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/keithito/eval-877000-4.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/3_keithito/4_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;The buses aren&amp;rsquo;t the problem, they actually provide a solution.&lt;/p&gt;
&lt;p&gt;(63 chars, 13 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/3_keithito/5_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/keithito/eval-877000-5.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/3_keithito/5_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;h3 id=&#34;ground-truth-との比較&#34;&gt;Ground truth との比較&lt;/h3&gt;
&lt;p&gt;最後に、元のデータセットとの比較です。学習データからサンプルを取ってきて比較します。自前実装で生成したもの、ground truthの順に貼ります。&lt;/p&gt;
&lt;p&gt;Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition.&lt;/p&gt;
&lt;p&gt;(152 chars, 30 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/5_ljspeech_sample/0_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/lj/LJ001-0001.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/5_ljspeech_sample/0_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;in being comparatively modern.&lt;/p&gt;
&lt;p&gt;(30 chars, 5 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/5_ljspeech_sample/1_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/lj/LJ001-0002.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/5_ljspeech_sample/1_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;For although the Chinese took impressions from wood blocks engraved in relief for centuries before the woodcutters of the Netherlands, by a similar process.&lt;/p&gt;
&lt;p&gt;(156 chars, 26 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/5_ljspeech_sample/2_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/lj/LJ001-0003.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/5_ljspeech_sample/2_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;produced the block books, which were the immediate predecessors of the true printed book,&lt;/p&gt;
&lt;p&gt;(89 chars, 16 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/5_ljspeech_sample/3_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/lj/LJ001-0004.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/5_ljspeech_sample/3_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;the invention of movable metal letters in the middle of the fifteenth century may justly be considered as the invention of the art of printing.&lt;/p&gt;
&lt;p&gt;(143 chars, 26 words)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/5_ljspeech_sample/4_step720000.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/tacotron/lj/LJ001-0005.mp3&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/audio/tacotron/5_ljspeech_sample/4_step720000_alignment.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;元音声があまり良いクリーンな音声ではないとはいえ、まー元音声とは大きな違いがありますねー、、厳しいです。スペクトログラムを見ている限りでは（貼ってないですが、すいません）、明らかに高周波数成分の予測が上手く言っていないことはわかっています。ナイーブなアイデアではありますが、GANを導入すると良くなるのではないかと思っています。&lt;/p&gt;
&lt;h3 id=&#34;おまけ生成する度に変わる音声&#34;&gt;おまけ：生成する度に変わる音声&lt;/h3&gt;
&lt;p&gt;実験する過程で副次的に得られた結果ではあるのですが、テスト時に一部dropoutを有効にしていると&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;、生成する度に音声が異なる（韻律が微妙に変わる）、といった現象を経験しています。以下、前に検証した際の実験ノートのリンクを貼っておきます。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://nbviewer.jupyter.org/gist/r9y9/fe1945b73cd5b98e97c61410fe26a851#Try-same-input-multiple-times&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://nbviewer.jupyter.org/gist/r9y9/fe1945b73cd5b98e97c61410fe26a851#Try-same-input-multiple-times&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;まとめ--感想など&#34;&gt;まとめ &amp;amp; 感想など&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Tacotronを実装しました &lt;a href=&#34;https://github.com/r9y9/tacotron_pytorch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/tacotron_pytorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;24時間のデータセットに対して、20万ステップ程度（数日くらい）学習させたらそれなりにまともな音声が生成できるようになりました。70万ステップ（一週間と少しくらい）学習させましたが、ずっとロスは下がり続ける一方で、50万くらいからはあまり大きな品質改善は見られなかったように思います。&lt;/li&gt;
&lt;li&gt;Googleの論文と（ほぼ&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;）同じように実装したつもりですが、品質はそこまで高くならなかったように思います。End-to-end では、 &lt;strong&gt;データの量と品質&lt;/strong&gt; がかなり重要なので、それが主な原因だと思っています。（僕の実装に、多少バグがあるかもしれませんが、、、&lt;/li&gt;
&lt;li&gt;EOS (End-of-sentence) では、理想的には要素がすべて0のスペクトログラムが出力されるはずなのですが、実際にはやはりそうもいかないので、判定には以下のようなしきい値処理を用いました。ここで貼った音声は全部この仕組みで動いており、単純ですがそれなりに上手く機能しているようです。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;def is_end_of_frames(output, eps=0.2):
    return (output.data &amp;lt;= eps).all()
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;論文からは非自明な点の一つとして、エンコーダの出力のうち、入力のゼロ詰めした部分をマスキングするかどうか、といった点があります。これは、既存実装によってもまちまちで、例えば &lt;a href=&#34;https://github.com/keithito/tacotron&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron&lt;/a&gt; ではマスキングしていませんが、&lt;a href=&#34;https://github.com/barronalex/Tacotron/blob/2de9e507456cbe2b680cbc6b2beb6a761bd2eebd/models/tacotron.py#L51&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;barronalex/Tacotron&lt;/a&gt; ではマスクしています。僕はマスクする場合としない場合と両方試したのですが（ここに貼った結果は、マスクしていない場合のものです）、マスクしないほうが若干良くなったような気もします。理想的にはマスクするべきだと思ったのですが、実際に試したところどちらかが圧倒的に悪いという結果ではありませんでした。発見した大きな違いの一つは、マスクなしの場合はアテンションは大まかにmonotonicになる一方で、マスクありの場合は、無音区間ではエンコーダ出力の冒頭にアテンションの重みが大きくなる（ので、monotonicではない）、と言ったことがありました。マスクありの音声サンプル、アライメントの可視化は、（少し古いですが）&lt;a href=&#34;http://nbviewer.jupyter.org/github/r9y9/tacotron_pytorch/blob/bdad19fdff22016c7457a979707655bb7a605cd8/notebooks/Test%20Tacotron.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ここ&lt;/a&gt; にあります。参考までに、Tensorflowでエンコーダの出力マスクする場合は、&lt;code&gt;memory_sequence_length&lt;/code&gt; を指定します &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BahdanauAttention&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BahdanauAttention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;日本語でやったり、multi-speaker でやったりしたかったのですが、とにかく実験に時間がかかるので、今のところ僕の中では優先度が低めになってしまいました。時間と計算資源に余裕があれば、やりたいのですが…&lt;/li&gt;
&lt;li&gt;日本語でやるには、英語と同じようにはいきません。というのも、char-levelで考えた際に、語彙が大きすぎるので。やるならば、十分大きな日本語テキストコーパスからembeddingを別途学習して（Tacotronでは、モデル自体にembeddingが入っています）、その他の部分を音声つきコーパスで学習する、といった方法が良いかなと思います。CSJコーパスは結構向いているんじゃないかと思っています。&lt;/li&gt;
&lt;li&gt;multi-speakerモデルを考える場合、どこにembeddingを差し込むのか、といったことが重要になってきますが、&lt;a href=&#34;https://github.com/keithito/tacotron/issues/18&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron/issues/18&lt;/a&gt; や &lt;a href=&#34;https://github.com/keithito/tacotron/issues/24&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron/issues/24&lt;/a&gt; に少し議論があるので、興味のある人は見てみるとよいかもしれません。DeepVoiceの論文も参考になるかと思います&lt;/li&gt;
&lt;li&gt;最新のTensorFlowでは、griffin lim や stft（GPUで走る、勾配が求められる）が実装されているので、tacotronモデルを少し拡張して、サンプルレベルでロスを考える、といったことが簡単に試せると思います（ある意味WaveNetです）。ただし、ものすごく計算リソースを必要とするのが容易に想像がつくので、僕はやっていません。GPU落ちてこないかな、、、&lt;/li&gt;
&lt;li&gt;Tacotronの拡張として、speaker embedding以外にも、いろんな潜在変数を埋め込んでみると、楽しそうに思いました。例えば話速、感情とか。&lt;/li&gt;
&lt;li&gt;TensorFlowのseq2seqあたりのドキュメント/コードをよく読んでいたのですが、APIが抽象化されすぎていてつらいなと思いました。例えばAttentionWrapper、コードを読まずに挙動を理解するのは無理なのではと思いました &lt;a href=&#34;https://github.com/r9y9/tacotron_pytorch/issues/2#issuecomment-334255759&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/tacotron_pytorch/issues/2#issuecomment-334255759&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/keithito/tacotron&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keithito/tacotron&lt;/a&gt; は本当によく書かれているなと思ったので、TensorFlowに長けている方には、おすすめです&lt;/li&gt;
&lt;li&gt;僕の実装では、バッチサイズ32でGPUメモリ5GB程度しか食わないので、Tacotronは比較的軽いモデルなのだなーと思いました。物体検出で有名な single shot multibox detector (通称SSD) なんかは、バッチサイズ16とかでも平気で12GBとか使ってくるので（一年近く前の経験ですが）、無限にGPUリソースがほしくなってきます&lt;/li&gt;
&lt;li&gt;これが僕にとって、はじめてまともにseq2seqを実装した経験でした。色々勉強したのですが、Attention mechanism に関しては、 &lt;a href=&#34;http://colinraffel.com/blog/online-and-linear-time-attention-by-enforcing-monotonic-alignments.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://colinraffel.com/blog/online-and-linear-time-attention-by-enforcing-monotonic-alignments.html&lt;/a&gt; がとても参考になりました。あとで知ったのですが、monotonic attentionの著者は僕が昔から使っている音楽信号処理のライブラリ &lt;a href=&#34;https://github.com/librosa/librosa&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;librosa&lt;/a&gt; のコミッタでした（僕も弱小コミッタの一人）。とても便利で、よくテストされているので、おすすめです。オープンソースのTacotron実装でも、音声処理にも使われています&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;
&lt;p&gt;End-to-End 音声合成は、言語処理のフロントエンドを（最低限の前処理を除き）必要としないという素晴らしさがあります。SampleRNN、Char2wavと他にも色々ありますが、今後もっと発展していくのではないかと思っています。おしまい。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1703.10135&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tacotron: Towards End-to-End Speech Synthesis / arXiv:1703.10135&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;URLには現時点のgitのコミットハッシュが入っています。最新版は、 &lt;a href=&#34;https://github.com/r9y9/tacotron_pytorch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/tacotron_pytorch&lt;/a&gt; から直接辿ってください。&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/keithito/tacotron/pull/43#issuecomment-332068107&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/keithito/tacotron/pull/43#issuecomment-332068107&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;dropoutを切ってしまうと、アライメントが死んでしまうというバグ？に苦しんでおり…未だ原因を突き止められていません&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;たとえばロスはちょっと違って、高周波数帯域に比べて低周波数帯域の重みを少し大きくしていたりしています。これは既存のtf実装に従いました。&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>GAN 日本語音声合成 [arXiv:1709.08041]</title>
      <link>https://r9y9.github.io/blog/2017/10/10/gantts-jp/</link>
      <pubDate>Tue, 10 Oct 2017 11:45:32 +0900</pubDate>
      <guid>https://r9y9.github.io/blog/2017/10/10/gantts-jp/</guid>
      <description>&lt;p&gt;&lt;strong&gt;10/11 追記&lt;/strong&gt;: IEEE TASLPのペーパー (Open access) が公開されたようなので、リンクを貼っておきます: &lt;a href=&#34;https://ieeexplore.ieee.org/document/8063435/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ieeexplore.ieee.org/document/8063435/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;arXiv論文リンク: &lt;a href=&#34;https://arxiv.org/abs/1709.08041&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1709.08041&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/10/09/gantts/&#34;&gt;前回の記事&lt;/a&gt; の続きです。これでこのシリーズは終わりの予定です。&lt;/p&gt;
&lt;p&gt;前回は英語音声合成でしたが、以前書いた &lt;a href=&#34;https://r9y9.github.io/blog/2017/08/16/japanese-dnn-tts/&#34;&gt;DNN日本語音声合成の記事&lt;/a&gt; で使ったデータと同じものを使い、日本語音声合成をやってみましたので、結果を残しておきます。&lt;/p&gt;
&lt;h2 id=&#34;実験&#34;&gt;実験&lt;/h2&gt;
&lt;h3 id=&#34;実験条件&#34;&gt;実験条件&lt;/h3&gt;
&lt;p&gt;HTSのNIT-ATR503のデモデータ (&lt;a href=&#34;https://github.com/r9y9/nnmnkwii_gallery/blob/4899437e22528399ca50c34097a2db2bed782f8b/data/NIT-ATR503_COPYING&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ライセンス&lt;/a&gt;) から、wavデータ503発話を用います。442を学習用、56を評価用、残り5をテスト用にします（※英語音声とtrain/evalの比率は同じです）。継続長モデルは、state-levelではなくphone-levelです。サンプリング周波数が48kHzなので、mgcの次元を25から60に増やしました。モデル構造は、すべて英語音声合成の場合と同じです。ADV loss は0次を除くmgcを用いて計算しました。F0は入れていません。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/gantts&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gantts&lt;/a&gt; の &lt;a href=&#34;https://github.com/r9y9/gantts/tree/jp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;jpブランチ&lt;/a&gt; をチェックアウトして、以下のシェルを実行すると、ここに貼った結果が得られます。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; ./jp_tts_demo.sh jp_tts_order59
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ただし、シェル中に、&lt;code&gt;HTS_ROOT&lt;/code&gt; という変数があり、シェル実行前に、環境に合わせてディレクトリを指定する必要があります。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;diff --git a/jp_tts_demo.sh b/jp_tts_demo.sh
index 7a8f12c..b18e604 100755
--- a/jp_tts_demo.sh
+++ b/jp_tts_demo.sh
@@ -8,7 +8,7 @@ experiment_id=$1
 fs=48000

 # Needs adjastment
-HTS_DEMO_ROOT=~/local/HTS-demo_NIT-ATR503-M001
+HTS_DEMO_ROOT=HTS日本語デモの場所を指定してください

 # Flags
 run_duration_training=1
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;変換音声の比較&#34;&gt;変換音声の比較&lt;/h3&gt;
&lt;h4 id=&#34;音響モデルのみ適用&#34;&gt;音響モデルのみ適用&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;自然音声&lt;/li&gt;
&lt;li&gt;ベースライン&lt;/li&gt;
&lt;li&gt;GAN&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;の順に音声を貼ります。聴きやすいように、soxで音量を正規化しています。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;nitech_jp_atr503_m001_j49&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nit-atr503/nitech_jp_atr503_m001_j49.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/acoustic_only/baseline/test/nitech_jp_atr503_m001_j49.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/acoustic_only/gan/test/nitech_jp_atr503_m001_j49.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;nitech_jp_atr503_m001_j50&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nit-atr503/nitech_jp_atr503_m001_j50.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/acoustic_only/baseline/test/nitech_jp_atr503_m001_j50.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/acoustic_only/gan/test/nitech_jp_atr503_m001_j50.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;nitech_jp_atr503_m001_j51&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nit-atr503/nitech_jp_atr503_m001_j51.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/acoustic_only/baseline/test/nitech_jp_atr503_m001_j51.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/acoustic_only/gan/test/nitech_jp_atr503_m001_j51.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;nitech_jp_atr503_m001_j52&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nit-atr503/nitech_jp_atr503_m001_j52.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/acoustic_only/baseline/test/nitech_jp_atr503_m001_j52.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/acoustic_only/gan/test/nitech_jp_atr503_m001_j52.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;nitech_jp_atr503_m001_j53&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nit-atr503/nitech_jp_atr503_m001_j53.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/acoustic_only/baseline/test/nitech_jp_atr503_m001_j53.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/acoustic_only/gan/test/nitech_jp_atr503_m001_j53.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;h4 id=&#34;音響モデル継続長モデルを適用&#34;&gt;音響モデル＋継続長モデルを適用&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;nitech_jp_atr503_m001_j49&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nit-atr503/nitech_jp_atr503_m001_j49.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/duration_acoustic/baseline/test/nitech_jp_atr503_m001_j49.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/duration_acoustic/gan/test/nitech_jp_atr503_m001_j49.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;nitech_jp_atr503_m001_j50&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nit-atr503/nitech_jp_atr503_m001_j50.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/duration_acoustic/baseline/test/nitech_jp_atr503_m001_j50.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/duration_acoustic/gan/test/nitech_jp_atr503_m001_j50.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;nitech_jp_atr503_m001_j51&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nit-atr503/nitech_jp_atr503_m001_j51.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/duration_acoustic/baseline/test/nitech_jp_atr503_m001_j51.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/duration_acoustic/gan/test/nitech_jp_atr503_m001_j51.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;nitech_jp_atr503_m001_j52&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nit-atr503/nitech_jp_atr503_m001_j52.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/duration_acoustic/baseline/test/nitech_jp_atr503_m001_j52.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/duration_acoustic/gan/test/nitech_jp_atr503_m001_j52.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;nitech_jp_atr503_m001_j53&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/nit-atr503/nitech_jp_atr503_m001_j53.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/duration_acoustic/baseline/test/nitech_jp_atr503_m001_j53.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/jp_tts_order59/duration_acoustic/gan/test/nitech_jp_atr503_m001_j53.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;どうでしょうか。ちょっと早口になってしまっている箇所もありますが、全体的には明瞭性が上がって、品質が改善されたような感じがします。若干ノイジーな感じは、音響モデルにRNNを使えば改善されるのですが、今回は計算リソースの都合上、Feed-forward型のサンプルとなっています。&lt;/p&gt;
&lt;h3 id=&#34;gv&#34;&gt;GV&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;nitech_jp_atr503_m001_j49&lt;/code&gt; に対して計算した結果です。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/jp-gantts/nitech_jp_atr503_m001_j49_gv.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;英語音声合成の実験でも確認しているのですが、mgcの次元を大きく取ると、高次元でGVが若干落ちる傾向にあります。ただし、&lt;a href=&#34;https://twitter.com/r9y9/status/915213687891169280&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;一週間前の僕のツイート&lt;/a&gt; によると、なぜかそんなこともなく（当時ばりばりのプロトタイピングの時期だったので、コードが残っておらず、いまは再現できないという、、すいません）、僕が何かミスをしている可能性もあります。ただ、品質はそんなに悪くないように思います。&lt;/p&gt;
&lt;h3 id=&#34;変調スペクトル&#34;&gt;変調スペクトル&lt;/h3&gt;
&lt;p&gt;評価用セットで平均を取ったものです。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/jp-gantts/ms.png&#34; /&gt;&lt;/div&gt;
&lt;h3 id=&#34;特徴量の分布&#34;&gt;特徴量の分布&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;nitech_jp_atr503_m001_j49&lt;/code&gt; に対して計算した結果です。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/jp-gantts/nitech_jp_atr503_m001_j49_scatter.png&#34; /&gt;&lt;/div&gt;
&lt;h3 id=&#34;おまけ-htsデモと聴き比べ&#34;&gt;おまけ: HTSデモと聴き比べ&lt;/h3&gt;
&lt;p&gt;HTSデモを実行すると生成されるサンプルとの聴き比べです。注意事項ですが、&lt;strong&gt;実験条件がまったく異なります&lt;/strong&gt;。あくまで参考程度にどうぞ。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;HTSデモ&lt;/li&gt;
&lt;li&gt;ベースライン&lt;/li&gt;
&lt;li&gt;GAN&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;の順に音声を貼ります。&lt;/p&gt;
&lt;p&gt;1 こんにちは&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/hts_nit_atr503_2mix/phrase01.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/test/baseline/phrase01.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/test/gan/phrase01.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;2 それではさようなら&lt;/p&gt;
&lt;p&gt;HTS&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/hts_nit_atr503_2mix/phrase02.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/test/baseline/phrase02.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/test/gan/phrase02.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;3 はじめまして&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/hts_nit_atr503_2mix/phrase03.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/test/baseline/phrase03.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/test/gan/phrase03.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;4 ようこそ名古屋工業大学へ&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/hts_nit_atr503_2mix/phrase04.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/test/baseline/phrase04.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/test/gan/phrase04.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;5 今夜の名古屋の天気は雨です&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/hts_nit_atr503_2mix/phrase05.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/test/baseline/phrase05.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-gantts/test/gan/phrase05.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;
&lt;p&gt;アイデアはシンプル、効果は素晴らしいという、僕の好きな（試したくなる）研究の紹介でした。ありがとうございました。&lt;/p&gt;
&lt;p&gt;GANシリーズのその他記事へのリンクは以下の通りです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/10/05/ganvc/&#34;&gt;GAN 声質変換 (en) 編はこちら&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/10/09/gantts/&#34;&gt;GAN 音声合成 (en) 編はこちら&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;追記: 図を作るのに使ったノートブックは &lt;a href=&#34;http://nbviewer.jupyter.org/gist/r9y9/185a56417cee27d9f785b8caf1c9f5ec&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;こちら&lt;/a&gt; においておきました。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1709.08041&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yuki Saito, Shinnosuke Takamichi, Hiroshi Saruwatari, &amp;ldquo;Statistical Parametric Speech Synthesis Incorporating Generative Adversarial Networks&amp;rdquo;, arXiv:1709.08041 [cs.SD], Sep. 2017&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>【音声合成編】Statistical Parametric Speech Synthesis Incorporating Generative Adversarial Networks [arXiv:1709.08041]</title>
      <link>https://r9y9.github.io/blog/2017/10/09/gantts/</link>
      <pubDate>Mon, 09 Oct 2017 02:00:00 +0900</pubDate>
      <guid>https://r9y9.github.io/blog/2017/10/09/gantts/</guid>
      <description>&lt;p&gt;&lt;strong&gt;10/11 追記&lt;/strong&gt;: IEEE TASLPのペーパー (Open access) が公開されたようなので、リンクを貼っておきます: &lt;a href=&#34;https://ieeexplore.ieee.org/document/8063435/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ieeexplore.ieee.org/document/8063435/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;arXiv論文リンク: &lt;a href=&#34;https://arxiv.org/abs/1709.08041&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1709.08041&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/10/05/ganvc/&#34;&gt;前回の記事&lt;/a&gt; の続きです。音響モデルの学習にGANを使うというアイデアは、声質変換だけでなく音声合成にも応用できます。&lt;a href=&#34;http://festvox.org/cmu_arctic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CMU ARCTIC&lt;/a&gt; を使った英語音声合成の実験を行って、ある程度良い結果がでたので、まとめようと思います。音声サンプルだけ聴きたい方は真ん中の方まで読み飛ばしてください。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;コードはこちら: &lt;a href=&#34;https:github.com/r9y9/gantts&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/gantts | PyTorch implementation of GAN-based text-to-speech and voice conversion (VC) &lt;/a&gt; (VCのコードも一緒です)&lt;/li&gt;
&lt;li&gt;音声サンプル付きデモノートブックはこちら: &lt;a href=&#34;http://nbviewer.jupyter.org/github/r9y9/gantts/blob/master/notebooks/Test%20TTS.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The effects of adversarial training in text-to-speech synthesis | nbviewer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;前回の記事でも書いた注意書きですが、厳密に同じ結果を再現しようとは思っていません。同様のアイデアを試す、といったことに主眼を置いています。&lt;/p&gt;
&lt;h2 id=&#34;実験&#34;&gt;実験&lt;/h2&gt;
&lt;h3 id=&#34;実験条件&#34;&gt;実験条件&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://festvox.org/cmu_arctic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CMU ARCTIC&lt;/a&gt; から、話者 &lt;code&gt;slt&lt;/code&gt; のwavデータそれぞれ1131発話すべてを用います。
&lt;a href=&#34;https://github.com/CSTR-Edinburgh/merlin&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Merlin&lt;/a&gt;  の slt デモの条件と同様に、1000を学習用、126を評価用、残り5をテスト用にします。継続長モデル（state-level）には &lt;strong&gt;Bidirectional-LSTM RNN&lt;/strong&gt; を、音響モデルには &lt;strong&gt;Feed-forward型&lt;/strong&gt; のニューラルネットを使用しました&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;。継続長モデル、音響モデルの両方にGANを取り入れました。論文の肝である &lt;strong&gt;ADV loss&lt;/strong&gt; についてですが、mgcのみ（0次は除く）を使って計算するパターンと、mgc + lf0で計算するパターンとで比較しました&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;p&gt;実験の結果 (ADV loss: mgcのみ) は、 &lt;a href=&#34;https://github.com/r9y9/gantts/tree/a5ec247ba7ee1a160875661f8899f56f8010be5b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a5ec247&lt;/a&gt; をチェックアウトして、下記のシェルを実行すると再現できます。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;./tts_demo.sh tts_test
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;データのダウンロード、特徴抽出、モデル学習、音声サンプル合成まで一通り行われます。&lt;code&gt;tts_test&lt;/code&gt; の部分は何でもよいです。tensorboard用に吐くログイベント名、モデル出力先、音声サンプル出力先の決定に使われます。詳細はコードを参照ください。 (ADV loss: mgc + lf0) の結果は、&lt;a href=&#34;https://github.com/r9y9/gantts/blob/a5ec247ba7ee1a160875661f8899f56f8010be5b/hparams.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ハイパーパラメータ&lt;/a&gt;を下記のように変更してシェルを実行すると再現できます。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;diff --git a/hparams.py b/hparams.py
index d82296c..e73dc57 100644
--- a/hparams.py
+++ b/hparams.py
@@ -175,7 +175,7 @@ tts_acoustic = tf.contrib.training.HParams(
     # Streams used for computing adversarial loss
     # NOTE: you should probably change discriminator&#39;s `in_dim`
     # if you change the adv_streams
-    adversarial_streams=[True, False, False, False],
+    adversarial_streams=[True, True, False, False],
     # Don&#39;t switch this on unless you are sure what you are doing
     # If True, you will need to adjast `in_dim` for discriminator.
     # Rationale for this is that power coefficients are less meaningful
@@ -202,7 +202,7 @@ tts_acoustic = tf.contrib.training.HParams(
     # Discriminator
     discriminator=&amp;quot;MLP&amp;quot;,
     discriminator_params={
-        &amp;quot;in_dim&amp;quot;: 24,
+        &amp;quot;in_dim&amp;quot;: 25,
         &amp;quot;out_dim&amp;quot;: 1,
         &amp;quot;num_hidden&amp;quot;: 2,
         &amp;quot;hidden_dim&amp;quot;: 256,
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;変換音声の比較&#34;&gt;変換音声の比較&lt;/h3&gt;
&lt;h4 id=&#34;音響モデルのみ適用-adv-loss-mgcのみ&#34;&gt;音響モデルのみ適用 (ADV loss: mgcのみ)&lt;/h4&gt;
&lt;p&gt;継続長モデルを適用しない、かつ ADV lossにmgcのみを用いる場合です。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;自然音声&lt;/li&gt;
&lt;li&gt;ベースライン&lt;/li&gt;
&lt;li&gt;GAN&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;の順に音声を貼ります。聴きやすいように、soxで音量を正規化しています。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;arctic_b0535&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/slt/arctic_b0535.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/acoustic_only/baseline/test/arctic_b0535.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/acoustic_only/gan/test/arctic_b0535.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;arctic_b0536&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/slt/arctic_b0536.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/acoustic_only/baseline/test/arctic_b0536.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/acoustic_only/gan/test/arctic_b0536.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;arctic_b0537&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/slt/arctic_b0537.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/acoustic_only/baseline/test/arctic_b0537.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/acoustic_only/gan/test/arctic_b0537.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;arctic_b0538&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/slt/arctic_b0538.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/acoustic_only/baseline/test/arctic_b0538.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/acoustic_only/gan/test/arctic_b0538.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;arctic_b0539&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/slt/arctic_b0539.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/acoustic_only/baseline/test/arctic_b0539.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/acoustic_only/gan/test/arctic_b0539.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;VCの場合と同じように、音声の明瞭性が上がったように思います。&lt;/p&gt;
&lt;h4 id=&#34;音響モデル継続長モデルを適用-adv-loss-mgcのみ&#34;&gt;音響モデル＋継続長モデルを適用 (ADV loss: mgcのみ)&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;arctic_b0535&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/slt/arctic_b0535.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/duration_acoustic/baseline/test/arctic_b0535.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/duration_acoustic/gan/test/arctic_b0535.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;arctic_b0536&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/slt/arctic_b0536.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/duration_acoustic/baseline/test/arctic_b0536.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/duration_acoustic/gan/test/arctic_b0536.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;arctic_b0537&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/slt/arctic_b0537.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/duration_acoustic/baseline/test/arctic_b0537.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/duration_acoustic/gan/test/arctic_b0537.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;arctic_b0538&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/slt/arctic_b0538.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/duration_acoustic/baseline/test/arctic_b0538.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/duration_acoustic/gan/test/arctic_b0538.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;arctic_b0539&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/slt/arctic_b0539.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/duration_acoustic/baseline/test/arctic_b0539.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/duration_acoustic/gan/test/arctic_b0539.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;音声の明瞭性が上がっているとは思いますが、継続長に関しては、ベースライン/GANで差異がほとんどないように感じられると思います。これは、（僕が実験した範囲では少なくとも）DiscriminatorがGeneartorに勝ちやすくて (音響モデルの場合は、そんなことはない)、 ADV lossが下がるどころか上がってしまい、結果 MGE lossを最小化する場合とほとんど変わっていない、という結果になっています。論文に記載の内容とは異なり、state-levelの継続長モデルではあるものの、ハイパーパラメータなどなどいろいろ変えて試したのですが、上手くいきませんでした。&lt;/p&gt;
&lt;h4 id=&#34;adv-loss-mgc-vs-mgc--lf0&#34;&gt;ADV loss: mgc vs mgc + lf0&lt;/h4&gt;
&lt;p&gt;次に、ロスの比較です。F0の変化に着目しやすいように、継続長モデルを使わず、音響モデルのみを適用します。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;自然音声&lt;/li&gt;
&lt;li&gt;ADV loss (mgcのみ, 24次元)&lt;/li&gt;
&lt;li&gt;ADV loss (mgc + lf0, 25次元)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;の順に音声を貼ります。また、WORLD (dio + stonemask) で分析したF0の可視化結果も併せて貼っておきます。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;arctic_b0535&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/slt/arctic_b0535.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/acoustic_only/gan/test/arctic_b0535.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24_advf0/acoustic_only/gan/test/arctic_b0535.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/gantts/arctic_b0535_f0.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;arctic_b0536&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/slt/arctic_b0536.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/acoustic_only/gan/test/arctic_b0536.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24_advf0/acoustic_only/gan/test/arctic_b0536.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/gantts/arctic_b0536_f0.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;arctic_b0537&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/slt/arctic_b0537.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/acoustic_only/gan/test/arctic_b0537.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24_advf0/acoustic_only/gan/test/arctic_b0537.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/gantts/arctic_b0537_f0.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;arctic_b0538&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/slt/arctic_b0538.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/acoustic_only/gan/test/arctic_b0538.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24_advf0/acoustic_only/gan/test/arctic_b0538.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/gantts/arctic_b0538_f0.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;arctic_b0539&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/slt/arctic_b0539.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24/acoustic_only/gan/test/arctic_b0539.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/gantts/tts_order24_advf0/acoustic_only/gan/test/arctic_b0539.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/gantts/arctic_b0539_f0.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;どうでしょうか。上手くいっている場合も（arctic_b537とか）あれば、上手くいっていない場合 (arctic_b539とか) もあるように思います。僕にはF0が不自然に揺れているように感じ場合が多くありました。ここでは5つしか音声を貼っていませんが、その他126個の評価用音声でも聴き比べていると、ADV lossにF0を入れない方がよい気がしました（あくまで僕の主観ですが&lt;/p&gt;
&lt;p&gt;このあたりは、F0の抽出法、補間法に強く依存しそうです。今回は、F0抽出のパラメータをまったくチューニングしていないので、そのせいもあった（f0分析エラーに引っ張られて悪くなった）のかもしれません。&lt;/p&gt;
&lt;h3 id=&#34;global-variance-は補償されているのか&#34;&gt;Global variance は補償されているのか？&lt;/h3&gt;
&lt;p&gt;F0の話は終わりで、スペクトル特徴量に着目して結果を分析していきます。以下、ADV loss (mgcのみ)、継続長モデル＋音響モデルを適用したサンプルでの分析結果です。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/gantts/arctic_b0537_gv.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;大まかに、論文で示されているのと同様の結果が得られました。なお、これは &lt;code&gt;arctic_b0537&lt;/code&gt; の一発話に対して計算したもので、テストセットの平均ではありません（すいません）。また、これはテストセット中のサンプルの中で、GVが補償されていることがわかりやすい例をピックアップしました。ただし、他のテストサンプルにおいても同様の傾向が見られるのは確認しています。&lt;/p&gt;
&lt;h3 id=&#34;modulation-spectrum-変調スペクトル-は補償されているのか&#34;&gt;Modulation spectrum (変調スペクトル) は補償されているのか？&lt;/h3&gt;
&lt;p&gt;評価用の音声126発話それぞれで変調スペクトルを計算し、それらの平均を取り、適当な特徴量の次元をピックアップしたものを示します。横軸は変調周波数です。一番右端が50Hzです。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/gantts/ms.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;arctic_b0537&lt;/code&gt; の一発話に対して計算したものです。&lt;strong&gt;VCの場合とは異なり&lt;/strong&gt;、ベースライン、GANともに、低次元であっても10Hzを越えた辺りから自然音声とは大きく異っています。これはなぜなのか、僕にはまだわかっていません。また、VCの場合と同様に、高次元になるほど、GANベースの方が変調スペクトルは自然音声に近いこともわかります。GANによって、変調スペクトルはある程度補償されていると言えると思います。&lt;/p&gt;
&lt;h3 id=&#34;特徴量の分布&#34;&gt;特徴量の分布&lt;/h3&gt;
 &lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/gantts/arctic_b0537_scatter.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;arctic_b0537&lt;/code&gt; の一発話に対して計算したものです。論文で示されているほど顕著ではない気がしますが、おおまかに同様の結果が得られました。&lt;/p&gt;
&lt;h2 id=&#34;感想&#34;&gt;感想&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;GANのチューニングは難しい。人力（直感）ハイパーパラメータのチューニングを試しましたが、大変でした。そしてあまりうまくできた自信がありません。潤沢な計算資源でなんとかしたい…&lt;/li&gt;
&lt;li&gt;GANの学習は不安定（に感じる）が、通常の MSE loss の学習は安定で、かつBidirectional LSTM RNNは安定してよいです（結果をここに貼っていなくて申し訳ですが）。ただし、計算にものすごく時間がかかるのと、GPUメモリをかなり使うので、とりあえず通常のfeed forward型で実験した結果をまとめました&lt;/li&gt;
&lt;li&gt;state-levelの継続長モデルに、GANを使うのはあまり上手くできませんでした。ここに貼ったサンプルからはわからないのですが（すいません）、GとDが上手く競い合わず、Dが勝ってしまう場合がほとんどでした（結果それが一番まし）。上手く競い合わせるようとすると、早口音声が生成されてしまったり、と失敗がありました。&lt;/li&gt;
&lt;li&gt;F0を ADV lossに加えると、より自然音声に近づくと感じる場合もあるが、一方でF0が不自然に揺れてしまう場合もありました。これはF0の抽出法、補間法にも依存するので、調査が必要です&lt;/li&gt;
&lt;li&gt;mgc, lf0, vuv, bapすべてで ADV lossに加えると、残念な結果を見ることになりました。理想的にはこれでも上手くいくと思って最初に試したのですが、だめでした。興味のある人はためしてみてください&lt;/li&gt;
&lt;li&gt;mgcの0次（パワー成分）は、ADV lossに加えない方がよい。考えてみると、特にフレーム単位のモデルの場合（RNNではなく）、パワー情報はnatural/generated の識別にはほとんど寄与しなさそうです。これはArxivの方の論文には書いていないのですが（僕の見逃しでなければ）、&lt;a href=&#34;http://sython.org/papers/ASJ/saito2017asja.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ASJの原稿&lt;/a&gt; には書いてあるんですよね。一つのハマりどころでした&lt;/li&gt;
&lt;li&gt;DにRNNを使った実験も少しやってみたのですが、うまく競い合わせるのが難しそうでした。DにRNNを使うのは本質的には良いと思っているので、この辺りはもう少し色々試行錯誤したいと思っています&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;
&lt;p&gt;GANの学習は大変でしたが、上手く学習できれば品質向上につながることを確認できました。今後、計算リソースが空き次第、RNNでの実験も進めようと思うのと、日本語でやってみようと思っています。&lt;/p&gt;
&lt;p&gt;GANシリーズのその他記事へのリンクは以下の通りです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/10/05/ganvc/&#34;&gt;GAN 声質変換 (en) 編はこちら&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/10/10/gantts-jp/&#34;&gt;GAN 音声合成 (ja) 編はこちら&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;p&gt;Arxivにあるペーパーだけでなく、その他いろいろ参考にしました。ありがとうございます。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1709.08041&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yuki Saito, Shinnosuke Takamichi, Hiroshi Saruwatari, &amp;ldquo;Statistical Parametric Speech Synthesis Incorporating Generative Adversarial Networks&amp;rdquo;, arXiv:1709.08041 [cs.SD], Sep. 2017&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sython.org/papers/SIG-SLP/saito201702slp.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yuki Saito, Shinnosuke Takamichi, and Hiroshi Saruwatari, &amp;ldquo;Training algorithm to deceive anti-spoofing verification for DNN-based text-to-speech synthesis,&amp;rdquo; IPSJ SIG Technical Report, 2017-SLP-115, no. 1, pp. 1-6, Feb., 2017. (in Japanese)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jstage.jst.go.jp/article/transinf/E100.D/8/E100.D_2017EDL8034/_article&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yuki Saito, Shinnosuke Takamichi, and Hiroshi Saruwatari, &amp;ldquo;Voice conversion using input-to-output highway networks,&amp;rdquo; IEICE Transactions on Information and Systems, Vol.E100-D, No.8, pp.1925&amp;ndash;1928, Aug. 2017&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.slideshare.net/ShinnosukeTakamichi/dnnantispoofing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.slideshare.net/ShinnosukeTakamichi/dnnantispoofing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.slideshare.net/YukiSaito8/Saito2017icassp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.slideshare.net/YukiSaito8/Saito2017icassp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/SythonUK/whisperVC&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/SythonUK/whisperVC&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sython.org/papers/ASJ/saito2017asja.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yuki Saito, Shinnosuke Takamichi, and Hiroshi Saruwatari, &amp;ldquo;Experimental investigation of divergences in adversarial DNN-based speech synthesis,&amp;rdquo; Proc. ASJ, Spring meeting, 1-8-7, &amp;ndash;, Sep., 2017. (in Japanese)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;継続長モデル、音響モデルともにRNNを使うと良くなることがわかっているのですが、計算リソースの都合上、今回は音響モデルはFeed-forwardにしました。Feed-forwardだと30分で終わる計算が、RNNだと数時間かかってしまうので…&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;他も色々やったのですが、だいたい失敗でした。&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>【声質変換編】Statistical Parametric Speech Synthesis Incorporating Generative Adversarial Networks [arXiv:1709.08041]</title>
      <link>https://r9y9.github.io/blog/2017/10/05/ganvc/</link>
      <pubDate>Thu, 05 Oct 2017 23:25:36 +0900</pubDate>
      <guid>https://r9y9.github.io/blog/2017/10/05/ganvc/</guid>
      <description>&lt;p&gt;&lt;strong&gt;10/11 追記&lt;/strong&gt;: IEEE TASLPのペーパー (Open access) が公開されたようなので、リンクを貼っておきます: &lt;a href=&#34;https://ieeexplore.ieee.org/document/8063435/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ieeexplore.ieee.org/document/8063435/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;arXiv論文リンク: &lt;a href=&#34;https://arxiv.org/abs/1709.08041&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1709.08041&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;2017年9月末に、表題の &lt;a href=&#34;https://arxiv.org/abs/1709.08041&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;論文&lt;/a&gt; が公開されたのと、&lt;a href=&#34;https://github.com/r9y9/nnmnkwii&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nnmnkwii&lt;/a&gt; という designed for easy and fast prototyping を目指すライブラリを作ったのもあるので、実装してみました。僕が実験した限りでは、声質変換 (Voice conversion; VC) では安定して良くなりました（音声合成ではまだ実験中です）。この記事では、声質変換について僕が実験した結果をまとめようと思います。音声合成については、また後日まとめます&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;コードはこちら: &lt;a href=&#34;https:github.com/r9y9/gantts&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/gantts | PyTorch implementation of GAN-based text-to-speech and voice conversion (VC) &lt;/a&gt; (TTSのコードも一緒です)&lt;/li&gt;
&lt;li&gt;音声サンプルを聴きたい方はこちら: &lt;a href=&#34;http://nbviewer.jupyter.org/github/r9y9/gantts/blob/master/notebooks/Test%20VC.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The effects of adversarial training in voice conversion | nbviewer&lt;/a&gt; (※解説はまったくありませんのであしからず)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;なお、厳密に同じ結果を再現しようとは思っていません。同様のアイデアを試す、といったことに主眼を置いています。コードに関しては、ここに貼った結果を再現できるように気をつけました。&lt;/p&gt;
&lt;h2 id=&#34;概要&#34;&gt;概要&lt;/h2&gt;
&lt;p&gt;一言でいえば、音響モデルの学習に Generative Adversarial Net (&lt;strong&gt;GAN&lt;/strong&gt;) を導入する、といったものです。少し具体的には、&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;音響モデル（生成モデル）が生成した音響特徴量を偽物か本物かを見分けようとする識別モデルと、&lt;/li&gt;
&lt;li&gt;生成誤差を小さくしつつ (Minimum Generation Error loss; &lt;strong&gt;MGE loss&lt;/strong&gt; の最小化) 、生成した特徴量を識別モデルに本物だと誤認識させようとする (Adversarial loss; &lt;strong&gt;ADV loss&lt;/strong&gt; の最小化) 生成モデル&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;を交互に学習することで、自然音声の特徴量と生成した特徴量の分布を近づけるような、より良い音響モデルを獲得する、といった方法です。&lt;/p&gt;
&lt;h2 id=&#34;ベースライン&#34;&gt;ベースライン&lt;/h2&gt;
&lt;p&gt;ベースラインとしては、 &lt;strong&gt;MGE training&lt;/strong&gt; が挙げられています。DNN音声合成でよくあるロス関数として、音響特徴量 (静的特徴量 + 動的特徴量) に対する Mean Squared Error (&lt;strong&gt;MSE loss&lt;/strong&gt;) というものがあります。これは、特徴量の各次元毎に誤差に正規分布を考えて、その対数尤度を最大化することを意味します。
しかし、&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;静的特徴量と動的特徴量の間には本来 deterministic な関係があることが無視されていること&lt;/li&gt;
&lt;li&gt;ロスがフレーム単位で計算されるので、 (動的特徴量が含まれているとはいえ) 時間構造が無視されてしまっていること&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;から、それらの問題を解決するために、系列単位で、かつパラメータ生成後の静的特徴量の領域でロスを計算する方法、MGE training が提案されています。&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h2 id=&#34;実験&#34;&gt;実験&lt;/h2&gt;
&lt;h3 id=&#34;実験条件&#34;&gt;実験条件&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://festvox.org/cmu_arctic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CMU ARCTIC&lt;/a&gt; から、話者 &lt;code&gt;clb&lt;/code&gt; と &lt;code&gt;slt&lt;/code&gt; のwavデータそれぞれ500発話を用います。439を学習用、56を評価用、残り5をテスト用にします。音響特徴量には、WORLDを使って59次のメルケプストラムを抽出し、0次を除く59次元のベクトルを各フレーム毎の特徴量とします。F0、非周期性指標に関しては、元話者のものをそのまま使い、差分スペクトル法を用いて波形合成を行いました。F0の変換はしていません。音響モデルには、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jstage.jst.go.jp/article/transinf/E100.D/8/E100.D_2017EDL8034/_article&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yuki Saito, Shinnosuke Takamichi, and Hiroshi Saruwatari, &amp;ldquo;Voice conversion using input-to-output highway networks,&amp;rdquo; IEICE Transactions on Information and Systems, Vol.E100-D, No.8, pp.1925&amp;ndash;1928, Aug. 2017&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;で述べられている highway network を用います。ただし、活性化関数をReLUからLeakyReLUにしたり、Dropoutを入れたり、アーキテクチャは微妙に変えています。前者は、調べたら勾配が消えにくくて学習の不安定なGANに良いと書いてある記事があったので（ちゃんと理解しておらず安直ですが、実験したところ悪影響はなさそうでしたので様子見）、後者は、GANの学習の安定化につながった気がします（少なくともTTSでは）。Discriminatorには、Dropout付きの多層ニューラルネットを使いました。MGE loss と ADV loss をバランスする重み &lt;code&gt;w_d&lt;/code&gt; は、 1.0 にしました。層の数、ニューロンの数等、その他詳細が知りたい方は、コードを参照してください。実験に使用したコードの正確なバージョンは  &lt;a href=&#34;https://github.com/r9y9/gantts/tree/ccbb51b51634b272f0a71f29ad4c28edd8ce3429&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ccbb51b&lt;/a&gt; です。ハイパーパラメータは &lt;a href=&#34;https://github.com/r9y9/gantts/blob/ccbb51b51634b272f0a71f29ad4c28edd8ce3429/hparams.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;こちら&lt;/a&gt; です。&lt;/p&gt;
&lt;p&gt;ここで示す結果を再現したい場合は、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;コードをチェックアウト&lt;/li&gt;
&lt;li&gt;パッケージと依存関係をインストール&lt;/li&gt;
&lt;li&gt;&lt;code&gt;clb&lt;/code&gt; と &lt;code&gt;slt&lt;/code&gt; のデータをダウンロード（僕の場合は、 &lt;code&gt;~/data/cmu_arctic&lt;/code&gt; にあります&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;そして、以下のスクリプトを実行すればOKです。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;./vc_demo.sh ~/data/cmu_arctic
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;なお実行には、GPUメモリが4GBくらいは必要です（バッチサイズ32の場合）。GTX 1080Ti + i7-7700K の計算環境で、約1時間半くらいで終わります。スクリプト実行が完了すれば、&lt;code&gt;generated&lt;/code&gt; ディレクトリに、ベースライン/GAN それぞれで変換した音声が出力されます。以下に順に示す図については、&lt;a href=&#34;http://nbviewer.jupyter.org/github/r9y9/gantts/blob/master/notebooks/Test%20VC.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;デモノートブック&lt;/a&gt; を実行すると作ることができます。&lt;/p&gt;
&lt;h3 id=&#34;変換音声の比較&#34;&gt;変換音声の比較&lt;/h3&gt;
&lt;p&gt;テストセットの5つのデータに対しての変換音声、およびその元音声、ターゲット音声を比較できるように貼っておきます。下記の順番です。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;元話者の音声&lt;/li&gt;
&lt;li&gt;ターゲット話者の音声&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MGE Loss&lt;/strong&gt; を最小化して得られたモデルによる変換音声&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MGE loss + ADV loss&lt;/strong&gt; を最小化して得られたモデルによる変換音声&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;比較しやすいように、音量はsoxで正規化しました。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;arctic_a0496&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/src/arctic_a0496.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/tgt/arctic_a0496.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/baseline/test/arctic_a0496.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/gan/test/arctic_a0496.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;arctic_a0497&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/src/arctic_a0497.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/tgt/arctic_a0497.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/baseline/test/arctic_a0497.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/gan/test/arctic_a0497.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;arctic_a0498&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/src/arctic_a0498.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/tgt/arctic_a0498.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/baseline/test/arctic_a0498.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/gan/test/arctic_a0498.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;arctic_a0499&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/src/arctic_a0499.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/tgt/arctic_a0499.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/baseline/test/arctic_a0499.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/gan/test/arctic_a0499.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;strong&gt;arctic_a0500&lt;/strong&gt;&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/src/arctic_a0500.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/tgt/arctic_a0500.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/baseline/test/arctic_a0500.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/ganvc/gan/test/arctic_a0500.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;&lt;code&gt;clb&lt;/code&gt;, &lt;code&gt;slt&lt;/code&gt; は違いがわかりにくいと以前誰かから指摘されたのですが、これに慣れてしまいました。わかりづらかったらすいません。僕の耳では、明瞭性が上がって、良くなっているように思います。&lt;/p&gt;
&lt;h3 id=&#34;global-variance-は補償されているのか&#34;&gt;Global variance は補償されているのか？&lt;/h3&gt;
&lt;p&gt;統計ベースの手法では、変換音声の &lt;strong&gt;Global variance (GV)&lt;/strong&gt; が落ちてしまい、品質が劣化してしまう問題がよく知られています。GANベースの手法によって、この問題に対処できているのかどうか、実際に確認しました。以下に、データセット中の一サンプルを適当にピックアップして、GVを計算したものを示します。縦軸は対数、横軸はメルケプストラムの次元です。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/ganvc/gv.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;おおおまか、論文で示されているのと同等の結果を得ることができました。&lt;/p&gt;
&lt;h3 id=&#34;modulation-spectrum-変調スペクトル-は補償されているのか&#34;&gt;Modulation spectrum (変調スペクトル) は補償されているのか？&lt;/h3&gt;
&lt;p&gt;GVをより一般化ものとして、変調スペクトルという概念があります。端的に言えば、パラメータ系列の時間方向に対する離散フーリエ変換の二乗（の対数※定義によるかもですが、ここでは対数をとったもの）です。統計処理によって劣化した変換音声は、変調スペクトルが自然音声と比べて小さくなっていることが知られています。というわけで、GANベースの方法によって、変調スペクトルは補償されているのか？ということを調べてみました。これは、論文には書いていません（が、きっとされていると思います）。以下に、評価用の音声56発話それぞれで変調スペクトルを計算し、それらの平均を取り、適当な特徴量の次元をピックアップしたものを示します。横軸は変調周波数です。一番右端が50Hzです。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/ganvc/ms.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;特に高次元の変調スペクトルに対して、ベースラインは大きく落ちている一方で、GANベースでは比較的自然音声と近いことがわかります。しかし、高次元になるほど、自然音声とGANベースでも違いが出ているのがわかります。改善の余地はありそうですね。&lt;/p&gt;
&lt;h3 id=&#34;特徴量の分布&#34;&gt;特徴量の分布&lt;/h3&gt;
&lt;p&gt;論文で示されているscatter plotですが、同じことをやってみました。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/ganvc/scatter.png&#34; /&gt;&lt;/div&gt;
&lt;p&gt;概ね、論文通りの結果となっています。&lt;/p&gt;
&lt;h3 id=&#34;詐称率について&#34;&gt;詐称率について&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;w_d&lt;/code&gt; を変化させて、詐称率がどうなるかは実験していないのですが、&lt;code&gt;w_d = 1.0&lt;/code&gt; の場合に、だいたい0.7 ~ 0.9 くらいに収まることを確認しました。TTSでは0.99くらいの、論文と同様の結果が出ました。くらい、というのは、どのくらい Discriminator を学習させるか、初期化としてのMGE学習（例えば25epochくらい）のあと生成された特徴量に対して学習させるのか、それとも初期化とは別でベースライン用のモデル（100epochとか）を使って生成された特徴量に対して学習させるのか、によって変わってくるのと、その辺りが論文からではあまりわからなかったのと、学習率や最適化アルゴリズムやデータによっても変わってくるのと、詐称率の計算は品質にはまったく関係ないのもあって、あまり真面目にやっていません。すいません&lt;/p&gt;
&lt;h2 id=&#34;感想&#34;&gt;感想&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;効果は劇的、明らかに良くなりました。素晴らしいですね&lt;/li&gt;
&lt;li&gt;論文で書かれている反復回数 (25epochとか)よりも、100, 200と多く学習させる方がよかったです（知覚的な差は微妙ですが）ロスは下がり続けていました。&lt;/li&gt;
&lt;li&gt;実装はそんなに大変ではなかったですが、GANの学習が難しい感じがしました（VCではあまり失敗しないが、TTSではよく失敗する。落とし所を探し中&lt;/li&gt;
&lt;li&gt;Adam は学習は速いが、過学習ししやすい。GANも不安定になりがちな気がしました&lt;/li&gt;
&lt;li&gt;Adagrad は収束は遅いが、安定&lt;/li&gt;
&lt;li&gt;MGE loss と ADV loss の重みの計算は、適当にclipするようにしました。しなくてもだいたい収束しますが、バグがあると簡単に発散しますね〜haha&lt;/li&gt;
&lt;li&gt;gradient clipping をいれました。TTSでは少なくとも良くなった気がします。VCはなしでも安定しているようです。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;
&lt;p&gt;とても良くなりました。素晴らしいです。今回もWORLDにお世話になりました。続いて、TTSでも実験を進めていきます。&lt;/p&gt;
&lt;p&gt;GANシリーズのその他記事へのリンクは以下の通りです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/10/09/gantts/&#34;&gt;GAN 音声合成 (en) 編はこちら&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/blog/2017/10/10/gantts-jp/&#34;&gt;GAN 音声合成 (ja) 編はこちら&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;p&gt;Arxivにあるペーパーだけでなく、その他いろいろ参考にしました。ありがとうございます。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1709.08041&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yuki Saito, Shinnosuke Takamichi, Hiroshi Saruwatari, &amp;ldquo;Statistical Parametric Speech Synthesis Incorporating Generative Adversarial Networks&amp;rdquo;, arXiv:1709.08041 [cs.SD], Sep. 2017&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sython.org/papers/SIG-SLP/saito201702slp.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yuki Saito, Shinnosuke Takamichi, and Hiroshi Saruwatari, &amp;ldquo;Training algorithm to deceive anti-spoofing verification for DNN-based text-to-speech synthesis,&amp;rdquo; IPSJ SIG Technical Report, 2017-SLP-115, no. 1, pp. 1-6, Feb., 2017. (in Japanese)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jstage.jst.go.jp/article/transinf/E100.D/8/E100.D_2017EDL8034/_article&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yuki Saito, Shinnosuke Takamichi, and Hiroshi Saruwatari, &amp;ldquo;Voice conversion using input-to-output highway networks,&amp;rdquo; IEICE Transactions on Information and Systems, Vol.E100-D, No.8, pp.1925&amp;ndash;1928, Aug. 2017&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.slideshare.net/ShinnosukeTakamichi/dnnantispoofing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.slideshare.net/ShinnosukeTakamichi/dnnantispoofing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.slideshare.net/YukiSaito8/Saito2017icassp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.slideshare.net/YukiSaito8/Saito2017icassp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/SythonUK/whisperVC&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/SythonUK/whisperVC&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Kobayashi, Kazuhiro, et al. &amp;ldquo;Statistical Singing Voice Conversion with Direct Waveform Modification based on the Spectrum Differential.&amp;rdquo; Fifteenth Annual Conference of the International Speech Communication Association. 2014.&lt;/li&gt;
&lt;/ul&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;論文では有効性が示されていますが、僕が試した範囲内で、かつ僕の耳にによれば、あまり大きな改善は確認できていません。客観的な評価は、そのうちする予定です。&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>DNN音声合成のためのライブラリの紹介とDNN日本語音声合成の実装例</title>
      <link>https://r9y9.github.io/blog/2017/08/16/japanese-dnn-tts/</link>
      <pubDate>Wed, 16 Aug 2017 23:10:56 +0900</pubDate>
      <guid>https://r9y9.github.io/blog/2017/08/16/japanese-dnn-tts/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/nnmnkwii&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nnmnkwii&lt;/a&gt; というDNN音声合成のためのライブラリを公開しましたので、その紹介をします。&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://t.co/p8MnOxkVoH&#34;&gt;https://t.co/p8MnOxkVoH&lt;/a&gt; Library to build speech synthesis systems designed for easy and fast prototyping. Open sourced:)&lt;/p&gt;&amp;mdash; 山本りゅういち (@r9y9) &lt;a href=&#34;https://twitter.com/r9y9/status/897117170265501696&#34;&gt;August 14, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;ドキュメントの最新版は &lt;a href=&#34;https://r9y9.github.io/nnmnkwii/latest/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://r9y9.github.io/nnmnkwii/latest/&lt;/a&gt; です。以下に、いくつかリンクを貼っておきます。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/nnmnkwii/v0.0.1/design_jp.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;なぜ作ったのか、その背景の説明と設計 (日本語)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/nnmnkwii/v0.0.1/nnmnkwii_gallery/notebooks/00-Quick%20start%20guide.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;クイックガイド&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://r9y9.github.io/nnmnkwii/v0.0.1/nnmnkwii_gallery/notebooks/tts/01-DNN-based%20statistical%20speech%20synthesis%20%28en%29.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DNN英語音声合成のチュートリアル&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;よろしければご覧ください&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;p&gt;ドキュメントは、だいたい英語でお硬い雰囲気で書いたので、この記事では、日本語でカジュアルに背景などを説明しようと思うのと、（ドキュメントには英語音声合成の例しかないので）HTSのデモに同梱のATR503文のデータセットを使って、&lt;strong&gt;DNN日本語音声合成&lt;/strong&gt; を実装する例を示したいと思います。結果だけ知りたい方は、音声サンプルが下の方にあるので、適当に読み飛ばしてください。&lt;/p&gt;
&lt;h2 id=&#34;なぜ作ったのか&#34;&gt;なぜ作ったのか&lt;/h2&gt;
&lt;p&gt;一番大きな理由は、僕が &lt;strong&gt;対話環境（Jupyter, IPython等）&lt;/strong&gt; で使えるツールがほしかったからです&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;。
僕は結構前からREPL (Read-Eval-Print-Loop) 信者で、プログラミングのそれなりの時間をREPLで過ごします。
IDEも好きですし、emacsも好きなのですが、同じくらいJupyterや&lt;a href=&#34;https://github.com/JuliaLang/julia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Julia&lt;/a&gt;のREPLが好きです。
用途に応じて使い分けますが、特に何かデータを分析する必要があるような時に、即座にデータを可視化できるJupyter notebookは、僕にとってプログラミングに欠かせないものになっています。&lt;/p&gt;
&lt;p&gt;ところが、HTSの後継として生まれたDNN音声合成ツールである &lt;a href=&#34;https://github.com/CSTR-Edinburgh/merlin&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Merlin&lt;/a&gt; は、コマンドラインツールとして使われる想定のもので、僕の要望を満たしてくれるものではありませんでした。
とはいえ、Merlinは優秀な音声研究者たちの産物であり、当然役に立つ部分も多く、使っていました。しかし、ことプロトタイピングにおいては、やはり対話環境でやりたいなあという思いが強まっていきました。&lt;/p&gt;
&lt;p&gt;新しく作るのではなく、Merlinを使い続ける、Merlinを改善する方針も考えました。僕がMerlinを使い始めた頃、Merlinはpython3で動かなかったので、動くように &lt;a href=&#34;https://github.com/CSTR-Edinburgh/merlin/pull/141&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;プルリク&lt;/a&gt; を出したこともあるのですが、まぁレビューに数カ月もかかってしまったので、これは新しいものを作った方がいいな、と思うに至りました。&lt;/p&gt;
&lt;p&gt;以上が、僕が新しくツール作ろうと思った理由です。&lt;/p&gt;
&lt;h2 id=&#34;特徴&#34;&gt;特徴&lt;/h2&gt;
&lt;p&gt;さて、Merlinに対する敬意と不満から生まれたツールでありますが、その特徴を簡単にまとめます。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;対話環境&lt;/strong&gt; での使用を前提に、設計されています。コマンドラインツールはありません。ユーザが必要に応じて作ればよい、という考えです。&lt;/li&gt;
&lt;li&gt;DNN音声合成のデモをノートブック形式で提供しています。&lt;/li&gt;
&lt;li&gt;大規模データでも扱えるように、データセットとデータセットのイテレーション（フレーム毎、発話毎の両方）のユーティリティが提供されています&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Merlinとは異なり、音響モデルは提供しません&lt;/strong&gt;。自分で実装する必要があります（が、今の時代簡単ですよね、lstmでも数行で書けるので&lt;/li&gt;
&lt;li&gt;任意の深層学習フレームワークと併せて使えるように、設計されています&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;（&lt;a href=&#34;https://r9y9.github.io/nnmnkwii/latest/references/autograd.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;autogradパッケージ&lt;/a&gt;のみ、今のところPyTorch依存です&lt;/li&gt;
&lt;li&gt;言語特徴量の抽出の部分は、Merlinのコードをリファクタして用いています。そのせいもあって、Merlinのデモと同等のパフォーマンスを簡単に実現できます。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;対象ユーザ&#34;&gt;対象ユーザ&lt;/h2&gt;
&lt;p&gt;まずはじめに、大雑把にいって、音声合成の研究（or その真似事）をしてみたい人が主な対象です。
自前のデータを元に、ブラックボックスでいいので音声合成エンジンを作りたい、という人には厳しいかもしれません。その前提を元に、少し整理します。&lt;/p&gt;
&lt;h3 id=&#34;こんな人におすすめです&#34;&gt;こんな人におすすめです&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Jupyter notebookが好きな人&lt;/li&gt;
&lt;li&gt;REPLが好きな人&lt;/li&gt;
&lt;li&gt;Pythonで処理を完結させたい人&lt;/li&gt;
&lt;li&gt;オープンソースの文化に寛容な人&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;音声合成の研究を始めてみたい人&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;こんな人には向かないかも&#34;&gt;こんな人には向かないかも&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;コマンドラインツールこそ至高な人&lt;/li&gt;
&lt;li&gt;パイプライン処理こそ至高な人&lt;/li&gt;
&lt;li&gt;SPTKのコマンドラインツール至高な人&lt;/li&gt;
&lt;li&gt;信頼のある機関が作ったツールしか使わない人&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;音声研究者ガチ勢で、自前のツールで満足している人&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;dnn日本語音声合成の実装例&#34;&gt;DNN日本語音声合成の実装例&lt;/h2&gt;
&lt;p&gt;さて、前置きはこのくらいにして、日本語音声合成の実装例を示します。シンプルなFeed forwardなネットワークと、Bi-directional LSTM RNNの2パターンを、ノートブック形式で作成しました。&lt;/p&gt;
&lt;p&gt;ソースコードは、 &lt;a href=&#34;https://github.com/r9y9/nnmnkwii_gallery&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/nnmnkwii_gallery&lt;/a&gt; にあります。以下に、現状点での直リンク（gitのコミットハッシュがURLに入っています）を貼っておきます。nbviewerに飛びます。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://nbviewer.jupyter.org/github/r9y9/nnmnkwii_gallery/blob/bd4bd260eb22d0000ac2776b204b3a5afb693c49/notebooks/tts/jp-01-DNN-based%20statistical%20speech%20synthesis.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Feed forwardなネットワークを使った音声合成のノートブックへの直リンク&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://nbviewer.jupyter.org/github/r9y9/nnmnkwii_gallery/blob/bd4bd260eb22d0000ac2776b204b3a5afb693c49/notebooks/tts/jp-02-Bidirectional-LSTM%20based%20RNNs%20for%20speech%20synthesis.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bi-directional LSTM RNNを使った音声合成のノートブックへの直リンク&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;興味のある人は、ローカルに落として実行してみてください。CUDA環境があることが前提ですが、通常のFeed forwardのネットワークを用いたデモは、
特徴抽出の時間（初回実行時に必要）を除けば、5分で学習&amp;amp;波形生成が終わります。Bi-directional LSTMのデモは、僕の環境 (i7-7700K, GTX 1080Ti) では、約2時間で終わります。GPUメモリが少ない場合は、バッチサイズを小さくしなければならず、より時間がかかるかもしれません。&lt;/p&gt;
&lt;h3 id=&#34;データセット&#34;&gt;データセット&lt;/h3&gt;
&lt;p&gt;今回は、HTSのNIT-ATR503のデモデータ (&lt;a href=&#34;https://github.com/r9y9/nnmnkwii_gallery/blob/4899437e22528399ca50c34097a2db2bed782f8b/data/NIT-ATR503_COPYING&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ライセンス&lt;/a&gt;) を拝借します。ライブラリを使って音声合成を実現するためのデータとして、最低限以下が必要です。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(state or phone level) フルコンテキストラベル&lt;/li&gt;
&lt;li&gt;Wavファイル&lt;/li&gt;
&lt;li&gt;質問ファイル（言語特徴量の抽出に必要）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上2つは、今回はHTSのデモスクリプトからまるまるそのまま使います（※HTSのデモスクリプトを回す必要はありません）。質問ファイルは、コンテキストクラスタリングに使われる質問ファイルを元に、質問数を（本当に）適当に減らして、Merlinのデモの質問ファイルからCQSに該当する質問を加えて、作成しました。
フルコンテキストラベルには、phone-levelでアライメントされたものを使いますが、
state-levelでアライメントされたものを使えば、性能は上がると思います。今回は簡単のためにphone-levelのアライメントを使います。&lt;/p&gt;
&lt;p&gt;質問の選定には、改善の余地があることがわかっていますが、あくまでデモということで、ご了承ください。&lt;/p&gt;
&lt;h3 id=&#34;音声合成の結果&#34;&gt;音声合成の結果&lt;/h3&gt;
&lt;p&gt;全体の処理に興味がある場合は別途ノートブックを見てもらうとして、ここでは結果だけ貼っておきます。
HTSのデモからとってきた例文5つに対して、それぞれ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Feed forward neural networks (MyNetとします) で生成したもの&lt;/li&gt;
&lt;li&gt;Bi-directional LSTM recurrent neural networks (MyRNNとします)で生成したもの&lt;/li&gt;
&lt;li&gt;HTSデモで生成したもの (HTSとします)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;の順番に、音声ファイルを添付しておきます。聴きやすいように、soxで正規化しています。それではどうぞ。&lt;/p&gt;
&lt;p&gt;1 こんにちは&lt;/p&gt;
&lt;p&gt;MyNet&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-01-tts/phrase01.wav&#34; type=&#34;audio/wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;MyRNN&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-02-tts/phrase01.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;HTS&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/hts_nit_atr503_2mix/phrase01.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;2 それではさようなら&lt;/p&gt;
&lt;p&gt;MyNet&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-01-tts/phrase02.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;MyRNN&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-02-tts/phrase02.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;HTS&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/hts_nit_atr503_2mix/phrase02.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;3 はじめまして&lt;/p&gt;
&lt;p&gt;MyNet&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-01-tts/phrase03.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;MyRNN&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-02-tts/phrase03.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;HTS&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/hts_nit_atr503_2mix/phrase03.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;4 ようこそ名古屋工業大学へ&lt;/p&gt;
&lt;p&gt;MyNet&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-01-tts/phrase04.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;MyRNN&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-02-tts/phrase04.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;HTS&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/hts_nit_atr503_2mix/phrase04.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;5 今夜の名古屋の天気は雨です&lt;/p&gt;
&lt;p&gt;MyNet&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-01-tts/phrase05.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;MyRNN&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/jp-02-tts/phrase05.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;HTS&lt;/p&gt;
&lt;audio controls=&#34;controls&#34; &gt;
&lt;source src=&#34;https://r9y9.github.io/audio/hts_nit_atr503_2mix/phrase05.wav&#34; autoplay/&gt;
Your browser does not support the audio element.
&lt;/audio&gt;
&lt;p&gt;一応HTSで生成された音声も貼りましたが、そもそも実験条件が違いすぎる&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;ので、単純に比較することはできません。
せめて HTS ＋ STRAIGHTと比較したかったところですが、僕はSTRAIGHTを持っていないので、残念ながらできません、悲しみ。&lt;/p&gt;
&lt;p&gt;しかし、それなりにまともな音声が出ているような気がします。&lt;/p&gt;
&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;
&lt;p&gt;いままでさんざん、汎用性とは程遠いクソコードを書いてきましたが、今回こそは少しはマシなものを作ろうと思って作りました。僕以外の人にも役に立てば幸いです。あと、この記事を書いた目的は、いろんな人に使ってみてほしいのと、使ってみた結果のフィードバックがほしい（バグ見つけた、そもそもエラーで動かん、ここがクソ、等）ということなので、フィードバックをくださると助かります。よろしくお願いします。&lt;/p&gt;
&lt;p&gt;ちなみに名前ですが、ななみ or しちみと読んでください。何でもいいのですが、常識的に考えてあぁ確かに読めないなぁと思いました（小並感）。ドキュメントにあるロゴは、昔三次元物体追跡の実験をしていたときに撮ったく某モンのポイントクラウドですが、そのうち七味的な画像に変えようと思っています。適当ですいません&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;リンク切れが怖いので、v0.0.1のリンクを貼りました。できれば、最新版をご覧ください。 &lt;a href=&#34;https://r9y9.github.io/nnmnkwii/latest/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://r9y9.github.io/nnmnkwii/latest/&lt;/a&gt; こちらからたどれます&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;知っている人にはまたか、と言われそう&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;音響モデルの提供をライブラリの範囲外とすることで、間接的に達成されています&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;バグにエンカウントしたらすぐに使うのをやめてしまう人には、向いていないかもしれません。&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Merlinは、エジンバラ大学の優秀な研究者の方々によって作られています&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;f0分析、スペクトル包絡抽出、非周期性成分の抽出法がすべてことなる、またポストフィルタの種類も異なる。条件をある程度揃えて比較するのが面倒そうだったので（なにせHTSを使ったモデルの学習は数時間かかるし…）、手を抜きました、すいません&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>DNN統計的音声合成ツールキット Merlin の中身を理解をする</title>
      <link>https://r9y9.github.io/blog/2017/08/16/trying-to-understand-merlin/</link>
      <pubDate>Wed, 16 Aug 2017 03:00:00 +0900</pubDate>
      <guid>https://r9y9.github.io/blog/2017/08/16/trying-to-understand-merlin/</guid>
      <description>&lt;p&gt;この記事では、音声合成ツールキットであるMerlinが、具体的に何をしているのか（特徴量の正規化、無音区間の削除、ポストフィルタなど、コードを読まないとわからないこと）、その中身を僕が理解した範囲でまとめます。
なお、HMM音声合成について簡単に理解していること（HMMとは、状態とは、フルコンテキストラベルとは、くらい）を前提とします。&lt;/p&gt;
&lt;h2 id=&#34;はじめに&#34;&gt;はじめに&lt;/h2&gt;
&lt;p&gt;Merlinの概要については以下をご覧ください。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://ssw9.net/papers/ssw9_PS2-13_Wu.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wu, Zhizheng, Oliver Watts, and Simon King. &amp;ldquo;Merlin: An open source neural network speech synthesis system.&amp;rdquo; Proc. SSW, Sunnyvale, USA (2016).&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://ssw9.net/papers/ssw9_DS-3_Ronanki.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;A Demonstration of the
Merlin Open Source Neural Network Speech Synthesis System&amp;rdquo;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cstr-edinburgh.github.io/merlin/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;公式ドキュメント&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Merlinにはデモスクリプトがついています。基本的にユーザが使うインタフェースはrun_merlin.pyというコマンドラインスクリプトで、
デモスクリプトではrun_merlin.pyに用途に応じた設定ファイルを与えることで、継続長モデルの学習/音響モデルの学習/パラメータ生成など、音声合成に必要なステップを実現しています。&lt;/p&gt;
&lt;p&gt;デモスクリプトを実行すると、音声データ (wav) と言語特徴量（HTSのフルコンテキストラベル）から、変換音声が合成されるところまでまるっとやってくれるのですが、それだけでは内部で何をやっているのか、理解することはできません。
ツールキットを使う目的が、自分が用意したデータセットで音声合成器を作りたい、といった場合には、特に内部を知る必要はありません。
また、設定ファイルをちょこっといじるだけでこと済むのであれば、知る必要はないかもしれません。
しかし、モデル構造を変えたい、学習アルゴリズムを変えたい、ポストフィルタを入れたい、といったように、少し進んだ使い方をしようとすれば、内部構造を理解しないとできないことも多いと思います。&lt;/p&gt;
&lt;p&gt;run_merlin.py はあらゆる処理 (具体的にはあとで述べます) のエントリーポイントになっているがゆえに、コードはなかなかに複雑になっています&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;。この記事では、run_merlin.pyがいったい何をしているのかを読み解いた結果をまとめます。&lt;/p&gt;
&lt;h2 id=&#34;merlinでは提供しないこと&#34;&gt;Merlinでは提供しないこと&lt;/h2&gt;
&lt;p&gt;Merlinが何を提供してくれるのかを理解する前に、何を提供しないのか、をざっくりと整理します。以下のとおりです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Text-processing (&lt;strong&gt;Frontend&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;Speech analysis/synthesis (&lt;strong&gt;Backend&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;HTSと同様に、frontend, backendといった部分は提供していません。Merlinの論文にもあるように、HTSの影響を受けているようです。&lt;/p&gt;
&lt;p&gt;Frontendには、英語ならFestival、BackendにはWORLDやSTRAIGHTを使ってよろしくやってね、というスタンスです。
Backendに関しては、Merlinのインストールガイドにあるように、WOLRDをインストールするように促されます。&lt;/p&gt;
&lt;p&gt;デモスクリプトでは、Frontendによって生成されたフルコンテキストラベル（HTS書式）が事前に同梱されているので、Festivalをインストールする必要はありません。
misc以下に、Festivalを使ってフルコンテキストラベルを作るスクリプト (make_labels) があるので、デモデータ以外のデータセットを使う場合は、それを使います。&lt;/p&gt;
&lt;h2 id=&#34;steps&#34;&gt;Steps&lt;/h2&gt;
&lt;p&gt;本編です。slt_arcticのデモスクリプトに従い、いくらかのステップに分けて、詳細に見ていきます。なお、以下デモスクリプトと書いた際には、slt_arcticのデモスクリプトを指すものとします。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;継続長モデルの学習&lt;/li&gt;
&lt;li&gt;音響モデルの学習&lt;/li&gt;
&lt;li&gt;変換音声の合成&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;なお、Merlinのスクリプトによってはかれるデータは、基本的に&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;x.astype(np.float32).tofile(&amp;quot;foobar.bin&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;といった感じで、32bit浮動小数点のnumpyの配列がヘッダなしのバイナリフォーマットで保存されています。デバッグ時には、&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;np.fromfile(&amp;quot;foobar.bin&amp;quot;, dtype=np.float32)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;として、ファイルを読み込んでインスペクトするのが便利です。注意事項として、ややこしいことに、拡張子は信頼できません。&lt;code&gt;.lab&lt;/code&gt; という拡張子であっても、フルコンテキストラベルのテキストファイルである場合もあれば、上述のようにバイナリフォーマットである可能性もあります。つらいですね！&lt;/p&gt;
&lt;h3 id=&#34;継続長モデルの学習&#34;&gt;継続長モデルの学習&lt;/h3&gt;
&lt;p&gt;継続長モデルとは、言語特徴量から、継続長を予測するモデルです。Merlinでは、phone-level / state-level のどちらかを選択可能です。Merlinの提供するDNN音声合成では、継続長の予測→音響特徴量の予測→合成、といったスタイルをとります。
デフォルトでは、state-levelで継続長（具体的には一状態当たりの継続フレーム数）を予測します。状態レベルのアライメントのほうが、時間解像度の高いコンテキストを得られ、結果音声合成の品質が良くなるので、デフォルトになっているのだと思います。 &lt;a href=&#34;https://github.com/CSTR-Edinburgh/merlin/issues/18&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/CSTR-Edinburgh/merlin/issues/18&lt;/a&gt; に少し議論があります。&lt;/p&gt;
&lt;p&gt;デモスクリプトを実行すると、 &lt;code&gt;experiments/slt_arctic_demo/duration_model/&lt;/code&gt; 以下に継続長モデル用のデータがは出力されます。いくつか重要なものについて、以下に示します。&lt;/p&gt;
&lt;h4 id=&#34;data&#34;&gt;data&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;label_phone_align&lt;/code&gt;: 音素レベルでのフルコンテキストラベルです&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dur&lt;/code&gt;: 状態別継続長です。正確には、&lt;code&gt;T&lt;/code&gt; をフルコンテキストラベル中の音素数として、&lt;code&gt;(T, 5)&lt;/code&gt; の配列が発話ごとに保存されます。5は音素あたりのHMMの状態数で、慣例的に？5が使用されているような気がします。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;inter_module&#34;&gt;inter_module&lt;/h4&gt;
&lt;p&gt;中間結果のファイル群です&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;binary_label_416/&lt;/code&gt;: HTS形式の質問ファイルを元に生成した、言語特徴量行列です。デモスクリプトでは、416個の質問があるので、一状態あたり416次元の特徴ベクトルになります。binaryな特徴量（母音か否か）と連続的な特徴量（単語中のsylalbleの数等）があります。&lt;code&gt;(T, 416)&lt;/code&gt; の行列が、発話ごとに保存されています。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;label_norm_HTS_416.dat&lt;/code&gt;: 416次元の特徴ベクトルの正規化に必要な情報です。デモスクリプトでは、言語特徴量に関してはmin/max正規化が行われるので、minおよびmaxの416次元のベクトル（計416*2次元）が保存されています。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nn_dur_5/&lt;/code&gt;: 無音区間が除去された、状態別継続長です。フォルダ名からは察することは難しいですが、無音区間が除去されています。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nn_no_silence_lab_416/&lt;/code&gt;: 無音区間が除去された、言語特徴量行列です。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nn_no_silence_lab_norm_416/&lt;/code&gt;: 無音区間が除去された、min/max正規化された言語特徴量行列です。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nn_norm_dur_5/&lt;/code&gt; 無音区間が除去された、mean/variance正規化された状態別継続長です。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;norm_info_dur_5_MVN.dat&lt;/code&gt;: 継続長の正規化に必要な情報です。具体的には、Mean-variance正規化（N(0, 1)になるようにする）が行われるので、平均と標準偏差（not 分散）が入っています。状態レベルでのアライメントを使用する場合は、5*2で計10次元のベクトルです。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ref_data/&lt;/code&gt;: RMSEなどの評価基準を計算する際に使われる継続長のテストデータです。&lt;code&gt;data/dur&lt;/code&gt; ディレクトリの継続長データを元に、無音区間が除去されたものです&lt;/li&gt;
&lt;li&gt;&lt;code&gt;var/&lt;/code&gt;: 継続長の分散（not 標準偏差）です。パラメータ生成 (MLPG) に使われる想定のデータです&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;けっこうたくさんありますね。これだけでも、いかに多くのことがrun_merlin.pyによってなされているか、わかるかと思います。&lt;/p&gt;
&lt;h4 id=&#34;入力出力&#34;&gt;入力/出力&lt;/h4&gt;
&lt;p&gt;中間ファイルがたくさんあってややこしいですが、整理すると、ネットワーク学習に用いる入力と出力は以下になります。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;入力: &lt;code&gt;nn_no_silence_lab_norm_416&lt;/code&gt;, 一発話あたりの特徴量のshape: &lt;code&gt;(T, 416)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;出力: &lt;code&gt;nn_norm_dur_5&lt;/code&gt;, 一発話あたりの特徴量のshape: &lt;code&gt;(T, 5)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;学習されたモデルは、 &lt;code&gt;nnets_model&lt;/code&gt;というフォルダに保存されます。&lt;/p&gt;
&lt;h3 id=&#34;音響モデルの学習&#34;&gt;音響モデルの学習&lt;/h3&gt;
&lt;p&gt;音響モデルとは、言語特徴量からメルケプストラム、F0、非周期性成分などの音響特徴量を予測するモデルです。Merlinのデモスクリプトでは、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;メルケプストラム: 60次元（動的特徴量を合わせると、180次元)&lt;/li&gt;
&lt;li&gt;対数F0: 1次元（動的特徴量を合わせると、3次元)&lt;/li&gt;
&lt;li&gt;有声 or 無声フラグ (voiced/unvoiced; vuv): 1次元&lt;/li&gt;
&lt;li&gt;非周期性成分: 1次元（動的特徴量を合わせると、3次元)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;の計187次元の音響特徴量を予測するモデルを考えます。継続長モデルのときと同様に、出力されるファイルについていくらか説明します。&lt;/p&gt;
&lt;h4 id=&#34;data-1&#34;&gt;data&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;bap&lt;/code&gt;: 発話毎に計算された非周期性成分が入っています。bapはband averaged aperiodicityの略です（専門家の人にとっては当たり前かと思いますが、一応&lt;/li&gt;
&lt;li&gt;&lt;code&gt;label_phone_align&lt;/code&gt;: phone-levelでアライメントされたHTSのコンテキストラベルが入っています。デフォルトの設定では使いません。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;label_state_align&lt;/code&gt;: state-levelでアライメントされたHTSのコンテキストラベルが入っています&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lf0&lt;/code&gt;: 対数F0です。なお、WORLDではかれるF0は無声区間で0を取りますが、無声区間の部分を線形補間することによって、非ゼロの値で補完しています。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mgc&lt;/code&gt;: メルケプストラムです（フォルダ名は、慣習的にメル一般化ケプストラムを表す &lt;code&gt;mgc&lt;/code&gt;となっていますが、デモスクリプトでは実際にはメルケプストラムです）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;inter_module-1&#34;&gt;inter_module&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;binary_label_425/&lt;/code&gt;: 言語特徴量の行列です。継続長モデルの場合と違って、フレーム単位で生成されているのと、フレーム単位ならではの特徴量（音素中の何フレーム目なのか、等）が追加されています。フレーム数を &lt;code&gt;T&lt;/code&gt; として、 &lt;code&gt;(T, 425)&lt;/code&gt; の配列が発話ごとに保存されています。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;label_norm_HTS_425.dat&lt;/code&gt;: 言語特徴量のmin/max正規化に必要なmin/maxのベクトルです。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nn_mgc_lf0_vuv_bap_187/&lt;/code&gt;: mgc, lf0, vuv, bapを結合した音響特徴量です。よくcmp (composed featureから来ていると思われる) と表されるものです。ディレクトリ名からは判別が付きませんが、無音区間は削除されています。ややこしい&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nn_no_silence_lab_425/&lt;/code&gt;: &lt;code&gt;binary_label_425&lt;/code&gt; の言語特徴量から無音区間を削除したものです&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nn_no_silence_lab_norm_425/&lt;/code&gt;: それをさらにmin/max正規化したものです&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nn_norm_mgc_lf0_vuv_bap_187/&lt;/code&gt;: &lt;code&gt;nn_mgc_lf0_vuv_bap_187/&lt;/code&gt;の音響特徴量をN(0, 1)になるようにmean/variance正規化したものです&lt;/li&gt;
&lt;li&gt;&lt;code&gt;norm_info_mgc_lf0_vuv_bap_187_MVN.dat&lt;/code&gt;: 音響特徴量の正規化に必要な、平均と標準偏差です&lt;/li&gt;
&lt;li&gt;&lt;code&gt;var/&lt;/code&gt;: mgc, lf0, bap, vuvそれぞれの分散です。このうちvuvは、パラメータ生成時にMLPGを行いませんが、保存はされています。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;入力出力-1&#34;&gt;入力/出力&lt;/h4&gt;
&lt;p&gt;継続長モデルの場合と同様の中間特徴量が出力されています。改めて整理すると、音響モデルの学習に使用する入力と出力は、以下のとおりです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;入力: &lt;code&gt;nn_no_silence_lab_norm_425/&lt;/code&gt;, 一発話あたりの特徴量のshape: &lt;code&gt;(T, 425)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;出力: &lt;code&gt;nn_norm_mgc_lf0_vuv_bap_187&lt;/code&gt;, 一発話あたりの特徴量のshape: &lt;code&gt;(T, 187)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;学習されたモデルは、 &lt;code&gt;nnets_model&lt;/code&gt;というフォルダに保存されます。&lt;/p&gt;
&lt;h3 id=&#34;波形生成&#34;&gt;波形生成&lt;/h3&gt;
&lt;p&gt;得られた継続長モデルと音響モデルから、波形を生成する処理は、大雑把にいって以下の手順で行われます。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;フルコンテキストラベルから得られる言語特徴量を元に、継続長モデルを使って継続長を予測する&lt;/li&gt;
&lt;li&gt;予測された継続長を使って、フルコンテキストラベルを書き換える。より具体的には、状態毎の start_time, end_time の部分を書き換える。&lt;/li&gt;
&lt;li&gt;書き換えられたフルコンテキストラベルから、音響モデル用のフレームレベルの言語特徴量を計算し、音響モデルを使って音響特徴量を予測する&lt;/li&gt;
&lt;li&gt;予測された音響特徴量（static + delta + delta-delta) から、静的特徴量をMLPGによって生成する。MLPGによって生成するのは、mgc, lf0, bapのみで、vuvについてはそのまま使う。波形合成にはvuvを直接使うのではなく、vuv &amp;lt; 0.5以下のf0を0として扱う。&lt;/li&gt;
&lt;li&gt;生成されたメルケプストラムに対して、Merlinお手製ポストフィルタを掛ける&lt;/li&gt;
&lt;li&gt;得られた音響特徴量 (mgc, f0, bap) から、WORLDを使って波形合成をする&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以上です。Merlinの良い所の一つに、ログをたくさんはいてくれるというのがあります。しかし、このうちポストフィルタ（デフォルトでONです）に関しては一切（デフォルトでは）ログがはかれず、気づくのに時間がかかりました。&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;また、個人的な感覚ですが、このポストフィルタの影響は絶大に思いました。コードを見て、何をしているのか僕には理解できませんでしたが、ヒューリスティックな方法も含んでいるように思いました。興味のある方は、 波形合成用のconfファイルを開いて、&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[Waveform]
do_post_filtering: False
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;のように、&lt;code&gt;[Waveform]&lt;/code&gt; セクションに &lt;code&gt;do_post_filtering&lt;/code&gt; という項目を加えて、生成結果を聴き比べてみることをおすすめします。ポストフィルタによって劇的に音質が改善されているのがわかると思います。さらに興味のある方は、コードを読んでみてください。参考文献も探しましたが、僕には見つかりませんでした。ご存知の方がいれば教えていただきたいです。&lt;/p&gt;
&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;
&lt;p&gt;Merlin、最初は使いにくいなと思っていましたが、頑張って読んでみれば、とても勉強になりました（使いやすいとは言っていない）。後半はだれて、適当なまとめになってしまったかもしれません、すいません。もろもろの不満から&lt;a href=&#34;https://github.com/r9y9/nnmnkwii&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;新しいツール&lt;/a&gt;を作りましたが、それはまた別の機会に紹介したいと思います。ありがとうございました。&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://jrmeyer.github.io/merlin/2017/02/14/Installing-Merlin.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://jrmeyer.github.io/merlin/2017/02/14/Installing-Merlin.html&lt;/a&gt; によれば、This is a very clearly written Python script だそうです…。僕に読解力がないだけの可能性があります&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;自分で作ったモデルが、どうしてもmerlinに勝てない、なぜだ、と悩んでいたとき、Merlinに言及している論文の一つに、ポストフィルタを使っているとの記述があり、探ってみるとたしかにあった、という感じでした。&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>PythonによるニューラルネットのToyコード</title>
      <link>https://r9y9.github.io/blog/2014/05/11/python-feed-forward-neural-network-toy-code/</link>
      <pubDate>Sun, 11 May 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/05/11/python-feed-forward-neural-network-toy-code/</guid>
      <description>&lt;p&gt;1000番煎じだけど、知り合いにニューラルネットを教えていて、その過程で書いたコード。わかりやすさ重視。&lt;/p&gt;
&lt;p&gt;このために、誤差伝播法をn回導出しました（意訳：何回もメモなくしました）&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/usr/bin/python
# coding: utf-8

# ニューラルネットワーク(Feed-Forward Neural Networks)の学習、認識の
# デモコードです。
# 誤差伝搬法によってニューラルネットを学習します。
# XORの学習、テストの簡単なデモコードもついています
# 2014/05/10 Ryuichi Yamamoto

import numpy as np

def sigmoid(x):
    return 1.0 / (1.0 + np.exp(-x))

def dsigmoid(y):
    return y * (1.0 - y)

class NeuralNet:
    def __init__(self, num_input, num_hidden, num_output):
        &amp;quot;&amp;quot;&amp;quot;
        パラメータの初期化
        &amp;quot;&amp;quot;&amp;quot;
        # 入力層から隠れ層への重み行列
        self.W1 = np.random.uniform(-1.0, 1.0, (num_input, num_hidden))
        self.hidden_bias = np.ones(num_hidden, dtype=float)
        # 隠れ層から出力層への重み行列
        self.W2 = np.random.uniform(-1.0, 1.0, (num_hidden, num_output))
        self.output_bias = np.ones(num_output, dtype=float)

    def forward(self, x):
        &amp;quot;&amp;quot;&amp;quot;
        前向き伝搬の計算
        &amp;quot;&amp;quot;&amp;quot;
        h = sigmoid(np.dot(self.W1.T, x) + self.hidden_bias)
        return sigmoid(np.dot(self.W2.T, h) + self.output_bias)

    def cost(self, data, target):
        &amp;quot;&amp;quot;&amp;quot;
        最小化したい誤差関数
        &amp;quot;&amp;quot;&amp;quot;
        N = data.shape[0]
        E = 0.0
        for i in range(N):
            y, t = self.forward(data[i]), target[i]
            E += np.sum((y - t) * (y - t))
        return 0.5 * E / float(N)

    def train(self, data, target, epoches=30000, learning_rate=0.1,\
              monitor_period=None):
        &amp;quot;&amp;quot;&amp;quot;
        Stochastic Gradient Decent (SGD) による学習
        &amp;quot;&amp;quot;&amp;quot;
        for epoch in range(epoches):
            # 学習データから1サンプルをランダムに選ぶ
            index = np.random.randint(0, data.shape[0])
            x, t = data[index], target[index]

            # 入力から出力まで前向きに信号を伝搬
            h = sigmoid(np.dot(self.W1.T, x) + self.hidden_bias)
            y = sigmoid(np.dot(self.W2.T, h) + self.output_bias)

            # 隠れ層-&amp;gt;出力層の重みの修正量を計算
            output_delta = (y - t) * dsigmoid(y)
            grad_W2 = np.dot(np.atleast_2d(h).T, np.atleast_2d(output_delta))

            # 隠れ層-&amp;gt;出力層の重みを更新
            self.W2 -= learning_rate * grad_W2
            self.output_bias -= learning_rate * output_delta

            # 入力層-&amp;gt;隠れ層の重みの修正量を計算
            hidden_delta = np.dot(self.W2, output_delta) * dsigmoid(h)
            grad_W1 = np.dot(np.atleast_2d(x).T, np.atleast_2d(hidden_delta))

            # 入力層-&amp;gt;隠れ層の重みを更新
            self.W1 -= learning_rate * grad_W1
            self.hidden_bias -= learning_rate * hidden_delta

            # 現在の目的関数の値を出力
            if monitor_period != None and epoch % monitor_period == 0:
                print &amp;quot;Epoch %d, Cost %f&amp;quot; % (epoch, self.cost(data, target))

        print &amp;quot;Training finished.&amp;quot;

    def predict(self, x):
        &amp;quot;&amp;quot;&amp;quot;
        出力層の最も反応するニューロンの番号を返します
        &amp;quot;&amp;quot;&amp;quot;
        return np.argmax(self.forward(x))

if __name__ == &amp;quot;__main__&amp;quot;:
    import argparse

    parser = argparse.ArgumentParser(description=&amp;quot;Specify options&amp;quot;)
    parser.add_argument(&amp;quot;--epoches&amp;quot;, dest=&amp;quot;epoches&amp;quot;, type=int, required=True)
    parser.add_argument(&amp;quot;--learning_rate&amp;quot;, dest=&amp;quot;learning_rate&amp;quot;,\
                        type=float, default=0.1)
    parser.add_argument(&amp;quot;--hidden&amp;quot;, dest=&amp;quot;hidden&amp;quot;, type=int, default=20)
    args = parser.parse_args()

    nn = NeuralNet(2, args.hidden, 1)

    data = np.array([[0, 0], [0 ,1], [1, 0], [1, 1]])
    target = np.array([0, 1, 1, 0])

    nn.train(data, target, args.epoches, args.learning_rate,\
             monitor_period=1000)

    for x in data:
        print &amp;quot;%s : predicted %s&amp;quot; % (x, nn.forward(x))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/usr/bin/python
# coding: utf-8

# MNISTを用いたニューラルネットによる手書き数字認識のデモコードです
# 学習方法やパラメータによりますが、だいたい 90 ~ 97% くらいの精度出ます。
# 使い方は、コードを読むか、
# python mnist_net.py -h
# としてください
# 参考までに、
# python mnist_net.py --epoches 50000 --learning_rate 0.1 --hidden 100
# とすると、テストセットに対して、93.2%の正解率です
# 僕の環境では、学習、認識合わせて（だいたい）5分くらいかかりました。
# 2014/05/10 Ryuichi Yamamoto

import numpy as np
from sklearn.externals import joblib
import cPickle
import gzip
import os

# 作成したニューラルネットのパッケージ
import net

def load_mnist_dataset(dataset):
    &amp;quot;&amp;quot;&amp;quot;
    MNISTのデータセットをダウンロードします
    &amp;quot;&amp;quot;&amp;quot;
    # Download the MNIST dataset if it is not present
    data_dir, data_file = os.path.split(dataset)
    if (not os.path.isfile(dataset)) and data_file == &#39;mnist.pkl.gz&#39;:
        import urllib
        origin = &#39;http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz&#39;
        print &#39;Downloading data from %s&#39; % origin
        urllib.urlretrieve(origin, dataset)

    f = gzip.open(dataset, &#39;rb&#39;)
    train_set, valid_set, test_set = cPickle.load(f)
    f.close()

    return train_set, valid_set, test_set

def augument_labels(labels, order):
    &amp;quot;&amp;quot;&amp;quot;
    1次元のラベルデータを、ラベルの種類数(order)次元に拡張します
    &amp;quot;&amp;quot;&amp;quot;
    new_labels = []
    for i in range(labels.shape[0]):
        v = np.zeros(order)
        v[labels[i]] = 1
        new_labels.append(v)

    return np.array(new_labels).reshape((labels.shape[0], order))

if __name__ == &amp;quot;__main__&amp;quot;:
    import argparse

    parser = argparse.ArgumentParser(description=&amp;quot;MNIST手書き数字認識のデモ&amp;quot;)
    parser.add_argument(&amp;quot;--epoches&amp;quot;, dest=&amp;quot;epoches&amp;quot;, type=int, required=True)
    parser.add_argument(&amp;quot;--learning_rate&amp;quot;, dest=&amp;quot;learning_rate&amp;quot;,\
                        type=float, default=0.1)
    parser.add_argument(&amp;quot;--hidden&amp;quot;, dest=&amp;quot;hidden&amp;quot;, type=int, default=100)
    args = parser.parse_args()

    train_set, valid_set, test_set = load_mnist_dataset(&amp;quot;mnist.pkl.gz&amp;quot;)
    n_labels = 10 # 0,1,2,3,4,5,6,7,9
    n_features = 28*28

    # モデルを新しく作る
    nn = net.NeuralNet(n_features, args.hidden, n_labels)

    # モデルを読み込む
    # nn = joblib.load(&amp;quot;./nn_mnist.pkl&amp;quot;)

    nn.train(train_set[0], augument_labels(train_set[1], n_labels),\
             args.epoches, args.learning_rate, monitor_period=2000)

    ## テスト
    test_data, labels = test_set
    results = np.arange(len(test_data), dtype=np.int)
    for n in range(len(test_data)):
        results[n] = nn.predict(test_data[n])
        # print &amp;quot;%d : predicted %s, expected %s&amp;quot; % (n, results[n], labels[n])
    print &amp;quot;recognition rate: &amp;quot;, (results == labels).mean()

    # モデルを保存
    model_filename = &amp;quot;nn_mnist.pkl&amp;quot;
    joblib.dump(nn, model_filename, compress=9)
    print &amp;quot;The model parameters are dumped to &amp;quot; + model_filename
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/python-neural-net-toy-codes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/python-neural-net-toy-codes&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;以下のようなコマンドを叩いて、正解率が97%くらいになるまで学習してから入力層から隠れ層への重みを可視化してみた&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# python mnist_net.py --epoches 50000 --learning_rate 0.1 --hidden 100 # epochesは適当に
&lt;/code&gt;&lt;/pre&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/nn_mnist_W1_100.png&#34; alt=&#34;Input to Hidden weight filters after trained on MNIST.&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;興味深いことに、RBMと違って重み行列の解釈はしにくい。生成モデルの尤度を最大化することと、誤差を最小化することはこんなにも違うんだなぁというこなみな感想&lt;/p&gt;
&lt;p&gt;RBMについては、以下へ&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://r9y9.github.io/blog/2014/03/06/restricted-boltzmann-machines-mnist/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Restricted Boltzmann Machines with MNIST - LESS IS MORE&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;おわり&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Restricted Boltzmann Machines with MNIST</title>
      <link>https://r9y9.github.io/blog/2014/03/06/restricted-boltzmann-machines-mnist/</link>
      <pubDate>Thu, 06 Mar 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/03/06/restricted-boltzmann-machines-mnist/</guid>
      <description>&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/RBM_mnist_Hidden_500_layers.png &#34;RBM training result on MNIST handwritten digit dataset. Each image represents a filter learned by RBM.&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;ディープ某を使った研究を再現してみたくて、最近某ニューラルネットに手を出し始めた。で、手始めにRestricted Boltzmann Machinesを実装してみたので、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MNISTを使って学習した結果の重み（22*22=484個）を貼っとく（↑）&lt;/li&gt;
&lt;li&gt;得た知見をまとめとく&lt;/li&gt;
&lt;li&gt;Goのコード貼っとく&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ってな感じで書いておく&lt;/p&gt;
&lt;p&gt;(本当はRBMについて自分なりの解釈を書こうと思ったのだけど、それはまた今度)&lt;/p&gt;
&lt;h2 id=&#34;実験条件&#34;&gt;実験条件&lt;/h2&gt;
&lt;p&gt;データベースはmnist。手書き数字認識で有名なアレ。学習の条件は、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;隠れ層のユニット数: 500&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;mini-batch size: 20&lt;/li&gt;
&lt;li&gt;iterationの回数: 15&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;対数尤度の変化&#34;&gt;対数尤度の変化&lt;/h2&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/RBM_mnist_Hidden_500_log_likelihood.png &#34;Pseudo log-likelihood on mnist databae.&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;以下グラフに表示している生データ&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0 -196.59046099622128
1 -70.31708616742365
2 -65.29499371647965
3 -62.37983267378022
4 -61.5359019358253
5 -60.917772257650164
6 -59.64207778426757
7 -59.42201674307857
8 -59.18497336138633
9 -58.277168243126305
10 -58.36279288392401
11 -58.57805165724595
12 -57.71043215987184
13 -58.17783142034138
14 -57.53629129936344
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;尤度上がると安心する。厳密に対数尤度を計算することは難しいので、&lt;a href=&#34;http://deeplearning.net/tutorial/rbm.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Restricted Boltzmann Machines (RBM) | DeepLearning Tutorial&lt;/a&gt; にある擬似尤度を参考にした&lt;/p&gt;
&lt;h2 id=&#34;学習時間&#34;&gt;学習時間&lt;/h2&gt;
&lt;p&gt;うちのcore2duoのPCで4時間弱だった気がする（うろ覚え&lt;/p&gt;
&lt;p&gt;隠れ層のユニット数100だと、40分ほどだった&lt;/p&gt;
&lt;h2 id=&#34;知見&#34;&gt;知見&lt;/h2&gt;
&lt;p&gt;今の所、試行錯誤して自分が得た知見は、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sample by sampleのSGDよりmini-batch SGDの方が安定して尤度上がる&lt;/li&gt;
&lt;li&gt;mini-batch sizeを大きくしすぎると学習が進まない。20くらいがちょうど良かった&lt;/li&gt;
&lt;li&gt;k-CD のkを大きくしてもさほど学習結果変わらない（計算コストはけっこう増すけど）&lt;/li&gt;
&lt;li&gt;persistent CDを使ってもあまりよくならない（計算コストはけっこう増すけど）&lt;/li&gt;
&lt;li&gt;やっぱ1-CDで十分だった&lt;/li&gt;
&lt;li&gt;データの正規化方法によって結構結果も変わる。ノイズを足すかどうか、とか&lt;/li&gt;
&lt;li&gt;学習率超重要すぎわろた。今回の場合は0.1くらいかちょうど良かった&lt;/li&gt;
&lt;li&gt;隠れ層のユニット数が大きいほど学習が上手く行けばと尤度は上がる(?)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;まぁだいたい &lt;a href=&#34;http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Practical Guide to Training Restricted Boltzmann Machines (PDF)&lt;/a&gt; に書いてあるけど、実際に肌で感じて理解した。persistent CDはもうちょっと成果出て欲しい。データ変えると成果出るんかな？&lt;/p&gt;
&lt;h2 id=&#34;コード&#34;&gt;コード&lt;/h2&gt;
&lt;p&gt;コアの部分だけ、一応&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package rbm

import (
	&amp;quot;encoding/json&amp;quot;
	&amp;quot;errors&amp;quot;
	&amp;quot;fmt&amp;quot;
	&amp;quot;github.com/r9y9/nn&amp;quot; // sigmoid, matrix
	&amp;quot;math&amp;quot;
	&amp;quot;math/rand&amp;quot;
	&amp;quot;os&amp;quot;
	&amp;quot;time&amp;quot;
)

// References:
// [1] G. Hinton, &amp;quot;A Practical Guide to Training Restricted Boltzmann Machines&amp;quot;,
// UTML TR 2010-003.
// url: http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf
//
// [2] A. Fischer and C. Igel. &amp;quot;An introduction to restricted Boltzmann machines&amp;quot;,
// Proc. of the 17th Iberoamerican Congress on Pattern Recognition (CIARP),
// Volume 7441 of LNCS, pages 14–36. Springer, 2012
// url: http://image.diku.dk/igel/paper/AItRBM-proof.pdf
//
// [3] Restricted Boltzmann Machines (RBM),  DeepLearning tutorial
// url: http://deeplearning.net/tutorial/rbm.html

// Notes about implementation:
// Notation used in this code basically follows [2].
// e.g. W for weight, B for bias of visible layer, C for bias of hidden layer.

// Graphical representation of Restricted Boltzmann Machines (RBM).
//
//     ○ ○ .... ○  h(hidden layer), c(bias)
//     /\ /\ /    /\
//    ○ ○ ○ ... ○ v(visible layer), b(bias)
type RBM struct {
	W               [][]float64 // Weight
	B               []float64   // Bias of visible layer
	C               []float64   // Bias of hidden layer
	NumHiddenUnits  int
	NumVisibleUnits int
	Option          TrainingOption
}

type TrainingOption struct {
	LearningRate        float64
	OrderOfGibbsSamping int // It is known that 1 is enough for many cases.
	Epoches             int
	MiniBatchSize       int
	L2Regularization    bool
	RegularizationRate  float64
}

// NewRBM creates new RBM instance. It requires input data and number of
// hidden units to initialize RBM.
func NewRBM(numVisibleUnits, numHiddenUnits int) *RBM {
	rbm := new(RBM)
	rand.Seed(time.Now().UnixNano())

	rbm.W = nn.MakeMatrix(numHiddenUnits, numVisibleUnits)
	rbm.B = make([]float64, numVisibleUnits)
	rbm.C = make([]float64, numHiddenUnits)
	rbm.NumVisibleUnits = numVisibleUnits
	rbm.NumHiddenUnits = numHiddenUnits

	rbm.InitRBM()
	return rbm
}

// NewRBMWithParameters returns RBM instance given RBM parameters.
// This func will be used in Deep Belief Networks.
func NewRBMWithParameters(W [][]float64, B, C []float64) (*RBM, error) {
	rbm := new(RBM)

	rbm.NumVisibleUnits = len(B)
	rbm.NumHiddenUnits = len(C)

	if len(W) != rbm.NumHiddenUnits || len(W[0]) != rbm.NumVisibleUnits {
		return nil, errors.New(&amp;quot;Shape of weight matrix is wrong.&amp;quot;)
	}

	rand.Seed(time.Now().UnixNano())
	rbm.W = W
	rbm.B = B
	rbm.C = C

	return rbm, nil
}

// LoadRBM loads RBM from a dump file and return its instatnce.
func LoadRBM(filename string) (*RBM, error) {
	file, err := os.Open(filename)
	if err != nil {
		return nil, err
	}
	defer file.Close()

	decoder := json.NewDecoder(file)
	rbm := &amp;amp;RBM{}
	err = decoder.Decode(rbm)

	if err != nil {
		return nil, err
	}

	return rbm, nil
}

// Dump writes RBM parameters to file in json format.
func (rbm *RBM) Dump(filename string) error {
	file, err := os.Create(filename)
	if err != nil {
		return err
	}
	defer file.Close()

	encoder := json.NewEncoder(file)
	err = encoder.Encode(rbm)
	if err != nil {
		return err
	}

	return nil
}

// Heuristic initialization of visible bias.
func (rbm *RBM) InitVisibleBiasUsingTrainingData(data [][]float64) {
	// Init B (bias of visible layer)
	activeRateInVisibleLayer := rbm.getActiveRateInVisibleLayer(data)
	for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
		rbm.B[j] = math.Log(activeRateInVisibleLayer[j] / (1.0 - activeRateInVisibleLayer[j]))
	}
}

func (rbm *RBM) getActiveRateInVisibleLayer(data [][]float64) []float64 {
	rate := make([]float64, rbm.NumVisibleUnits)
	for _, sample := range data {
		for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
			rate[j] += sample[j]
		}
	}
	for j := range rate {
		rate[j] /= float64(len(data))
	}
	return rate
}

// InitRBM performes a heuristic parameter initialization.
func (rbm *RBM) InitRBM() {
	// Init W
	for i := 0; i &amp;lt; rbm.NumHiddenUnits; i++ {
		for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
			rbm.W[i][j] = 0.01 * rand.NormFloat64()
		}
	}

	for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
		rbm.B[j] = 0.0
	}

	// Init C (bias of hidden layer)
	for i := 0; i &amp;lt; rbm.NumHiddenUnits; i++ {
		rbm.C[i] = 0.0
	}
}

// P_H_Given_V returns the conditinal probability of a hidden unit given a set of visible units.
func (rbm *RBM) P_H_Given_V(hiddenIndex int, v []float64) float64 {
	sum := 0.0
	for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
		sum += rbm.W[hiddenIndex][j] * v[j]
	}
	return nn.Sigmoid(sum + rbm.C[hiddenIndex])
}

// P_V_Given_H returns the conditinal probability of a visible unit given a set of hidden units.
func (rbm *RBM) P_V_Given_H(visibleIndex int, h []float64) float64 {
	sum := 0.0
	for i := 0; i &amp;lt; rbm.NumHiddenUnits; i++ {
		sum += rbm.W[i][visibleIndex] * h[i]
	}
	return nn.Sigmoid(sum + rbm.B[visibleIndex])
}

// GibbsSampling performs k-Gibbs sampling algorithm,
// where k is the number of iterations in gibbs sampling.
func (rbm *RBM) GibbsSampling(v []float64, k int) []float64 {
	// Initial value is set to input
	vUsedInSamping := make([]float64, len(v))
	copy(vUsedInSamping, v)

	for t := 0; t &amp;lt; k; t++ {
		sampledH := make([]float64, rbm.NumHiddenUnits)
		for i := 0; i &amp;lt; rbm.NumHiddenUnits; i++ {
			p := rbm.P_H_Given_V(i, vUsedInSamping)
			if p &amp;gt; rand.Float64() {
				sampledH[i] = 1.0
			} else {
				sampledH[i] = 0.0
			}
		}
		for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
			p := rbm.P_V_Given_H(j, sampledH)
			if p &amp;gt; rand.Float64() {
				vUsedInSamping[j] = 1.0
			} else {
				vUsedInSamping[j] = 0.0
			}
		}
	}

	return vUsedInSamping
}

func flip(x []float64, bit int) []float64 {
	y := make([]float64, len(x))
	copy(y, x)
	y[bit] = 1.0 - x[bit]
	return y
}

// FreeEnergy returns F(v), the free energy of RBM given a visible vector v.
// refs: eq. (25) in [1].
func (rbm *RBM) FreeEnergy(v []float64) float64 {
	energy := 0.0

	for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
		energy -= rbm.B[j] * v[j]
	}

	for i := 0; i &amp;lt; rbm.NumHiddenUnits; i++ {
		sum := rbm.C[i]
		for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
			sum += rbm.W[i][j] * v[j]
		}
		energy -= math.Log(1 + math.Exp(sum))
	}

	return energy
}

// PseudoLogLikelihood returns pseudo log-likelihood for a given input data.
func (rbm *RBM) PseudoLogLikelihood(v []float64) float64 {
	bitIndex := rand.Int() % len(v)
	fe := rbm.FreeEnergy(v)
	feFlip := rbm.FreeEnergy(flip(v, bitIndex))
	cost := float64(rbm.NumVisibleUnits) * math.Log(nn.Sigmoid(feFlip-fe))
	return cost
}

// PseudoLogLikelihood returns pseudo log-likelihood for a given dataset (or mini-batch).
func (rbm *RBM) PseudoLogLikelihoodForAllData(data [][]float64) float64 {
	sum := 0.0
	for i := range data {
		sum += rbm.PseudoLogLikelihood(data[i])
	}
	cost := sum / float64(len(data))
	return cost
}

// ComputeGradient returns gradients of RBM parameters for a given (mini-batch) dataset.
func (rbm *RBM) ComputeGradient(data [][]float64) ([][]float64, []float64, []float64) {
	gradW := nn.MakeMatrix(rbm.NumHiddenUnits, rbm.NumVisibleUnits)
	gradB := make([]float64, rbm.NumVisibleUnits)
	gradC := make([]float64, rbm.NumHiddenUnits)

	for _, v := range data {
		// Gibbs Sampling
		gibbsStart := v
		vAfterSamping := rbm.GibbsSampling(gibbsStart, rbm.Option.OrderOfGibbsSamping)

		// pre-computation that is used in gradient computation
		p_h_given_v1 := make([]float64, rbm.NumHiddenUnits)
		p_h_given_v2 := make([]float64, rbm.NumHiddenUnits)
		for i := 0; i &amp;lt; rbm.NumHiddenUnits; i++ {
			p_h_given_v1[i] = rbm.P_H_Given_V(i, v)
			p_h_given_v2[i] = rbm.P_H_Given_V(i, vAfterSamping)
		}

		// Gompute gradient of W
		for i := 0; i &amp;lt; rbm.NumHiddenUnits; i++ {
			for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
				gradW[i][j] += p_h_given_v1[i]*v[j] - p_h_given_v2[i]*vAfterSamping[j]
			}
		}

		// Gompute gradient of B
		for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
			gradB[j] += v[j] - vAfterSamping[j]
		}

		// Gompute gradient of C
		for i := 0; i &amp;lt; rbm.NumHiddenUnits; i++ {
			gradC[i] += p_h_given_v1[i] - p_h_given_v2[i]
		}
	}

	return gradW, gradB, gradC
}

func (rbm *RBM) ParseTrainingOption(option TrainingOption) error {
	rbm.Option = option

	if rbm.Option.MiniBatchSize &amp;lt;= 0 {
		return errors.New(&amp;quot;Number of mini-batchs must be larger than zero.&amp;quot;)
	}
	if rbm.Option.Epoches &amp;lt;= 0 {
		return errors.New(&amp;quot;Epoches must be larger than zero.&amp;quot;)
	}
	if rbm.Option.OrderOfGibbsSamping &amp;lt;= 0 {
		return errors.New(&amp;quot;Order of Gibbs sampling must be larger than zero.&amp;quot;)
	}
	if rbm.Option.LearningRate == 0 {
		return errors.New(&amp;quot;Learning rate must be specified to train RBMs.&amp;quot;)
	}

	return nil
}

// Train performs Contrastive divergense learning algorithm to train RBM.
// The alrogithm is basedd on (mini-batch) Stochastic Gradient Ascent.
func (rbm *RBM) Train(data [][]float64, option TrainingOption) error {
	err := rbm.ParseTrainingOption(option)
	if err != nil {
		return err
	}

	numMiniBatches := len(data) / rbm.Option.MiniBatchSize

	for epoch := 0; epoch &amp;lt; option.Epoches; epoch++ {
		// Monitoring
		fmt.Println(epoch, rbm.PseudoLogLikelihoodForAllData(data))

		for m := 0; m &amp;lt; numMiniBatches; m++ {
			// Compute Gradient
			batch := data[m*rbm.Option.MiniBatchSize : (m+1)*rbm.Option.MiniBatchSize]
			gradW, gradB, gradC := rbm.ComputeGradient(batch)

			// Update W
			for i := 0; i &amp;lt; rbm.NumHiddenUnits; i++ {
				for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
					rbm.W[i][j] += rbm.Option.LearningRate * gradW[i][j] / float64(rbm.Option.MiniBatchSize)
					if rbm.Option.L2Regularization {
						rbm.W[i][j] *= (1.0 - rbm.Option.RegularizationRate)
					}
				}
			}

			// Update B
			for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
				rbm.B[j] += rbm.Option.LearningRate * gradB[j] / float64(rbm.Option.MiniBatchSize)
			}

			// Update C
			for i := 0; i &amp;lt; rbm.NumHiddenUnits; i++ {
				rbm.C[i] += rbm.Option.LearningRate * gradC[i] / float64(rbm.Option.MiniBatchSize)
			}
		}
	}

	return nil
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;使い方とかは察して（どうせ誰も使わないはず&lt;/p&gt;
&lt;p&gt;今は、通常のRBMのvisible layerを連続値に拡張した Gaussian Bernoulli RBMを学習しようとしてるんだけど、これがムズイ。実装ミスもあるかもだけど、局所解に落ちまくってる気がする。&lt;/p&gt;
&lt;p&gt;Gaussian Bernoulli RBM、Deep Belief Networks, Deep Neural Networksについてはまた今度&lt;/p&gt;
&lt;p&gt;2014/05/11
要望があったので、もろもろコードあげました
&lt;a href=&#34;https://github.com/r9y9/nnet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/nnet&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;参考資料&#34;&gt;参考資料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://image.diku.dk/igel/paper/AItRBM-proof.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Introduction to Restricted Boltzmann Machines (PDF)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Practical Guide to Training Restricted Boltzmann Machines (PDF)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://mglab.blogspot.jp/2012/08/restricted-boltzmann-machine.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Restricted Boltzmann Machineの学習手法についての簡単なまとめ | 映像奮闘記&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://d.hatena.ne.jp/saket/20121212&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ゆるふわ Restricted Boltzmann Machine | Risky Dune&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://deeplearning.net/tutorial/rbm.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Restricted Boltzmann Machines (RBM) | DeepLearning Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://imonad.com/rbm/restricted-boltzmann-machine/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Restricted Boltzmann Machine - Short Tutorial | iMonad&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://scikit-learn.org/stable/auto_examples/plot_rbm_logistic_classification.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Restricted Boltzmann Machine features for digit classification | scikit-learn&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
