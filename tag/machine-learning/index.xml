<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning | LESS IS MORE</title>
    <link>https://r9y9.github.io/tag/machine-learning/</link>
      <atom:link href="https://r9y9.github.io/tag/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Machine Learning</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Copyright © Ryuichi YAMAMOTO All rights reserved.</copyright><lastBuildDate>Sun, 26 Apr 2015 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://r9y9.github.io/media/icon_hu71488a41e9448d472219f1cc71ecc0ad_259818_512x512_fill_lanczos_center_3.png</url>
      <title>Machine Learning</title>
      <link>https://r9y9.github.io/tag/machine-learning/</link>
    </image>
    
    <item>
      <title>JuliaTokyo #3 Speech Signal Processing in Julia</title>
      <link>https://r9y9.github.io/blog/2015/04/26/juliatokyo3-speech-signal-processing-in-julia/</link>
      <pubDate>Sun, 26 Apr 2015 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2015/04/26/juliatokyo3-speech-signal-processing-in-julia/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://juliatokyo.connpass.com/event/13218/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JuliaTokyo #3&lt;/a&gt;でLT発表してきました。前回の&lt;a href=&#34;https://juliatokyo.connpass.com/event/8010/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JuliaTokyo #2&lt;/a&gt;でも発表したので、二回目でした。&lt;/p&gt;
&lt;h2 id=&#34;スライド&#34;&gt;スライド&lt;/h2&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/h4geMoK1msYqdY&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt; &lt;div style=&#34;margin-bottom:5px&#34;&gt; &lt;strong&gt; &lt;a href=&#34;//www.slideshare.net/ryuichiy/juliatokyo-3-speech-signal-processing-in-julia-47403938&#34; title=&#34;JuliaTokyo #3 Speech Signal Processing in Julia&#34; target=&#34;_blank&#34;&gt;JuliaTokyo #3 Speech Signal Processing in Julia&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href=&#34;//www.slideshare.net/ryuichiy&#34; target=&#34;_blank&#34;&gt;Ryuichi YAMAMOTO&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;コード&#34;&gt;コード&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/JuliaTokyo3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/JuliaTokyo3&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;三行まとめ&#34;&gt;三行まとめ&lt;/h2&gt;
&lt;p&gt;発表の内容を三行でまとめると、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;音声ファイルの読み込み（or 書き込み）は[WAV.jl]((&lt;a href=&#34;https://github.com/dancasimiro/WAV.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/dancasimiro/WAV.jl&lt;/a&gt;)を使おう&lt;/li&gt;
&lt;li&gt;基本的なデジタル信号処理は &lt;a href=&#34;https://github.com/JuliaDSP/DSP.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JuliaDSP/DSP.jl&lt;/a&gt; をチェック（※JuliaDSPにはウェーブレットとかもあるよ）&lt;/li&gt;
&lt;li&gt;音声に特化した信号処理は、&lt;a href=&#34;https://github.com/r9y9/WORLD.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/WORLD.jl&lt;/a&gt; がオススメです&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;という感じです。&lt;/p&gt;
&lt;p&gt;応用例として、歌声を分離する話（&lt;a href=&#34;https://github.com/r9y9/RobustPCA.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;デモコード&lt;/a&gt;）、統計的声質変換（&lt;a href=&#34;http://r9y9.github.io/blog/2014/11/12/statistical-voice-conversion-code/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;統計的声質変換クッソムズすぎワロタ（チュートリアル編） - LESS IS MORE&lt;/a&gt;）、画像をスペクトログラムに足しこむ話とか、さっと紹介しました。&lt;/p&gt;
&lt;h2 id=&#34;補足&#34;&gt;補足&lt;/h2&gt;
&lt;p&gt;僕が使う/作ったパッケージを、あとで見返せるように最後のスライドにまとめておいたのですが、改めてここで整理しておきます。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/dancasimiro/WAV.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;dancasimiro/WAV&lt;/a&gt; WAVファイルの読み込み&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/JuliaDSP/DSP.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JuliaDSP/DSP&lt;/a&gt; 窓関数、スペクトログラム、デジタルフィルタ&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/WORLD.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/WORLD&lt;/a&gt; 音声分析・合成フレームワーク&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/MelGeneralizedCepstrums.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/MelGeneralizedCepstrums&lt;/a&gt; メル一般化ケプストラム分析&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/SynthesisFilters.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/SynthesisFilters&lt;/a&gt; メル一般化ケプストラムからの波形合成&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/SPTK.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/SPTK&lt;/a&gt; 音声信号処理ツールキット&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/RobustPCA.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/RobustPCA&lt;/a&gt; ロバスト主成分分析(歌声分離へ応用)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/REAPER.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/REAPER&lt;/a&gt; 基本周波数推定&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/VoiceConversion.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/VoiceConversion&lt;/a&gt; 統計的声質変換&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上から順に、&lt;del&gt;汎用的かなーと思います&lt;/del&gt;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;。僕が書いたパッケージの中では、&lt;strong&gt;WORLDのみ&lt;/strong&gt;公式パッケージにしています。理由は単純で、その他のパッケージはあまりユーザがいないだろうなーと思ったからです。かなりマニアックであったり、今後の方針が決まってなかったり（ごめんなさい）、応用的過ぎて全然汎用的でなかったり。WORLDは自信を持ってオススメできますので、Juliaで音声信号処理をやってみようかなと思った方は、ぜひお試しください。&lt;/p&gt;
&lt;h2 id=&#34;ざっくり感想&#34;&gt;ざっくり感想&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;＃Juliaわからん 本当に素晴らしいと思うので、僕も積極的に #Juliaわからん とつぶやいていこうと思います（詳しくは &lt;a href=&#34;https://twitter.com/chezou&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@chezou&lt;/a&gt; さんの記事をどうぞ &lt;a href=&#34;http://chezou.hatenablog.com/entry/2015/04/26/222518&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#JuliaTokyo で #juliaわからん という雑なレポジトリを立てた話をしたら julia.tokyo ができてた  - once upon a time,&lt;/a&gt;）。僕は、Julia に Theano が欲しいです。&lt;code&gt;T.grad&lt;/code&gt; 強力すぎる&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ccall&lt;/code&gt; かんたんとか言いましたが、ミスった書き方をしたときのエラーメッセージはあまり親切ではないので、つまずきやすいかも。僕は気合で何とかしています。&lt;/li&gt;
&lt;li&gt;Julia遅いんだけど？？？と言われたら、&lt;a href=&#34;https://twitter.com/bicycle1885&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@bicycle1885&lt;/a&gt; さんの &lt;a href=&#34;http://www.slideshare.net/KentaSato/whats-wrong-47403774&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;What&amp;rsquo;s wrong with this Julia?&lt;/a&gt; を投げつけようと思います。&lt;/li&gt;
&lt;li&gt;かなり聴衆が限定的になってしまう話をしてしまったので、次発表するならJulia 言語自体の話をしようかなと思いました&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;最後に&#34;&gt;最後に&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/sorami&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@sorami&lt;/a&gt;さんを筆頭とする運営の方々、本当にありがとうございました！楽しかったです。&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;とスライドに書いたけど、考えなおすと、僕が思う品質の高さ順、の方が正確です、失礼しました。MelGeneneralizedCepstrumsは一番気合入れて書いたけど、ユーザーがいるかといったらいないし、RobustPCAはさっと書いただけだけど、アルゴリズムとしては汎用的だし。またRobustPCAだけ毛色が違いますが、応用例で紹介したのでリストに入れておきました。&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>統計的声質変換クッソムズすぎワロタ（チュートリアル編）</title>
      <link>https://r9y9.github.io/blog/2014/11/12/statistical-voice-conversion-code/</link>
      <pubDate>Wed, 12 Nov 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/11/12/statistical-voice-conversion-code/</guid>
      <description>&lt;h2 id=&#34;はじめに&#34;&gt;はじめに&lt;/h2&gt;
&lt;p&gt;こんばんは。統計的声質変換（以降、簡単に声質変換と書きます）って面白いなーと思っているのですが、興味を持つ人が増えたらいいなと思い、今回は簡単なチュートリアルを書いてみます。間違っている箇所があれば、指摘してもらえると助かります。よろしくどうぞ。&lt;/p&gt;
&lt;p&gt;前回の記事（&lt;a href=&#34;http://r9y9.github.io/blog/2014/07/13/statistical-voice-conversion-wakaran/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;統計的声質変換クッソムズすぎワロタ（実装の話） - LESS IS MORE&lt;/a&gt;）では変換部分のコードのみを貼りましたが、今回はすべてのコードを公開します。なので、記事内で示す声質変換の結果を、この記事を読んでいる方が再現することも可能です。対象読者は、特に初学者の方で、声質変換を始めたいけれど論文からコードに落とすにはハードルが高いし、コードを動かしながら仕組みを理解していきたい、という方を想定しています。役に立てば幸いです。&lt;/p&gt;
&lt;h2 id=&#34;コード&#34;&gt;コード&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/VoiceConversion.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/VoiceConversion.jl&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://julialang.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Julia&lt;/a&gt; という言語で書かれています。Juliaがどんな言語かをさっと知るのには、以下のスライドがお勧めです。人それぞれ好きな言語で書けばいいと思いますが、個人的にJuliaで書くことになった経緯は、最後の方に簡単にまとめました。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/39141184&#34; width=&#34;425&#34; height=&#34;355&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt; &lt;div style=&#34;margin-bottom:5px&#34;&gt; &lt;strong&gt; &lt;a href=&#34;//www.slideshare.net/kentaroiizuka/julia-39141184&#34; title=&#34;プログラミング言語 Julia の紹介&#34; target=&#34;_blank&#34;&gt;プログラミング言語 Julia の紹介&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href=&#34;//www.slideshare.net/kentaroiizuka&#34; target=&#34;_blank&#34;&gt;Kentaro Iizuka&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&#34;サードパーティライブラリ&#34;&gt;サードパーティライブラリ&lt;/h2&gt;
&lt;p&gt;声質変換は多くのコンポーネントによって成り立っていますが、すべてを自分で書くのは現実的ではありません。僕は、主に以下のライブラリを活用しています。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://ml.cs.yamanashi.ac.jp/world/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WORLD&lt;/a&gt; - 音声分析合成のフレームワークとして、あるいは単にスペクトル包絡を抽出するツールとして使っています。&lt;a href=&#34;https://github.com/r9y9/WORLD.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Juliaラッパー&lt;/a&gt;を書きました。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;sp-tk.sourceforge.net&#34;&gt;SPTK&lt;/a&gt; - メル対数スペクトル近似（Mel-Log Spectrum Approximation; MLSA）フィルタを変換処理に使っています。これも&lt;a href=&#34;https://github.com/r9y9/SPTK.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Juliaラッパー&lt;/a&gt;を書きました。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://scikit-learn.org/stable/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sklearn&lt;/a&gt; - sklearn.mixture をGMMの学習に使っています。pythonのライブラリは、juliaから簡単に呼べます。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;音声分析合成に関しては、アカデミック界隈ではよく使われている&lt;a href=&#34;http://www.wakayama-u.ac.jp/~kawahara/STRAIGHTadv/index_j.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;STRAIGHT&lt;/a&gt;がありますが、WORLDの方がライセンスもゆるくソースも公開されていて、かつ性能も劣らない（正確な話は、森勢先生の論文を参照してください）ので、おすすめです。&lt;/p&gt;
&lt;h2 id=&#34;voiceconversionjlhttpsgithubcomr9y9voiceconversionjl-でできること&#34;&gt;&lt;a href=&#34;https://github.com/r9y9/VoiceConversion.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VoiceConversion.jl&lt;/a&gt; でできること&lt;/h2&gt;
&lt;h3 id=&#34;追記-20150107&#34;&gt;追記 2015/01/07&lt;/h3&gt;
&lt;p&gt;この記事を書いた段階のv0.0.1は、依存ライブラリの変更のため、現在は動きません。すみません。何のためのタグだ、という気がしてきますが、、最低限masterは動作するようにしますので、そちらをお試しください（基本的には、新しいコードの方が改善されています）。それでも動かないときは、issueを投げてください。&lt;/p&gt;
&lt;p&gt;2014/11/10現在（v0.0.1のタグを付けました）、できることは以下の通りです（外部ライブラリを叩いているものを含む）。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;音声波形からのメルケプストラムの抽出&lt;/li&gt;
&lt;li&gt;DPマッチングによるパラレルデータの作成&lt;/li&gt;
&lt;li&gt;GMMの学習&lt;/li&gt;
&lt;li&gt;GMMベースのframe-by-frame特徴量変換&lt;/li&gt;
&lt;li&gt;GMMベースのtrajectory特徴量変換&lt;/li&gt;
&lt;li&gt;GMMベースのtrajectory特徴量変換（GV考慮版）&lt;/li&gt;
&lt;li&gt;音声分析合成系WORLDを使った声質変換&lt;/li&gt;
&lt;li&gt;MLSAフィルタを使った差分スペクトルに基づく声質変換&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;これらのうち、trajectory変換以外を紹介します。&lt;/p&gt;
&lt;h2 id=&#34;チュートリアルcmu_arcticを使ったgmmベースの声質変換特徴抽出からパラレルデータの作成gmmの学習変換合成処理まで&#34;&gt;チュートリアル：CMU_ARCTICを使ったGMMベースの声質変換（特徴抽出からパラレルデータの作成、GMMの学習、変換・合成処理まで）&lt;/h2&gt;
&lt;p&gt;データセットに&lt;a href=&#34;http://festvox.org/cmu_arctic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CMU_ARCTIC&lt;/a&gt;を使って、GMMベースの声質変換（clb -&amp;gt; slt）を行う方法を説明します。なお、VoiceConversion.jl のv0.0.1を使います。ubuntuで主に動作確認をしていますが、macでも動くと思います。&lt;/p&gt;
&lt;h2 id=&#34;0-前準備&#34;&gt;0. 前準備&lt;/h2&gt;
&lt;h3 id=&#34;01-データセットのダウンロード&#34;&gt;0.1. データセットのダウンロード&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://festvox.org/cmu_arctic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Festvox: CMU_ARCTIC Databases&lt;/a&gt; を使います。コマンド一発ですべてダウンロードする&lt;a href=&#34;https://gist.github.com/r9y9/ff67c05aeb87410eae2e&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;スクリプト&lt;/a&gt;を書いたので、ご自由にどうぞ。&lt;/p&gt;
&lt;h3 id=&#34;02-juliaのインストール&#34;&gt;0.2. juliaのインストール&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://julialang.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;公式サイト&lt;/a&gt;からバイナリをダウンロードするか、&lt;a href=&#34;https://github.com/JuliaLang/julia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;githubのリポジトリ&lt;/a&gt;をクローンしてビルドしてください。バージョンは、現在の最新安定版のv0.3.2を使います。&lt;/p&gt;
&lt;p&gt;記事内では、juliaの基本的な使い方については解説しないので、前もってある程度調べておいてもらえると、スムーズに読み進められるかと思います。&lt;/p&gt;
&lt;h3 id=&#34;03-voiceconversionjl-のインストール&#34;&gt;0.3. VoiceConversion.jl のインストール&lt;/h3&gt;
&lt;p&gt;juliaを起動して、以下のコマンドを実行してください。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;julia&amp;gt; Pkg.clone(&amp;quot;https://github.com/r9y9/VoiceConversion.jl&amp;quot;)
julia&amp;gt; Pkg.build(&amp;quot;VoiceConversion&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;サードパーティライブラリは、sklearnを除いてすべて自動でインストールされます。sklearnは、例えば以下のようにしてインストールしておいてください。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo pip install sklearn
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;これで準備は完了です！&lt;/p&gt;
&lt;h2 id=&#34;1-音声波形からのメルケプストラムの抽出&#34;&gt;1. 音声波形からのメルケプストラムの抽出&lt;/h2&gt;
&lt;p&gt;まずは、音声から声質変換に用いる特徴量を抽出します。特徴量としては、声質変換や音声合成の分野で広く使われているメルケプストラムを使います。メルケプストラムの抽出は、&lt;code&gt;scripts/mcep.jl&lt;/code&gt; を使うことでできます。&lt;/p&gt;
&lt;h3 id=&#34;20141115-追記&#34;&gt;2014/11/15 追記&lt;/h3&gt;
&lt;p&gt;実行前に、&lt;code&gt;julia&amp;gt; Pkg.add(&amp;quot;WAV&amp;quot;)&lt;/code&gt; として、WAVパッケージをインストールしておいてください。(2014/11/15時点のmasterでは自動でインストールされますが、v0.0.1ではインストールされません、すいません）。また、メルケプストラムの出力先ディレクトリは事前に作成しておいてください（最新のスクリプトでは自動で作成されます）。&lt;/p&gt;
&lt;p&gt;以下のようにして、2話者分の特徴量を抽出しましょう。以下のスクリプトでは、 &lt;code&gt;~/data/cmu_arctic/&lt;/code&gt; にデータがあることを前提としています。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# clb
julia mcep.jl ~/data/cmu_arctic/cmu_us_clb_arctic/wav/ ~/data/cmu_arctic_jld/speakers/clb/
# slt
julia mcep.jl ~/data/cmu_arctic/cmu_us_slt_arctic/wav/ ~/data/cmu_arctic_jld/speakers/slt/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;基本的な使い方は、&lt;code&gt;mcep.jl &amp;lt;wavファイルがあるディレクトリ&amp;gt; &amp;lt;メルケプストラムが出力されるディレクトリ&amp;gt;&lt;/code&gt; になっています。オプションについては、 &lt;code&gt;mcep.jl -h&lt;/code&gt; としてヘルプを見るか、コードを直接見てください。&lt;/p&gt;
&lt;p&gt;抽出されたメルケプストラムは、HDF5フォーマットで保存されます。メルケプストラムの中身を見てみると、以下のような感じです。可視化には、PyPlotパッケージが必要です。Juliaを開いて、&lt;code&gt;julia&amp;gt; Pkg.add(&amp;quot;PyPlot&amp;quot;)&lt;/code&gt; とすればOKです。IJuliaを使いたい場合（僕は使っています）は、&lt;code&gt;julia&amp;gt; Pkg.add(&amp;quot;IJulia&amp;quot;)&lt;/code&gt; としてIJuliaもインストールしておきましょう。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# メルケプストラムの可視化

using HDF5, JLD, PyPlot

x = load(&amp;quot;clb/arctic_a0028.jld&amp;quot;)

figure(figsize=(16, 6), dpi=80, facecolor=&amp;quot;w&amp;quot;, edgecolor=&amp;quot;k&amp;quot;)
imshow(x[&amp;quot;feature_matrix&amp;quot;], origin=&amp;quot;lower&amp;quot;, aspect=&amp;quot;auto&amp;quot;)
colorbar()
&lt;/code&gt;&lt;/pre&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/clb_a0028_melcepstrum.png &#34;Mel-cepstrum of clb_a0028.&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;0次成分だけ取り出してみると、以下のようになります。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# メルケプストラムの0次成分のみを可視化

figure(figsize=(16, 6), dpi=80, facecolor=&amp;quot;w&amp;quot;, edgecolor=&amp;quot;k&amp;quot;)
plot(vec(x[&amp;quot;feature_matrix&amp;quot;][1,:]), linewidth=2.0, label=&amp;quot;0th order mel-cesptrum of clb_a0028&amp;quot;)
xlim(0, size(x[&amp;quot;feature_matrix&amp;quot;], 2)-10) # 末尾がsilenceだった都合上…（決め打ち）
xlabel(&amp;quot;Frame&amp;quot;)
legend(loc=&amp;quot;upper right&amp;quot;)
ylim(-10, -2) # 見やすいように適当に決めました
&lt;/code&gt;&lt;/pre&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/clb_a0028_melcepstrum_0th.png &#34;Mel-cepstrum of clb_a0028 0th.&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;こんな感じです。話者clbの&lt;code&gt;clb_a0028.wav&lt;/code&gt;を聞きながら、特徴量見てみてください。0次の成分からは、音量の大小が読み取れると思います。&lt;/p&gt;
&lt;h2 id=&#34;2-dpマッチングによるパラレルデータの作成&#34;&gt;2. DPマッチングによるパラレルデータの作成&lt;/h2&gt;
&lt;p&gt;次に、2話者分の特徴量を時間同期して連結します。基本的に声質変換では、音韻の違いによらない特徴量（非言語情報）の対応関係を学習するために、同一発話内容の特徴量を時間同期し（音韻の違いによる変動を可能な限りなくすため）、学習データとして用います。このデータのことを、パラレルデータと呼びます。&lt;/p&gt;
&lt;p&gt;パラレルデータの作成には、DPマッチングを使うのが一般的です。&lt;code&gt;scripts/align.jl&lt;/code&gt; を使うとできます。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;julia align.jl ~/data/cmu_arctic_jld/speakers/clb ~/data/cmu_arctic_jld/speakers/slt ~/data/cmu_arctic_jld/parallel/clb_and_slt/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;使い方は、&lt;code&gt;align.jl &amp;lt;話者1（clb）の特徴量のパス&amp;gt; &amp;lt;話者2（slt）の特徴量のパス&amp;gt; &amp;lt;パラレルデータの出力先&amp;gt;&lt;/code&gt; になっています。&lt;/p&gt;
&lt;p&gt;きちんと時間同期されているかどうか、0次成分を見て確認してみましょう。&lt;/p&gt;
&lt;p&gt;時間同期を取る前のメルケプストラムを以下に示します。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# 時間同期前のメルケプストラム（0次）を可視化

x = load(&amp;quot;clb/arctic_a0028.jld&amp;quot;)
y = load(&amp;quot;slt/arctic_a0028.jld&amp;quot;)

figure(figsize=(16, 6), dpi=80, facecolor=&amp;quot;w&amp;quot;, edgecolor=&amp;quot;k&amp;quot;)
plot(vec(x[&amp;quot;feature_matrix&amp;quot;][1,:]), linewidth=2.0, label=&amp;quot;0th order mel-cesptrum of clb_a0028&amp;quot;)
plot(vec(y[&amp;quot;feature_matrix&amp;quot;][1,:]), linewidth=2.0, label=&amp;quot;0th order mel-cesptrum of slt_a0028&amp;quot;)
xlim(0, min(size(x[&amp;quot;feature_matrix&amp;quot;], 2), size(y[&amp;quot;feature_matrix&amp;quot;], 2))-10) # 決め打ち
xlabel(&amp;quot;Frame&amp;quot;)
legend(loc=&amp;quot;upper right&amp;quot;)
ylim(-10, -2) # 決め打ち
&lt;/code&gt;&lt;/pre&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/clb_and_slt_a0028_melcepstrum_0th.png &#34;0th order mel-cepstrum (not aligned)&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;ちょっとずれてますね&lt;/p&gt;
&lt;p&gt;次に、時間同期後のメルケプストラムを示します。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# 時間同期後のメルケプストラム（0次）を可視化

parallel = load(&amp;quot;arctic_a0028_parallel.jld&amp;quot;)

figure(figsize=(16, 6), dpi=80, facecolor=&amp;quot;w&amp;quot;, edgecolor=&amp;quot;k&amp;quot;)
plot(vec(parallel[&amp;quot;src&amp;quot;][&amp;quot;feature_matrix&amp;quot;][1,:]), linewidth=2.0, &amp;quot;b&amp;quot;, label=&amp;quot;0th order mel-cesptrum of clb_a0028&amp;quot;)
plot(vec(parallel[&amp;quot;tgt&amp;quot;][&amp;quot;feature_matrix&amp;quot;][1,:]), linewidth=2.0, &amp;quot;g&amp;quot;, label=&amp;quot;0th order mel-cesptrum of slt_a0028&amp;quot;)
xlim(0, size(parallel[&amp;quot;tgt&amp;quot;][&amp;quot;feature_matrix&amp;quot;], 2))
xlabel(&amp;quot;Frame&amp;quot;)
legend()
&lt;/code&gt;&lt;/pre&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/clb_and_slt_a0028_melcepstrum_0th_aligned.png &#34;0th order mel-cepstrum (aligned)&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;ずれが修正されているのがわかりますね。注意として、&lt;code&gt;align.jl&lt;/code&gt; の中身を追えばわかるのですが、無音区間をしきい値判定で検出して、パラレルデータから除外しています。&lt;/p&gt;
&lt;p&gt;結果、時間同期されたパラレルデータは以下のようになります。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# パラレルデータの可視化

figure(figsize=(16, 6), dpi=80, facecolor=&amp;quot;w&amp;quot;, edgecolor=&amp;quot;k&amp;quot;)
imshow(vcat(parallel[&amp;quot;src&amp;quot;][&amp;quot;feature_matrix&amp;quot;], parallel[&amp;quot;tgt&amp;quot;][&amp;quot;feature_matrix&amp;quot;]), origin=&amp;quot;lower&amp;quot;, aspect=&amp;quot;auto&amp;quot;)
colorbar()
&lt;/code&gt;&lt;/pre&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/clb_and_slt_a0028_parallel.png &#34;example of parallel data&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;このパラレルデータを（複数の発話分さらに結合して）使って、特徴量の対応関係を学習していきます。モデルには、GMMを使います。&lt;/p&gt;
&lt;h2 id=&#34;3-gmmの学習&#34;&gt;3. GMMの学習&lt;/h2&gt;
&lt;p&gt;GMMの学習には、&lt;code&gt;sklearn.mixture.GMM&lt;/code&gt; を使います。GMMは古典的な生成モデルで、実装は探せばたくさん見つかるので、既存の有用なライブラリを使えば十分です。（余談ですが、pythonのライブラリを簡単に呼べるのはjuliaの良いところの一つですね）&lt;/p&gt;
&lt;p&gt;&lt;code&gt;scripts/train_gmm.jl&lt;/code&gt; を使うと、モデルのダンプ、julia &amp;lt;-&amp;gt; python間のデータフォーマットの変換等、もろもろやってくれます。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;julia train_gmm.jl ~/data/cmu_arctic_jld/parallel/clb_and_slt/ clb_and_slt_gmm32_order40.jld --max 200 --n_components 32 --n_iter=100 --n_init=1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;使い方は、&lt;code&gt;train_gmm.jl &amp;lt;パラレルデータのパス&amp;gt; &amp;lt;出力するモデルデータのパス&amp;gt;&lt;/code&gt; になっています。上の例では、学習に用いる発話数、GMMの混合数、反復回数等を指定しています。オプションの詳細はスクリプトをご覧ください。&lt;/p&gt;
&lt;p&gt;僕の環境では、上記のコマンドを叩くと2時間くらいかかりました。学習が終わったところで、学習済みのモデルのパラメータを可視化してみましょう。&lt;/p&gt;
&lt;p&gt;まずは平均を見てみます。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# GMMの平均ベクトルを（いくつか）可視化
gmm = load(&amp;quot;clb_and_slt_gmm32_order40.jld&amp;quot;)

figure(figsize=(16, 6), dpi=80, facecolor=&amp;quot;w&amp;quot;, edgecolor=&amp;quot;k&amp;quot;)
for k=1:3
    plot(gmm[&amp;quot;means&amp;quot;][:,k], linewidth=2.0, label=&amp;quot;mean of mixture $k&amp;quot;)
end
legend()
&lt;/code&gt;&lt;/pre&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/clb_and_slt_gmm32_order40_mean.png &#34;means of trained GMM&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;共分散の一部可視化してみると、以下のようになります。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# GMMの共分散行列を一部可視化

figure(figsize=(16, 6), dpi=80, facecolor=&amp;quot;w&amp;quot;, edgecolor=&amp;quot;k&amp;quot;)
imshow(gmm[&amp;quot;covars&amp;quot;][:,:,2])
colorbar()
clim(0.0, 0.16)
&lt;/code&gt;&lt;/pre&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/clb_and_slt_gmm32_order40_covar.png &#34;covariance of trained GMM&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;まぁこんなもんですね。&lt;/p&gt;
&lt;h2 id=&#34;4-音声分析合成worldを用いたgmmベースのframe-by-frame声質変換&#34;&gt;4. 音声分析合成WORLDを用いたGMMベースのframe-by-frame声質変換&lt;/h2&gt;
&lt;p&gt;さて、ようやく声質変換の準備が整いました。学習したモデルを使って、GMMベースのframe-by-frame声質変換（clb -&amp;gt; slt ）をやってみましょう。具体的な変換アルゴリズムは、論文（例えば&lt;a href=&#34;http://isw3.naist.jp/~tomoki/Tomoki/Journals/IEEE-Nov-2007_MLVC.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;戸田先生のこれ&lt;/a&gt;）をチェックしてみてください。音声分析合成系にはWORLDを使います。&lt;/p&gt;
&lt;p&gt;一般的な声質変換では、まず音声を以下の三つの成分に分解します。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基本周波数&lt;/li&gt;
&lt;li&gt;スペクトル包絡（今回いじりたい部分）&lt;/li&gt;
&lt;li&gt;非周期性成分&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;その後、スペクトル包絡に対して変換を行い、変換後のパラメータを使って音声波形を合成するといったプロセスを取ります。これらは、&lt;code&gt;scripts/vc.jl&lt;/code&gt; を使うと簡単にできるようになっています。本当にWORLDさまさまです。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;julia vc.jl ~/data/cmu_arctic/cmu_us_clb_arctic/wav/arctic_a0028.wav clb_and_slt_gmm32_order40.jld clb_to_slt_a0028.wav --order 40
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;使い方は、&lt;code&gt;vc.jl &amp;lt;変換対象の音声ファイル&amp;gt; &amp;lt;変換モデル&amp;gt; &amp;lt;出力wavファイル名&amp;gt;&lt;/code&gt; となっています。&lt;/p&gt;
&lt;p&gt;上記のコマンドを実行すると、GMMベースのframe-by-frame声質変換の結果が音声ファイルに出力されます。以下に結果を貼っておくので、聞いてみてください。&lt;/p&gt;
&lt;h3 id=&#34;変換元となる音声-clb_a0028&#34;&gt;変換元となる音声 clb_a0028&lt;/h3&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;166&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/176093202&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false&#34;&gt;&lt;/iframe&gt;
&lt;h3 id=&#34;変換目標となる話者-slt_a0028&#34;&gt;変換目標となる話者 slt_a0028&lt;/h3&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;166&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/176093240&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false&#34;&gt;&lt;/iframe&gt;
&lt;h3 id=&#34;変換結果-clb_to_slt_a0028&#34;&gt;変換結果 clb_to_slt_a0028&lt;/h3&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;166&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/176093403&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;話者性はなんとなく目標話者に近づいている気がしますが、音質が若干残念な感じですね。。&lt;/p&gt;
&lt;h2 id=&#34;5-差分スペクトル補正に基づく声質変換&#34;&gt;5. 差分スペクトル補正に基づく声質変換&lt;/h2&gt;
&lt;p&gt;最後に、より高品質な声質変換を達成可能な差分スペクトル補正に基づく声質変換を紹介します。差分スペクトル補正に基づく声質変換では、基本周波数や非周期性成分をいじれない代わりに音質はかなり改善します。以前書いた記事（&lt;a href=&#34;http://r9y9.github.io/blog/2014/07/05/statistical-voice-conversion-muzui/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;統計的声質変換クッソムズすぎワロタ - LESS IS MORE&lt;/a&gt;）から、着想に関連する部分を引用します。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;これまでは、音声を基本周波数、非周期性成分、スペクトル包絡に分解して、スペクトル包絡を表す特徴量を変換し、変換後の特徴量を元に波形を再合成していました。ただ、よくよく考えると、そもそも基本周波数、非周期性成分をいじる必要がない場合であれば、わざわざ分解して再合成する必要なくね？声質の部分のみ変換するようなフィルタかけてやればよくね？という考えが生まれます。実は、そういったアイデアに基づく素晴らしい手法があります。それが、差分スペクトル補正に基づく声質変換です。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;差分スペクトル補正に基づく声質変換の詳細ついては、最近inter speechに論文が出たようなので、そちらをご覧ください。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://isw3.naist.jp/~kazuhiro-k/resource/kobayashi14IS.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Kobayashi 2014] Kobayashi, Kazuhiro, et al. &amp;ldquo;Statistical Singing Voice Conversion with Direct Waveform Modification based on the Spectrum Differential.&amp;rdquo; Fifteenth Annual Conference of the International Speech Communication Association. 2014.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;こばくん、論文を宣伝しておきますね＾＾&lt;/p&gt;
&lt;h3 id=&#34;51-差分特徴量の学習&#34;&gt;5.1 差分特徴量の学習&lt;/h3&gt;
&lt;p&gt;さて、差分スペクトル補正に基づく声質変換行うには、変換元話者$X$と目標話者$Y$の特徴量の同時分布$P(X,Y)$を学習するのではなく、$P(X, Y-X)$ （日本語で書くとややこしいのですが、変換元話者の特徴量$X$と、変換元話者と目標話者の差分特徴量$Y-X$の同時分布）を学習します。これは、 &lt;code&gt;train_gmm.jl&lt;/code&gt; を使ってGMMを学習する際に、&lt;code&gt;--diff&lt;/code&gt; とオプションをつけるだけでできます。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;julia train_gmm.jl ~/data/cmu_arctic_jld/parallel/clb_and_slt/ clb_to_slt_gmm32_order40_diff.jld --max 200 --n_components 32 --n_iter=100 --n_init=1 --diff
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可視化してみます。&lt;/p&gt;
&lt;p&gt;平均&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/clb_to_slt_gmm32_order40_mean.png &#34;means of trained DIFFGMM&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;共分散&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/clb_to_slt_gmm32_order40_covar.png &#34;covar of trained DIFFGMM&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;さっき学習したGMMとは、共分散はかなり形が違いますね。高次元成分でも、分散が比較的大きな値をとっているように見えます。形が異っているのは見てすぐにわかりますが、では具体的には何が異っているのか、それはなぜなのか、きちんと考えると面白そうですね。&lt;/p&gt;
&lt;h3 id=&#34;52-mlsaフィルタによる声質変換&#34;&gt;5.2 MLSAフィルタによる声質変換&lt;/h3&gt;
&lt;p&gt;差分スペクトル補正に基づく声質変換では、WORLDを使って音声の分析合成を行うのではなく、生の音声波形を入力として、MLSAフィルタをかけるのみです。これは、 &lt;code&gt;scripts/diffvc.jl&lt;/code&gt; を使うと簡単にできます。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;julia diffvc.jl ~/data/cmu_arctic/cmu_us_clb_arctic/wav/arctic_a0028.wav clb_to_slt_gmm32_order40_diff.jld clb_to_slt_a0028_diff.wav --order 40
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;さて、結果を聞いてみましょう。&lt;/p&gt;
&lt;h3 id=&#34;53-差分スペクトル補正に基づく声質変換結果&#34;&gt;5.3 差分スペクトル補正に基づく声質変換結果&lt;/h3&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;166&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/176093513&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;アイデアはシンプル、結果は良好、最高の手法ですね（べた褒め&lt;/p&gt;
&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;
&lt;p&gt;以上、長くなりましたが、統計的声質変換についてのチュートリアルはこれで終わります。誰の役に立つのか知らないけれど、役に立てば嬉しいです。トラジェクトリ変換やGVを考慮したバージョンなど、今回紹介していないものも実装しているので、詳しくは&lt;a href=&#34;https://github.com/r9y9/VoiceConversion.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Githubのリポジトリ&lt;/a&gt;をチェックしてください。バグをレポートしてくれたりすると、僕は喜びます。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;h3 id=&#34;以前書いた声質変換に関する記事&#34;&gt;以前書いた声質変換に関する記事&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://r9y9.github.io/blog/2014/07/05/statistical-voice-conversion-muzui/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;統計的声質変換クッソムズすぎワロタ - LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://r9y9.github.io/blog/2014/07/13/statistical-voice-conversion-wakaran/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;統計的声質変換クッソムズすぎワロタ（実装の話） - LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;論文&#34;&gt;論文&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://isw3.naist.jp/~tomoki/Tomoki/Journals/IEEE-Nov-2007_MLVC.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Toda 2007] T. Toda, A. W. Black, and K. Tokuda, “Voice conversion based on maximum likelihood estimation of spectral parameter trajectory,” IEEE
Trans. Audio, Speech, Lang. Process, vol. 15, no. 8, pp. 2222–2235,
Nov. 2007.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://isw3.naist.jp/~kazuhiro-k/resource/kobayashi14IS.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Kobayashi 2014] Kobayashi, Kazuhiro, et al. &amp;ldquo;Statistical Singing Voice Conversion with Direct Waveform Modification based on the Spectrum Differential.&amp;rdquo; Fifteenth Annual Conference of the International Speech Communication Association. 2014.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;faq&#34;&gt;FAQ&lt;/h2&gt;
&lt;h3 id=&#34;前はpythonで書いてなかった&#34;&gt;前はpythonで書いてなかった？&lt;/h3&gt;
&lt;p&gt;はい、https://gist.github.com/r9y9/88bda659c97f46f42525 ですね。正確には、GMMの学習・変換処理はpythonで書いて、特徴抽出、パラレルデータの作成、波形合成はGo言語で書いていました。が、Goとpythonでデータのやりとり、Goとpythonをいったり来たりするのが面倒になってしまって、一つの言語に統一したいと思うようになりました。Goで機械学習は厳しいと感じていたので、pythonで書くかなぁと最初は思ったのですが、WORLDやSPTKなど、Cのライブラリをpythonから使うのが思いの他面倒だったので（&lt;a href=&#34;https://github.com/r9y9/SPTK&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SPTKのpythonラッパー&lt;/a&gt;は書きましたが）、Cやpythonとの連携がしやすく、スクリプト言語でありながらCに速度面で引けをとらないjuliaに興味があったので、juliaですべて完結するようにしました。かなり実験的な試みでしたが、今はかなり満足しています。juliaさいこー&lt;/p&gt;
&lt;h3 id=&#34;新規性は&#34;&gt;新規性は？&lt;/h3&gt;
&lt;p&gt;ありません&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NMFで音源分離を試してみる</title>
      <link>https://r9y9.github.io/blog/2014/10/19/nmf-music-source-separation/</link>
      <pubDate>Sun, 19 Oct 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/10/19/nmf-music-source-separation/</guid>
      <description>&lt;p&gt;ずーーっと前に、 &lt;a href=&#34;http://r9y9.github.io/blog/2013/07/27/nmf-euclid/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NMFアルゴリズムの導出（ユークリッド距離版） - LESS IS MORE&lt;/a&gt; で実際に実装してみてやってみると書いていたのに、まったくやっていなかったことに気づいたのでやりました。&lt;/p&gt;
&lt;p&gt;音楽に対してやってみたのですが、簡単な曲だったら、まぁぼちぼち期待通りに動いたかなぁという印象です。コードとノートを挙げたので、興味のある方はどうぞ。&lt;/p&gt;
&lt;h2 id=&#34;github&#34;&gt;Github&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/julia-nmf-ss-toy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/julia-nmf-ss-toy&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;ノート&#34;&gt;ノート&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/r9y9/julia-nmf-ss-toy/blob/master/NMF-based%20Music%20Source%20Separation%20Demo.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NMF-based Music Source Separation Demo.ipynb | nbviewer&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;nmfのコード-julia&#34;&gt;NMFのコード (Julia)&lt;/h2&gt;
&lt;p&gt;NMFの実装の部分だけ抜き出しておきます。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function nmf_euc(Y::AbstractMatrix, K::Int=4;
                        maxiter::Int=100)
    H = rand(size(Y, 1), K)
    U = rand(K, size(Y, 2))
    const ϵ = 1.0e-21
    for i=1:maxiter
        H = H .* (Y*U&#39;) ./ (H*U*U&#39; + ϵ)
        U = U .* (H&#39;*Y) ./ (H&#39;*H*U + ϵ)
        U = U ./ maximum(U)
    end
    return H, U
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;いやー簡単ですねー。&lt;a href=&#34;http://r9y9.github.io/blog/2013/07/27/nmf-euclid/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NMFアルゴリズムの導出（ユークリッド距離版） - LESS IS MORE&lt;/a&gt; で導出した更新式ほぼそのままになってます（異なる点としては、ゼロ除算回避をしているのと、Uをイテレーション毎に正規化していることくらい）。&lt;/p&gt;
&lt;p&gt;B3, B4くらいの人にとっては参考になるかもしれないと思ってあげてみた。&lt;/p&gt;
&lt;p&gt;おわり&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gamma Process Non-negative Matrix Factorization (GaP-NMF) in Julia</title>
      <link>https://r9y9.github.io/blog/2014/08/20/gap-nmf-julia/</link>
      <pubDate>Wed, 20 Aug 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/08/20/gap-nmf-julia/</guid>
      <description>&lt;p&gt;最近 &lt;a href=&#34;julialang.org&#34;&gt;Julia&lt;/a&gt; で遊んでいて、その過程で非負値行列因子分解（NMF）のノンパラ版の一つであるGamma Process Non-negative Matrix Factorization (GaP-NMF) を書いてみました。（まぁmatlabコードの写経なんですが）&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/BNMF.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/BNMF.jl&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;元論文:
&lt;a href=&#34;http://soundlab.cs.princeton.edu/publications/2010_icml_gapnmf.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bayesian Nonparametric Matrix Factorization for Recorded Music&lt;/a&gt;
by Matthew D. Hoffman et al. in ICML 2010.&lt;/p&gt;
&lt;h2 id=&#34;デモ&#34;&gt;デモ&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/r9y9/BNMF.jl/blob/master/notebook/GaP-NMF.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://nbviewer.ipython.org/github/r9y9/BNMF.jl/blob/master/notebook/GaP-NMF.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;適当な音声（音楽じゃなくてごめんなさい）に対して、GaP-NMFをfittingしてみた結果のメモです。$K=100$ で始めて、100回ほどイテレーションを回すと適度な数（12くらい）にtruncateしているのがわかると思います。予めモデルの複雑度を指定しなくても、データから適当な数を自動決定してくれる、ノンパラベイズの良いところですね。&lt;/p&gt;
&lt;h2 id=&#34;ハマったところ&#34;&gt;ハマったところ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;GIGの期待値を求めるのに必要な第二種変形ベッセル関数は、exponentially scaled versionを使いましょう。じゃないとInf地獄を見ることになると思います（つらい）。Juliaで言うなら &lt;a href=&#34;https://julia.readthedocs.org/en/latest/stdlib/base/?highlight=besselkx#Base.besselkx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;besselkx&lt;/a&gt; で、scipyで言うなら &lt;a href=&#34;http://students.mimuw.edu.pl/~pbechler/scipy_doc/generated/scipy.special.kve.html#scipy.special.kve&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;scipy.special.kve&lt;/a&gt; です。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;雑感&#34;&gt;雑感&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;MatlabのコードをJuliaに書き直すのは簡単。ところどころ作法が違うけど（例えば配列の要素へのアクセスはmatlabはA(i,j)でJuliaはA[i,j]）、だいたい一緒&lt;/li&gt;
&lt;li&gt;というかJuliaがMatlabに似すぎ？&lt;/li&gt;
&lt;li&gt;Gamma分布に従う乱数は、&lt;a href=&#34;https://github.com/JuliaStats/Distributions.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Distributions,jl&lt;/a&gt; を使えばめっちゃ簡単に生成できた。素晴らしすぎる&lt;/li&gt;
&lt;li&gt;行列演算がシンプルにかけてホント楽。pythonでもmatlabでもそうだけど（Goだとこれができないんですよ…）&lt;/li&gt;
&lt;li&gt;第二種変形ベッセル関数とか、scipy.special にあるような特殊関数が標準である。素晴らしい。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;python版と速度比較&#34;&gt;Python版と速度比較&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/dawenl/bp_nmf/tree/master/code/gap_nmf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bp_nmf/code/gap_nmf&lt;/a&gt; と比較します。matlabはもってないので比較対象からはずします、ごめんなさい&lt;/p&gt;
&lt;p&gt;Gistにベンチマークに使ったスクリプトと実行結果のメモを置いときました
&lt;a href=&#34;https://gist.github.com/r9y9/3d0c6a90dd155801c4c1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gist.github.com/r9y9/3d0c6a90dd155801c4c1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;結果だけ書いておくと、あらゆる現実を（ry の音声にGaP-NMFをepochs=100でfittingするのにかかった時間は、&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Julia: Mean elapsed time: 21.92968243 [sec]
Python: Mean elapsed time: 18.3550617 [sec]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;という結果になりました。つまりJuliaのほうが1.2倍くらい遅かった（僕の実装が悪い可能性は十分ありますが）。どこがボトルネックになっているのか調べていないので、気が向いたら調べます。Juliaの方が速くなったらいいなー&lt;/p&gt;
&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;
&lt;p&gt;GaP-NMFの実装チャレンジは二回目でした。（たぶん）一昨年、年末に実家に帰るときに、何を思ったのか急に実装したくなって、電車の中で論文を読んで家に着くなり実装するというエクストリームわけわからんことをしていましたが、その時はNaN and Inf地獄に負けてしまいました。Pythonで書いていましたが、今見るとそのコードマジクソでした。&lt;/p&gt;
&lt;p&gt;そして二回目である今回、最初はmatlabコードを見ずに自力で書いていたんですが、またもやInf地獄に合いもうだめだと思って、matlabコードを写経しました。あんま成長していないようです（つらい）&lt;/p&gt;
&lt;p&gt;Julia歴二週間くらいですが、良い感じなので使い続けて見ようと思います。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Goでニューラルネットいくつか書いたけどやっぱPythonが楽でいいですね</title>
      <link>https://r9y9.github.io/blog/2014/07/29/neural-networks-in-go-and-python/</link>
      <pubDate>Tue, 29 Jul 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/07/29/neural-networks-in-go-and-python/</guid>
      <description>&lt;p&gt;いまいち成果出ないので気分転換にブログをだらだら書いてみるテストです。&lt;/p&gt;
&lt;h2 id=&#34;まえがき&#34;&gt;まえがき&lt;/h2&gt;
&lt;p&gt;半年くらい前に、某深層学習に興味を持ってやってみようかなーと思っていた時期があって、その時にGoでいくつかニューラルネットを書きました（参考：&lt;a href=&#34;http://r9y9.github.io/blog/2014/03/06/restricted-boltzmann-machines-mnist/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Restricted Boltzmann Machines with MNIST - LESS IS MORE&lt;/a&gt;、&lt;a href=&#34;https://github.com/r9y9/nnet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;githubに上げたコード&lt;/a&gt;）。なぜGoだったかというと、僕がGoに興味を持ち始めていたからというのが大きいです。Goを知る前は、たくさん計算するようなコードを書くときはC++だったけれど、C++は色々つらいものがあるし、GoはC++には速度面で劣るもののそこそこ速く、かつスクリプト的な書きやすさもあります。C++のデバッグやメンテに費やす膨大な時間に比べれば、計算時間が1.5~2倍に増えるくらい気にしないというスタンスで、僕はC++のかわりGoを使おうとしていました（※今でも間違っているとは思いませんが、とはいえ、厳しいパフォーマンスを求められる場合や既存の資産を有効活用したい場合など、必要な場面ではC++を書いています）。&lt;/p&gt;
&lt;h2 id=&#34;goで機械学習&#34;&gt;Goで機械学習&lt;/h2&gt;
&lt;p&gt;僕は機械学習がけっこう好きなので、Goでコード書くかーと思っていたのですが、結果としてまったく捗りませんでした。ニューラルネットをてきとーに書いたくらいです。&lt;/p&gt;
&lt;p&gt;検索するとわかりますが、現状、他の主流な言語に比べて圧倒的に数値計算のライブラリが少ないです。特に、線形代数、行列演算のデファクト的なライブラリがないのはつらいです。いくつか代表的なものをあげます。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/skelterjohn/go.matrix&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;skelterjohn/go.matrix&lt;/a&gt; - もうまったくメンテされていないし、たぶんするつもりはないと思います。使い勝手は、僕にとってはそんなに悪くなかった（試しに&lt;a href=&#34;https://gist.github.com/r9y9/9030922&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NMF&lt;/a&gt;を書いてみた）ですが、実装は純粋なGoで書かれていて、GPUを使って計算するのが流行りな時代では、例えば大きなニューラルネットをパラメータを変えながら何度も学習するのにはしんどいと思いました。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gonum/matrix&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gonum/matrix&lt;/a&gt; - 比較的最近出てきたライブラリで、&lt;a href=&#34;https://code.google.com/p/biogo/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;biogo&lt;/a&gt; から行列演算に関する部分を切り出して作られたもののようです。行列演算の内部でblasを使っていて、かつ将来的にはcublasにも対応したい、みたいな投稿をGoogle Groupsで見たのもあって、半年くらい前にはgoで行列演算を行うならこのライブラリを使うべきだと判断しました（以前けっこう調べました：&lt;a href=&#34;http://qiita.com/r9y9/items/7f93a89e3a88bb4ed263&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gonum/matrix のデザインコンセプトに関するメモ - Qiita&lt;/a&gt;）。しかし、それほど頻繁にアップデートされていませんし、機能もまだ少ないです。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;自分で作るかー、という考えも生まれなかったことはないですが、端的に言えばそれを行うだけのやる気がありませんでした。まぁ本当に必要だったら多少難しくてもやるのですが、ほら、僕達にはpythonがあるじゃないですか…&lt;/p&gt;
&lt;h2 id=&#34;pythonで機械学習&#34;&gt;Pythonで機械学習&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.google.co.jp/search?q=python&amp;#43;%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92&amp;amp;oq=python&amp;#43;%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;python 機械学習 - Google 検索&lt;/a&gt; 約 119,000 件（2014/07/29現在）&lt;/p&gt;
&lt;p&gt;もうみんなやってますよね。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.google.co.jp/search?q=Golang&amp;#43;%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92&amp;amp;oq=Golang&amp;#43;%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Golang 機械学習 - Google 検索&lt;/a&gt; 約 9,130 件（2014/07/29現在）&lt;/p&gt;
&lt;p&gt;いつかpythonのように増えるんでしょうか。正直に言って、わかりません（正確には、あんま考えていませんごめんなさい）&lt;/p&gt;
&lt;p&gt;さて、僕もよくpython使います。機械学習のコードを書くときは、だいたいpythonを使うようになりました（昔はC++で書いていました）。なぜかって、numpy, scipyのおかげで、とても簡潔に、かつ上手く書けばそこそこ速く書けるからです。加えて、ライブラリがとても豊富なんですよね、機械学習にかかわらず。numpy, scipyに加えて、matplotlibという優秀な描画ライブラリがあるのが、僕がpythonを使う大きな理由になっています。&lt;/p&gt;
&lt;p&gt;pythonの機械学習ライブラリは、&lt;a href=&#34;http://scikit-learn.org/stable/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;scikit-learn&lt;/a&gt; が特に有名でしょうか。僕もちょいちょい使います。使っていて最近おどろいたのは、scipy.mixtureには通常のGMMだけでなく変分GMM、無限混合GMMも入っていることですよね。自分で実装しようとしたら、たぶんとても大変です。昔変分GMMの更新式を導出したことがありますが、何度も心が折れそうになりました。いやー、いい時代になったもんですよ…（遠い目&lt;/p&gt;
&lt;h2 id=&#34;pythonでニューラルネットpylearn2を使おう&#34;&gt;Pythonでニューラルネット（pylearn2を使おう）&lt;/h2&gt;
&lt;p&gt;Deep何とかを含め流行りのニューラルネットが使える機械学習のライブラリでは、僕は &lt;a href=&#34;https://github.com/lisa-lab/pylearn2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pylearn2&lt;/a&gt; がよさ気だなーと思っています。理由は、高速かつ拡張性が高いからです。pylearn2は、数学的な記号表現からGPUコード（GPUがなければCPU向けのコード）を生成するmathコンパイラ &lt;a href=&#34;https://github.com/Theano/Theano&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Theano&lt;/a&gt; で書かれているためpythonでありながら高速で、かつ機械学習に置いて重要なコンポーネントであるデータ、モデル、アルゴリズムが上手く分離されて設計されているのがいいところかなと思います（全部ごっちゃに書いていませんか？僕はそうですごめんなさい。データはともかくモデルと学習を上手く切り分けるの難しい）。A Machine Learning library based on Theanoとのことですが、Deep learningで有名な &lt;a href=&#34;http://lisa.iro.umontreal.ca/index_en.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lisa lab&lt;/a&gt; 発ということもあり、ニューラルネットのライブラリという印象が少し強いですね。&lt;/p&gt;
&lt;p&gt;一つ重要なこととして、このライブラリはかなり研究者向けです。ブラックボックスとして使うのではなく、中身を読んで必要に応じて自分で拡張を書きたい人に向いているかと思います。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://arxiv.org/pdf/1308.4214v1.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ian J. Goodfellow, David Warde-Farley, Pascal Lamblin, Vincent Dumoulin, Mehdi Mirza, Razvan Pascanu, James Bergstra, Frédéric Bastien, and Yoshua Bengio. “Pylearn2: a machine learning research library”. arXiv preprint arXiv:1308.4214&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;↑の論文のIntroductionの部分に、その旨は明記されています。と、論文のリンクを貼っておいてなんですが、&lt;a href=&#34;http://www-etud.iro.umontreal.ca/~goodfeli/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ian Goodfellow&lt;/a&gt; のホームページにもっと簡潔に書いてありました。以下、引用します。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I wrote most of Pylearn2, a python library designed to make machine learning research convenient. Its mission is to provide a toolbox of interchangeable parts that provide a lot of flexibility for setting up machine learning experiments, providing enough extensibility that pretty much any research idea is feasible within the context of the library. This is in contrast to other machine learning libraries such as scikits-learn that are designed to be black boxes that just work. Think of pylearn2 as user friendly for machine learning researchers and scikits-learn as user friendly for developers that want to apply machine learning.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;pylearn2では、Multi-layer Perceptron (MLP)、Deep Bolztmann Machines (DBM)、新しいものでMaxout Network等、手軽に試すことができます（まぁゆうて計算はめっちゃ時間かかるけど）。先述の通りmathコンパイラの &lt;a href=&#34;https://github.com/Theano/Theano&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Theano&lt;/a&gt; を使って実装されているので、GPUがある場合はGPUを使って計算してくれます。環境構築に関しては、今はAWSという便利なサービスがあるので、GPUを持っていなくてもウェブ上でポチポチしてるだけで簡単にGPU環境を構築できます（参考：&lt;a href=&#34;http://r9y9.github.io/blog/2014/07/20/pylearn2-on-ec2-g2-2xlarge/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pylearn2, theanoをEC2 g2.x2large で動かす方法 - LESS IS MORE&lt;/a&gt;）。本当にいい時代になったものですね（二回目&lt;/p&gt;
&lt;p&gt;pylearn2、コードやドキュメント、活発なgithubでの開発、議論を見ていて、素晴らしいなーと思いました（まだ使い始めたばかりの僕の意見にあまり信憑性はないのですが…）。僕もこれくらい汎用性、拡張性のあるコードを書きたい人生でした…（自分の書いたニューラルネットのコードを見ながら）&lt;/p&gt;
&lt;h2 id=&#34;pylearn2は遅いって&#34;&gt;Pylearn2は遅いって？&lt;/h2&gt;
&lt;p&gt;本当に速さを求めるなら &lt;a href=&#34;https://code.google.com/p/cuda-convnet2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cuda-convnet2&lt;/a&gt; や &lt;a href=&#34;http://caffe.berkeleyvision.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cafee&lt;/a&gt;、もしくは直でcudaのAPIをだな…と言いたいところですが、確かにpylearn2は他の深層学習のライブラリに比べて遅いようです。最近、Convolutional Neural Network (CNN) に関するベンチマークがGithubで公開されていました。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/soumith/convnet-benchmarks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;soumith/convnet-benchmarks&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;現時点でまだ work in progressと書いてありますが、参考になると思います。優劣の問題ではなく、必要に応じて使い分ければいいと僕は思っています。&lt;/p&gt;
&lt;p&gt;さてさて、本当はここから僕が書いたGoのニューラルネットのコードがいかにクソかという話を書こうかと思ったのですが、長くなったのでまた今度にします。&lt;/p&gt;
&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Goでニューラルネットとか機械学習をやるのは現状しんどいし（&lt;a href=&#34;https://github.com/sjwhitworth/golearn&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;golearn&lt;/a&gt;とかあるけど、まだまだearly stage）、おとなしくpython使うのが無難&lt;/li&gt;
&lt;li&gt;pythonはやっぱり楽。ライブラリ豊富だし。ニューラルネットならpylearn2がおすすめ。ただし自分で拡張まで書きたい人向けです。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;散々pythonいいよゆうてますが、どちらかといえば僕はGoの方が好きです。機械学習には現状pythonを使うのがいいんじゃないかなーと思って、Goでニューラルネットを書いていた時を思い出しながらつらつらと書いてみました。&lt;/p&gt;
&lt;p&gt;おわり。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pylearn2, theanoをEC2 g2.x2large で動かす方法</title>
      <link>https://r9y9.github.io/blog/2014/07/20/pylearn2-on-ec2-g2-2xlarge/</link>
      <pubDate>Sun, 20 Jul 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/07/20/pylearn2-on-ec2-g2-2xlarge/</guid>
      <description>&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/dbm_learned_from_mnist.png &#34;Weight visualization of Restricted bolztomann machine trained on MNIST dataset.&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;h2 id=&#34;モチベーション&#34;&gt;モチベーション&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;手元のへぼマシンでニューラルネットの学習を回わす&lt;/li&gt;
&lt;li&gt;半日たっても終わらない&lt;/li&gt;
&lt;li&gt;最近だとGPU使って計算を高速化するのが流行りだが、手元にGPUはない&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;http://www.kurtsp.com/deep-learning-in-python-with-pylearn2-and-amazon-ec2.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Learning in Python with Pylearn2 and Amazon EC2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;手元にGPUがない…？大丈夫！Amazon EC2を使えば良さそう！！！&lt;/p&gt;
&lt;p&gt;というわけで、めんどくさいと言わずにec2にお手軽計算環境を整えます。ec2でGPUが乗ったものだと、g2.2xlargeがよさそうですね。&lt;/p&gt;
&lt;p&gt;ちなみに↑の図、pylearn2のtutorialのRestricted Bolzmann MachinesをMNISTで学習した結果なんですが、手元のマシンだとだいたい6時間くらい？（忘れた）だったのがg2.2xlargeだと30分もかかってない（ごめんなさい時間図るの忘れた）。$0.65/hourと安いんだし（他のインスタンスに比べればそりゃ高いけど）、もう手元のマシンで計算するの時間の無駄だしやめようと思います。&lt;/p&gt;
&lt;p&gt;さてさて、今回環境構築に少しはまったので、もうはまらないように簡単にまとめておきます。&lt;/p&gt;
&lt;h2 id=&#34;結論&#34;&gt;結論&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://aws.amazon.com/marketplace/pp/B00FYCDDTE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Amazon Linux AMI with NVIDIA GRID GPU Driver on AWS Marketplace &lt;/a&gt;&lt;/p&gt;
&lt;p&gt;すでにNVIDIAのドライバとCUDA（5.5）が入ったインスタンスをベースに使いましょう。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://xanxys.hatenablog.jp/entry/2014/05/17/135932&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EC2(g2.2xlarge)でOpenGLを使う方法&lt;/a&gt; で挙げられているように普通のlinuxを使う方法もありますが、ハマる可能性大です。僕はubuntuが使いたかったので最初はubuntu 14.04 server でドライバ、cuda (5.5 or 6.0) のインストールを試しましたが同じように失敗しました。&lt;/p&gt;
&lt;p&gt;イケイケと噂の音声認識ライブラリKaldiの&lt;a href=&#34;https://220-135-252-130.hinet-ip.hinet.net/speechwiki/index.php/Kaldi#installing_and_testing_CUDA-6.0_in_Ubuntu_14.04&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ドキュメントらしきもの&lt;/a&gt;を見ると、Ubuntu 14.04でもcuda 6.0インストールできるっぽいんですけどね…だめでした。頑張ればできるかもしれませんが、よほど強いメリットがない場合は、おとなしくpre-installされたインスタンスを使うのが吉だと思います。&lt;/p&gt;
&lt;h2 id=&#34;セットアップ&#34;&gt;セットアップ&lt;/h2&gt;
&lt;p&gt;↑で上げたインスタンスにはGPUドライバやCUDAは入っていますが、theanoもpylearn2もnumpyもscipyも入っていません。よって、それらは手動でインストールする必要があります。&lt;/p&gt;
&lt;p&gt;というわけで、インストールするシェルをメモって置きます。試行錯誤したあとに適当にまとめたshellなので、なんか抜けてたらごめんなさい。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://gist.github.com/r9y9/50f13ba28b5b158c25ae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gist.github.com/r9y9/50f13ba28b5b158c25ae&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash

# Pylearn2 setup script for Amazon Linux AMI with NVIDIA GRID GPU Driver.
# http://goo.gl/3KeXXW

sudo yum update -y
sudo yum install -y emacs tmux python-pip
sudo yum install -y python-devel git blas-devel lapack-devel

# numpy, scipy, matplotlib, etc.
sudo pip install numpy
sudo pip install scipy
sudo pip install cython
sudo pip install ipython nose

# matplotlib
sudo yum install -y libpng-devel freetype-devel
sudo pip install matplotlib

# Scikit-learn
sudo pip install scikit-learn

# Theano
sudo pip install --upgrade git+git://github.com/Theano/Theano.git

# Enable GPU for theano
echo &#39;[global]
floatX = float32
device = gpu0

[nvcc]
fastmath = True&#39; &amp;gt; .theanorc

# pylearn2
git clone git://github.com/lisa-lab/pylearn2.git
cd pylearn2
sudo python setup.py develop
cd ..

echo &amp;quot;export PYLEARN2_DATA_PATH=/home/ec2-user/data&amp;quot; &amp;gt;&amp;gt; .bashrc

# MNIST dataset
mkdir -p data/mnist/
cd data/mnist/
wget http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
gunzip train-images-idx3-ubyte.gz
wget http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
gunzip train-labels-idx1-ubyte.gz
wget http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
gunzip t10k-images-idx3-ubyte.gz
wget http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
gunzip t10k-labels-idx1-ubyte.gz
cd ../..
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;簡単ですね&lt;/p&gt;
&lt;p&gt;また、上記のような手順を踏まなくても、Community AMIs でpylearn2で検索するとすでにpylearn2が入ったAMIが出てくるので、それを使うのもありかもです（僕は試してません）。&lt;/p&gt;
&lt;p&gt;僕がAMIを公開してもいいんですが、今のところする予定はありません&lt;/p&gt;
&lt;h1 id=&#34;まとめ&#34;&gt;まとめ&lt;/h1&gt;
&lt;p&gt;そこそこ良い計算環境がさくっとできました、まる。ラーメン食べたい&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://aws.amazon.com/jp/ec2/instance-types/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;インスタンスタイプ - Amazon EC2 (クラウド上の仮想サーバー Amazon Elastic Compute Cloud) | アマゾン ウェブ サービス（AWS 日本語）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.kurtsp.com/deep-learning-in-python-with-pylearn2-and-amazon-ec2.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Learning in Python with Pylearn2 and Amazon EC2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://xanxys.hatenablog.jp/entry/2014/05/17/135932&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EC2(g2.2xlarge)でOpenGLを使う方法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>統計的声質変換クッソムズすぎワロタ（実装の話）</title>
      <link>https://r9y9.github.io/blog/2014/07/13/statistical-voice-conversion-wakaran/</link>
      <pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/07/13/statistical-voice-conversion-wakaran/</guid>
      <description>&lt;p&gt;2014/07/28 追記：
重み行列の構築の部分を改良したのでちょいアップデート。具体的にはdense matrixとして構築してからスパース行列に変換していたのを、はじめからスパース行列として構築するようにして無駄にメモリを使わないようにしました。あとdiffが見やすいようにgistにあげました
&lt;a href=&#34;https://gist.github.com/r9y9/88bda659c97f46f42525&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gist.github.com/r9y9/88bda659c97f46f42525&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;まえがき&#34;&gt;まえがき&lt;/h2&gt;
&lt;p&gt;前回、&lt;a href=&#34;http://r9y9.github.io/blog/2014/07/05/statistical-voice-conversion-muzui/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;統計的声質変換クッソムズすぎワロタ - LESS IS MORE&lt;/a&gt; という記事を書いたら研究者の方々等ちょいちょい反応してくださって嬉しかったです。差分スペクトル補正、その道の人が聴いても音質がいいそう。これはいい情報です。&lt;/p&gt;
&lt;p&gt;Twitter引用:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; lang=&#34;en&#34;&gt;&lt;p&gt;統計的声質変換クッソムズすぎワロタ - LESS IS MORE &lt;a href=&#34;http://t.co/8RkeXIf6Ym&#34;&gt;http://t.co/8RkeXIf6Ym&lt;/a&gt; &lt;a href=&#34;https://twitter.com/r9y9&#34;&gt;@r9y9&lt;/a&gt;さんから ムズすぎと言いながら，最後の音はしっかり出ているあたり凄いなぁ．&lt;/p&gt;&amp;mdash; M. Morise (忍者系研究者) (@m_morise) &lt;a href=&#34;https://twitter.com/m_morise/statuses/485339123171852289&#34;&gt;July 5, 2014&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; lang=&#34;en&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://twitter.com/ballforest&#34;&gt;@ballforest&lt;/a&gt; 従来のパラメータ変換と比較すると、音質は従来よりもよさそうな気はしますがスペクトル包絡の性差ががっつりと影響しそうな気もするんですよね。&lt;/p&gt;&amp;mdash; 縄文人（妖精系研究者なのです） (@dicekicker) &lt;a href=&#34;https://twitter.com/dicekicker/statuses/485380534122463232&#34;&gt;July 5, 2014&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;異性間に関しては、実験が必要ですね。異性間だとF0が結構変わってくると思いますが、差分スペクトル補正の場合そもそもF0をいじらないという前提なので、F0とスペクトル包絡が完全に独立でない（ですよね？）以上、同姓間に比べて音質は劣化する気はします。簡単にやったところ、少なくとも僕の主観的には劣化しました&lt;/p&gt;
&lt;p&gt;ところで、結構いい感じにできたぜひゃっはーと思って、先輩に聞かせてみたら違いわかんねと言われて心が折れそうになりました。やはり現実はつらいです。&lt;/p&gt;
&lt;h2 id=&#34;実装の話&#34;&gt;実装の話&lt;/h2&gt;
&lt;p&gt;さて、今回は少し実装のことを書こうと思います。学習&amp;amp;変換部分はPythonで書いています。その他はGo（※Goの話は書きません）。&lt;/p&gt;
&lt;h3 id=&#34;トラジェクトリベースのパラメータ変換が遅いのは僕の実装が悪いからでした本当に申し訳ありませんでしたorz&#34;&gt;トラジェクトリベースのパラメータ変換が遅いのは僕の実装が悪いからでした本当に申し訳ありませんでしたorz&lt;/h3&gt;
&lt;p&gt;前回トラジェクトリベースは処理が激重だと書きました。なんと、4秒程度の音声（フレームシフト5msで777フレーム）に対して変換部分に600秒ほどかかっていたのですが（重すぎワロタ）、結果から言えばPythonでも12秒くらいまでに高速化されました（混合数64, メルケプの次元数40+デルタ=80次元、分散共分散はfull）。本当にごめんなさい。&lt;/p&gt;
&lt;p&gt;何ヶ月か前、ノリでトラジェクトリベースの変換を実装しようと思ってサクッと書いたのがそのままで、つまりとても効率の悪い実装になっていました。具体的には放置していた問題が二つあって、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ナイーブな逆行列の計算&lt;/li&gt;
&lt;li&gt;スパース性の無視&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;です。特に後者はかなりパフォーマンスに影響していました&lt;/p&gt;
&lt;h3 id=&#34;ナイーブな逆行列の計算&#34;&gt;ナイーブな逆行列の計算&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://d.hatena.ne.jp/sleepy_yoshi/20120513/p1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;numpy.linalg.invとnumpy.linalg.solveを用いた逆行列計算 - 睡眠不足？！ (id:sleepy_yoshi)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;numpy.linalg.inv&lt;/code&gt;を使っていましたよね。しかも&lt;code&gt;numpy.linalg.solve&lt;/code&gt;のほうが速いことを知っていながら。一ヶ月前の自分を問い詰めたい。&lt;code&gt;numpy.linalg.solve&lt;/code&gt;で置き換えたら少し速くなりました。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;600秒 -&amp;gt; 570秒&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;1.05倍の高速化&lt;/p&gt;
&lt;h3 id=&#34;スパース性の無視&#34;&gt;スパース性の無視&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://isw3.naist.jp/~tomoki/Tomoki/Journals/IEEE-Nov-2007_MLVC.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;T. Toda, A. W. Black, and K. Tokuda, “Voice conversion based on maximum likelihood estimation of spectral parameter trajectory,” IEEE Trans. Audio, Speech, Lang. Process, vol. 15, no. 8, pp. 2222–2235, Nov. 2007&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;論文を見ていただければわかるのですが、トラジェクトリベースの変換法における多くの計算は、行列を使って表すことができます。で、論文中の$W$という行列は、サイズがめちゃくちゃでかいのですがほとんどの要素は0です。この性質を使わない理由はないですよね？？&lt;/p&gt;
&lt;p&gt;…残念なことに、僕は密行列として扱って計算していました。ほら、疎行列ってちょっと扱いづらいじゃないですか…めんどくさそう…と思って放置してました。ごめんなさい&lt;/p&gt;
&lt;p&gt;pythonで疎行列を扱うなら、scipy.sparseを使えば良さそうです。結果、$W$を疎行列として扱うことで行列演算は大きく高速化されました。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;570秒 -&amp;gt; 12秒くらい&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;単純に考えると50倍の高速化ですか。本当にアホだった。最初からscipy.sparse使っておけばよかったです。&lt;/p&gt;
&lt;p&gt;scipy.sparseの使い方は以下を参考にしました。みなさんぜひ使いましょう&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://sucrose.hatenablog.com/entry/2013/04/07/130625&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Python で疎行列(SciPy) - 唯物是真 @Scaled_Wurm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://docs.scipy.org/doc/scipy/reference/sparse.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sparse matrices (scipy.sparse) — SciPy v0.14.0 Reference Guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://lucidfrontier45.wordpress.com/2011/08/02/scipysparse_matmul/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;scipy.sparseで疎行列の行列積 | frontier45&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;コード&#34;&gt;コード&lt;/h2&gt;
&lt;p&gt;メモ的な意味で主要なコードを貼っておきます。
&lt;a href=&#34;https://gist.github.com/r9y9/88bda659c97f46f42525&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gist.github.com/r9y9/88bda659c97f46f42525&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/usr/bin/python
# coding: utf-8

import numpy as np
from numpy import linalg
from sklearn.mixture import GMM
import scipy.linalg
import scipy.sparse
import scipy.sparse.linalg

class GMMMap:
    &amp;quot;&amp;quot;&amp;quot;GMM-based frame-by-frame speech parameter mapping.

    GMMMap represents a class to transform spectral features of a source
    speaker to that of a target speaker based on Gaussian Mixture Models
    of source and target joint spectral features.

    Notation
    --------
    Source speaker&#39;s feature: X = {x_t}, 0 &amp;lt;= t &amp;lt; T
    Target speaker&#39;s feature: Y = {y_t}, 0 &amp;lt;= t &amp;lt; T
    where T is the number of time frames.

    Parameters
    ----------
    gmm : scipy.mixture.GMM
        Gaussian Mixture Models of source and target joint features

    swap : bool
        True: source -&amp;gt; target
        False target -&amp;gt; source

    Attributes
    ----------
    num_mixtures : int
        the number of Gaussian mixtures

    weights : array, shape (`num_mixtures`)
        weights for each gaussian

    src_means : array, shape (`num_mixtures`, `order of spectral feature`)
        means of GMM for a source speaker

    tgt_means : array, shape (`num_mixtures`, `order of spectral feature`)
        means of GMM for a target speaker

    covarXX : array, shape (`num_mixtures`, `order of spectral feature`,
        `order of spectral feature`)
        variance matrix of source speaker&#39;s spectral feature

    covarXY : array, shape (`num_mixtures`, `order of spectral feature`,
        `order of spectral feature`)
        covariance matrix of source and target speaker&#39;s spectral feature

    covarYX : array, shape (`num_mixtures`, `order of spectral feature`,
        `order of spectral feature`)
        covariance matrix of target and source speaker&#39;s spectral feature

    covarYY : array, shape (`num_mixtures`, `order of spectral feature`,
        `order of spectral feature`)
        variance matrix of target speaker&#39;s spectral feature

    D : array, shape (`num_mixtures`, `order of spectral feature`,
        `order of spectral feature`)
        covariance matrices of target static spectral features

    px : scipy.mixture.GMM
        Gaussian Mixture Models of source speaker&#39;s features

    Reference
    ---------
      - [Toda 2007] Voice Conversion Based on Maximum Likelihood Estimation
        of Spectral Parameter Trajectory.
        http://isw3.naist.jp/~tomoki/Tomoki/Journals/IEEE-Nov-2007_MLVC.pdf

    &amp;quot;&amp;quot;&amp;quot;
    def __init__(self, gmm, swap=False):
        # D is the order of spectral feature for a speaker
        self.num_mixtures, D = gmm.means_.shape[0], gmm.means_.shape[1]/2
        self.weights = gmm.weights_

        # Split source and target parameters from joint GMM
        self.src_means = gmm.means_[:, 0:D]
        self.tgt_means = gmm.means_[:, D:]
        self.covarXX = gmm.covars_[:, :D, :D]
        self.covarXY = gmm.covars_[:, :D, D:]
        self.covarYX = gmm.covars_[:, D:, :D]
        self.covarYY = gmm.covars_[:, D:, D:]

        # swap src and target parameters
        if swap:
            self.tgt_means, self.src_means = self.src_means, self.tgt_means
            self.covarYY, self.covarXX = self.covarXX, self.covarYY
            self.covarYX, self.covarXY = self.XY, self.covarYX

        # Compute D eq.(12) in [Toda 2007]
        self.D = np.zeros(self.num_mixtures*D*D).reshape(self.num_mixtures, D, D)
        for m in range(self.num_mixtures):
            xx_inv_xy = np.linalg.solve(self.covarXX[m], self.covarXY[m])
            self.D[m] = self.covarYY[m] - np.dot(self.covarYX[m], xx_inv_xy)

        # p(x), which is used to compute posterior prob. for a given source
        # spectral feature in mapping stage.
        self.px = GMM(n_components=self.num_mixtures, covariance_type=&amp;quot;full&amp;quot;)
        self.px.means_ = self.src_means
        self.px.covars_ = self.covarXX
        self.px.weights_ = self.weights

    def convert(self, src):
        &amp;quot;&amp;quot;&amp;quot;
        Mapping source spectral feature x to target spectral feature y
        so that minimize the mean least squared error.
        More specifically, it returns the value E(p(y|x)].

        Parameters
        ----------
        src : array, shape (`order of spectral feature`)
            source speaker&#39;s spectral feature that will be transformed

        Return
        ------
        converted spectral feature
        &amp;quot;&amp;quot;&amp;quot;
        D = len(src)

        # Eq.(11)
        E = np.zeros((self.num_mixtures, D))
        for m in range(self.num_mixtures):
            xx = np.linalg.solve(self.covarXX[m], src - self.src_means[m])
            E[m] = self.tgt_means[m] + self.covarYX[m].dot(xx)

        # Eq.(9) p(m|x)
        posterior = self.px.predict_proba(np.atleast_2d(src))

        # Eq.(13) conditinal mean E[p(y|x)]
        return posterior.dot(E)

class TrajectoryGMMMap(GMMMap):
    &amp;quot;&amp;quot;&amp;quot;
    Trajectory-based speech parameter mapping for voice conversion
    based on the maximum likelihood criterion.

    Parameters
    ----------
    gmm : scipy.mixture.GMM
        Gaussian Mixture Models of source and target speaker joint features

    gv : scipy.mixture.GMM (default=None)
        Gaussian Mixture Models of target speaker&#39;s global variance of spectral
        feature

    swap : bool (default=False)
        True: source -&amp;gt; target
        False target -&amp;gt; source

    Attributes
    ----------
    TODO

    Reference
    ---------
      - [Toda 2007] Voice Conversion Based on Maximum Likelihood Estimation
        of Spectral Parameter Trajectory.
        http://isw3.naist.jp/~tomoki/Tomoki/Journals/IEEE-Nov-2007_MLVC.pdf
    &amp;quot;&amp;quot;&amp;quot;
    def __init__(self, gmm, T, gv=None, swap=False):
        GMMMap.__init__(self, gmm, swap)

        self.T = T
        # shape[1] = d(src) + d(src_delta) + d(tgt) + d(tgt_delta)
        D = gmm.means_.shape[1] / 4

        ## Setup for Trajectory-based mapping
        self.__construct_weight_matrix(T, D)

        ## Setup for GV post-filtering
        # It is assumed that GV is modeled as a single mixture GMM
        if gv != None:
            self.gv_mean = gv.means_[0]
            self.gv_covar = gv.covars_[0]
            self.Pv = np.linalg.inv(self.gv_covar)

    def __construct_weight_matrix(self, T, D):
        # Construct Weight matrix W
        # Eq.(25) ~ (28)
        for t in range(T):
            w0 = scipy.sparse.lil_matrix((D, D*T))
            w1 = scipy.sparse.lil_matrix((D, D*T))
            w0[0:,t*D:(t+1)*D] = scipy.sparse.diags(np.ones(D), 0)

            if t-1 &amp;gt;= 0:
                tmp = np.zeros(D)
                tmp.fill(-0.5)
                w1[0:,(t-1)*D:t*D] = scipy.sparse.diags(tmp, 0)
            if t+1 &amp;lt; T:
                tmp = np.zeros(D)
                tmp.fill(0.5)
                w1[0:,(t+1)*D:(t+2)*D] = scipy.sparse.diags(tmp, 0)

            W_t = scipy.sparse.vstack([w0, w1])

            # Slower
            # self.W[2*D*t:2*D*(t+1),:] = W_t

            if t == 0:
                self.W = W_t
            else:
                self.W = scipy.sparse.vstack([self.W, W_t])

        self.W = scipy.sparse.csr_matrix(self.W)

        assert self.W.shape == (2*D*T, D*T)

    def convert(self, src):
        &amp;quot;&amp;quot;&amp;quot;
        Mapping source spectral feature x to target spectral feature y
        so that maximize the likelihood of y given x.

        Parameters
        ----------
        src : array, shape (`the number of frames`, `the order of spectral feature`)
            a sequence of source speaker&#39;s spectral feature that will be
            transformed

        Return
        ------
        a sequence of transformed spectral features
        &amp;quot;&amp;quot;&amp;quot;
        T, D = src.shape[0], src.shape[1]/2

        if T != self.T:
            self.__construct_weight_matrix(T, D)

        # A suboptimum mixture sequence  (eq.37)
        optimum_mix = self.px.predict(src)

        # Compute E eq.(40)
        self.E = np.zeros((T, 2*D))
        for t in range(T):
            m = optimum_mix[t] # estimated mixture index at time t
            xx = np.linalg.solve(self.covarXX[m], src[t] - self.src_means[m])
            # Eq. (22)
            self.E[t] = self.tgt_means[m] + np.dot(self.covarYX[m], xx)
        self.E = self.E.flatten()

        # Compute D eq.(41). Note that self.D represents D^-1.
        self.D = np.zeros((T, 2*D, 2*D))
        for t in range(T):
            m = optimum_mix[t]
            xx_inv_xy = np.linalg.solve(self.covarXX[m], self.covarXY[m])
            # Eq. (23)
            self.D[t] = self.covarYY[m] - np.dot(self.covarYX[m], xx_inv_xy)
            self.D[t] = np.linalg.inv(self.D[t])
        self.D = scipy.linalg.block_diag(*self.D)

        # represent D as a sparse matrix
        self.D = scipy.sparse.csr_matrix(self.D)

        # Compute target static features
        # eq.(39)
        covar = self.W.T.dot(self.D.dot(self.W))
        y = scipy.sparse.linalg.spsolve(covar, self.W.T.dot(self.D.dot(self.E)),\
                                        use_umfpack=False)
        return y.reshape((T, D))
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;結論&#34;&gt;結論&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;疎行列の演算を考えるときは、間違ってもめんどくさいとか思わずに疎行列を積極的に使おう&lt;/li&gt;
&lt;li&gt;統計的声質変換ムズすぎ&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;おまけめも&#34;&gt;おまけめも&lt;/h2&gt;
&lt;p&gt;僕が変換精度を改善するために考えていることのめも&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;統計的な手法を使う限りover-smoothingの問題はついてくる。ならば、逆にover-smoothingされることで都合の良い特徴量を考えることはできないか&lt;/li&gt;
&lt;li&gt;メルケプとかそもそもスペクトル包絡をコンパクトにparamtricに表現するために考えられたもの（だと思ってる）ので、高品質な変換を考えるならばスペクトル包絡をそのまま使うなりした方がいいんじゃないか。とはいえスペクトル包絡をそのまま使うのはぼちぼち高次元なので、個人性に依存する部分を残した形で非線形次元削減したらどうか（例えばニューラルネットを使って統計的に個人性に依存する部分を見つけ出すとか）&lt;/li&gt;
&lt;li&gt;time-dependentな関係をモデル化しないとだめじゃないか、確率モデルとして。RNNとか普通に使えそうだし、まぁHMMでもよい&lt;/li&gt;
&lt;li&gt;音素境界を推定して、segment単位で変換するのも良いかも&lt;/li&gt;
&lt;li&gt;識別モデルもっと使ってもいいんじゃないか&lt;/li&gt;
&lt;li&gt;波形合成にSPTKのmlsadfコマンド使ってる？あれ実はフレーム間のメルケプが線形補間されてるんですよね。本当に線形補間でいいんでしょうか？他の補間法も試したらどうですかね&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;こんなかんじですか。おやすみなさい&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>統計的声質変換クッソムズすぎワロタ</title>
      <link>https://r9y9.github.io/blog/2014/07/05/statistical-voice-conversion-muzui/</link>
      <pubDate>Sat, 05 Jul 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/07/05/statistical-voice-conversion-muzui/</guid>
      <description>&lt;h2 id=&#34;20141012-追記&#34;&gt;2014/10/12 追記&lt;/h2&gt;
&lt;p&gt;少なくともGVのコードに致命的なバグがあったことがわかりました。よって、あまりあてにしないでください…（ごめんなさい&lt;/p&gt;
&lt;p&gt;こんにちは。&lt;/p&gt;
&lt;p&gt;最近、統計的声質変換の勉強をしていました。で、メジャーなGMM（混合ガウスモデル）ベースの変換を色々やってみたので、ちょろっと書きます。実は（というほどでもない?）シンプルなGMMベースの方法だと音質クッソ悪くなってしまうんですが、色々試してやっとまともに聞ける音質になったので、試行錯誤の形跡を残しておくとともに、音声サンプルを貼っておきます。ガチ勢の方はゆるりと見守ってください&lt;/p&gt;
&lt;p&gt;基本的に、以下の論文を参考にしています&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://isw3.naist.jp/~tomoki/Tomoki/Journals/IEEE-Nov-2007_MLVC.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;T. Toda, A. W. Black, and K. Tokuda, “Voice conversion based on maximum likelihood estimation of spectral parameter trajectory,” IEEE Trans. Audio, Speech, Lang. Process, vol. 15, no. 8, pp. 2222–2235, Nov. 2007&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;gmmベースの声質変換の基本&#34;&gt;GMMベースの声質変換の基本&lt;/h2&gt;
&lt;p&gt;シンプルなGMMベースの声質変換は大きく二つのフェーズに分けられます。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;参照話者と目標話者のスペクトル特徴量の結合GMM $P(x,y)$を学習する&lt;/li&gt;
&lt;li&gt;入力$x$が与えらたとき、$P(y|x)$が最大となるようにスペクトル特徴量を変換する&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;あらかじめ話者間の関係をデータから学習しておくことで、未知の入力が来た時にも変換が可能になるわけです。&lt;/p&gt;
&lt;p&gt;具体的な変換プロセスとしては、音声を&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基本周波数&lt;/li&gt;
&lt;li&gt;非周期性成分&lt;/li&gt;
&lt;li&gt;スペクトル包絡&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;の3つに分解し、スペクトル包絡の部分（≒声質を表す特徴量）に対して変換を行い、最後に波形を再合成するといった方法がよく用いられます。基本周波数や非周期性成分も変換することがありますが、ここではとりあえず扱いません&lt;/p&gt;
&lt;p&gt;シンプルな方法では、フレームごとに独立に変換を行います。&lt;/p&gt;
&lt;p&gt;GMMベースのポイントは、東大の齋藤先生の以下のツイートを引用しておきます。&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; lang=&#34;en&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://twitter.com/shurabaP&#34;&gt;@shurabaP&lt;/a&gt; GMMベースの声質変換の肝は、入力xが与えられた時の出力yの条件付き確率P(y|x) が最大になるようにyを選ぶという確率的な考えです。私のショボい自作スクリプトですが、HTKを使ったGMMの学習レシピは研究室内部用に作ってあるので、もし必要なら公開しますよ。&lt;/p&gt;&amp;mdash; Daisuke Saito (@dsk_saito) &lt;a href=&#34;https://twitter.com/dsk_saito/statuses/48442052534472706&#34;&gt;March 17, 2011&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;ちなみに僕はscipy.mixture.GMMを使いました。HTKヤダー&lt;/p&gt;
&lt;h2 id=&#34;やってみる&#34;&gt;やってみる&lt;/h2&gt;
&lt;p&gt;さて、実際にやってみます。データベースには、[CMU_ARCTIC speech synthesis databases](ht
tp://www.festvox.org/cmu_arctic/)を使います。今回は、女性話者の二人を使いました。&lt;/p&gt;
&lt;p&gt;音声の分析合成には、&lt;a href=&#34;http://ml.cs.yamanashi.ac.jp/world/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WORLD&lt;/a&gt;を使います。WORLDで求めたスペクトル包絡からメルケプストラム（今回は32次元）に変換したものを特徴量として使いました。&lt;/p&gt;
&lt;p&gt;学習では、学習サンプル10641フレーム（23フレーズ）、GMMの混合数64、full-covarianceで学習しました。&lt;/p&gt;
&lt;h3 id=&#34;変換元となる話者参照話者&#34;&gt;変換元となる話者（参照話者）&lt;/h3&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;166&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/157362625&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false&#34;&gt;&lt;/iframe&gt;
&lt;h3 id=&#34;変換対象となる話者目標話者&#34;&gt;変換対象となる話者（目標話者）&lt;/h3&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;166&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/157362613&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false&#34;&gt;&lt;/iframe&gt;
&lt;h3 id=&#34;gmmベースのframe-by-frameな声質変換の結果&#34;&gt;GMMベースのframe-by-frameな声質変換の結果&lt;/h3&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;166&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/157371966&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;はぁー、正直聞けたもんじゃないですね。声質は目標話者に近づいている感がありますが、何分音質が悪い。学習条件を色々変えて試行錯誤しましたけどダメでした&lt;/p&gt;
&lt;h2 id=&#34;gmmベースの声質変換の弱点&#34;&gt;GMMベースの声質変換の弱点&lt;/h2&gt;
&lt;p&gt;さて、なぜダメかを考えます。もう考えつくされてる感あるけど、大事なところだけ整理します&lt;/p&gt;
&lt;h3 id=&#34;フレーム毎に独立な変換処理&#34;&gt;フレーム毎に独立な変換処理&lt;/h3&gt;
&lt;p&gt;まず、音声が時間的に独立なわけないですよね。フレームごとに独立に変換すると、時間的に不連続な点が出てきてしまいます。その結果、ちょっとノイジーな音声になってしまったのではないかと考えられます。&lt;/p&gt;
&lt;p&gt;これに対する解決法としては、戸田先生の論文にあるように、動的特徴量も併せてGMMを学習して、系列全体の確率が最大となるように変換を考えるトラジェクトリベースのパラメータ生成方法があります。&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;さて、やってみます。参照音声、目標音声は↑で使ったサンプルと同じです。&lt;/p&gt;
&lt;h3 id=&#34;トラジェクトリベースの声質変換の結果&#34;&gt;トラジェクトリベースの声質変換の結果&lt;/h3&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;166&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/157371969&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;あんま変わらないですね。計算量めっちゃ食うのに、本当につらい。実装が間違ってる可能性もあるけど…&lt;/p&gt;
&lt;p&gt;他の方法を考えるとするならば、まぁいっぱいあると思うんですが、スペクトル包絡なんて時間的に不連続にコロコロ変わるようなもんでもない気がするので、確率モデルとしてそういう依存関係を考慮した声質変換があってもいいもんですけどね。あんま見てない気がします。&lt;/p&gt;
&lt;p&gt;ちょっと調べたら見つかったもの↓&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://koasas.kaist.ac.kr/bitstream/10203/17632/1/25.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kim, E.K., Lee, S., Oh, Y.-H. (1997). &amp;ldquo;Hidden Markov Model Based Voice Conversion Using Dynamic Characteristics of Speaker&amp;rdquo;, Proc. of Eurospeech’97, Rhodes, Greece, pp. 2519-2522.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;過剰な平滑化&#34;&gt;過剰な平滑化&lt;/h3&gt;
&lt;p&gt;これはGMMに限った話ではないですが、GMMベースのFrame-by-Frameな声質変換の場合でいえば、変換後の特徴量は条件付き期待値を取ることになるので、まぁ常識的に考えて平滑化されますよね。&lt;/p&gt;
&lt;p&gt;これに対する解法としては、GV（Global Variance）を考慮する方法があります。これは戸田先生が提案されたものですね。&lt;/p&gt;
&lt;p&gt;さて、やってみます。wktk&lt;/p&gt;
&lt;h3 id=&#34;gvを考慮したトラジェクトリベースの声質変換の結果&#34;&gt;GVを考慮したトラジェクトリベースの声質変換の結果&lt;/h3&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;166&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/157371971&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;多少ましになった気もしなくもないけど、やっぱり音質はいまいちですね。そして計算量は激マシします。本当につらい。学会で聞いたGVありの音声はもっと改善してた気がするんだけどなー音声合成の話だけど。僕の実装が間違ってるんですかね…&lt;/p&gt;
&lt;h2 id=&#34;ムズすぎわろた&#34;&gt;ムズすぎわろた&lt;/h2&gt;
&lt;p&gt;以上、いくつか試しましたが、統計的声質変換は激ムズだということがわかりました。え、ここで終わるの？という感じですが、最後に一つ別の手法を紹介します。&lt;/p&gt;
&lt;h2 id=&#34;差分スペクトル補正に基づく統計的声質変換&#34;&gt;差分スペクトル補正に基づく統計的声質変換&lt;/h2&gt;
&lt;p&gt;これまでは、音声を基本周波数、非周期性成分、スペクトル包絡に分解して、スペクトル包絡を表す特徴量を変換し、変換後の特徴量を元に波形を再合成していました。ただ、よくよく考えると、そもそも基本周波数、非周期性成分をいじる必要がない場合であれば、わざわざ分解して再合成する必要なくね？声質の部分のみ変換するようなフィルタかけてやればよくね？という考えが生まれます。実は、そういったアイデアに基づく素晴らしい手法があります。それが、差分スペクトル補正に基づく声質変換です。&lt;/p&gt;
&lt;p&gt;詳細は、以下の予稿をどうぞ&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.phontron.com/paper/kobayashi14asj.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;小林 和弘, 戸田 智基, Graham Neubig, Sakriani Sakti, 中村 哲. &amp;ldquo;差分スペクトル補正に基づく統計的歌声声質変換&amp;rdquo;, 日本音響学会2014年春季研究発表会(ASJ). 東京. 2014年3月.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;では、やってみます。歌声ではなく話し声ですが。他の声質変換の結果とも聴き比べてみてください。&lt;/p&gt;
&lt;h3 id=&#34;差分スペクトル補正に基づく声質変換の結果&#34;&gt;差分スペクトル補正に基づく声質変換の結果&lt;/h3&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;166&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/157362603&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;かなり音声の自然性は上がりましたね。これはヘタすると騙されるレベル。本当に素晴らしいです。しかも簡単にできるので、お勧めです。↑のは、GMMに基づくframe-by-frameな変換です。計算量も軽いので、リアルタイムでもいけますね。&lt;/p&gt;
&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;
&lt;p&gt;声質変換であれこれ試行錯誤して、ようやくスタートラインにたてた感があります。今後は新しい方法を考えようかなーと思ってます。&lt;/p&gt;
&lt;p&gt;おわり&lt;/p&gt;
&lt;h2 id=&#34;おわび&#34;&gt;おわび&lt;/h2&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; lang=&#34;en&#34;&gt;&lt;p&gt;お盆の間に学習ベースの声質変換のプログラム書く（宿題） &lt;a href=&#34;https://twitter.com/hashtag/%E5%AE%A3%E8%A8%80?src=hash&#34;&gt;#宣言&lt;/a&gt;&lt;/p&gt;&amp;mdash; 山本りゅういち (@r9y9) &lt;a href=&#34;https://twitter.com/r9y9/statuses/366928228465655808&#34;&gt;August 12, 2013&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;約1年かかりました……。本当に申し訳ありませんでした(´･_･`)&lt;/p&gt;
&lt;h2 id=&#34;追記&#34;&gt;追記&lt;/h2&gt;
&lt;p&gt;Twitterで教えてもらいました。トラジェクトリベースで学習も変換も行う研究もありました&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; lang=&#34;en&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://twitter.com/r9y9&#34;&gt;@r9y9&lt;/a&gt; つ トラジェクトリＧＭＭな特徴量変換 &lt;a href=&#34;http://t.co/kUn7bp9EUt&#34;&gt;http://t.co/kUn7bp9EUt&lt;/a&gt;&lt;/p&gt;&amp;mdash; 縄文人（妖精系研究者なのです） (@dicekicker) &lt;a href=&#34;https://twitter.com/dicekicker/statuses/485376823308455936&#34;&gt;July 5, 2014&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;ただ、これはトラジェクトリベースのパラメータ生成法の提案であって、トラジェクトリモデル自体を学習してるわけではないんだよなー。普通に考えると学習もトラジェクトリで考える方法があっていい気がするが、 &lt;del&gt;まだ見てないですね。&lt;/del&gt; ありました。追記参照&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>PythonによるニューラルネットのToyコード</title>
      <link>https://r9y9.github.io/blog/2014/05/11/python-feed-forward-neural-network-toy-code/</link>
      <pubDate>Sun, 11 May 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/05/11/python-feed-forward-neural-network-toy-code/</guid>
      <description>&lt;p&gt;1000番煎じだけど、知り合いにニューラルネットを教えていて、その過程で書いたコード。わかりやすさ重視。&lt;/p&gt;
&lt;p&gt;このために、誤差伝播法をn回導出しました（意訳：何回もメモなくしました）&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/usr/bin/python
# coding: utf-8

# ニューラルネットワーク(Feed-Forward Neural Networks)の学習、認識の
# デモコードです。
# 誤差伝搬法によってニューラルネットを学習します。
# XORの学習、テストの簡単なデモコードもついています
# 2014/05/10 Ryuichi Yamamoto

import numpy as np

def sigmoid(x):
    return 1.0 / (1.0 + np.exp(-x))

def dsigmoid(y):
    return y * (1.0 - y)

class NeuralNet:
    def __init__(self, num_input, num_hidden, num_output):
        &amp;quot;&amp;quot;&amp;quot;
        パラメータの初期化
        &amp;quot;&amp;quot;&amp;quot;
        # 入力層から隠れ層への重み行列
        self.W1 = np.random.uniform(-1.0, 1.0, (num_input, num_hidden))
        self.hidden_bias = np.ones(num_hidden, dtype=float)
        # 隠れ層から出力層への重み行列
        self.W2 = np.random.uniform(-1.0, 1.0, (num_hidden, num_output))
        self.output_bias = np.ones(num_output, dtype=float)

    def forward(self, x):
        &amp;quot;&amp;quot;&amp;quot;
        前向き伝搬の計算
        &amp;quot;&amp;quot;&amp;quot;
        h = sigmoid(np.dot(self.W1.T, x) + self.hidden_bias)
        return sigmoid(np.dot(self.W2.T, h) + self.output_bias)

    def cost(self, data, target):
        &amp;quot;&amp;quot;&amp;quot;
        最小化したい誤差関数
        &amp;quot;&amp;quot;&amp;quot;
        N = data.shape[0]
        E = 0.0
        for i in range(N):
            y, t = self.forward(data[i]), target[i]
            E += np.sum((y - t) * (y - t))
        return 0.5 * E / float(N)

    def train(self, data, target, epoches=30000, learning_rate=0.1,\
              monitor_period=None):
        &amp;quot;&amp;quot;&amp;quot;
        Stochastic Gradient Decent (SGD) による学習
        &amp;quot;&amp;quot;&amp;quot;
        for epoch in range(epoches):
            # 学習データから1サンプルをランダムに選ぶ
            index = np.random.randint(0, data.shape[0])
            x, t = data[index], target[index]

            # 入力から出力まで前向きに信号を伝搬
            h = sigmoid(np.dot(self.W1.T, x) + self.hidden_bias)
            y = sigmoid(np.dot(self.W2.T, h) + self.output_bias)

            # 隠れ層-&amp;gt;出力層の重みの修正量を計算
            output_delta = (y - t) * dsigmoid(y)
            grad_W2 = np.dot(np.atleast_2d(h).T, np.atleast_2d(output_delta))

            # 隠れ層-&amp;gt;出力層の重みを更新
            self.W2 -= learning_rate * grad_W2
            self.output_bias -= learning_rate * output_delta

            # 入力層-&amp;gt;隠れ層の重みの修正量を計算
            hidden_delta = np.dot(self.W2, output_delta) * dsigmoid(h)
            grad_W1 = np.dot(np.atleast_2d(x).T, np.atleast_2d(hidden_delta))

            # 入力層-&amp;gt;隠れ層の重みを更新
            self.W1 -= learning_rate * grad_W1
            self.hidden_bias -= learning_rate * hidden_delta

            # 現在の目的関数の値を出力
            if monitor_period != None and epoch % monitor_period == 0:
                print &amp;quot;Epoch %d, Cost %f&amp;quot; % (epoch, self.cost(data, target))

        print &amp;quot;Training finished.&amp;quot;

    def predict(self, x):
        &amp;quot;&amp;quot;&amp;quot;
        出力層の最も反応するニューロンの番号を返します
        &amp;quot;&amp;quot;&amp;quot;
        return np.argmax(self.forward(x))

if __name__ == &amp;quot;__main__&amp;quot;:
    import argparse

    parser = argparse.ArgumentParser(description=&amp;quot;Specify options&amp;quot;)
    parser.add_argument(&amp;quot;--epoches&amp;quot;, dest=&amp;quot;epoches&amp;quot;, type=int, required=True)
    parser.add_argument(&amp;quot;--learning_rate&amp;quot;, dest=&amp;quot;learning_rate&amp;quot;,\
                        type=float, default=0.1)
    parser.add_argument(&amp;quot;--hidden&amp;quot;, dest=&amp;quot;hidden&amp;quot;, type=int, default=20)
    args = parser.parse_args()

    nn = NeuralNet(2, args.hidden, 1)

    data = np.array([[0, 0], [0 ,1], [1, 0], [1, 1]])
    target = np.array([0, 1, 1, 0])

    nn.train(data, target, args.epoches, args.learning_rate,\
             monitor_period=1000)

    for x in data:
        print &amp;quot;%s : predicted %s&amp;quot; % (x, nn.forward(x))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/usr/bin/python
# coding: utf-8

# MNISTを用いたニューラルネットによる手書き数字認識のデモコードです
# 学習方法やパラメータによりますが、だいたい 90 ~ 97% くらいの精度出ます。
# 使い方は、コードを読むか、
# python mnist_net.py -h
# としてください
# 参考までに、
# python mnist_net.py --epoches 50000 --learning_rate 0.1 --hidden 100
# とすると、テストセットに対して、93.2%の正解率です
# 僕の環境では、学習、認識合わせて（だいたい）5分くらいかかりました。
# 2014/05/10 Ryuichi Yamamoto

import numpy as np
from sklearn.externals import joblib
import cPickle
import gzip
import os

# 作成したニューラルネットのパッケージ
import net

def load_mnist_dataset(dataset):
    &amp;quot;&amp;quot;&amp;quot;
    MNISTのデータセットをダウンロードします
    &amp;quot;&amp;quot;&amp;quot;
    # Download the MNIST dataset if it is not present
    data_dir, data_file = os.path.split(dataset)
    if (not os.path.isfile(dataset)) and data_file == &#39;mnist.pkl.gz&#39;:
        import urllib
        origin = &#39;http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz&#39;
        print &#39;Downloading data from %s&#39; % origin
        urllib.urlretrieve(origin, dataset)

    f = gzip.open(dataset, &#39;rb&#39;)
    train_set, valid_set, test_set = cPickle.load(f)
    f.close()

    return train_set, valid_set, test_set

def augument_labels(labels, order):
    &amp;quot;&amp;quot;&amp;quot;
    1次元のラベルデータを、ラベルの種類数(order)次元に拡張します
    &amp;quot;&amp;quot;&amp;quot;
    new_labels = []
    for i in range(labels.shape[0]):
        v = np.zeros(order)
        v[labels[i]] = 1
        new_labels.append(v)

    return np.array(new_labels).reshape((labels.shape[0], order))

if __name__ == &amp;quot;__main__&amp;quot;:
    import argparse

    parser = argparse.ArgumentParser(description=&amp;quot;MNIST手書き数字認識のデモ&amp;quot;)
    parser.add_argument(&amp;quot;--epoches&amp;quot;, dest=&amp;quot;epoches&amp;quot;, type=int, required=True)
    parser.add_argument(&amp;quot;--learning_rate&amp;quot;, dest=&amp;quot;learning_rate&amp;quot;,\
                        type=float, default=0.1)
    parser.add_argument(&amp;quot;--hidden&amp;quot;, dest=&amp;quot;hidden&amp;quot;, type=int, default=100)
    args = parser.parse_args()

    train_set, valid_set, test_set = load_mnist_dataset(&amp;quot;mnist.pkl.gz&amp;quot;)
    n_labels = 10 # 0,1,2,3,4,5,6,7,9
    n_features = 28*28

    # モデルを新しく作る
    nn = net.NeuralNet(n_features, args.hidden, n_labels)

    # モデルを読み込む
    # nn = joblib.load(&amp;quot;./nn_mnist.pkl&amp;quot;)

    nn.train(train_set[0], augument_labels(train_set[1], n_labels),\
             args.epoches, args.learning_rate, monitor_period=2000)

    ## テスト
    test_data, labels = test_set
    results = np.arange(len(test_data), dtype=np.int)
    for n in range(len(test_data)):
        results[n] = nn.predict(test_data[n])
        # print &amp;quot;%d : predicted %s, expected %s&amp;quot; % (n, results[n], labels[n])
    print &amp;quot;recognition rate: &amp;quot;, (results == labels).mean()

    # モデルを保存
    model_filename = &amp;quot;nn_mnist.pkl&amp;quot;
    joblib.dump(nn, model_filename, compress=9)
    print &amp;quot;The model parameters are dumped to &amp;quot; + model_filename
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/python-neural-net-toy-codes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/python-neural-net-toy-codes&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;以下のようなコマンドを叩いて、正解率が97%くらいになるまで学習してから入力層から隠れ層への重みを可視化してみた&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# python mnist_net.py --epoches 50000 --learning_rate 0.1 --hidden 100 # epochesは適当に
&lt;/code&gt;&lt;/pre&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/nn_mnist_W1_100.png&#34; alt=&#34;Input to Hidden weight filters after trained on MNIST.&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;興味深いことに、RBMと違って重み行列の解釈はしにくい。生成モデルの尤度を最大化することと、誤差を最小化することはこんなにも違うんだなぁというこなみな感想&lt;/p&gt;
&lt;p&gt;RBMについては、以下へ&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://r9y9.github.io/blog/2014/03/06/restricted-boltzmann-machines-mnist/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Restricted Boltzmann Machines with MNIST - LESS IS MORE&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;おわり&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Restricted Boltzmann Machines with MNIST</title>
      <link>https://r9y9.github.io/blog/2014/03/06/restricted-boltzmann-machines-mnist/</link>
      <pubDate>Thu, 06 Mar 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/03/06/restricted-boltzmann-machines-mnist/</guid>
      <description>&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/RBM_mnist_Hidden_500_layers.png &#34;RBM training result on MNIST handwritten digit dataset. Each image represents a filter learned by RBM.&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;ディープ某を使った研究を再現してみたくて、最近某ニューラルネットに手を出し始めた。で、手始めにRestricted Boltzmann Machinesを実装してみたので、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MNISTを使って学習した結果の重み（22*22=484個）を貼っとく（↑）&lt;/li&gt;
&lt;li&gt;得た知見をまとめとく&lt;/li&gt;
&lt;li&gt;Goのコード貼っとく&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ってな感じで書いておく&lt;/p&gt;
&lt;p&gt;(本当はRBMについて自分なりの解釈を書こうと思ったのだけど、それはまた今度)&lt;/p&gt;
&lt;h2 id=&#34;実験条件&#34;&gt;実験条件&lt;/h2&gt;
&lt;p&gt;データベースはmnist。手書き数字認識で有名なアレ。学習の条件は、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;隠れ層のユニット数: 500&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;mini-batch size: 20&lt;/li&gt;
&lt;li&gt;iterationの回数: 15&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;対数尤度の変化&#34;&gt;対数尤度の変化&lt;/h2&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/RBM_mnist_Hidden_500_log_likelihood.png &#34;Pseudo log-likelihood on mnist databae.&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;以下グラフに表示している生データ&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0 -196.59046099622128
1 -70.31708616742365
2 -65.29499371647965
3 -62.37983267378022
4 -61.5359019358253
5 -60.917772257650164
6 -59.64207778426757
7 -59.42201674307857
8 -59.18497336138633
9 -58.277168243126305
10 -58.36279288392401
11 -58.57805165724595
12 -57.71043215987184
13 -58.17783142034138
14 -57.53629129936344
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;尤度上がると安心する。厳密に対数尤度を計算することは難しいので、&lt;a href=&#34;http://deeplearning.net/tutorial/rbm.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Restricted Boltzmann Machines (RBM) | DeepLearning Tutorial&lt;/a&gt; にある擬似尤度を参考にした&lt;/p&gt;
&lt;h2 id=&#34;学習時間&#34;&gt;学習時間&lt;/h2&gt;
&lt;p&gt;うちのcore2duoのPCで4時間弱だった気がする（うろ覚え&lt;/p&gt;
&lt;p&gt;隠れ層のユニット数100だと、40分ほどだった&lt;/p&gt;
&lt;h2 id=&#34;知見&#34;&gt;知見&lt;/h2&gt;
&lt;p&gt;今の所、試行錯誤して自分が得た知見は、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sample by sampleのSGDよりmini-batch SGDの方が安定して尤度上がる&lt;/li&gt;
&lt;li&gt;mini-batch sizeを大きくしすぎると学習が進まない。20くらいがちょうど良かった&lt;/li&gt;
&lt;li&gt;k-CD のkを大きくしてもさほど学習結果変わらない（計算コストはけっこう増すけど）&lt;/li&gt;
&lt;li&gt;persistent CDを使ってもあまりよくならない（計算コストはけっこう増すけど）&lt;/li&gt;
&lt;li&gt;やっぱ1-CDで十分だった&lt;/li&gt;
&lt;li&gt;データの正規化方法によって結構結果も変わる。ノイズを足すかどうか、とか&lt;/li&gt;
&lt;li&gt;学習率超重要すぎわろた。今回の場合は0.1くらいかちょうど良かった&lt;/li&gt;
&lt;li&gt;隠れ層のユニット数が大きいほど学習が上手く行けばと尤度は上がる(?)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;まぁだいたい &lt;a href=&#34;http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Practical Guide to Training Restricted Boltzmann Machines (PDF)&lt;/a&gt; に書いてあるけど、実際に肌で感じて理解した。persistent CDはもうちょっと成果出て欲しい。データ変えると成果出るんかな？&lt;/p&gt;
&lt;h2 id=&#34;コード&#34;&gt;コード&lt;/h2&gt;
&lt;p&gt;コアの部分だけ、一応&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package rbm

import (
	&amp;quot;encoding/json&amp;quot;
	&amp;quot;errors&amp;quot;
	&amp;quot;fmt&amp;quot;
	&amp;quot;github.com/r9y9/nn&amp;quot; // sigmoid, matrix
	&amp;quot;math&amp;quot;
	&amp;quot;math/rand&amp;quot;
	&amp;quot;os&amp;quot;
	&amp;quot;time&amp;quot;
)

// References:
// [1] G. Hinton, &amp;quot;A Practical Guide to Training Restricted Boltzmann Machines&amp;quot;,
// UTML TR 2010-003.
// url: http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf
//
// [2] A. Fischer and C. Igel. &amp;quot;An introduction to restricted Boltzmann machines&amp;quot;,
// Proc. of the 17th Iberoamerican Congress on Pattern Recognition (CIARP),
// Volume 7441 of LNCS, pages 14–36. Springer, 2012
// url: http://image.diku.dk/igel/paper/AItRBM-proof.pdf
//
// [3] Restricted Boltzmann Machines (RBM),  DeepLearning tutorial
// url: http://deeplearning.net/tutorial/rbm.html

// Notes about implementation:
// Notation used in this code basically follows [2].
// e.g. W for weight, B for bias of visible layer, C for bias of hidden layer.

// Graphical representation of Restricted Boltzmann Machines (RBM).
//
//     ○ ○ .... ○  h(hidden layer), c(bias)
//     /\ /\ /    /\
//    ○ ○ ○ ... ○ v(visible layer), b(bias)
type RBM struct {
	W               [][]float64 // Weight
	B               []float64   // Bias of visible layer
	C               []float64   // Bias of hidden layer
	NumHiddenUnits  int
	NumVisibleUnits int
	Option          TrainingOption
}

type TrainingOption struct {
	LearningRate        float64
	OrderOfGibbsSamping int // It is known that 1 is enough for many cases.
	Epoches             int
	MiniBatchSize       int
	L2Regularization    bool
	RegularizationRate  float64
}

// NewRBM creates new RBM instance. It requires input data and number of
// hidden units to initialize RBM.
func NewRBM(numVisibleUnits, numHiddenUnits int) *RBM {
	rbm := new(RBM)
	rand.Seed(time.Now().UnixNano())

	rbm.W = nn.MakeMatrix(numHiddenUnits, numVisibleUnits)
	rbm.B = make([]float64, numVisibleUnits)
	rbm.C = make([]float64, numHiddenUnits)
	rbm.NumVisibleUnits = numVisibleUnits
	rbm.NumHiddenUnits = numHiddenUnits

	rbm.InitRBM()
	return rbm
}

// NewRBMWithParameters returns RBM instance given RBM parameters.
// This func will be used in Deep Belief Networks.
func NewRBMWithParameters(W [][]float64, B, C []float64) (*RBM, error) {
	rbm := new(RBM)

	rbm.NumVisibleUnits = len(B)
	rbm.NumHiddenUnits = len(C)

	if len(W) != rbm.NumHiddenUnits || len(W[0]) != rbm.NumVisibleUnits {
		return nil, errors.New(&amp;quot;Shape of weight matrix is wrong.&amp;quot;)
	}

	rand.Seed(time.Now().UnixNano())
	rbm.W = W
	rbm.B = B
	rbm.C = C

	return rbm, nil
}

// LoadRBM loads RBM from a dump file and return its instatnce.
func LoadRBM(filename string) (*RBM, error) {
	file, err := os.Open(filename)
	if err != nil {
		return nil, err
	}
	defer file.Close()

	decoder := json.NewDecoder(file)
	rbm := &amp;amp;RBM{}
	err = decoder.Decode(rbm)

	if err != nil {
		return nil, err
	}

	return rbm, nil
}

// Dump writes RBM parameters to file in json format.
func (rbm *RBM) Dump(filename string) error {
	file, err := os.Create(filename)
	if err != nil {
		return err
	}
	defer file.Close()

	encoder := json.NewEncoder(file)
	err = encoder.Encode(rbm)
	if err != nil {
		return err
	}

	return nil
}

// Heuristic initialization of visible bias.
func (rbm *RBM) InitVisibleBiasUsingTrainingData(data [][]float64) {
	// Init B (bias of visible layer)
	activeRateInVisibleLayer := rbm.getActiveRateInVisibleLayer(data)
	for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
		rbm.B[j] = math.Log(activeRateInVisibleLayer[j] / (1.0 - activeRateInVisibleLayer[j]))
	}
}

func (rbm *RBM) getActiveRateInVisibleLayer(data [][]float64) []float64 {
	rate := make([]float64, rbm.NumVisibleUnits)
	for _, sample := range data {
		for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
			rate[j] += sample[j]
		}
	}
	for j := range rate {
		rate[j] /= float64(len(data))
	}
	return rate
}

// InitRBM performes a heuristic parameter initialization.
func (rbm *RBM) InitRBM() {
	// Init W
	for i := 0; i &amp;lt; rbm.NumHiddenUnits; i++ {
		for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
			rbm.W[i][j] = 0.01 * rand.NormFloat64()
		}
	}

	for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
		rbm.B[j] = 0.0
	}

	// Init C (bias of hidden layer)
	for i := 0; i &amp;lt; rbm.NumHiddenUnits; i++ {
		rbm.C[i] = 0.0
	}
}

// P_H_Given_V returns the conditinal probability of a hidden unit given a set of visible units.
func (rbm *RBM) P_H_Given_V(hiddenIndex int, v []float64) float64 {
	sum := 0.0
	for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
		sum += rbm.W[hiddenIndex][j] * v[j]
	}
	return nn.Sigmoid(sum + rbm.C[hiddenIndex])
}

// P_V_Given_H returns the conditinal probability of a visible unit given a set of hidden units.
func (rbm *RBM) P_V_Given_H(visibleIndex int, h []float64) float64 {
	sum := 0.0
	for i := 0; i &amp;lt; rbm.NumHiddenUnits; i++ {
		sum += rbm.W[i][visibleIndex] * h[i]
	}
	return nn.Sigmoid(sum + rbm.B[visibleIndex])
}

// GibbsSampling performs k-Gibbs sampling algorithm,
// where k is the number of iterations in gibbs sampling.
func (rbm *RBM) GibbsSampling(v []float64, k int) []float64 {
	// Initial value is set to input
	vUsedInSamping := make([]float64, len(v))
	copy(vUsedInSamping, v)

	for t := 0; t &amp;lt; k; t++ {
		sampledH := make([]float64, rbm.NumHiddenUnits)
		for i := 0; i &amp;lt; rbm.NumHiddenUnits; i++ {
			p := rbm.P_H_Given_V(i, vUsedInSamping)
			if p &amp;gt; rand.Float64() {
				sampledH[i] = 1.0
			} else {
				sampledH[i] = 0.0
			}
		}
		for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
			p := rbm.P_V_Given_H(j, sampledH)
			if p &amp;gt; rand.Float64() {
				vUsedInSamping[j] = 1.0
			} else {
				vUsedInSamping[j] = 0.0
			}
		}
	}

	return vUsedInSamping
}

func flip(x []float64, bit int) []float64 {
	y := make([]float64, len(x))
	copy(y, x)
	y[bit] = 1.0 - x[bit]
	return y
}

// FreeEnergy returns F(v), the free energy of RBM given a visible vector v.
// refs: eq. (25) in [1].
func (rbm *RBM) FreeEnergy(v []float64) float64 {
	energy := 0.0

	for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
		energy -= rbm.B[j] * v[j]
	}

	for i := 0; i &amp;lt; rbm.NumHiddenUnits; i++ {
		sum := rbm.C[i]
		for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
			sum += rbm.W[i][j] * v[j]
		}
		energy -= math.Log(1 + math.Exp(sum))
	}

	return energy
}

// PseudoLogLikelihood returns pseudo log-likelihood for a given input data.
func (rbm *RBM) PseudoLogLikelihood(v []float64) float64 {
	bitIndex := rand.Int() % len(v)
	fe := rbm.FreeEnergy(v)
	feFlip := rbm.FreeEnergy(flip(v, bitIndex))
	cost := float64(rbm.NumVisibleUnits) * math.Log(nn.Sigmoid(feFlip-fe))
	return cost
}

// PseudoLogLikelihood returns pseudo log-likelihood for a given dataset (or mini-batch).
func (rbm *RBM) PseudoLogLikelihoodForAllData(data [][]float64) float64 {
	sum := 0.0
	for i := range data {
		sum += rbm.PseudoLogLikelihood(data[i])
	}
	cost := sum / float64(len(data))
	return cost
}

// ComputeGradient returns gradients of RBM parameters for a given (mini-batch) dataset.
func (rbm *RBM) ComputeGradient(data [][]float64) ([][]float64, []float64, []float64) {
	gradW := nn.MakeMatrix(rbm.NumHiddenUnits, rbm.NumVisibleUnits)
	gradB := make([]float64, rbm.NumVisibleUnits)
	gradC := make([]float64, rbm.NumHiddenUnits)

	for _, v := range data {
		// Gibbs Sampling
		gibbsStart := v
		vAfterSamping := rbm.GibbsSampling(gibbsStart, rbm.Option.OrderOfGibbsSamping)

		// pre-computation that is used in gradient computation
		p_h_given_v1 := make([]float64, rbm.NumHiddenUnits)
		p_h_given_v2 := make([]float64, rbm.NumHiddenUnits)
		for i := 0; i &amp;lt; rbm.NumHiddenUnits; i++ {
			p_h_given_v1[i] = rbm.P_H_Given_V(i, v)
			p_h_given_v2[i] = rbm.P_H_Given_V(i, vAfterSamping)
		}

		// Gompute gradient of W
		for i := 0; i &amp;lt; rbm.NumHiddenUnits; i++ {
			for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
				gradW[i][j] += p_h_given_v1[i]*v[j] - p_h_given_v2[i]*vAfterSamping[j]
			}
		}

		// Gompute gradient of B
		for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
			gradB[j] += v[j] - vAfterSamping[j]
		}

		// Gompute gradient of C
		for i := 0; i &amp;lt; rbm.NumHiddenUnits; i++ {
			gradC[i] += p_h_given_v1[i] - p_h_given_v2[i]
		}
	}

	return gradW, gradB, gradC
}

func (rbm *RBM) ParseTrainingOption(option TrainingOption) error {
	rbm.Option = option

	if rbm.Option.MiniBatchSize &amp;lt;= 0 {
		return errors.New(&amp;quot;Number of mini-batchs must be larger than zero.&amp;quot;)
	}
	if rbm.Option.Epoches &amp;lt;= 0 {
		return errors.New(&amp;quot;Epoches must be larger than zero.&amp;quot;)
	}
	if rbm.Option.OrderOfGibbsSamping &amp;lt;= 0 {
		return errors.New(&amp;quot;Order of Gibbs sampling must be larger than zero.&amp;quot;)
	}
	if rbm.Option.LearningRate == 0 {
		return errors.New(&amp;quot;Learning rate must be specified to train RBMs.&amp;quot;)
	}

	return nil
}

// Train performs Contrastive divergense learning algorithm to train RBM.
// The alrogithm is basedd on (mini-batch) Stochastic Gradient Ascent.
func (rbm *RBM) Train(data [][]float64, option TrainingOption) error {
	err := rbm.ParseTrainingOption(option)
	if err != nil {
		return err
	}

	numMiniBatches := len(data) / rbm.Option.MiniBatchSize

	for epoch := 0; epoch &amp;lt; option.Epoches; epoch++ {
		// Monitoring
		fmt.Println(epoch, rbm.PseudoLogLikelihoodForAllData(data))

		for m := 0; m &amp;lt; numMiniBatches; m++ {
			// Compute Gradient
			batch := data[m*rbm.Option.MiniBatchSize : (m+1)*rbm.Option.MiniBatchSize]
			gradW, gradB, gradC := rbm.ComputeGradient(batch)

			// Update W
			for i := 0; i &amp;lt; rbm.NumHiddenUnits; i++ {
				for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
					rbm.W[i][j] += rbm.Option.LearningRate * gradW[i][j] / float64(rbm.Option.MiniBatchSize)
					if rbm.Option.L2Regularization {
						rbm.W[i][j] *= (1.0 - rbm.Option.RegularizationRate)
					}
				}
			}

			// Update B
			for j := 0; j &amp;lt; rbm.NumVisibleUnits; j++ {
				rbm.B[j] += rbm.Option.LearningRate * gradB[j] / float64(rbm.Option.MiniBatchSize)
			}

			// Update C
			for i := 0; i &amp;lt; rbm.NumHiddenUnits; i++ {
				rbm.C[i] += rbm.Option.LearningRate * gradC[i] / float64(rbm.Option.MiniBatchSize)
			}
		}
	}

	return nil
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;使い方とかは察して（どうせ誰も使わないはず&lt;/p&gt;
&lt;p&gt;今は、通常のRBMのvisible layerを連続値に拡張した Gaussian Bernoulli RBMを学習しようとしてるんだけど、これがムズイ。実装ミスもあるかもだけど、局所解に落ちまくってる気がする。&lt;/p&gt;
&lt;p&gt;Gaussian Bernoulli RBM、Deep Belief Networks, Deep Neural Networksについてはまた今度&lt;/p&gt;
&lt;p&gt;2014/05/11
要望があったので、もろもろコードあげました
&lt;a href=&#34;https://github.com/r9y9/nnet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/nnet&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;参考資料&#34;&gt;参考資料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://image.diku.dk/igel/paper/AItRBM-proof.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Introduction to Restricted Boltzmann Machines (PDF)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Practical Guide to Training Restricted Boltzmann Machines (PDF)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://mglab.blogspot.jp/2012/08/restricted-boltzmann-machine.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Restricted Boltzmann Machineの学習手法についての簡単なまとめ | 映像奮闘記&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://d.hatena.ne.jp/saket/20121212&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ゆるふわ Restricted Boltzmann Machine | Risky Dune&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://deeplearning.net/tutorial/rbm.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Restricted Boltzmann Machines (RBM) | DeepLearning Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://imonad.com/rbm/restricted-boltzmann-machine/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Restricted Boltzmann Machine - Short Tutorial | iMonad&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://scikit-learn.org/stable/auto_examples/plot_rbm_logistic_classification.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Restricted Boltzmann Machine features for digit classification | scikit-learn&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Naive Bayesの復習（実装編）: MNISTを使って手書き数字認識</title>
      <link>https://r9y9.github.io/blog/2013/08/06/naive-bayes-mnist/</link>
      <pubDate>Tue, 06 Aug 2013 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2013/08/06/naive-bayes-mnist/</guid>
      <description>&lt;p&gt;前回は学習アルゴリズムを導出したので、今回はそれを実装する。Gaussian Naive Bayesのみやった。例によって、アルゴリズムを書く時間よりも言語の使い方等を調べてる時間などの方が圧倒的に多いという残念感だったけど、とりあえずメモる。python, numpy, scipy, matplotlibすべて忘れてた。どれも便利だから覚えよう…&lt;/p&gt;
&lt;p&gt;そもそもナイーブベイズやろうとしてたのも、MNISTのdigit recognitionがやりたかったからなので、実際にやってみた。&lt;/p&gt;
&lt;p&gt;コードはgithubに置いた &lt;a href=&#34;https://github.com/r9y9/naive_bayes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/naive_bayes&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;結果だけ知りたい人へ：正解率  76 %くらいでした。まぁこんなもんですね&lt;/p&gt;
&lt;h2 id=&#34;手書き数字認識&#34;&gt;手書き数字認識&lt;/h2&gt;
&lt;p&gt;手書き数字の画像データから、何が書かれているのか当てる。こういうタスクを手書き数字認識と言う。郵便番号の自動認識が有名ですね。&lt;/p&gt;
&lt;p&gt;今回は、MNISTという手書き数字のデータセットを使って、0〜9の数字認識をやる。MNISTについて詳しくは本家へ→&lt;a href=&#34;http://yann.lecun.com/exdb/mnist/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;THE MNIST DATABASE of handwritten digits&lt;/a&gt;
ただし、MNISTのデータセットは直接使わず、Deep Learningのチュートリアルで紹介されていた（&lt;a href=&#34;http://deeplearning.net/tutorial/gettingstarted.html#gettingstarted&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ここ&lt;/a&gt;）、pythonのcPickleから読める形式に変換されているデータを使った。感謝&lt;/p&gt;
&lt;h2 id=&#34;とりあえずやってみる&#34;&gt;とりあえずやってみる&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone https://github.com/r9y9/naive_bayes
$ cd naive_bayes
$ python mnist_digit_recognition.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;プログラムの中身は以下のようになってる。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MNISTデータセットのダウンロード&lt;/li&gt;
&lt;li&gt;モデルの学習&lt;/li&gt;
&lt;li&gt;テスト&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;実行すると、学習されたGaussianの平均が表示されて、最後に認識結果が表示される。今回は、単純に画像のピクセル毎に独立なGaussianを作ってるので、尤度の計算にめちゃくちゃ時間かかる。実装のせいもあるけど。なので、デフォでは50サンプルのみテストするようにした。&lt;/p&gt;
&lt;h2 id=&#34;学習されたgaussianの平均&#34;&gt;学習されたGaussianの平均&lt;/h2&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/mnist_mean_of_gaussian.png &#34;gaussian means&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;学習されたGaussianの平均をプロットしたもの。上のコードを実行すると表示される。&lt;/p&gt;
&lt;p&gt;それっぽい。学習データは50000サンプル&lt;/p&gt;
&lt;h2 id=&#34;認識結果&#34;&gt;認識結果&lt;/h2&gt;
&lt;p&gt;時間がかかるけど、テストデータ10000個に対してやってみると、結果は以下のようになった。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;0.7634 (7634/10000)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;まぁナイーブベイズなんてこんなもん。もちろん、改善のしようはいくらでもあるけれども。ちなみにDeep learningのチュートリアルで使われてたDBN.pyだと0.987くらいだった。&lt;/p&gt;
&lt;h2 id=&#34;感想&#34;&gt;感想&lt;/h2&gt;
&lt;p&gt;相関が強い特徴だと上手くいかんのは当たり前で、ピクセル毎にGaussianなんて作らずに（ピクセル間の相関を無視せずに）、少しまともな特徴抽出をかませば、8割りは超えるんじゃないかなぁと思う。&lt;/p&gt;
&lt;p&gt;あとこれ、実装してても機械学習的な面白さがまったくない（上がれ目的関数ｩｩーー！的な）ので、あまりおすすめしません。おわり。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://r9y9.github.io/blog/2013/07/28/naive-bayes-formulation/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;導出編→Naive Bayesの復習（導出編）&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.slideshare.net/shima__shima/python-13349162&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;機械学習のPythonとの出会い（１）：単純ベイズ基礎編 - slideshare&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Naive Bayesの復習（導出編）</title>
      <link>https://r9y9.github.io/blog/2013/07/28/naive-bayes-formulation/</link>
      <pubDate>Sun, 28 Jul 2013 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2013/07/28/naive-bayes-formulation/</guid>
      <description>&lt;p&gt;すぐ忘れるのでメモ。ナイーブベイズの学習アルゴリズムの導出とか、そもそもナイーブベイズが定番過ぎて意外とやったことなかった気もするので、復習がてらやってみた。&lt;/p&gt;
&lt;p&gt;ちょっと修正 2013/07/30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ナイーブベイズについて整理&lt;/li&gt;
&lt;li&gt;学習アルゴリズムの導出&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;naive-bayes-ナイーブベイズ&#34;&gt;Naive bayes （ナイーブベイズ）&lt;/h2&gt;
&lt;p&gt;スパムフィルタで使われたことで有名な確率モデルで、シンプルだけどそこそこ実用的なのが良い所。Naive bayesという名前は、特徴ベクトル間に条件付き独立性を仮定してることにある（実際は相関あることが多いけど、まぁ簡単のためって感じ）。具体的に例を挙げて言うと、例えば文書分類タスクの場合、各単語は独立に生起するという仮定を置くことに相当する。&lt;/p&gt;
&lt;p&gt;まずはモデルを書き下す。入力データを$\mathbf{x}$（D次元）、ラベルを$y$（離散値）とすると、ナイーブベイズでは以下のように同時確率をモデル化する。&lt;/p&gt;
&lt;div&gt;
\begin{align}
p(\mathbf{x}, y) &amp;= p(y)p(\mathbf{x}|y)\\
&amp;= p(y)p(x_{1}, x_{2}, \dots, x_{D}|y)\\
&amp;= p(y)\prod_{d=1}^{D} p(x_{d}|y)
\end{align}
&lt;/div&gt;
&lt;p&gt;カンタン。基本的にdは次元に対するインデックス、nはデータに対するインデックスとして書く。&lt;/p&gt;
&lt;p&gt;ポイントは特徴ベクトル間に条件付き独立性の仮定を置いていること（二度目）で、それによってパラメータの数が少なくて済む。&lt;/p&gt;
&lt;h2 id=&#34;分類&#34;&gt;分類&lt;/h2&gt;
&lt;p&gt;一番確率の高いラベルを選べばいい。数式で書くと以下のようになる。&lt;/p&gt;
&lt;div&gt;
\begin{align}
\hat{y} &amp;= \argmax_{y} [p(y|\mathbf{x})]\\
 &amp;= \argmax_{y} [p(\mathbf{x}, y)]\\
 &amp;= \argmax_{y} \Bigl[ p(y)\prod_{d=1}^{D} p(x_{d}|y)\Bigr]
\end{align}
&lt;/div&gt;
&lt;p&gt;argmaxを取る上では、$y$に依存しない項は無視していいので、事後確率の最大化は、同時確率の最大化に等しくなる。&lt;/p&gt;
&lt;h2 id=&#34;学習アルゴリズムの導出&#34;&gt;学習アルゴリズムの導出&lt;/h2&gt;
&lt;p&gt;ここからが本番。学習データを$X = {\mathbf{x}_{n}}_{n=1}^{N}$、対応する正解ラベルを$Y = {y_n}_{n=1}^{N} $として、最尤推定により学習アルゴリズムを導出する。実際はMAP推定をすることが多いけど、今回は省略。拡張は簡単。&lt;/p&gt;
&lt;h3 id=&#34;尤度関数&#34;&gt;尤度関数&lt;/h3&gt;
&lt;p&gt;各サンプルが独立に生起したと仮定すると、尤度関数は以下のように書ける。&lt;/p&gt;
&lt;div&gt;
\begin{align}
L(X,Y; \mathbf{\theta}) &amp;= \prod_{n=1}^{N}p(y_{n})p(\mathbf{x_{n}}|y_{n})\\
&amp;= \prod_{n=1}^{N} \Bigl[ p(y_{n})\prod_{d=1}^{D}p(x_{nd}|y_{n})\Bigr]
\end{align}
&lt;/div&gt;
&lt;p&gt;対数を取って、&lt;/p&gt;
&lt;div&gt;
\begin{align}
\log L(X,Y; \mathbf{\theta}) =  \sum_{n=1}^{N}\Bigl[\log p(y_{n}) + \sum_{d=1}^{D}\log p(x_{nd}|y_{n})\Bigr]
\end{align}
&lt;/div&gt;
&lt;p&gt;学習アルゴリズムは、この関数の最大化として導くことができる。&lt;/p&gt;
&lt;h3 id=&#34;ところで&#34;&gt;ところで&lt;/h3&gt;
&lt;p&gt;特徴ベクトルにどのような分布を仮定するかでアルゴリズムが少し変わるので、今回は以下の二つをやってみる。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ベルヌーイ分布&lt;/li&gt;
&lt;li&gt;正規分布&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;前者は、binary featureを使う場合で、後者は、continuous featureを使う場合を想定してる。画像のピクセル値とか連続値を扱いたい場合は、正規分布が無難。その他、多項分布を使うこともあるけど、ベルヌーイ分布の場合とほとんど一緒なので今回は省略&lt;/p&gt;
&lt;p&gt;ラベルに対する事前分布は、ラベルが離散値なので多項分布（間違ってた）categorical distributionとする。日本語でなんて言えばいいのか…&lt;a href=&#34;http://en.wikipedia.org/wiki/Categorical_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;wikipedia&lt;/a&gt; 参考&lt;/p&gt;
&lt;h2 id=&#34;bernoulli-naive-bayes&#34;&gt;Bernoulli naive bayes&lt;/h2&gt;
&lt;p&gt;特徴ベクトルにベルヌーイ分布を仮定する場合。0 or 1のbinary featureを使う場合にはこれでおｋ．ベルヌーイ分布は以下&lt;/p&gt;
&lt;div&gt;
\begin{align}
p(x;q) = q^{x}(1-q)^{1-x}
\end{align}
&lt;/div&gt;
&lt;p&gt;特徴ベクトルに対するパラメータは、ラベル数×特徴ベクトルの次元数（L×D）個ある。対数尤度関数（Gとする）は、以下のように書ける。&lt;/p&gt;
&lt;div&gt;
\begin{align}
G &amp;=  \sum_{n=1}^{N}\Bigl[ \log \pi_{y_{n}} \notag \\
 &amp;+ \sum_{d=1}^{D} \bigl[ x_{nd} \log q_{y_{n}d} + (1-x_{nd}) \log (1-q_{y_{n}d}) \bigr] \Bigr]
\end{align}
&lt;/div&gt;
&lt;p&gt;ここで、$\pi_{y_{n}}$ はcategorical distributionのパラメータ。&lt;/p&gt;
&lt;h3 id=&#34;微分方程式を解く&#34;&gt;微分方程式を解く&lt;/h3&gt;
&lt;p&gt;あとは微分してゼロ。ラベルに対するインデックスをl 、学習データ中のラベルlが出現する回数を$N_{l} = \sum_{n=1}^{N} \delta(y_{n}= l)$、さらにその中で$x_{nd}=1 $となる回数を$N_{ld} = \sum_{n=1}^{N} \delta(y_{n}= l) \cdot x_{nd} $とすると、&lt;/p&gt;
&lt;div&gt;
\begin{align}
\frac{\partial G}{\partial q_{ld}} &amp;= \frac{N_{ld}}{q_{ld}} - \frac{N_{l} - N_{ld}}{1-q_{ld}}  = 0
\end{align}
&lt;/div&gt;
&lt;p&gt;よって、&lt;/p&gt;
&lt;div&gt;
\begin{align}
q_{ld} = \frac{N_{ld}}{N_{l}} \label{eq:naive1}
\end{align}
&lt;/div&gt;
&lt;p&gt;できました。厳密に数式で書こうとするとめんどくさい。日本語で書くと、&lt;/p&gt;
&lt;div&gt;
\begin{align}
パラメータ = \frac{特徴ベクトルの出現回数}{ラベルの出現回数}
\end{align}
&lt;/div&gt;
&lt;p&gt;って感じでしょうか。&lt;/p&gt;
&lt;p&gt;categoricalのパラメータについては、めんどくさくなってきたのでやらないけど、もう直感的に以下。ラグランジュの未定定数法でおｋ&lt;/p&gt;
&lt;div&gt;
\begin{align}
\pi_{l} = \frac{N_{l}}{N} \label{eq:naive2}
\end{align}
&lt;/div&gt;
&lt;p&gt;学習は、式 ($\ref{eq:naive1}$)、($\ref{eq:naive2}$) を計算すればおｋ．やっと終わった。。。長かった。&lt;/p&gt;
&lt;h2 id=&#34;gaussian-naive-bayes&#34;&gt;Gaussian naive bayes&lt;/h2&gt;
&lt;p&gt;次。$x$が連続変数で、その分布に正規分布（Gaussian）を仮定する場合。まず、正規分布は以下のとおり。&lt;/p&gt;
&lt;div&gt;
\begin{align}
p(x; \mu, \sigma^{2}) = \frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\Bigl\{-\frac{(x-\mu)^{2}}{2\sigma^{2}}\Bigr\}
\end{align}
&lt;/div&gt;
&lt;p&gt;正規分布を使う場合、特徴ベクトルに対するパラメータは、ラベル数×特徴ベクトルの次元数×2個ある。×2となっているのは、平均と分散の分。対数尤度関数は、以下のようになる&lt;/p&gt;
&lt;div&gt;
\begin{align}
G &amp;=  \sum_{n=1}^{N}\Bigl[ \log \pi_{y_{n}} \notag \\
 &amp;+ \sum_{d=1}^{D} \bigl[ -\frac{1}{2}\log 2\pi - \log\sigma_{y_{n}d} -  \frac{(x_{nd}-\mu_{y_{n}d})^2}{2\sigma_{y_{n}d}} \bigr] \Bigr]
\end{align}
&lt;/div&gt;
&lt;h3 id=&#34;微分方程式を解く-1&#34;&gt;微分方程式を解く&lt;/h3&gt;
&lt;p&gt;計算は省略するけど、偏微分してゼロと置けば、結果は以下のようになる。式が若干煩雑だけど、基本的には正規分布の最尤推定をしてるだけ。&lt;/p&gt;
&lt;div&gt;
\begin{align}
\mu_{ld} = \frac{1}{N_{l}} \sum_{n=1}^{N} x_{nd} \cdot \delta(y_{n} =l) = \frac{N_{ld}}{N_{l}} \label{eq:naive3}
\end{align}
&lt;/div&gt;
&lt;div&gt;
\begin{align}
\sigma_{ld} = \frac{1}{N_{l}} \sum_{n=1}^{N} (x_{nd}-\mu_{ld})^{2} \cdot \delta (y_{n}= l) \label{eq:naive4}
\end{align}
&lt;/div&gt;
&lt;p&gt;学習では、式 ($\ref{eq:naive2}$)、($\ref{eq:naive3}$)、($\ref{eq:naive4}$)を計算すればおｋ．式 ($\ref{eq:naive3}$)は式 ($\ref{eq:naive1}$)と一緒なんだけど、正規分布の場合はxが連続値なので注意。分散が特徴ベクトルの次元によらず一定とすれば、パラメータの数をぐっと減らすこともできる。&lt;/p&gt;
&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;
&lt;p&gt;これで終わり。予想以上に書くのに時間かかった…。今日logistic regressionを見直してて、ふとnaive bayesやったことないなーと思って、まぁ試すだけならscipy使えば一瞬なんだろうけどちょっと導出までやってみようと思った。&lt;/p&gt;
&lt;p&gt;実装編→&lt;a href=&#34;http://r9y9.github.io/blog/2013/08/06/naive-bayes-mnist/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Naive Bayesの復習（実装編）: MNISTを使って手書き数字認識&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://d.hatena.ne.jp/saket/20130212/1360678478&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;scikit.learn手法徹底比較！ ナイーブベイズ編Add Star - Risky Dune&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cs.cmu.edu/~epxing/Class/10701-10s/Lecture/lecture5.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gaussian Naïve Bayes, andLogistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://aidiary.hatenablog.com/entry/20100613/1276389337&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ナイーブベイズを用いたテキスト分類 - 人工知能に関する断創録&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>NMFアルゴリズムの導出（ユークリッド距離版）</title>
      <link>https://r9y9.github.io/blog/2013/07/27/nmf-euclid/</link>
      <pubDate>Sat, 27 Jul 2013 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2013/07/27/nmf-euclid/</guid>
      <description>&lt;h2 id=&#34;はじめに&#34;&gt;はじめに&lt;/h2&gt;
&lt;p&gt;シングルトラックにミックスダウンされた音楽から、その構成する要素（例えば、楽器とか）を分離したいと思うことがある。
音源分離と言えば、最近はNon-negative Matrix Factorization (非負値行列因子分解; NMF) が有名。
実装は非常に簡単だけど、実際にやってみるとどの程度の音源分離性能が出るのか気になったので、やってみる。&lt;/p&gt;
&lt;p&gt;と思ったけど、まずNMFについて整理してたら長くなったので、実装は今度にして、まずアルゴリズムを導出してみる。&lt;/p&gt;
&lt;h2 id=&#34;20141019-追記&#34;&gt;2014/10/19 追記&lt;/h2&gt;
&lt;p&gt;実装しました&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/julia-nmf-ss-toy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/julia-nmf-ss-toy&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;nmfの問題設定&#34;&gt;NMFの問題設定&lt;/h2&gt;
&lt;p&gt;NMFとは、与えられた行列を非負という制約の元で因子分解する方法のこと。
音楽の場合、対象はスペクトログラムで、式で書くとわかりやすい。
スペクトログラムを $\mathbf{Y}: [\Omega \times T] $
とすると、&lt;/p&gt;
&lt;div&gt;
\begin{align}
\mathbf{Y} \simeq \mathbf{H} \mathbf{U}
\end{align}
&lt;/div&gt;
&lt;p&gt;となる、$\mathbf{H}: [\Omega \times K]、\mathbf{U}: [K \times T]$ を求めるのがNMFの問題。
ここで、Hが基底、Uがアクティビティ行列に相当する。
NMFは、元の行列Yと分解後の行列の距離の最小化問題として定式化できる。&lt;/p&gt;
&lt;div&gt;
\begin{align}
\mathbf{H}, \mathbf{U} = \mathop{\rm arg~min}\limits_{\mathbf{H}, \mathbf{U}} D (\mathbf{Y}|\mathbf{H}\mathbf{U}), \hspace{3mm} {\rm subect\ to} \hspace{3mm} H_{\omega,k}, U_{k, t} &gt; 0
\end{align}
&lt;/div&gt;
&lt;p&gt;すごくシンプル。Dは距離関数で色んなものがある。ユークリッド距離、KLダイバージェンス、板倉斎藤距離、βダイバージェンスとか。&lt;/p&gt;
&lt;h2 id=&#34;ユークリッド距離の最小化&#34;&gt;ユークリッド距離の最小化&lt;/h2&gt;
&lt;p&gt;ここではユークリッド距離（Frobeniusノルムともいう）として、二乗誤差最小化問題を解くことにする。
一番簡単なので。最小化すべき目的関数は次のようになる。&lt;/p&gt;
&lt;div&gt;
\begin{align}
D (\mathbf{Y}|\mathbf{H}\mathbf{U}) =&amp; || \mathbf{Y}-\mathbf{HU}||_{F} \\
=&amp; \sum_{\omega, k}|Y_{\omega,t} - \sum_{k}H_{\omega, k}U_{k, t}|^{2}
\end{align}
&lt;/div&gt;
&lt;p&gt;行列同士の二乗誤差の最小化は、要素毎の二乗誤差の和の最小化ということですね。展開すると、次のようになる。&lt;/p&gt;
&lt;div&gt;
\begin{align}
\sum_{\omega, k}|Y_{\omega,t} - \sum_{k}H_{\omega, k}U_{k, t}|^{2}
= \sum_{\omega, t}(|Y_{\omega, t}|^2 -2Y_{\omega, t} \sum_{k}H_{\omega, k}U_{k, t} + |\sum_{k}H_{\omega, k}U_{k, t}|^2)
\end{align}
&lt;/div&gt;
&lt;p&gt;微分してゼロ！としたいところだけど、3つ目の項を見ると、絶対値の中に和が入っているので、そうはいかない。
なので、補助関数法を使う。
基本的なアイデアは、目的関数の直接の最適化が難しい場合には、上界関数を立てることで間接的に最小化するということ。&lt;/p&gt;
&lt;p&gt;3項目に対してイェンセンの不等式を適応すると、&lt;/p&gt;
&lt;div&gt;
\begin{align}
|\sum_{k}H_{\omega,k}U_{k,t}|^{2} \le \sum_{k} \frac{H_{\omega,k}^{2}U_{k, t}^{2}}{\lambda_{k, \omega, t}}
\end{align}
&lt;/div&gt;
&lt;p&gt;これで、右辺は $ H_{\omega,k}, U_{k, t} $ について二次関数になったので、微分できてはっぴー。
上の不等式を使えば、実際に最小化する目的関数は、次のようになる。&lt;/p&gt;
&lt;div&gt;
\begin{align}
G := \sum_{\omega, t}(|Y_{\omega, t}|^2 -2Y_{\omega, t} \sum_{k}H_{\omega, k}U_{k, t} + \sum_{k} \frac{H_{\omega,k}^{2}U_{k, t}^{2}}{\lambda_{k, \omega, t}})
\end{align}
&lt;/div&gt;
&lt;p&gt;Gを最小化すれば、間接的に元の目的関数も小さくなる。&lt;/p&gt;
&lt;h2 id=&#34;更新式の導出&#34;&gt;更新式の導出&lt;/h2&gt;
&lt;p&gt;あとは更新式を導出するだけ。
まず、目的関数を上から押さえるイメージで、イェンセンの不等式の等号条件から補助変数の更新式を求める。
この場合、kに関して和が1になることに注意して、&lt;/p&gt;
&lt;div&gt;
\begin{align}
\lambda_{k,\omega,t} = \frac{H_{\omega, k}U_{k, t}}{\sum_{k&#39;}H_{\omega, k&#39;}U_{k&#39;, t}}
\end{align}
&lt;/div&gt;
&lt;p&gt;次に、目的関数Gを $H_{\omega,k}, U_{k,t} $で偏微分する。&lt;/p&gt;
&lt;div&gt;
\begin{align}
\frac{\partial G}{\partial H_{\omega,k}} &amp;= \sum_{t} (-2 Y_{\omega,t}U_{k,t} + 2 \frac{H_{\omega, k}U_{k, t}^2}{\lambda_{k,\omega,t}}) &amp;= 0\\
\frac{\partial G}{\partial U_{k, t}} &amp;= \sum_{\omega} (-2 Y_{\omega,t}H_{\omega,k} + 2 \frac{H_{\omega, k}^2U_{k, t}}{\lambda_{k,\omega,t}}) &amp;= 0
\end{align}
&lt;/div&gt;
&lt;p&gt;少し変形すれば、以下の式を得る。&lt;/p&gt;
&lt;div&gt;
\begin{align}
H_{\omega,k} = \frac{\sum_{t}Y_{\omega,t}U_{k,t}}{\sum_{t}\frac{U_{k, t}^2}{\lambda_{k,\omega,t}}}, \hspace{3mm}
U_{k,t} = \frac{\sum_{\omega}Y_{\omega,t}H_{\omega,k}}{\sum_{\omega}\frac{H_{\omega, k}^2}{\lambda_{k,\omega,t}}}
\end{align}
&lt;/div&gt;
&lt;p&gt;補助変数を代入すれば、出来上がり。&lt;/p&gt;
&lt;div&gt;
\begin{align}
H_{\omega,k} = H_{\omega,k} \frac{\sum_{t}Y_{\omega,t}U_{k,t}}{\sum_{t}U_{k, t}\sum_{k&#39;}H_{\omega, k&#39;}U_{k&#39;, t}}, \hspace{3mm}
U_{k,t} = U_{k,t}\frac{\sum_{\omega}Y_{\omega,t}H_{\omega,k}}{\sum_{\omega}H_{\omega, k}\sum_{k&#39;}H_{\omega, k&#39;}U_{k&#39;, t}}
\end{align}
&lt;/div&gt;
&lt;h2 id=&#34;行列表記で&#34;&gt;行列表記で&lt;/h2&gt;
&lt;p&gt;これで終わり…ではなく、もう少しスマートに書きたい。
ここで、少し実装を意識して行列表記を使って書きなおす。
行列の積は、AB（A: [m x n] 行列、B: [n x l] 行列）のようにAの列数とBの行数が等しくなることに注意して、
ほんの少し変形すれば最終的には次のように書ける。&lt;/p&gt;
&lt;div&gt;
\begin{align}
H_{\omega,k} &amp;= H_{\omega,k} \frac{[\mathbf{Y}\mathbf{U}^{\mathrm{T}}]_{\omega,k}}{[\mathbf{H}\mathbf{U}\mathbf{U}^{\mathrm{T}}]_{\omega,k}}, \\
U_{k,t} &amp;= U_{k,t}\frac{[\mathbf{H}^{\mathrm{T}}\mathbf{Y}]_{k, t}}{[\mathbf{H}^{\mathrm{T}}\mathbf{H}\mathbf{U}]_{k,t}}
\end{align}
&lt;/div&gt;
&lt;p&gt;乗法更新式というやつですね。
元々の行列の要素が非負なら、掛けても非負のままですよってこと。
NMFのアルゴリズムは、この更新式を目的関数が収束するまで計算するだけ、簡単。Pythonなら数行で書ける。&lt;/p&gt;
&lt;h2 id=&#34;メモ&#34;&gt;メモ&lt;/h2&gt;
&lt;p&gt;自分で導出していて思ったことをメモっておこうと思う。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;更新式は、行列の要素毎に独立して求められるんだなぁということ。
&lt;ul&gt;
&lt;li&gt;まぁ要素毎に偏微分して等式立ててるからそうなんだけど。更新の順番によって、収束する値、速度が変わるといったことはないんだろうか。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;行列演算とスカラー演算が同じ式に同時に含まれていることがあるので注意。例えば、最終的な更新式の割り算は、要素毎のスカラー演算で、行列演算ではない。&lt;/li&gt;
&lt;li&gt;何かいっぱいシグマがあるけど、めげない。計算ミスしやすい、つらい。&lt;/li&gt;
&lt;li&gt;NMFという名前から行列操作を意識してしまうけど、更新式の導出の過程に行列の微分とか出てこない。更新式の導出は、行列の要素個々に対して行うイメージ。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NMFなんて簡単、と言われますが（要出典）、実際にやってみると結構めんどくさいなー、と思いました（小並感&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
