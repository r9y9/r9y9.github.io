<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Speech Signal Processing | LESS IS MORE</title>
    <link>https://r9y9.github.io/tag/speech-signal-processing/</link>
      <atom:link href="https://r9y9.github.io/tag/speech-signal-processing/index.xml" rel="self" type="application/rss+xml" />
    <description>Speech Signal Processing</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Copyright © Ryuichi YAMAMOTO All rights reserved.</copyright><lastBuildDate>Sun, 06 Sep 2015 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://r9y9.github.io/media/icon_hu80af6620f812c701b45cf64bd91e2f1f_1204_512x512_fill_lanczos_center_3.png</url>
      <title>Speech Signal Processing</title>
      <link>https://r9y9.github.io/tag/speech-signal-processing/</link>
    </image>
    
    <item>
      <title>pysptk: SPTKのpythonラッパーを作った (part 2)</title>
      <link>https://r9y9.github.io/blog/2015/09/06/pysptk/</link>
      <pubDate>Sun, 06 Sep 2015 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2015/09/06/pysptk/</guid>
      <description>&lt;p&gt;2015/09/05:&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://t.co/WFBmYEIVce&#34;&gt;https://t.co/WFBmYEIVce&lt;/a&gt; SPTKのpythonラッパー（マシなやつ）完成&lt;br&gt;ドキュメント &lt;a href=&#34;http://t.co/jYhw1y3Bzg&#34;&gt;http://t.co/jYhw1y3Bzg&lt;/a&gt;&lt;br&gt;pip install pysptk でインストールできるようになりました。pypi童貞捨てれた&lt;/p&gt;&amp;mdash; 山本 龍一 / Ryuichi Yamamoto (@r9y9) &lt;a href=&#34;https://twitter.com/r9y9/status/639848868075560960?ref_src=twsrc%5Etfw&#34;&gt;September 4, 2015&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;ずいぶん前に、swig遊びをしがてらpythonのラッパーを書いていたんですが、cythonを使って新しく作りなおしました。かなりパワーアップしました。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install pysptk
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;でインストールできるので、よろしければどうぞ&lt;/p&gt;
&lt;h2 id=&#34;なぜ作ったのか&#34;&gt;なぜ作ったのか&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;cythonとsphinxで遊んでたらできた&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;使い方&#34;&gt;使い方&lt;/h2&gt;
&lt;p&gt;以下のデモを参考にどうぞ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/r9y9/pysptk/blob/51c103e5a7e9746c96cd78043df4e48fe2d6a3a8/examples/pysptk%20introduction.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Introduction to pysptk&lt;/a&gt;: メル一般化ケプストラム分析とか&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/r9y9/pysptk/blob/51c103e5a7e9746c96cd78043df4e48fe2d6a3a8/examples/Speech%20analysis%20and%20re-synthesis.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Speech analysis and re-synthesis&lt;/a&gt;: 音声の分析・再合成のデモ。合成音声はnotebook上で再生できます&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ドキュメント&#34;&gt;ドキュメント&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://pysptk.readthedocs.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://pysptk.readthedocs.org&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;ぼやき&#34;&gt;ぼやき&lt;/h2&gt;
&lt;p&gt;SPTKの関数、変な値入れるとexitしたりセグフォったりするので、ちゃんとテスト書いてほしいなあ&lt;/p&gt;
&lt;h2 id=&#34;関連&#34;&gt;関連&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://r9y9.github.io/blog/2014/08/10/sptk-from-python/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SPTKのPythonラッパーを書いた - LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>最近の音声信号処理遊びの進捗</title>
      <link>https://r9y9.github.io/blog/2015/08/23/speech-analysis-and-synthesis-in-julia/</link>
      <pubDate>Sun, 23 Aug 2015 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2015/08/23/speech-analysis-and-synthesis-in-julia/</guid>
      <description>&lt;h2 id=&#34;hello&#34;&gt;hello&lt;/h2&gt;
&lt;p&gt;遡ればもう約一年まえになるでしょうか、統計的声質遊びをしたいと思い、理論の勉強を始めたり、（特にJuliaで）コードを色々書いていました（お前ほんといろんな言語で遊んでるな、というツッコミはさておき）。&lt;a href=&#34;http://r9y9.github.io/blog/2014/11/12/statistical-voice-conversion-code/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;統計的声質変換クッソムズすぎワロタ（チュートリアル編） - LESS IS MORE&lt;/a&gt; を書いていた当初は、当然自分のためだけに書いていて、まぁアレな出来でしたが、最近気を取り直して多少マシに仕上げましたので、何となくブログに書いてみようかなーと思った次第です。というわけで、最近公式に登録したいくつかのパッケージを、まとめて簡単に紹介します。&lt;/p&gt;
&lt;p&gt;主な改善点は、windowsもちゃんとサポートするようにしたこと（誰得？）と、テストをきちんと書いたことと、julia的なインタフェースを意識するようにしたことですかね。3つ目はかなり曖昧ですが、まぁ気持ち使いやすくなったと思います。&lt;/p&gt;
&lt;h2 id=&#34;パッケージ&#34;&gt;パッケージ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/MelGeneralizedCepstrums.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MelGeneralizedCepstrums.jl&lt;/a&gt;: メル一般化ケプストラム分析&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/SynthesisFilters.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SynthesisFilters.jl&lt;/a&gt;: メル一般化ケプストラムからの音声波形合成&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/SPTK.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SPTK.jl&lt;/a&gt;: &lt;a href=&#34;http://sp-tk.sourceforge.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SPTK&lt;/a&gt;のラッパー&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;車輪の再発明はできるだけしたくなかったので、最初のほうはCライブラリのラッパーを書くことが多く、windowsとかめんどくさいしunix環境でしか動作確認してませんでしたが、&lt;a href=&#34;http://qiita.com/r9y9/items/e0567e2a21a5e3c36e51&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WindowsのJuliaから呼べるようなCライブラリの共有ライブラリ（DLL）を作る | qiita&lt;/a&gt; 重い腰を上げてwindowsでも動くように頑張ったことがあり（めんどくさいとか言って手を動かさないのホント良くないですね）、登録したパッケージはすべてwindowsでも動くようになりました。めでたし。&lt;a href=&#34;https://github.com/r9y9/WORLD.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WORLD.jl&lt;/a&gt; もwindowsで動くようにしました。&lt;/p&gt;
&lt;h2 id=&#34;melgeneralizedcepstrumsjl&#34;&gt;MelGeneralizedCepstrums.jl&lt;/h2&gt;
&lt;p&gt;メルケプストラムの推定とか。いくつか例を載せておきます&lt;/p&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/r9y9/MelGeneralizedCepstrums.jl/v0.0.1/examples/cepstrum.png&#34; alt=&#34;cepstrum based envelope.&#34; class=&#34;image&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/r9y9/MelGeneralizedCepstrums.jl/v0.0.1/examples/mel-cepstrum.png&#34; alt=&#34;mel-cepstrum based envelope.&#34; class=&#34;image&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/r9y9/MelGeneralizedCepstrums.jl/v0.0.1/examples/mel-generalized-cepstrum.png&#34; alt=&#34;mel-generalized-cepstrum based envelope.&#34; class=&#34;image&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/r9y9/MelGeneralizedCepstrums.jl/v0.0.1/examples/lpc-cepstrum.png&#34; alt=&#34;lpc-cepstrum based envelope.&#34; class=&#34;image&#34;&gt;
&lt;p&gt;詳細はこちらの&lt;a href=&#34;http://nbviewer.ipython.org/github/r9y9/MelGeneralizedCepstrums.jl/blob/v0.0.1/examples/Introduction%20to%20MelGeneralizedCeptrums.jl.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ノートブック&lt;/a&gt;へ&lt;/p&gt;
&lt;p&gt;メルケプストラム分析、メル一般化ケプストラム分析に関しては、SPTKの実装をjuliaで再実装してみました。結果、速度は1.0 ~ 1.5倍程度でおさまって、かつ数値的な安定性は増しています（メモリ使用量はお察し）。まぁ僕が頑張ったからというわけでなく、単にJuliaの線形方程式ソルバーがSPTKのものより安定しているというのが理由です。&lt;/p&gt;
&lt;h2 id=&#34;synthesisfiltersjl&#34;&gt;SynthesisFilters.jl&lt;/h2&gt;
&lt;p&gt;メルケプストラムからの波形合成とか。&lt;/p&gt;
&lt;p&gt;詳細はこちらの&lt;a href=&#34;http://nbviewer.ipython.org/github/r9y9/SynthesisFilters.jl/blob/v0.0.1/examples/Introduction%20to%20SynthesisFilters.jl.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ノートブック&lt;/a&gt;へ。いくつかの音声合成フィルタの合成音をノートブック上で比較することができます。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/r9y9/SynthesisFilters.jl/blob/mix-excitation/examples/Introduction%20to%20SynthesisFilters.jl.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;mixed excitation（っぽいの）を使ったバージョンのノートブック&lt;/a&gt;: 実装に自信がないので、そのうち消すかも。聴覚的にはこっちのほうが良いです。&lt;/p&gt;
&lt;h2 id=&#34;sptkjl&#34;&gt;SPTK.jl&lt;/h2&gt;
&lt;p&gt;公式のSPTKではなく、僕が少しいじったSPTK（windowsで動くようにしたり、APIとして使いやすいように関数内でexitしてた部分を適切なreturn code返すようにしたり、swipeというF0抽出のインタフェースをexposeしたり、など）をベースにしています。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/r9y9/SPTK.jl/blob/v0.0.1/examples/Introduction%20to%20SPTK.jl.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;デモ用のノートブック&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;MelGeneralizedCepstrums.jl と SynthesiFilters.jl は、ほとんどSPTK.jlで成り立っています。本質的に SPTK.jl にできて MelGeneralizedCepstrums.jl と SynthesiFilters.jlにできないことは基本的にないのですが、後者の方が、より簡単な、Julia的なインタフェースになっています。&lt;/p&gt;
&lt;p&gt;例えば、メルケプストラム、ケプストラム、LPCなど、スペクトルパラメータの型に応じて、適切なフィルタ係数に変換する、合成フィルタを選択するなど、multiple dispatchを有効に活用して、よりシンプルなインタフェースを提供するようにしました（というか自分がミスりたくなかったからそうしました）。&lt;/p&gt;
&lt;h2 id=&#34;おわり&#34;&gt;おわり&lt;/h2&gt;
&lt;p&gt;かなり適当に書きましたが、最近の進捗は、Juliaで書いていたパッケージ多少改善して、公式に登録したくらいでした。進捗まじ少なめ。あと些細なことですが、ipython（ijulia）に音埋め込むのクッソ簡単にできてびっくりしました（なんで今までやらなかったんだろう）。&lt;a href=&#34;https://github.com/jfsantos&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@jfsantos&lt;/a&gt; に感謝&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>JuliaTokyo #3 Speech Signal Processing in Julia</title>
      <link>https://r9y9.github.io/blog/2015/04/26/juliatokyo3-speech-signal-processing-in-julia/</link>
      <pubDate>Sun, 26 Apr 2015 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2015/04/26/juliatokyo3-speech-signal-processing-in-julia/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://juliatokyo.connpass.com/event/13218/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JuliaTokyo #3&lt;/a&gt;でLT発表してきました。前回の&lt;a href=&#34;https://juliatokyo.connpass.com/event/8010/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JuliaTokyo #2&lt;/a&gt;でも発表したので、二回目でした。&lt;/p&gt;
&lt;h2 id=&#34;スライド&#34;&gt;スライド&lt;/h2&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/h4geMoK1msYqdY&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt; &lt;div style=&#34;margin-bottom:5px&#34;&gt; &lt;strong&gt; &lt;a href=&#34;//www.slideshare.net/ryuichiy/juliatokyo-3-speech-signal-processing-in-julia-47403938&#34; title=&#34;JuliaTokyo #3 Speech Signal Processing in Julia&#34; target=&#34;_blank&#34;&gt;JuliaTokyo #3 Speech Signal Processing in Julia&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href=&#34;//www.slideshare.net/ryuichiy&#34; target=&#34;_blank&#34;&gt;Ryuichi YAMAMOTO&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;コード&#34;&gt;コード&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/JuliaTokyo3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/JuliaTokyo3&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;三行まとめ&#34;&gt;三行まとめ&lt;/h2&gt;
&lt;p&gt;発表の内容を三行でまとめると、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;音声ファイルの読み込み（or 書き込み）は[WAV.jl]((&lt;a href=&#34;https://github.com/dancasimiro/WAV.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/dancasimiro/WAV.jl&lt;/a&gt;)を使おう&lt;/li&gt;
&lt;li&gt;基本的なデジタル信号処理は &lt;a href=&#34;https://github.com/JuliaDSP/DSP.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JuliaDSP/DSP.jl&lt;/a&gt; をチェック（※JuliaDSPにはウェーブレットとかもあるよ）&lt;/li&gt;
&lt;li&gt;音声に特化した信号処理は、&lt;a href=&#34;https://github.com/r9y9/WORLD.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/WORLD.jl&lt;/a&gt; がオススメです&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;という感じです。&lt;/p&gt;
&lt;p&gt;応用例として、歌声を分離する話（&lt;a href=&#34;https://github.com/r9y9/RobustPCA.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;デモコード&lt;/a&gt;）、統計的声質変換（&lt;a href=&#34;http://r9y9.github.io/blog/2014/11/12/statistical-voice-conversion-code/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;統計的声質変換クッソムズすぎワロタ（チュートリアル編） - LESS IS MORE&lt;/a&gt;）、画像をスペクトログラムに足しこむ話とか、さっと紹介しました。&lt;/p&gt;
&lt;h2 id=&#34;補足&#34;&gt;補足&lt;/h2&gt;
&lt;p&gt;僕が使う/作ったパッケージを、あとで見返せるように最後のスライドにまとめておいたのですが、改めてここで整理しておきます。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/dancasimiro/WAV.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;dancasimiro/WAV&lt;/a&gt; WAVファイルの読み込み&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/JuliaDSP/DSP.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JuliaDSP/DSP&lt;/a&gt; 窓関数、スペクトログラム、デジタルフィルタ&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/WORLD.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/WORLD&lt;/a&gt; 音声分析・合成フレームワーク&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/MelGeneralizedCepstrums.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/MelGeneralizedCepstrums&lt;/a&gt; メル一般化ケプストラム分析&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/SynthesisFilters.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/SynthesisFilters&lt;/a&gt; メル一般化ケプストラムからの波形合成&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/SPTK.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/SPTK&lt;/a&gt; 音声信号処理ツールキット&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/RobustPCA.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/RobustPCA&lt;/a&gt; ロバスト主成分分析(歌声分離へ応用)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/REAPER.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/REAPER&lt;/a&gt; 基本周波数推定&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/r9y9/VoiceConversion.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r9y9/VoiceConversion&lt;/a&gt; 統計的声質変換&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上から順に、&lt;del&gt;汎用的かなーと思います&lt;/del&gt;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;。僕が書いたパッケージの中では、&lt;strong&gt;WORLDのみ&lt;/strong&gt;公式パッケージにしています。理由は単純で、その他のパッケージはあまりユーザがいないだろうなーと思ったからです。かなりマニアックであったり、今後の方針が決まってなかったり（ごめんなさい）、応用的過ぎて全然汎用的でなかったり。WORLDは自信を持ってオススメできますので、Juliaで音声信号処理をやってみようかなと思った方は、ぜひお試しください。&lt;/p&gt;
&lt;h2 id=&#34;ざっくり感想&#34;&gt;ざっくり感想&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;＃Juliaわからん 本当に素晴らしいと思うので、僕も積極的に #Juliaわからん とつぶやいていこうと思います（詳しくは &lt;a href=&#34;https://twitter.com/chezou&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@chezou&lt;/a&gt; さんの記事をどうぞ &lt;a href=&#34;http://chezou.hatenablog.com/entry/2015/04/26/222518&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#JuliaTokyo で #juliaわからん という雑なレポジトリを立てた話をしたら julia.tokyo ができてた  - once upon a time,&lt;/a&gt;）。僕は、Julia に Theano が欲しいです。&lt;code&gt;T.grad&lt;/code&gt; 強力すぎる&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ccall&lt;/code&gt; かんたんとか言いましたが、ミスった書き方をしたときのエラーメッセージはあまり親切ではないので、つまずきやすいかも。僕は気合で何とかしています。&lt;/li&gt;
&lt;li&gt;Julia遅いんだけど？？？と言われたら、&lt;a href=&#34;https://twitter.com/bicycle1885&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@bicycle1885&lt;/a&gt; さんの &lt;a href=&#34;http://www.slideshare.net/KentaSato/whats-wrong-47403774&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;What&amp;rsquo;s wrong with this Julia?&lt;/a&gt; を投げつけようと思います。&lt;/li&gt;
&lt;li&gt;かなり聴衆が限定的になってしまう話をしてしまったので、次発表するならJulia 言語自体の話をしようかなと思いました&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;最後に&#34;&gt;最後に&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/sorami&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@sorami&lt;/a&gt;さんを筆頭とする運営の方々、本当にありがとうございました！楽しかったです。&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;とスライドに書いたけど、考えなおすと、僕が思う品質の高さ順、の方が正確です、失礼しました。MelGeneneralizedCepstrumsは一番気合入れて書いたけど、ユーザーがいるかといったらいないし、RobustPCAはさっと書いただけだけど、アルゴリズムとしては汎用的だし。またRobustPCAだけ毛色が違いますが、応用例で紹介したのでリストに入れておきました。&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>統計的声質変換クッソムズすぎワロタ（チュートリアル編）</title>
      <link>https://r9y9.github.io/blog/2014/11/12/statistical-voice-conversion-code/</link>
      <pubDate>Wed, 12 Nov 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/11/12/statistical-voice-conversion-code/</guid>
      <description>&lt;h2 id=&#34;はじめに&#34;&gt;はじめに&lt;/h2&gt;
&lt;p&gt;こんばんは。統計的声質変換（以降、簡単に声質変換と書きます）って面白いなーと思っているのですが、興味を持つ人が増えたらいいなと思い、今回は簡単なチュートリアルを書いてみます。間違っている箇所があれば、指摘してもらえると助かります。よろしくどうぞ。&lt;/p&gt;
&lt;p&gt;前回の記事（&lt;a href=&#34;http://r9y9.github.io/blog/2014/07/13/statistical-voice-conversion-wakaran/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;統計的声質変換クッソムズすぎワロタ（実装の話） - LESS IS MORE&lt;/a&gt;）では変換部分のコードのみを貼りましたが、今回はすべてのコードを公開します。なので、記事内で示す声質変換の結果を、この記事を読んでいる方が再現することも可能です。対象読者は、特に初学者の方で、声質変換を始めたいけれど論文からコードに落とすにはハードルが高いし、コードを動かしながら仕組みを理解していきたい、という方を想定しています。役に立てば幸いです。&lt;/p&gt;
&lt;h2 id=&#34;コード&#34;&gt;コード&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/VoiceConversion.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/VoiceConversion.jl&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://julialang.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Julia&lt;/a&gt; という言語で書かれています。Juliaがどんな言語かをさっと知るのには、以下のスライドがお勧めです。人それぞれ好きな言語で書けばいいと思いますが、個人的にJuliaで書くことになった経緯は、最後の方に簡単にまとめました。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/39141184&#34; width=&#34;425&#34; height=&#34;355&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt; &lt;div style=&#34;margin-bottom:5px&#34;&gt; &lt;strong&gt; &lt;a href=&#34;//www.slideshare.net/kentaroiizuka/julia-39141184&#34; title=&#34;プログラミング言語 Julia の紹介&#34; target=&#34;_blank&#34;&gt;プログラミング言語 Julia の紹介&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href=&#34;//www.slideshare.net/kentaroiizuka&#34; target=&#34;_blank&#34;&gt;Kentaro Iizuka&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&#34;サードパーティライブラリ&#34;&gt;サードパーティライブラリ&lt;/h2&gt;
&lt;p&gt;声質変換は多くのコンポーネントによって成り立っていますが、すべてを自分で書くのは現実的ではありません。僕は、主に以下のライブラリを活用しています。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://ml.cs.yamanashi.ac.jp/world/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WORLD&lt;/a&gt; - 音声分析合成のフレームワークとして、あるいは単にスペクトル包絡を抽出するツールとして使っています。&lt;a href=&#34;https://github.com/r9y9/WORLD.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Juliaラッパー&lt;/a&gt;を書きました。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;sp-tk.sourceforge.net&#34;&gt;SPTK&lt;/a&gt; - メル対数スペクトル近似（Mel-Log Spectrum Approximation; MLSA）フィルタを変換処理に使っています。これも&lt;a href=&#34;https://github.com/r9y9/SPTK.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Juliaラッパー&lt;/a&gt;を書きました。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://scikit-learn.org/stable/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sklearn&lt;/a&gt; - sklearn.mixture をGMMの学習に使っています。pythonのライブラリは、juliaから簡単に呼べます。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;音声分析合成に関しては、アカデミック界隈ではよく使われている&lt;a href=&#34;http://www.wakayama-u.ac.jp/~kawahara/STRAIGHTadv/index_j.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;STRAIGHT&lt;/a&gt;がありますが、WORLDの方がライセンスもゆるくソースも公開されていて、かつ性能も劣らない（正確な話は、森勢先生の論文を参照してください）ので、おすすめです。&lt;/p&gt;
&lt;h2 id=&#34;voiceconversionjlhttpsgithubcomr9y9voiceconversionjl-でできること&#34;&gt;&lt;a href=&#34;https://github.com/r9y9/VoiceConversion.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VoiceConversion.jl&lt;/a&gt; でできること&lt;/h2&gt;
&lt;h3 id=&#34;追記-20150107&#34;&gt;追記 2015/01/07&lt;/h3&gt;
&lt;p&gt;この記事を書いた段階のv0.0.1は、依存ライブラリの変更のため、現在は動きません。すみません。何のためのタグだ、という気がしてきますが、、最低限masterは動作するようにしますので、そちらをお試しください（基本的には、新しいコードの方が改善されています）。それでも動かないときは、issueを投げてください。&lt;/p&gt;
&lt;p&gt;2014/11/10現在（v0.0.1のタグを付けました）、できることは以下の通りです（外部ライブラリを叩いているものを含む）。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;音声波形からのメルケプストラムの抽出&lt;/li&gt;
&lt;li&gt;DPマッチングによるパラレルデータの作成&lt;/li&gt;
&lt;li&gt;GMMの学習&lt;/li&gt;
&lt;li&gt;GMMベースのframe-by-frame特徴量変換&lt;/li&gt;
&lt;li&gt;GMMベースのtrajectory特徴量変換&lt;/li&gt;
&lt;li&gt;GMMベースのtrajectory特徴量変換（GV考慮版）&lt;/li&gt;
&lt;li&gt;音声分析合成系WORLDを使った声質変換&lt;/li&gt;
&lt;li&gt;MLSAフィルタを使った差分スペクトルに基づく声質変換&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;これらのうち、trajectory変換以外を紹介します。&lt;/p&gt;
&lt;h2 id=&#34;チュートリアルcmu_arcticを使ったgmmベースの声質変換特徴抽出からパラレルデータの作成gmmの学習変換合成処理まで&#34;&gt;チュートリアル：CMU_ARCTICを使ったGMMベースの声質変換（特徴抽出からパラレルデータの作成、GMMの学習、変換・合成処理まで）&lt;/h2&gt;
&lt;p&gt;データセットに&lt;a href=&#34;http://festvox.org/cmu_arctic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CMU_ARCTIC&lt;/a&gt;を使って、GMMベースの声質変換（clb -&amp;gt; slt）を行う方法を説明します。なお、VoiceConversion.jl のv0.0.1を使います。ubuntuで主に動作確認をしていますが、macでも動くと思います。&lt;/p&gt;
&lt;h2 id=&#34;0-前準備&#34;&gt;0. 前準備&lt;/h2&gt;
&lt;h3 id=&#34;01-データセットのダウンロード&#34;&gt;0.1. データセットのダウンロード&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://festvox.org/cmu_arctic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Festvox: CMU_ARCTIC Databases&lt;/a&gt; を使います。コマンド一発ですべてダウンロードする&lt;a href=&#34;https://gist.github.com/r9y9/ff67c05aeb87410eae2e&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;スクリプト&lt;/a&gt;を書いたので、ご自由にどうぞ。&lt;/p&gt;
&lt;h3 id=&#34;02-juliaのインストール&#34;&gt;0.2. juliaのインストール&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://julialang.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;公式サイト&lt;/a&gt;からバイナリをダウンロードするか、&lt;a href=&#34;https://github.com/JuliaLang/julia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;githubのリポジトリ&lt;/a&gt;をクローンしてビルドしてください。バージョンは、現在の最新安定版のv0.3.2を使います。&lt;/p&gt;
&lt;p&gt;記事内では、juliaの基本的な使い方については解説しないので、前もってある程度調べておいてもらえると、スムーズに読み進められるかと思います。&lt;/p&gt;
&lt;h3 id=&#34;03-voiceconversionjl-のインストール&#34;&gt;0.3. VoiceConversion.jl のインストール&lt;/h3&gt;
&lt;p&gt;juliaを起動して、以下のコマンドを実行してください。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;julia&amp;gt; Pkg.clone(&amp;quot;https://github.com/r9y9/VoiceConversion.jl&amp;quot;)
julia&amp;gt; Pkg.build(&amp;quot;VoiceConversion&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;サードパーティライブラリは、sklearnを除いてすべて自動でインストールされます。sklearnは、例えば以下のようにしてインストールしておいてください。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo pip install sklearn
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;これで準備は完了です！&lt;/p&gt;
&lt;h2 id=&#34;1-音声波形からのメルケプストラムの抽出&#34;&gt;1. 音声波形からのメルケプストラムの抽出&lt;/h2&gt;
&lt;p&gt;まずは、音声から声質変換に用いる特徴量を抽出します。特徴量としては、声質変換や音声合成の分野で広く使われているメルケプストラムを使います。メルケプストラムの抽出は、&lt;code&gt;scripts/mcep.jl&lt;/code&gt; を使うことでできます。&lt;/p&gt;
&lt;h3 id=&#34;20141115-追記&#34;&gt;2014/11/15 追記&lt;/h3&gt;
&lt;p&gt;実行前に、&lt;code&gt;julia&amp;gt; Pkg.add(&amp;quot;WAV&amp;quot;)&lt;/code&gt; として、WAVパッケージをインストールしておいてください。(2014/11/15時点のmasterでは自動でインストールされますが、v0.0.1ではインストールされません、すいません）。また、メルケプストラムの出力先ディレクトリは事前に作成しておいてください（最新のスクリプトでは自動で作成されます）。&lt;/p&gt;
&lt;p&gt;以下のようにして、2話者分の特徴量を抽出しましょう。以下のスクリプトでは、 &lt;code&gt;~/data/cmu_arctic/&lt;/code&gt; にデータがあることを前提としています。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# clb
julia mcep.jl ~/data/cmu_arctic/cmu_us_clb_arctic/wav/ ~/data/cmu_arctic_jld/speakers/clb/
# slt
julia mcep.jl ~/data/cmu_arctic/cmu_us_slt_arctic/wav/ ~/data/cmu_arctic_jld/speakers/slt/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;基本的な使い方は、&lt;code&gt;mcep.jl &amp;lt;wavファイルがあるディレクトリ&amp;gt; &amp;lt;メルケプストラムが出力されるディレクトリ&amp;gt;&lt;/code&gt; になっています。オプションについては、 &lt;code&gt;mcep.jl -h&lt;/code&gt; としてヘルプを見るか、コードを直接見てください。&lt;/p&gt;
&lt;p&gt;抽出されたメルケプストラムは、HDF5フォーマットで保存されます。メルケプストラムの中身を見てみると、以下のような感じです。可視化には、PyPlotパッケージが必要です。Juliaを開いて、&lt;code&gt;julia&amp;gt; Pkg.add(&amp;quot;PyPlot&amp;quot;)&lt;/code&gt; とすればOKです。IJuliaを使いたい場合（僕は使っています）は、&lt;code&gt;julia&amp;gt; Pkg.add(&amp;quot;IJulia&amp;quot;)&lt;/code&gt; としてIJuliaもインストールしておきましょう。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# メルケプストラムの可視化

using HDF5, JLD, PyPlot

x = load(&amp;quot;clb/arctic_a0028.jld&amp;quot;)

figure(figsize=(16, 6), dpi=80, facecolor=&amp;quot;w&amp;quot;, edgecolor=&amp;quot;k&amp;quot;)
imshow(x[&amp;quot;feature_matrix&amp;quot;], origin=&amp;quot;lower&amp;quot;, aspect=&amp;quot;auto&amp;quot;)
colorbar()
&lt;/code&gt;&lt;/pre&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/clb_a0028_melcepstrum.png &#34;Mel-cepstrum of clb_a0028.&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;0次成分だけ取り出してみると、以下のようになります。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# メルケプストラムの0次成分のみを可視化

figure(figsize=(16, 6), dpi=80, facecolor=&amp;quot;w&amp;quot;, edgecolor=&amp;quot;k&amp;quot;)
plot(vec(x[&amp;quot;feature_matrix&amp;quot;][1,:]), linewidth=2.0, label=&amp;quot;0th order mel-cesptrum of clb_a0028&amp;quot;)
xlim(0, size(x[&amp;quot;feature_matrix&amp;quot;], 2)-10) # 末尾がsilenceだった都合上…（決め打ち）
xlabel(&amp;quot;Frame&amp;quot;)
legend(loc=&amp;quot;upper right&amp;quot;)
ylim(-10, -2) # 見やすいように適当に決めました
&lt;/code&gt;&lt;/pre&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/clb_a0028_melcepstrum_0th.png &#34;Mel-cepstrum of clb_a0028 0th.&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;こんな感じです。話者clbの&lt;code&gt;clb_a0028.wav&lt;/code&gt;を聞きながら、特徴量見てみてください。0次の成分からは、音量の大小が読み取れると思います。&lt;/p&gt;
&lt;h2 id=&#34;2-dpマッチングによるパラレルデータの作成&#34;&gt;2. DPマッチングによるパラレルデータの作成&lt;/h2&gt;
&lt;p&gt;次に、2話者分の特徴量を時間同期して連結します。基本的に声質変換では、音韻の違いによらない特徴量（非言語情報）の対応関係を学習するために、同一発話内容の特徴量を時間同期し（音韻の違いによる変動を可能な限りなくすため）、学習データとして用います。このデータのことを、パラレルデータと呼びます。&lt;/p&gt;
&lt;p&gt;パラレルデータの作成には、DPマッチングを使うのが一般的です。&lt;code&gt;scripts/align.jl&lt;/code&gt; を使うとできます。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;julia align.jl ~/data/cmu_arctic_jld/speakers/clb ~/data/cmu_arctic_jld/speakers/slt ~/data/cmu_arctic_jld/parallel/clb_and_slt/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;使い方は、&lt;code&gt;align.jl &amp;lt;話者1（clb）の特徴量のパス&amp;gt; &amp;lt;話者2（slt）の特徴量のパス&amp;gt; &amp;lt;パラレルデータの出力先&amp;gt;&lt;/code&gt; になっています。&lt;/p&gt;
&lt;p&gt;きちんと時間同期されているかどうか、0次成分を見て確認してみましょう。&lt;/p&gt;
&lt;p&gt;時間同期を取る前のメルケプストラムを以下に示します。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# 時間同期前のメルケプストラム（0次）を可視化

x = load(&amp;quot;clb/arctic_a0028.jld&amp;quot;)
y = load(&amp;quot;slt/arctic_a0028.jld&amp;quot;)

figure(figsize=(16, 6), dpi=80, facecolor=&amp;quot;w&amp;quot;, edgecolor=&amp;quot;k&amp;quot;)
plot(vec(x[&amp;quot;feature_matrix&amp;quot;][1,:]), linewidth=2.0, label=&amp;quot;0th order mel-cesptrum of clb_a0028&amp;quot;)
plot(vec(y[&amp;quot;feature_matrix&amp;quot;][1,:]), linewidth=2.0, label=&amp;quot;0th order mel-cesptrum of slt_a0028&amp;quot;)
xlim(0, min(size(x[&amp;quot;feature_matrix&amp;quot;], 2), size(y[&amp;quot;feature_matrix&amp;quot;], 2))-10) # 決め打ち
xlabel(&amp;quot;Frame&amp;quot;)
legend(loc=&amp;quot;upper right&amp;quot;)
ylim(-10, -2) # 決め打ち
&lt;/code&gt;&lt;/pre&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/clb_and_slt_a0028_melcepstrum_0th.png &#34;0th order mel-cepstrum (not aligned)&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;ちょっとずれてますね&lt;/p&gt;
&lt;p&gt;次に、時間同期後のメルケプストラムを示します。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# 時間同期後のメルケプストラム（0次）を可視化

parallel = load(&amp;quot;arctic_a0028_parallel.jld&amp;quot;)

figure(figsize=(16, 6), dpi=80, facecolor=&amp;quot;w&amp;quot;, edgecolor=&amp;quot;k&amp;quot;)
plot(vec(parallel[&amp;quot;src&amp;quot;][&amp;quot;feature_matrix&amp;quot;][1,:]), linewidth=2.0, &amp;quot;b&amp;quot;, label=&amp;quot;0th order mel-cesptrum of clb_a0028&amp;quot;)
plot(vec(parallel[&amp;quot;tgt&amp;quot;][&amp;quot;feature_matrix&amp;quot;][1,:]), linewidth=2.0, &amp;quot;g&amp;quot;, label=&amp;quot;0th order mel-cesptrum of slt_a0028&amp;quot;)
xlim(0, size(parallel[&amp;quot;tgt&amp;quot;][&amp;quot;feature_matrix&amp;quot;], 2))
xlabel(&amp;quot;Frame&amp;quot;)
legend()
&lt;/code&gt;&lt;/pre&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/clb_and_slt_a0028_melcepstrum_0th_aligned.png &#34;0th order mel-cepstrum (aligned)&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;ずれが修正されているのがわかりますね。注意として、&lt;code&gt;align.jl&lt;/code&gt; の中身を追えばわかるのですが、無音区間をしきい値判定で検出して、パラレルデータから除外しています。&lt;/p&gt;
&lt;p&gt;結果、時間同期されたパラレルデータは以下のようになります。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# パラレルデータの可視化

figure(figsize=(16, 6), dpi=80, facecolor=&amp;quot;w&amp;quot;, edgecolor=&amp;quot;k&amp;quot;)
imshow(vcat(parallel[&amp;quot;src&amp;quot;][&amp;quot;feature_matrix&amp;quot;], parallel[&amp;quot;tgt&amp;quot;][&amp;quot;feature_matrix&amp;quot;]), origin=&amp;quot;lower&amp;quot;, aspect=&amp;quot;auto&amp;quot;)
colorbar()
&lt;/code&gt;&lt;/pre&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/clb_and_slt_a0028_parallel.png &#34;example of parallel data&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;このパラレルデータを（複数の発話分さらに結合して）使って、特徴量の対応関係を学習していきます。モデルには、GMMを使います。&lt;/p&gt;
&lt;h2 id=&#34;3-gmmの学習&#34;&gt;3. GMMの学習&lt;/h2&gt;
&lt;p&gt;GMMの学習には、&lt;code&gt;sklearn.mixture.GMM&lt;/code&gt; を使います。GMMは古典的な生成モデルで、実装は探せばたくさん見つかるので、既存の有用なライブラリを使えば十分です。（余談ですが、pythonのライブラリを簡単に呼べるのはjuliaの良いところの一つですね）&lt;/p&gt;
&lt;p&gt;&lt;code&gt;scripts/train_gmm.jl&lt;/code&gt; を使うと、モデルのダンプ、julia &amp;lt;-&amp;gt; python間のデータフォーマットの変換等、もろもろやってくれます。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;julia train_gmm.jl ~/data/cmu_arctic_jld/parallel/clb_and_slt/ clb_and_slt_gmm32_order40.jld --max 200 --n_components 32 --n_iter=100 --n_init=1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;使い方は、&lt;code&gt;train_gmm.jl &amp;lt;パラレルデータのパス&amp;gt; &amp;lt;出力するモデルデータのパス&amp;gt;&lt;/code&gt; になっています。上の例では、学習に用いる発話数、GMMの混合数、反復回数等を指定しています。オプションの詳細はスクリプトをご覧ください。&lt;/p&gt;
&lt;p&gt;僕の環境では、上記のコマンドを叩くと2時間くらいかかりました。学習が終わったところで、学習済みのモデルのパラメータを可視化してみましょう。&lt;/p&gt;
&lt;p&gt;まずは平均を見てみます。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# GMMの平均ベクトルを（いくつか）可視化
gmm = load(&amp;quot;clb_and_slt_gmm32_order40.jld&amp;quot;)

figure(figsize=(16, 6), dpi=80, facecolor=&amp;quot;w&amp;quot;, edgecolor=&amp;quot;k&amp;quot;)
for k=1:3
    plot(gmm[&amp;quot;means&amp;quot;][:,k], linewidth=2.0, label=&amp;quot;mean of mixture $k&amp;quot;)
end
legend()
&lt;/code&gt;&lt;/pre&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/clb_and_slt_gmm32_order40_mean.png &#34;means of trained GMM&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;共分散の一部可視化してみると、以下のようになります。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# GMMの共分散行列を一部可視化

figure(figsize=(16, 6), dpi=80, facecolor=&amp;quot;w&amp;quot;, edgecolor=&amp;quot;k&amp;quot;)
imshow(gmm[&amp;quot;covars&amp;quot;][:,:,2])
colorbar()
clim(0.0, 0.16)
&lt;/code&gt;&lt;/pre&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/clb_and_slt_gmm32_order40_covar.png &#34;covariance of trained GMM&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;まぁこんなもんですね。&lt;/p&gt;
&lt;h2 id=&#34;4-音声分析合成worldを用いたgmmベースのframe-by-frame声質変換&#34;&gt;4. 音声分析合成WORLDを用いたGMMベースのframe-by-frame声質変換&lt;/h2&gt;
&lt;p&gt;さて、ようやく声質変換の準備が整いました。学習したモデルを使って、GMMベースのframe-by-frame声質変換（clb -&amp;gt; slt ）をやってみましょう。具体的な変換アルゴリズムは、論文（例えば&lt;a href=&#34;http://isw3.naist.jp/~tomoki/Tomoki/Journals/IEEE-Nov-2007_MLVC.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;戸田先生のこれ&lt;/a&gt;）をチェックしてみてください。音声分析合成系にはWORLDを使います。&lt;/p&gt;
&lt;p&gt;一般的な声質変換では、まず音声を以下の三つの成分に分解します。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基本周波数&lt;/li&gt;
&lt;li&gt;スペクトル包絡（今回いじりたい部分）&lt;/li&gt;
&lt;li&gt;非周期性成分&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;その後、スペクトル包絡に対して変換を行い、変換後のパラメータを使って音声波形を合成するといったプロセスを取ります。これらは、&lt;code&gt;scripts/vc.jl&lt;/code&gt; を使うと簡単にできるようになっています。本当にWORLDさまさまです。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;julia vc.jl ~/data/cmu_arctic/cmu_us_clb_arctic/wav/arctic_a0028.wav clb_and_slt_gmm32_order40.jld clb_to_slt_a0028.wav --order 40
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;使い方は、&lt;code&gt;vc.jl &amp;lt;変換対象の音声ファイル&amp;gt; &amp;lt;変換モデル&amp;gt; &amp;lt;出力wavファイル名&amp;gt;&lt;/code&gt; となっています。&lt;/p&gt;
&lt;p&gt;上記のコマンドを実行すると、GMMベースのframe-by-frame声質変換の結果が音声ファイルに出力されます。以下に結果を貼っておくので、聞いてみてください。&lt;/p&gt;
&lt;h3 id=&#34;変換元となる音声-clb_a0028&#34;&gt;変換元となる音声 clb_a0028&lt;/h3&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;166&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/176093202&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false&#34;&gt;&lt;/iframe&gt;
&lt;h3 id=&#34;変換目標となる話者-slt_a0028&#34;&gt;変換目標となる話者 slt_a0028&lt;/h3&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;166&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/176093240&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false&#34;&gt;&lt;/iframe&gt;
&lt;h3 id=&#34;変換結果-clb_to_slt_a0028&#34;&gt;変換結果 clb_to_slt_a0028&lt;/h3&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;166&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/176093403&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;話者性はなんとなく目標話者に近づいている気がしますが、音質が若干残念な感じですね。。&lt;/p&gt;
&lt;h2 id=&#34;5-差分スペクトル補正に基づく声質変換&#34;&gt;5. 差分スペクトル補正に基づく声質変換&lt;/h2&gt;
&lt;p&gt;最後に、より高品質な声質変換を達成可能な差分スペクトル補正に基づく声質変換を紹介します。差分スペクトル補正に基づく声質変換では、基本周波数や非周期性成分をいじれない代わりに音質はかなり改善します。以前書いた記事（&lt;a href=&#34;http://r9y9.github.io/blog/2014/07/05/statistical-voice-conversion-muzui/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;統計的声質変換クッソムズすぎワロタ - LESS IS MORE&lt;/a&gt;）から、着想に関連する部分を引用します。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;これまでは、音声を基本周波数、非周期性成分、スペクトル包絡に分解して、スペクトル包絡を表す特徴量を変換し、変換後の特徴量を元に波形を再合成していました。ただ、よくよく考えると、そもそも基本周波数、非周期性成分をいじる必要がない場合であれば、わざわざ分解して再合成する必要なくね？声質の部分のみ変換するようなフィルタかけてやればよくね？という考えが生まれます。実は、そういったアイデアに基づく素晴らしい手法があります。それが、差分スペクトル補正に基づく声質変換です。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;差分スペクトル補正に基づく声質変換の詳細ついては、最近inter speechに論文が出たようなので、そちらをご覧ください。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://isw3.naist.jp/~kazuhiro-k/resource/kobayashi14IS.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Kobayashi 2014] Kobayashi, Kazuhiro, et al. &amp;ldquo;Statistical Singing Voice Conversion with Direct Waveform Modification based on the Spectrum Differential.&amp;rdquo; Fifteenth Annual Conference of the International Speech Communication Association. 2014.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;こばくん、論文を宣伝しておきますね＾＾&lt;/p&gt;
&lt;h3 id=&#34;51-差分特徴量の学習&#34;&gt;5.1 差分特徴量の学習&lt;/h3&gt;
&lt;p&gt;さて、差分スペクトル補正に基づく声質変換行うには、変換元話者$X$と目標話者$Y$の特徴量の同時分布$P(X,Y)$を学習するのではなく、$P(X, Y-X)$ （日本語で書くとややこしいのですが、変換元話者の特徴量$X$と、変換元話者と目標話者の差分特徴量$Y-X$の同時分布）を学習します。これは、 &lt;code&gt;train_gmm.jl&lt;/code&gt; を使ってGMMを学習する際に、&lt;code&gt;--diff&lt;/code&gt; とオプションをつけるだけでできます。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;julia train_gmm.jl ~/data/cmu_arctic_jld/parallel/clb_and_slt/ clb_to_slt_gmm32_order40_diff.jld --max 200 --n_components 32 --n_iter=100 --n_init=1 --diff
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可視化してみます。&lt;/p&gt;
&lt;p&gt;平均&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/clb_to_slt_gmm32_order40_mean.png &#34;means of trained DIFFGMM&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;共分散&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/clb_to_slt_gmm32_order40_covar.png &#34;covar of trained DIFFGMM&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;さっき学習したGMMとは、共分散はかなり形が違いますね。高次元成分でも、分散が比較的大きな値をとっているように見えます。形が異っているのは見てすぐにわかりますが、では具体的には何が異っているのか、それはなぜなのか、きちんと考えると面白そうですね。&lt;/p&gt;
&lt;h3 id=&#34;52-mlsaフィルタによる声質変換&#34;&gt;5.2 MLSAフィルタによる声質変換&lt;/h3&gt;
&lt;p&gt;差分スペクトル補正に基づく声質変換では、WORLDを使って音声の分析合成を行うのではなく、生の音声波形を入力として、MLSAフィルタをかけるのみです。これは、 &lt;code&gt;scripts/diffvc.jl&lt;/code&gt; を使うと簡単にできます。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;julia diffvc.jl ~/data/cmu_arctic/cmu_us_clb_arctic/wav/arctic_a0028.wav clb_to_slt_gmm32_order40_diff.jld clb_to_slt_a0028_diff.wav --order 40
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;さて、結果を聞いてみましょう。&lt;/p&gt;
&lt;h3 id=&#34;53-差分スペクトル補正に基づく声質変換結果&#34;&gt;5.3 差分スペクトル補正に基づく声質変換結果&lt;/h3&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;166&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/176093513&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;アイデアはシンプル、結果は良好、最高の手法ですね（べた褒め&lt;/p&gt;
&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;
&lt;p&gt;以上、長くなりましたが、統計的声質変換についてのチュートリアルはこれで終わります。誰の役に立つのか知らないけれど、役に立てば嬉しいです。トラジェクトリ変換やGVを考慮したバージョンなど、今回紹介していないものも実装しているので、詳しくは&lt;a href=&#34;https://github.com/r9y9/VoiceConversion.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Githubのリポジトリ&lt;/a&gt;をチェックしてください。バグをレポートしてくれたりすると、僕は喜びます。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;h3 id=&#34;以前書いた声質変換に関する記事&#34;&gt;以前書いた声質変換に関する記事&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://r9y9.github.io/blog/2014/07/05/statistical-voice-conversion-muzui/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;統計的声質変換クッソムズすぎワロタ - LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://r9y9.github.io/blog/2014/07/13/statistical-voice-conversion-wakaran/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;統計的声質変換クッソムズすぎワロタ（実装の話） - LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;論文&#34;&gt;論文&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://isw3.naist.jp/~tomoki/Tomoki/Journals/IEEE-Nov-2007_MLVC.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Toda 2007] T. Toda, A. W. Black, and K. Tokuda, “Voice conversion based on maximum likelihood estimation of spectral parameter trajectory,” IEEE
Trans. Audio, Speech, Lang. Process, vol. 15, no. 8, pp. 2222–2235,
Nov. 2007.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://isw3.naist.jp/~kazuhiro-k/resource/kobayashi14IS.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Kobayashi 2014] Kobayashi, Kazuhiro, et al. &amp;ldquo;Statistical Singing Voice Conversion with Direct Waveform Modification based on the Spectrum Differential.&amp;rdquo; Fifteenth Annual Conference of the International Speech Communication Association. 2014.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;faq&#34;&gt;FAQ&lt;/h2&gt;
&lt;h3 id=&#34;前はpythonで書いてなかった&#34;&gt;前はpythonで書いてなかった？&lt;/h3&gt;
&lt;p&gt;はい、https://gist.github.com/r9y9/88bda659c97f46f42525 ですね。正確には、GMMの学習・変換処理はpythonで書いて、特徴抽出、パラレルデータの作成、波形合成はGo言語で書いていました。が、Goとpythonでデータのやりとり、Goとpythonをいったり来たりするのが面倒になってしまって、一つの言語に統一したいと思うようになりました。Goで機械学習は厳しいと感じていたので、pythonで書くかなぁと最初は思ったのですが、WORLDやSPTKなど、Cのライブラリをpythonから使うのが思いの他面倒だったので（&lt;a href=&#34;https://github.com/r9y9/SPTK&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SPTKのpythonラッパー&lt;/a&gt;は書きましたが）、Cやpythonとの連携がしやすく、スクリプト言語でありながらCに速度面で引けをとらないjuliaに興味があったので、juliaですべて完結するようにしました。かなり実験的な試みでしたが、今はかなり満足しています。juliaさいこー&lt;/p&gt;
&lt;h3 id=&#34;新規性は&#34;&gt;新規性は？&lt;/h3&gt;
&lt;p&gt;ありません&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SPTKのJuliaラッパーも書いた</title>
      <link>https://r9y9.github.io/blog/2014/09/15/sptk-for-julia/</link>
      <pubDate>Mon, 15 Sep 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/09/15/sptk-for-julia/</guid>
      <description>&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/IMG_0960.JPG &#34;sea&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;夏も終わったようですね。またSPTKかという感じですが、Juliaから使うためのラッパーを書きました。必要そうなのはだいたいラップしたので、よろしければどうぞ。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/SPTK.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Julia wrapper for Speech Signal Processing Toolkit (SPTK) | Github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;かれこれ、Go, Python, Juliaと、3つの言語でラッパーを書いてしまいました。どれだけSPTK好きなんだと。そしてどれだけ言語触ってるんだ絞れと。うーん、とはいえどれも良いところと悪いところがあってですね（何も言ってない）、難しい…&lt;/p&gt;
&lt;p&gt;おしまい&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SPTKのPythonラッパーを書いた</title>
      <link>https://r9y9.github.io/blog/2014/08/10/sptk-from-python/</link>
      <pubDate>Sun, 10 Aug 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/08/10/sptk-from-python/</guid>
      <description>&lt;h2 id=&#34;20150906-追記&#34;&gt;2015/09/06 追記&lt;/h2&gt;
&lt;p&gt;ましなpythonラッパーを新しく作りました: &lt;a href=&#34;https://r9y9.github.io/blog/2015/09/06/pysptk/&#34;&gt;Pysptk: SPTKのpythonラッパーを作った (Part 2)&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;20140810-追記&#34;&gt;2014/08/10 追記&lt;/h2&gt;
&lt;p&gt;ipython notebookによる簡単なチュートリアルを貼っておきます&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/r9y9/SPTK/blob/master/notebook/SPTK%20calling%20from%20python.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SPTK を Pythonから呼ぶ | nbviewer&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;20141109&#34;&gt;2014/11/09&lt;/h2&gt;
&lt;p&gt;タイポ修正しました…&lt;/p&gt;
&lt;p&gt;scipy.mixture -&amp;gt; sklearn.mixture&lt;/p&gt;
&lt;p&gt;SPTKの中で最も価値がある（と僕が思っている）メルケプストラム分析、メルケプストラムからの波形合成（MLSA filter）がpythonから可能になります。&lt;/p&gt;
&lt;p&gt;ご自由にどうぞ&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/SPTK&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Speech Signal Processing Toolkit (SPTK) for API use with python | Github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;注意ですが、&lt;code&gt;SPTK.h&lt;/code&gt;にある関数を全部ラップしているわけではないです。僕が必要なものしか、現状はラップしていません（例えば、GMMとかラップする必要ないですよね？sklearn.mixture使えばいいし）。ただ、大方有用なものはラップしたと思います。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://r9y9.github.io/blog/2014/02/10/sptk-go-wrapper/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goで音声信号処理をしたいのでSPTKのGoラッパーを書く - LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Goでも書いたのにPythonでも書いてしまった。&lt;/p&gt;
&lt;p&gt;一年くらい前に元指導教員の先生と「Pythonから使えたらいいですよね」と話をしていました。先生、ようやく書きました。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>統計的声質変換クッソムズすぎワロタ（実装の話）</title>
      <link>https://r9y9.github.io/blog/2014/07/13/statistical-voice-conversion-wakaran/</link>
      <pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/07/13/statistical-voice-conversion-wakaran/</guid>
      <description>&lt;p&gt;2014/07/28 追記：
重み行列の構築の部分を改良したのでちょいアップデート。具体的にはdense matrixとして構築してからスパース行列に変換していたのを、はじめからスパース行列として構築するようにして無駄にメモリを使わないようにしました。あとdiffが見やすいようにgistにあげました
&lt;a href=&#34;https://gist.github.com/r9y9/88bda659c97f46f42525&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gist.github.com/r9y9/88bda659c97f46f42525&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;まえがき&#34;&gt;まえがき&lt;/h2&gt;
&lt;p&gt;前回、&lt;a href=&#34;http://r9y9.github.io/blog/2014/07/05/statistical-voice-conversion-muzui/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;統計的声質変換クッソムズすぎワロタ - LESS IS MORE&lt;/a&gt; という記事を書いたら研究者の方々等ちょいちょい反応してくださって嬉しかったです。差分スペクトル補正、その道の人が聴いても音質がいいそう。これはいい情報です。&lt;/p&gt;
&lt;p&gt;Twitter引用:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; lang=&#34;en&#34;&gt;&lt;p&gt;統計的声質変換クッソムズすぎワロタ - LESS IS MORE &lt;a href=&#34;http://t.co/8RkeXIf6Ym&#34;&gt;http://t.co/8RkeXIf6Ym&lt;/a&gt; &lt;a href=&#34;https://twitter.com/r9y9&#34;&gt;@r9y9&lt;/a&gt;さんから ムズすぎと言いながら，最後の音はしっかり出ているあたり凄いなぁ．&lt;/p&gt;&amp;mdash; M. Morise (忍者系研究者) (@m_morise) &lt;a href=&#34;https://twitter.com/m_morise/statuses/485339123171852289&#34;&gt;July 5, 2014&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; lang=&#34;en&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://twitter.com/ballforest&#34;&gt;@ballforest&lt;/a&gt; 従来のパラメータ変換と比較すると、音質は従来よりもよさそうな気はしますがスペクトル包絡の性差ががっつりと影響しそうな気もするんですよね。&lt;/p&gt;&amp;mdash; 縄文人（妖精系研究者なのです） (@dicekicker) &lt;a href=&#34;https://twitter.com/dicekicker/statuses/485380534122463232&#34;&gt;July 5, 2014&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;異性間に関しては、実験が必要ですね。異性間だとF0が結構変わってくると思いますが、差分スペクトル補正の場合そもそもF0をいじらないという前提なので、F0とスペクトル包絡が完全に独立でない（ですよね？）以上、同姓間に比べて音質は劣化する気はします。簡単にやったところ、少なくとも僕の主観的には劣化しました&lt;/p&gt;
&lt;p&gt;ところで、結構いい感じにできたぜひゃっはーと思って、先輩に聞かせてみたら違いわかんねと言われて心が折れそうになりました。やはり現実はつらいです。&lt;/p&gt;
&lt;h2 id=&#34;実装の話&#34;&gt;実装の話&lt;/h2&gt;
&lt;p&gt;さて、今回は少し実装のことを書こうと思います。学習&amp;amp;変換部分はPythonで書いています。その他はGo（※Goの話は書きません）。&lt;/p&gt;
&lt;h3 id=&#34;トラジェクトリベースのパラメータ変換が遅いのは僕の実装が悪いからでした本当に申し訳ありませんでしたorz&#34;&gt;トラジェクトリベースのパラメータ変換が遅いのは僕の実装が悪いからでした本当に申し訳ありませんでしたorz&lt;/h3&gt;
&lt;p&gt;前回トラジェクトリベースは処理が激重だと書きました。なんと、4秒程度の音声（フレームシフト5msで777フレーム）に対して変換部分に600秒ほどかかっていたのですが（重すぎワロタ）、結果から言えばPythonでも12秒くらいまでに高速化されました（混合数64, メルケプの次元数40+デルタ=80次元、分散共分散はfull）。本当にごめんなさい。&lt;/p&gt;
&lt;p&gt;何ヶ月か前、ノリでトラジェクトリベースの変換を実装しようと思ってサクッと書いたのがそのままで、つまりとても効率の悪い実装になっていました。具体的には放置していた問題が二つあって、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ナイーブな逆行列の計算&lt;/li&gt;
&lt;li&gt;スパース性の無視&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;です。特に後者はかなりパフォーマンスに影響していました&lt;/p&gt;
&lt;h3 id=&#34;ナイーブな逆行列の計算&#34;&gt;ナイーブな逆行列の計算&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://d.hatena.ne.jp/sleepy_yoshi/20120513/p1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;numpy.linalg.invとnumpy.linalg.solveを用いた逆行列計算 - 睡眠不足？！ (id:sleepy_yoshi)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;numpy.linalg.inv&lt;/code&gt;を使っていましたよね。しかも&lt;code&gt;numpy.linalg.solve&lt;/code&gt;のほうが速いことを知っていながら。一ヶ月前の自分を問い詰めたい。&lt;code&gt;numpy.linalg.solve&lt;/code&gt;で置き換えたら少し速くなりました。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;600秒 -&amp;gt; 570秒&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;1.05倍の高速化&lt;/p&gt;
&lt;h3 id=&#34;スパース性の無視&#34;&gt;スパース性の無視&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://isw3.naist.jp/~tomoki/Tomoki/Journals/IEEE-Nov-2007_MLVC.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;T. Toda, A. W. Black, and K. Tokuda, “Voice conversion based on maximum likelihood estimation of spectral parameter trajectory,” IEEE Trans. Audio, Speech, Lang. Process, vol. 15, no. 8, pp. 2222–2235, Nov. 2007&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;論文を見ていただければわかるのですが、トラジェクトリベースの変換法における多くの計算は、行列を使って表すことができます。で、論文中の$W$という行列は、サイズがめちゃくちゃでかいのですがほとんどの要素は0です。この性質を使わない理由はないですよね？？&lt;/p&gt;
&lt;p&gt;…残念なことに、僕は密行列として扱って計算していました。ほら、疎行列ってちょっと扱いづらいじゃないですか…めんどくさそう…と思って放置してました。ごめんなさい&lt;/p&gt;
&lt;p&gt;pythonで疎行列を扱うなら、scipy.sparseを使えば良さそうです。結果、$W$を疎行列として扱うことで行列演算は大きく高速化されました。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;570秒 -&amp;gt; 12秒くらい&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;単純に考えると50倍の高速化ですか。本当にアホだった。最初からscipy.sparse使っておけばよかったです。&lt;/p&gt;
&lt;p&gt;scipy.sparseの使い方は以下を参考にしました。みなさんぜひ使いましょう&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://sucrose.hatenablog.com/entry/2013/04/07/130625&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Python で疎行列(SciPy) - 唯物是真 @Scaled_Wurm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://docs.scipy.org/doc/scipy/reference/sparse.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sparse matrices (scipy.sparse) — SciPy v0.14.0 Reference Guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://lucidfrontier45.wordpress.com/2011/08/02/scipysparse_matmul/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;scipy.sparseで疎行列の行列積 | frontier45&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;コード&#34;&gt;コード&lt;/h2&gt;
&lt;p&gt;メモ的な意味で主要なコードを貼っておきます。
&lt;a href=&#34;https://gist.github.com/r9y9/88bda659c97f46f42525&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gist.github.com/r9y9/88bda659c97f46f42525&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/usr/bin/python
# coding: utf-8

import numpy as np
from numpy import linalg
from sklearn.mixture import GMM
import scipy.linalg
import scipy.sparse
import scipy.sparse.linalg

class GMMMap:
    &amp;quot;&amp;quot;&amp;quot;GMM-based frame-by-frame speech parameter mapping.

    GMMMap represents a class to transform spectral features of a source
    speaker to that of a target speaker based on Gaussian Mixture Models
    of source and target joint spectral features.

    Notation
    --------
    Source speaker&#39;s feature: X = {x_t}, 0 &amp;lt;= t &amp;lt; T
    Target speaker&#39;s feature: Y = {y_t}, 0 &amp;lt;= t &amp;lt; T
    where T is the number of time frames.

    Parameters
    ----------
    gmm : scipy.mixture.GMM
        Gaussian Mixture Models of source and target joint features

    swap : bool
        True: source -&amp;gt; target
        False target -&amp;gt; source

    Attributes
    ----------
    num_mixtures : int
        the number of Gaussian mixtures

    weights : array, shape (`num_mixtures`)
        weights for each gaussian

    src_means : array, shape (`num_mixtures`, `order of spectral feature`)
        means of GMM for a source speaker

    tgt_means : array, shape (`num_mixtures`, `order of spectral feature`)
        means of GMM for a target speaker

    covarXX : array, shape (`num_mixtures`, `order of spectral feature`,
        `order of spectral feature`)
        variance matrix of source speaker&#39;s spectral feature

    covarXY : array, shape (`num_mixtures`, `order of spectral feature`,
        `order of spectral feature`)
        covariance matrix of source and target speaker&#39;s spectral feature

    covarYX : array, shape (`num_mixtures`, `order of spectral feature`,
        `order of spectral feature`)
        covariance matrix of target and source speaker&#39;s spectral feature

    covarYY : array, shape (`num_mixtures`, `order of spectral feature`,
        `order of spectral feature`)
        variance matrix of target speaker&#39;s spectral feature

    D : array, shape (`num_mixtures`, `order of spectral feature`,
        `order of spectral feature`)
        covariance matrices of target static spectral features

    px : scipy.mixture.GMM
        Gaussian Mixture Models of source speaker&#39;s features

    Reference
    ---------
      - [Toda 2007] Voice Conversion Based on Maximum Likelihood Estimation
        of Spectral Parameter Trajectory.
        http://isw3.naist.jp/~tomoki/Tomoki/Journals/IEEE-Nov-2007_MLVC.pdf

    &amp;quot;&amp;quot;&amp;quot;
    def __init__(self, gmm, swap=False):
        # D is the order of spectral feature for a speaker
        self.num_mixtures, D = gmm.means_.shape[0], gmm.means_.shape[1]/2
        self.weights = gmm.weights_

        # Split source and target parameters from joint GMM
        self.src_means = gmm.means_[:, 0:D]
        self.tgt_means = gmm.means_[:, D:]
        self.covarXX = gmm.covars_[:, :D, :D]
        self.covarXY = gmm.covars_[:, :D, D:]
        self.covarYX = gmm.covars_[:, D:, :D]
        self.covarYY = gmm.covars_[:, D:, D:]

        # swap src and target parameters
        if swap:
            self.tgt_means, self.src_means = self.src_means, self.tgt_means
            self.covarYY, self.covarXX = self.covarXX, self.covarYY
            self.covarYX, self.covarXY = self.XY, self.covarYX

        # Compute D eq.(12) in [Toda 2007]
        self.D = np.zeros(self.num_mixtures*D*D).reshape(self.num_mixtures, D, D)
        for m in range(self.num_mixtures):
            xx_inv_xy = np.linalg.solve(self.covarXX[m], self.covarXY[m])
            self.D[m] = self.covarYY[m] - np.dot(self.covarYX[m], xx_inv_xy)

        # p(x), which is used to compute posterior prob. for a given source
        # spectral feature in mapping stage.
        self.px = GMM(n_components=self.num_mixtures, covariance_type=&amp;quot;full&amp;quot;)
        self.px.means_ = self.src_means
        self.px.covars_ = self.covarXX
        self.px.weights_ = self.weights

    def convert(self, src):
        &amp;quot;&amp;quot;&amp;quot;
        Mapping source spectral feature x to target spectral feature y
        so that minimize the mean least squared error.
        More specifically, it returns the value E(p(y|x)].

        Parameters
        ----------
        src : array, shape (`order of spectral feature`)
            source speaker&#39;s spectral feature that will be transformed

        Return
        ------
        converted spectral feature
        &amp;quot;&amp;quot;&amp;quot;
        D = len(src)

        # Eq.(11)
        E = np.zeros((self.num_mixtures, D))
        for m in range(self.num_mixtures):
            xx = np.linalg.solve(self.covarXX[m], src - self.src_means[m])
            E[m] = self.tgt_means[m] + self.covarYX[m].dot(xx)

        # Eq.(9) p(m|x)
        posterior = self.px.predict_proba(np.atleast_2d(src))

        # Eq.(13) conditinal mean E[p(y|x)]
        return posterior.dot(E)

class TrajectoryGMMMap(GMMMap):
    &amp;quot;&amp;quot;&amp;quot;
    Trajectory-based speech parameter mapping for voice conversion
    based on the maximum likelihood criterion.

    Parameters
    ----------
    gmm : scipy.mixture.GMM
        Gaussian Mixture Models of source and target speaker joint features

    gv : scipy.mixture.GMM (default=None)
        Gaussian Mixture Models of target speaker&#39;s global variance of spectral
        feature

    swap : bool (default=False)
        True: source -&amp;gt; target
        False target -&amp;gt; source

    Attributes
    ----------
    TODO

    Reference
    ---------
      - [Toda 2007] Voice Conversion Based on Maximum Likelihood Estimation
        of Spectral Parameter Trajectory.
        http://isw3.naist.jp/~tomoki/Tomoki/Journals/IEEE-Nov-2007_MLVC.pdf
    &amp;quot;&amp;quot;&amp;quot;
    def __init__(self, gmm, T, gv=None, swap=False):
        GMMMap.__init__(self, gmm, swap)

        self.T = T
        # shape[1] = d(src) + d(src_delta) + d(tgt) + d(tgt_delta)
        D = gmm.means_.shape[1] / 4

        ## Setup for Trajectory-based mapping
        self.__construct_weight_matrix(T, D)

        ## Setup for GV post-filtering
        # It is assumed that GV is modeled as a single mixture GMM
        if gv != None:
            self.gv_mean = gv.means_[0]
            self.gv_covar = gv.covars_[0]
            self.Pv = np.linalg.inv(self.gv_covar)

    def __construct_weight_matrix(self, T, D):
        # Construct Weight matrix W
        # Eq.(25) ~ (28)
        for t in range(T):
            w0 = scipy.sparse.lil_matrix((D, D*T))
            w1 = scipy.sparse.lil_matrix((D, D*T))
            w0[0:,t*D:(t+1)*D] = scipy.sparse.diags(np.ones(D), 0)

            if t-1 &amp;gt;= 0:
                tmp = np.zeros(D)
                tmp.fill(-0.5)
                w1[0:,(t-1)*D:t*D] = scipy.sparse.diags(tmp, 0)
            if t+1 &amp;lt; T:
                tmp = np.zeros(D)
                tmp.fill(0.5)
                w1[0:,(t+1)*D:(t+2)*D] = scipy.sparse.diags(tmp, 0)

            W_t = scipy.sparse.vstack([w0, w1])

            # Slower
            # self.W[2*D*t:2*D*(t+1),:] = W_t

            if t == 0:
                self.W = W_t
            else:
                self.W = scipy.sparse.vstack([self.W, W_t])

        self.W = scipy.sparse.csr_matrix(self.W)

        assert self.W.shape == (2*D*T, D*T)

    def convert(self, src):
        &amp;quot;&amp;quot;&amp;quot;
        Mapping source spectral feature x to target spectral feature y
        so that maximize the likelihood of y given x.

        Parameters
        ----------
        src : array, shape (`the number of frames`, `the order of spectral feature`)
            a sequence of source speaker&#39;s spectral feature that will be
            transformed

        Return
        ------
        a sequence of transformed spectral features
        &amp;quot;&amp;quot;&amp;quot;
        T, D = src.shape[0], src.shape[1]/2

        if T != self.T:
            self.__construct_weight_matrix(T, D)

        # A suboptimum mixture sequence  (eq.37)
        optimum_mix = self.px.predict(src)

        # Compute E eq.(40)
        self.E = np.zeros((T, 2*D))
        for t in range(T):
            m = optimum_mix[t] # estimated mixture index at time t
            xx = np.linalg.solve(self.covarXX[m], src[t] - self.src_means[m])
            # Eq. (22)
            self.E[t] = self.tgt_means[m] + np.dot(self.covarYX[m], xx)
        self.E = self.E.flatten()

        # Compute D eq.(41). Note that self.D represents D^-1.
        self.D = np.zeros((T, 2*D, 2*D))
        for t in range(T):
            m = optimum_mix[t]
            xx_inv_xy = np.linalg.solve(self.covarXX[m], self.covarXY[m])
            # Eq. (23)
            self.D[t] = self.covarYY[m] - np.dot(self.covarYX[m], xx_inv_xy)
            self.D[t] = np.linalg.inv(self.D[t])
        self.D = scipy.linalg.block_diag(*self.D)

        # represent D as a sparse matrix
        self.D = scipy.sparse.csr_matrix(self.D)

        # Compute target static features
        # eq.(39)
        covar = self.W.T.dot(self.D.dot(self.W))
        y = scipy.sparse.linalg.spsolve(covar, self.W.T.dot(self.D.dot(self.E)),\
                                        use_umfpack=False)
        return y.reshape((T, D))
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;結論&#34;&gt;結論&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;疎行列の演算を考えるときは、間違ってもめんどくさいとか思わずに疎行列を積極的に使おう&lt;/li&gt;
&lt;li&gt;統計的声質変換ムズすぎ&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;おまけめも&#34;&gt;おまけめも&lt;/h2&gt;
&lt;p&gt;僕が変換精度を改善するために考えていることのめも&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;統計的な手法を使う限りover-smoothingの問題はついてくる。ならば、逆にover-smoothingされることで都合の良い特徴量を考えることはできないか&lt;/li&gt;
&lt;li&gt;メルケプとかそもそもスペクトル包絡をコンパクトにparamtricに表現するために考えられたもの（だと思ってる）ので、高品質な変換を考えるならばスペクトル包絡をそのまま使うなりした方がいいんじゃないか。とはいえスペクトル包絡をそのまま使うのはぼちぼち高次元なので、個人性に依存する部分を残した形で非線形次元削減したらどうか（例えばニューラルネットを使って統計的に個人性に依存する部分を見つけ出すとか）&lt;/li&gt;
&lt;li&gt;time-dependentな関係をモデル化しないとだめじゃないか、確率モデルとして。RNNとか普通に使えそうだし、まぁHMMでもよい&lt;/li&gt;
&lt;li&gt;音素境界を推定して、segment単位で変換するのも良いかも&lt;/li&gt;
&lt;li&gt;識別モデルもっと使ってもいいんじゃないか&lt;/li&gt;
&lt;li&gt;波形合成にSPTKのmlsadfコマンド使ってる？あれ実はフレーム間のメルケプが線形補間されてるんですよね。本当に線形補間でいいんでしょうか？他の補間法も試したらどうですかね&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;こんなかんじですか。おやすみなさい&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>統計的声質変換クッソムズすぎワロタ</title>
      <link>https://r9y9.github.io/blog/2014/07/05/statistical-voice-conversion-muzui/</link>
      <pubDate>Sat, 05 Jul 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/07/05/statistical-voice-conversion-muzui/</guid>
      <description>&lt;h2 id=&#34;20141012-追記&#34;&gt;2014/10/12 追記&lt;/h2&gt;
&lt;p&gt;少なくともGVのコードに致命的なバグがあったことがわかりました。よって、あまりあてにしないでください…（ごめんなさい&lt;/p&gt;
&lt;p&gt;こんにちは。&lt;/p&gt;
&lt;p&gt;最近、統計的声質変換の勉強をしていました。で、メジャーなGMM（混合ガウスモデル）ベースの変換を色々やってみたので、ちょろっと書きます。実は（というほどでもない?）シンプルなGMMベースの方法だと音質クッソ悪くなってしまうんですが、色々試してやっとまともに聞ける音質になったので、試行錯誤の形跡を残しておくとともに、音声サンプルを貼っておきます。ガチ勢の方はゆるりと見守ってください&lt;/p&gt;
&lt;p&gt;基本的に、以下の論文を参考にしています&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://isw3.naist.jp/~tomoki/Tomoki/Journals/IEEE-Nov-2007_MLVC.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;T. Toda, A. W. Black, and K. Tokuda, “Voice conversion based on maximum likelihood estimation of spectral parameter trajectory,” IEEE Trans. Audio, Speech, Lang. Process, vol. 15, no. 8, pp. 2222–2235, Nov. 2007&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;gmmベースの声質変換の基本&#34;&gt;GMMベースの声質変換の基本&lt;/h2&gt;
&lt;p&gt;シンプルなGMMベースの声質変換は大きく二つのフェーズに分けられます。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;参照話者と目標話者のスペクトル特徴量の結合GMM $P(x,y)$を学習する&lt;/li&gt;
&lt;li&gt;入力$x$が与えらたとき、$P(y|x)$が最大となるようにスペクトル特徴量を変換する&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;あらかじめ話者間の関係をデータから学習しておくことで、未知の入力が来た時にも変換が可能になるわけです。&lt;/p&gt;
&lt;p&gt;具体的な変換プロセスとしては、音声を&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基本周波数&lt;/li&gt;
&lt;li&gt;非周期性成分&lt;/li&gt;
&lt;li&gt;スペクトル包絡&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;の3つに分解し、スペクトル包絡の部分（≒声質を表す特徴量）に対して変換を行い、最後に波形を再合成するといった方法がよく用いられます。基本周波数や非周期性成分も変換することがありますが、ここではとりあえず扱いません&lt;/p&gt;
&lt;p&gt;シンプルな方法では、フレームごとに独立に変換を行います。&lt;/p&gt;
&lt;p&gt;GMMベースのポイントは、東大の齋藤先生の以下のツイートを引用しておきます。&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; lang=&#34;en&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://twitter.com/shurabaP&#34;&gt;@shurabaP&lt;/a&gt; GMMベースの声質変換の肝は、入力xが与えられた時の出力yの条件付き確率P(y|x) が最大になるようにyを選ぶという確率的な考えです。私のショボい自作スクリプトですが、HTKを使ったGMMの学習レシピは研究室内部用に作ってあるので、もし必要なら公開しますよ。&lt;/p&gt;&amp;mdash; Daisuke Saito (@dsk_saito) &lt;a href=&#34;https://twitter.com/dsk_saito/statuses/48442052534472706&#34;&gt;March 17, 2011&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;ちなみに僕はscipy.mixture.GMMを使いました。HTKヤダー&lt;/p&gt;
&lt;h2 id=&#34;やってみる&#34;&gt;やってみる&lt;/h2&gt;
&lt;p&gt;さて、実際にやってみます。データベースには、[CMU_ARCTIC speech synthesis databases](ht
tp://www.festvox.org/cmu_arctic/)を使います。今回は、女性話者の二人を使いました。&lt;/p&gt;
&lt;p&gt;音声の分析合成には、&lt;a href=&#34;http://ml.cs.yamanashi.ac.jp/world/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WORLD&lt;/a&gt;を使います。WORLDで求めたスペクトル包絡からメルケプストラム（今回は32次元）に変換したものを特徴量として使いました。&lt;/p&gt;
&lt;p&gt;学習では、学習サンプル10641フレーム（23フレーズ）、GMMの混合数64、full-covarianceで学習しました。&lt;/p&gt;
&lt;h3 id=&#34;変換元となる話者参照話者&#34;&gt;変換元となる話者（参照話者）&lt;/h3&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;166&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/157362625&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false&#34;&gt;&lt;/iframe&gt;
&lt;h3 id=&#34;変換対象となる話者目標話者&#34;&gt;変換対象となる話者（目標話者）&lt;/h3&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;166&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/157362613&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false&#34;&gt;&lt;/iframe&gt;
&lt;h3 id=&#34;gmmベースのframe-by-frameな声質変換の結果&#34;&gt;GMMベースのframe-by-frameな声質変換の結果&lt;/h3&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;166&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/157371966&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;はぁー、正直聞けたもんじゃないですね。声質は目標話者に近づいている感がありますが、何分音質が悪い。学習条件を色々変えて試行錯誤しましたけどダメでした&lt;/p&gt;
&lt;h2 id=&#34;gmmベースの声質変換の弱点&#34;&gt;GMMベースの声質変換の弱点&lt;/h2&gt;
&lt;p&gt;さて、なぜダメかを考えます。もう考えつくされてる感あるけど、大事なところだけ整理します&lt;/p&gt;
&lt;h3 id=&#34;フレーム毎に独立な変換処理&#34;&gt;フレーム毎に独立な変換処理&lt;/h3&gt;
&lt;p&gt;まず、音声が時間的に独立なわけないですよね。フレームごとに独立に変換すると、時間的に不連続な点が出てきてしまいます。その結果、ちょっとノイジーな音声になってしまったのではないかと考えられます。&lt;/p&gt;
&lt;p&gt;これに対する解決法としては、戸田先生の論文にあるように、動的特徴量も併せてGMMを学習して、系列全体の確率が最大となるように変換を考えるトラジェクトリベースのパラメータ生成方法があります。&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;さて、やってみます。参照音声、目標音声は↑で使ったサンプルと同じです。&lt;/p&gt;
&lt;h3 id=&#34;トラジェクトリベースの声質変換の結果&#34;&gt;トラジェクトリベースの声質変換の結果&lt;/h3&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;166&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/157371969&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;あんま変わらないですね。計算量めっちゃ食うのに、本当につらい。実装が間違ってる可能性もあるけど…&lt;/p&gt;
&lt;p&gt;他の方法を考えるとするならば、まぁいっぱいあると思うんですが、スペクトル包絡なんて時間的に不連続にコロコロ変わるようなもんでもない気がするので、確率モデルとしてそういう依存関係を考慮した声質変換があってもいいもんですけどね。あんま見てない気がします。&lt;/p&gt;
&lt;p&gt;ちょっと調べたら見つかったもの↓&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://koasas.kaist.ac.kr/bitstream/10203/17632/1/25.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kim, E.K., Lee, S., Oh, Y.-H. (1997). &amp;ldquo;Hidden Markov Model Based Voice Conversion Using Dynamic Characteristics of Speaker&amp;rdquo;, Proc. of Eurospeech’97, Rhodes, Greece, pp. 2519-2522.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;過剰な平滑化&#34;&gt;過剰な平滑化&lt;/h3&gt;
&lt;p&gt;これはGMMに限った話ではないですが、GMMベースのFrame-by-Frameな声質変換の場合でいえば、変換後の特徴量は条件付き期待値を取ることになるので、まぁ常識的に考えて平滑化されますよね。&lt;/p&gt;
&lt;p&gt;これに対する解法としては、GV（Global Variance）を考慮する方法があります。これは戸田先生が提案されたものですね。&lt;/p&gt;
&lt;p&gt;さて、やってみます。wktk&lt;/p&gt;
&lt;h3 id=&#34;gvを考慮したトラジェクトリベースの声質変換の結果&#34;&gt;GVを考慮したトラジェクトリベースの声質変換の結果&lt;/h3&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;166&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/157371971&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;多少ましになった気もしなくもないけど、やっぱり音質はいまいちですね。そして計算量は激マシします。本当につらい。学会で聞いたGVありの音声はもっと改善してた気がするんだけどなー音声合成の話だけど。僕の実装が間違ってるんですかね…&lt;/p&gt;
&lt;h2 id=&#34;ムズすぎわろた&#34;&gt;ムズすぎわろた&lt;/h2&gt;
&lt;p&gt;以上、いくつか試しましたが、統計的声質変換は激ムズだということがわかりました。え、ここで終わるの？という感じですが、最後に一つ別の手法を紹介します。&lt;/p&gt;
&lt;h2 id=&#34;差分スペクトル補正に基づく統計的声質変換&#34;&gt;差分スペクトル補正に基づく統計的声質変換&lt;/h2&gt;
&lt;p&gt;これまでは、音声を基本周波数、非周期性成分、スペクトル包絡に分解して、スペクトル包絡を表す特徴量を変換し、変換後の特徴量を元に波形を再合成していました。ただ、よくよく考えると、そもそも基本周波数、非周期性成分をいじる必要がない場合であれば、わざわざ分解して再合成する必要なくね？声質の部分のみ変換するようなフィルタかけてやればよくね？という考えが生まれます。実は、そういったアイデアに基づく素晴らしい手法があります。それが、差分スペクトル補正に基づく声質変換です。&lt;/p&gt;
&lt;p&gt;詳細は、以下の予稿をどうぞ&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.phontron.com/paper/kobayashi14asj.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;小林 和弘, 戸田 智基, Graham Neubig, Sakriani Sakti, 中村 哲. &amp;ldquo;差分スペクトル補正に基づく統計的歌声声質変換&amp;rdquo;, 日本音響学会2014年春季研究発表会(ASJ). 東京. 2014年3月.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;では、やってみます。歌声ではなく話し声ですが。他の声質変換の結果とも聴き比べてみてください。&lt;/p&gt;
&lt;h3 id=&#34;差分スペクトル補正に基づく声質変換の結果&#34;&gt;差分スペクトル補正に基づく声質変換の結果&lt;/h3&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;166&#34; scrolling=&#34;no&#34; frameborder=&#34;no&#34; src=&#34;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/157362603&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;かなり音声の自然性は上がりましたね。これはヘタすると騙されるレベル。本当に素晴らしいです。しかも簡単にできるので、お勧めです。↑のは、GMMに基づくframe-by-frameな変換です。計算量も軽いので、リアルタイムでもいけますね。&lt;/p&gt;
&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;
&lt;p&gt;声質変換であれこれ試行錯誤して、ようやくスタートラインにたてた感があります。今後は新しい方法を考えようかなーと思ってます。&lt;/p&gt;
&lt;p&gt;おわり&lt;/p&gt;
&lt;h2 id=&#34;おわび&#34;&gt;おわび&lt;/h2&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; lang=&#34;en&#34;&gt;&lt;p&gt;お盆の間に学習ベースの声質変換のプログラム書く（宿題） &lt;a href=&#34;https://twitter.com/hashtag/%E5%AE%A3%E8%A8%80?src=hash&#34;&gt;#宣言&lt;/a&gt;&lt;/p&gt;&amp;mdash; 山本りゅういち (@r9y9) &lt;a href=&#34;https://twitter.com/r9y9/statuses/366928228465655808&#34;&gt;August 12, 2013&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;約1年かかりました……。本当に申し訳ありませんでした(´･_･`)&lt;/p&gt;
&lt;h2 id=&#34;追記&#34;&gt;追記&lt;/h2&gt;
&lt;p&gt;Twitterで教えてもらいました。トラジェクトリベースで学習も変換も行う研究もありました&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; lang=&#34;en&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://twitter.com/r9y9&#34;&gt;@r9y9&lt;/a&gt; つ トラジェクトリＧＭＭな特徴量変換 &lt;a href=&#34;http://t.co/kUn7bp9EUt&#34;&gt;http://t.co/kUn7bp9EUt&lt;/a&gt;&lt;/p&gt;&amp;mdash; 縄文人（妖精系研究者なのです） (@dicekicker) &lt;a href=&#34;https://twitter.com/dicekicker/statuses/485376823308455936&#34;&gt;July 5, 2014&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;ただ、これはトラジェクトリベースのパラメータ生成法の提案であって、トラジェクトリモデル自体を学習してるわけではないんだよなー。普通に考えると学習もトラジェクトリで考える方法があっていい気がするが、 &lt;del&gt;まだ見てないですね。&lt;/del&gt; ありました。追記参照&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>GOSSP - Go言語で音声信号処理</title>
      <link>https://r9y9.github.io/blog/2014/06/08/gossp-speech-signal-processing-for-go/</link>
      <pubDate>Sun, 08 Jun 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/06/08/gossp-speech-signal-processing-for-go/</guid>
      <description>&lt;h1 id=&#34;cからgoへ&#34;&gt;C++からGoへ&lt;/h1&gt;
&lt;p&gt;みなさん、C++で信号処理のアルゴリズムを書くのはつらいと思ったことはありませんか？C++で書くと速いのはいいけれど、いかんせん書くのが大変、コンパイルエラーは読みづらい、はたまたライブラリをビルドしようとしたら依存関係が上手く解決できず……そんな覚えはないでしょうか？謎のコンパイルエラーに悩みたくない、ガーベジコレクションほしい、Pythonのようにさくっと書きたい、型推論もほしい、でも動作は速い方がいい、そう思ったことはないでしょうか。&lt;/p&gt;
&lt;p&gt;そこでGoです。もちろん、そういった思いに完全に答えてくれるわけではありませんが、厳しいパフォーマンスを要求される場合でなければ、Goの方が良い場合も多いと僕は思っています。
とはいえ、まだ比較的新しい言語のため、ライブラリは少なく信号処理を始めるのも大変です。というわけで、僕がC++をやめてGoに移行したことを思い出し、Goでの信号処理の基礎と、今まで整備してきたGoでの音声信号処理ライブラリを紹介します。&lt;/p&gt;
&lt;p&gt;Goの良いところ/悪いところについては書きません。正直、本当は何の言語でもいいと思っていますが、僕はGoが好きなので、ちょっとでもGoで信号処理したいと思う人が増えるといいなーと思って書いてみます。&lt;/p&gt;
&lt;p&gt;あとで書きますが、僕が書いたコードで使えそうなものは、以下にまとめました。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/gossp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/gossp&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;基礎&#34;&gt;基礎&lt;/h1&gt;
&lt;h2 id=&#34;wavファイルの読み込み書き込み-wavhttpgodocorggithubcommjibsongo-dspwav&#34;&gt;Wavファイルの読み込み/書き込み &lt;a href=&#34;http://godoc.org/github.com/mjibson/go-dsp/wav&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[wav]&lt;/a&gt;&lt;/h2&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/speech_signal.png &#34;Speech signal example.&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;まずは音声ファイルの読み込みですね。wavファイルの読み込みさえできれば十分でしょう。&lt;/p&gt;
&lt;p&gt;これは、すでに有用なライブラリが存在します。&lt;a href=&#34;https://github.com/mjibson/go-dsp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GO-DSP&lt;/a&gt; とういデジタル信号処理のライブラリに含まれるwavパッケージを使いましょう。&lt;/p&gt;
&lt;p&gt;次のように書くことができます。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main

import (
	&amp;quot;fmt&amp;quot;
	&amp;quot;github.com/mjibson/go-dsp/wav&amp;quot;
	&amp;quot;log&amp;quot;
	&amp;quot;os&amp;quot;
)

func main() {
	// ファイルのオープン
	file, err := os.Open(&amp;quot;./test.wav&amp;quot;)
	if err != nil {
		log.Fatal(err)
	}

	// Wavファイルの読み込み
	w, werr := wav.ReadWav(file)
	if werr != nil {
		log.Fatal(werr)
	}

	// データを表示
	for i, val := range w.Data {
		fmt.Println(i, val)
	}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;簡単ですね。&lt;/p&gt;
&lt;p&gt;Goはウェブ周りの標準パッケージが充実しているので、以前&lt;a href=&#34;http://qiita.com/r9y9/items/35a1cf139332a3072fc8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;qiitaに書いた記事&lt;/a&gt;のように、wavファイルを受け取って何らかの処理をする、みたいなサーバも簡単に書くことができます&lt;/p&gt;
&lt;p&gt;wavファイルの書き込み＋ユーティリティを追加したかったので、僕は自分でカスタムしたパッケージを使っています。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/go-dsp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/go-dsp&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;高速フーリエ変換-fast-fourier-transform-fft-ffthttpgodocorggithubcommjibsongo-dspfft&#34;&gt;高速フーリエ変換 (Fast Fourier Transform; FFT) &lt;a href=&#34;http://godoc.org/github.com/mjibson/go-dsp/fft&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[fft]&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;言わずとしれたFFTです。音のスペクトルを求めるのに必須の処理です。で、Goではどうすればいいのか？ということですが、こちらもすでに有用なライブラリが存在します。&lt;a href=&#34;https://github.com/mjibson/go-dsp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GO-DSP&lt;/a&gt;に含まれる、fftパッケージを使いましょう。&lt;/p&gt;
&lt;p&gt;このfftパッケージは、go routinesを使って平行化されているため速いです。僕は、1次元のフーリエ変換以外めったに使いませんが、N次元のフーリエ変換をサポートしているのもこのライブラリのいいところです。&lt;/p&gt;
&lt;h3 id=&#34;参考&#34;&gt;参考&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://mattjibson.com/blog/2013/01/04/go-dsp-fft-performance-with-go-routines/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;go-dsp FFT performance with go routines · Matt Jibson&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;使い方は、とても簡単です。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main

import (
	&amp;quot;fmt&amp;quot;
	&amp;quot;github.com/mjibson/go-dsp/fft&amp;quot;
)

func main() {
	fmt.Println(fft.FFTReal([]float64{1, 2, 3, 4, 5, 6, 7, 8}))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;離散コサイン変換-discrete-cosine-transform-dct-dcthttpgodocorggithubcomr9y9gosspdct&#34;&gt;離散コサイン変換 (Discrete Cosine Transform; DCT) &lt;a href=&#34;http://godoc.org/github.com/r9y9/gossp/dct&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[dct]&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;DCTは、Mel-Frequency Cepstrum Coefficients (通称MFCC) 求めるのに必要な変換です。こちらは、残念ながら良さそうなライブラリがなかったので、自分で書きました。&lt;/p&gt;
&lt;p&gt;使い方はFFTとほとんど一緒です。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main

import (
	&amp;quot;fmt&amp;quot;
	&amp;quot;github.com/r9y9/gossp/dct&amp;quot;
)

func main() {
	y := dct.DCTOrthogonal([]float64{1, 2, 3, 4, 5, 6, 7, 8})
	fmt.Println(dct.IDCTOrthogonal(y)) // 直交変換では、逆変換すると元に戻る
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;さて、基本的なところは一旦ここまでです。次からは、少し音声寄りの信号処理手法の紹介です。&lt;/p&gt;
&lt;h1 id=&#34;時間周波数解析&#34;&gt;時間周波数解析&lt;/h1&gt;
&lt;h2 id=&#34;短時間フーリエ変換-short-time-fourier-transform-stft-stfthttpgodocorggithubcomr9y9gosspstft&#34;&gt;短時間フーリエ変換 (Short Time Fourier Transform; STFT) &lt;a href=&#34;http://godoc.org/github.com/r9y9/gossp/stft&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[stft]&lt;/a&gt;&lt;/h2&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/stft.png &#34;STFT spectrogram&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;STFTは、音声の時間周波数解析手法として定番の方法ですね。音声を可視化したり、何らかの認識アルゴリズムの特徴抽出に使ったり、まぁ色々です。&lt;/p&gt;
&lt;p&gt;次のようなコードを書くと、スペクトログラムが作れます&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main

import (
	&amp;quot;flag&amp;quot;
	&amp;quot;fmt&amp;quot;
	&amp;quot;github.com/r9y9/gossp&amp;quot;
	&amp;quot;github.com/r9y9/gossp/io&amp;quot;
	&amp;quot;github.com/r9y9/gossp/stft&amp;quot;
	&amp;quot;github.com/r9y9/gossp/window&amp;quot;
	&amp;quot;log&amp;quot;
	&amp;quot;math&amp;quot;
)

func main() {
	filename := flag.String(&amp;quot;i&amp;quot;, &amp;quot;input.wav&amp;quot;, &amp;quot;Input filename&amp;quot;)
	flag.Parse()

	w, werr := io.ReadWav(*filename)
	if werr != nil {
		log.Fatal(werr)
	}
	data := w.GetMonoData()

	s := &amp;amp;stft.STFT{
		FrameShift: int(float64(w.SampleRate) / 100.0), // 0.01 sec,
		FrameLen:   2048,
		Window:     window.CreateHanning(2048),
	}

	spectrogram := s.STFT(data)
	amplitudeSpectrogram, _ := gossp.SplitSpectrogram(spectrogram)
	PrintMatrixAsGnuplotFormat(amplitudeSpectrogram)
}

func PrintMatrixAsGnuplotFormat(matrix [][]float64) {
	fmt.Println(&amp;quot;#&amp;quot;, len(matrix[0]), len(matrix))
	for i, vec := range matrix {
		for j, val := range vec {
			fmt.Println(i, j, math.Log(val))
		}
		fmt.Println(&amp;quot;&amp;quot;)
	}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;上の画像は、gnuplotで表示したものです&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;set pm3d map
sp &amp;quot;spectrogram.txt&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;逆短時間フーリエ変換-inverse-short-time-fourier-transform-istft-stfthttpgodocorggithubcomr9y9gosspstft&#34;&gt;逆短時間フーリエ変換 (Inverse Short Time Fourier Transform; ISTFT) &lt;a href=&#34;http://godoc.org/github.com/r9y9/gossp/stft&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[stft]&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;ISTFTは、STFTの逆変換でスペクトログラムから時間領域の信号に戻すために使います。スペクトログラムを加工するような音源分離、ノイズ除去手法を使う場合には、必須の処理です。これはstftと同じパッケージ下にあります。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;	reconstructed := s.ISTFT(spectrogram)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;これで、スペクトログラムから音声を再構築することができます。&lt;/p&gt;
&lt;p&gt;逆変換の仕組みは、意外と難しかったりします。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.306.7858&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;D. W. Griffin and J. S. Lim, &amp;ldquo;Signal estimation from modified short-time Fourier transform,&amp;rdquo; IEEE Trans. ASSP, vol.32, no.2, pp.236–243, Apr. 1984.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://research.cs.tamu.edu/prism/lectures/sp/l6.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;L6: Short-time Fourier analysis and synthesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://yukara-13.hatenablog.com/entry/2013/11/17/210204&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pythonで短時間フーリエ変換（STFT）と逆変換 - 音楽プログラミングの超入門（仮）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;この辺を参考にしました。興味のある人は読んで見てください。&lt;/p&gt;
&lt;h2 id=&#34;連続ウェーブレット変換-continuous-wavelet-transform-cwt&#34;&gt;連続ウェーブレット変換 (Continuous Wavelet Transform; CWT)&lt;/h2&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/morlet_6_log.png &#34;Morlet Wavelet spectrogram&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;これは何回かブログで書きました。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://r9y9.github.io/blog/2013/10/20/continuous-wavelet-tranform/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FFTを使った連続ウェーブレット変換の高速化 - LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://r9y9.github.io/blog/2014/06/01/continuouos-wavelet-transform-types/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;連続ウェーブレット変換に使うマザーウェーブレット色々: Morlet, Paul, DOG - LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;コードは、テストがまだ通らないので開発中ということで…orz&lt;/p&gt;
&lt;h2 id=&#34;逆連続ウェーブレット変換-inverse-continuous-wavelet-transform-icwt&#34;&gt;逆連続ウェーブレット変換 (Inverse Continuous Wavelet Transform; ICWT)&lt;/h2&gt;
&lt;p&gt;連続ウェーブレット変換の逆変換ですね。これもけっこう難しいです。こちらもまだテストに通っていないので、開発中です。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://r9y9.github.io/blog/2013/10/21/signal-reconstruction-using-invere-cwt/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;逆連続ウェーブレット変換による信号の再構成 - LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;さて、この辺でまた一区切りです。次は、より音声に特化した信号処理手法を紹介します。&lt;/p&gt;
&lt;p&gt;※以降紹介するもののうち、多くは&lt;a href=&#34;http://sp-tk.sourceforge.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SPTK&lt;/a&gt;のGo-portになっていて、一部はcgoを使ってラップしただけです（後々はpure goにしたいけれど、特にメルケプストラム分析あたりは難しいのでできていません）&lt;/p&gt;
&lt;h1 id=&#34;音声分析系&#34;&gt;音声分析系&lt;/h1&gt;
&lt;h2 id=&#34;基本周波数推定-f0httpgodocorggithubcomr9y9gosspf0&#34;&gt;基本周波数推定 &lt;a href=&#34;http://godoc.org/github.com/r9y9/gossp/f0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[f0]&lt;/a&gt;&lt;/h2&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/arayuru_f0.png &#34;Fundamental frequency trajectory example.&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;ざっくり言えば音の高さを求める方法ですね。一応、音声に特化した方法をいくつか使えるようにしました。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://audition.ens.fr/adc/pdf/2002_JASA_YIN.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A. de Cheveigne and H. Kawahara. YIN, a fundamental frequency estimator for speech and music. J. Acoust. Soc. Am., 111(4):1917–1930, 2002.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cise.ufl.edu/~acamacho/publications/dissertation.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A. Camacho. SWIPE: A sawtooth waveform inspired pitch estimator for speech and music. PhD thesis, University of Florida, 2007.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ただしYINはもどきです。&lt;/p&gt;
&lt;p&gt;以前、&lt;a href=&#34;https://github.com/r9y9/go-world&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GO-WORLD&lt;/a&gt;という音声分析合成系WORLDのGoラッパーを書いたので、それを使えばF0推定手法Dioが使えます。&lt;/p&gt;
&lt;h3 id=&#34;参考-1&#34;&gt;参考&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://r9y9.github.io/blog/2014/03/22/go-world/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;音声分析変換合成システムWORLDのGoラッパーを書いた - LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;メルケプストラム分析-mgcephttpgodocorggithubcomr9y9gosspmgcep&#34;&gt;メルケプストラム分析 &lt;a href=&#34;http://godoc.org/github.com/r9y9/gossp/mgcep&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[mgcep]&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;音声合成界隈ではよく聞くメルケプストラム（※MFCCとは異なります）を求めるための分析手法です。メルケプストラムは、HMM（Hidden Markov Models; 隠れマルコフモデル）音声合成や統計的声質変換において、声道特徴（いわゆる、声質）のパラメータ表現としてよく使われています。メルケプストラムの前に、LPCとかPARCORとか色々あるのですが、現在のHMM音声合成で最もよく使われているのはメルケプストラムな気がするので、メルケプストラム分析があれば十分な気がします。&lt;/p&gt;
&lt;p&gt;これは、SPTKをcgoを使ってラップしました&lt;/p&gt;
&lt;h3 id=&#34;参考-2&#34;&gt;参考&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://ci.nii.ac.jp/naid/40004638236/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;徳田恵一, 小林隆夫, 深田俊明, 斎藤博徳, 今井 聖, “メルケプストラムをパラメータとする音声のスペクトル推定,” 信学論(A), vol.J74-A, no.8, pp.1240–1248, Aug. 1991.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;メル一般化ケプストラム分析-mgcephttpgodocorggithubcomr9y9gosspmgcep&#34;&gt;メル一般化ケプストラム分析 &lt;a href=&#34;http://godoc.org/github.com/r9y9/gossp/mgcep&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[mgcep]&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;メル一般化ケプストラム分析は、その名の通りメルケプストラム分析を一般化したものです。メルケプストラム分析はもちろん、LPCも包含します（詳細は、参考文献をチェックしてみてください）。論文をいくつかあさっている限り、あんまり使われていない気はしますが、これもSPTKをラップしてGoから使えるようにしました。メルケプストラム分析もメル一般化ケプストラム分析に含まれるので、mgcepという一つのパッケージにしました。&lt;/p&gt;
&lt;h3 id=&#34;参考-3&#34;&gt;参考&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.utdallas.edu/~john.hanse/nPublications/JP-55-SpeechComm-Yapanel-Hansen-PMVDR-Feb08.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tokuda, K., Masuko, T., Kobayashi, T., Imai, S., 1994. Mel-generalized Cepstral Analysis-A Uniﬁed Approach to Speech Spectral Estimation, ISCA ICSLP-94: Inter. Conf. Spoken Lang. Proc., Yokohama, Japan, pp. 1043–1046.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;音声合成系&#34;&gt;音声合成系&lt;/h1&gt;
&lt;h2 id=&#34;励起信号の生成-excitehttpgodocorggithubcomr9y9gosspexcite&#34;&gt;励起信号の生成 &lt;a href=&#34;http://godoc.org/github.com/r9y9/gossp/excite&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[excite]&lt;/a&gt;&lt;/h2&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/pulse_excite.png &#34;Exciation eignal.&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;p&gt;SPTKのexciteのGo実装です。いわゆるPulseExcitationという奴ですね。非周期成分まったく考慮しない単純な励起信号です。&lt;/p&gt;
&lt;p&gt;高品質な波形合成が必要な場合は、WORLDやSTRAIGHTを使うのが良いです。&lt;/p&gt;
&lt;h2 id=&#34;mlsa-mel-log-spectrum-approximation-デジタルフィルタ-vocoderhttpgodocorggithubcomr9y9gosspvocoder&#34;&gt;MLSA (Mel Log Spectrum Approximation) デジタルフィルタ &lt;a href=&#34;http://godoc.org/github.com/r9y9/gossp/vocoder&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[vocoder]&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;MLSAフィルタは、メルケプストラムと励起信号から音声波形を合成するためのデジタルフィルタです。HMM音声合成の波形合成部で使われています（今もきっと）。Pure goで書き直しました。&lt;/p&gt;
&lt;p&gt;昔、C++でも書いたことあります。&lt;/p&gt;
&lt;h3 id=&#34;参考-4&#34;&gt;参考&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://r9y9.github.io/blog/2013/12/01/mlsa-filter-with-c-plus-plus/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MLSA digital filter のC++実装 - LESS IS MORE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;mglsa-mel-genaralized-log-spectrum-approximation-デジタルフィルタ-vocoderhttpgodocorggithubcomr9y9gosspvocoder&#34;&gt;MGLSA (Mel Genaralized-Log Spectrum Approximation) デジタルフィルタ &lt;a href=&#34;http://godoc.org/github.com/r9y9/gossp/vocoder&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[vocoder]&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;MGLSAフィルタは、メル一般化ケプストラムから波形を合成するためのデジタルフィルタですね。これも pure goで書きました。&lt;/p&gt;
&lt;h2 id=&#34;sptkの再実装について&#34;&gt;&lt;strong&gt;※SPTKの再実装について&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;SPTKの実装をGoで書き直したものについては、SPTKの実装と結果が一致するかどうかを確認するテストを書いてあります。よって、誤った結果になるということは（計算誤差が影響する場合を除き）基本的にないので、お気になさらず。&lt;/p&gt;
&lt;h2 id=&#34;高品質な音声分析変換合成系-world-go-worldhttpgodocorggithubcomr9y9go-world&#34;&gt;高品質な音声分析変換合成系 WORLD &lt;a href=&#34;http://godoc.org/github.com/r9y9/go-world&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[go-world]&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://r9y9.github.io/blog/2014/03/22/go-world/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;音声分析変換合成システムWORLDのGoラッパーを書いた - LESS IS MORE&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;以前WORLDのGoラッパーを書いたので、色々使えると思います。統計ベースの音声合成とか、声質変換とか。僕は声質変換に使おうと思ってラップしました。&lt;/p&gt;
&lt;h1 id=&#34;おわりに&#34;&gt;おわりに&lt;/h1&gt;
&lt;p&gt;長々と書きましたが、Go言語での信号処理の基礎と、今まで整備してきた音声信号処理ライブラリを簡単に紹介しました。僕が書いたものは、まとめてGithubで公開しています。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/gossp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/gossp&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;使ってももらって、あわよくばバグとか報告してもらって、改善していければいいなーというのと、あとGithubのissue管理便利だし使おうと思ってGithubに上げました。&lt;/p&gt;
&lt;p&gt;みなさん、Goで音声信号処理始めてみませんか？&lt;/p&gt;
&lt;h1 id=&#34;余談&#34;&gt;余談&lt;/h1&gt;
&lt;h2 id=&#34;pythonではダメなのその他言語は&#34;&gt;Pythonではダメなの？その他言語は？&lt;/h2&gt;
&lt;p&gt;なんでGoなの？と思う人がいると思います。冒頭にも書いたとおり、正直好きなのにすればいいですが、適当に書いて速いのがいいならC++だし、型を意識せずさくっと書きたいならPythonだし、そこそこ速くて型があって型推論もあって、とかだったらGoがいいかなと僕は思います。&lt;/p&gt;
&lt;p&gt;Goの特徴（≒良さ）ついては、&lt;a href=&#34;http://www.slideshare.net/ymotongpoo/20130228-gobp-study-66-16830134&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;20130228 Goノススメ（BPStudy #66） | SlideShare&lt;/a&gt;
の11枚目が僕にはドンピシャです。&lt;/p&gt;
&lt;p&gt;numpy, scipy, matplotlib, scikit-learnあたりが最強すぎるので、僕はpythonも良く使います。&lt;/p&gt;
&lt;h2 id=&#34;きっかけ&#34;&gt;きっかけ&lt;/h2&gt;
&lt;p&gt;この記事を書いたきっかけは、友人にGoをおすすめしまくっていたのに全然聞いてくれなかったからでした。Goでも信号処理はできるよ&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>音声分析変換合成システムWORLDのGoラッパーを書いた</title>
      <link>https://r9y9.github.io/blog/2014/03/22/go-world/</link>
      <pubDate>Sat, 22 Mar 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/03/22/go-world/</guid>
      <description>&lt;h2 id=&#34;音声分析変換合成システムworld&#34;&gt;音声分析変換合成システムWORLD&lt;/h2&gt;
&lt;p&gt;WORLDとは、山梨大学の森勢先生が作られている高品質な音声分析変換合成システムです。非常に高品質かつ高速に動作するのが良い所です。詳細は以下のURLへ&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://ml.cs.yamanashi.ac.jp/world/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://ml.cs.yamanashi.ac.jp/world/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;オリジナルはC+＋で書かれていますが、Goからも使えるようにラッパーを書きました。非常にいいソフトウェアなので、もしよろしければどうぞ&lt;/p&gt;
&lt;h2 id=&#34;go-world&#34;&gt;GO-WORLD&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/go-world&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/go-world&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;使い方について、ほんの少し解説を書きます&lt;/p&gt;
&lt;p&gt;※ubuntu12.04でのみ動作確認してます。&lt;/p&gt;
&lt;h2 id=&#34;準備&#34;&gt;準備&lt;/h2&gt;
&lt;h3 id=&#34;1-worldのインストール&#34;&gt;1. WORLDのインストール&lt;/h3&gt;
&lt;p&gt;まずWORLDをインストールする必要があります。公式のパッケージではinstallerに相当するものがなかったので、作りました&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/world&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/world&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt; ./waf configure &amp;amp;&amp;amp; ./waf
 sudo ./waf install
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;でインストールできます。&lt;/p&gt;
&lt;p&gt;なお、WORLDは最新版ではなく0.1.2としています。最新版にすると自分の環境でビルドコケてしまったので…&lt;/p&gt;
&lt;h3 id=&#34;2-go-worldのインストール&#34;&gt;2. GO-WORLDのインストール&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;go get github.com/r9y9/go-world
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;簡単ですね！&lt;/p&gt;
&lt;h2 id=&#34;使い方&#34;&gt;使い方&lt;/h2&gt;
&lt;h3 id=&#34;1-インポートする&#34;&gt;1. インポートする&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;import &amp;quot;github.com/r9y9/go-world&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;2-worldのインスタンスを作る&#34;&gt;2. worldのインスタンスを作る&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;w := world.New(sampleRate, framePeriod) // e.g. (44100, 5)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;3-好きなworldのメソッドを呼ぶ&#34;&gt;3. 好きなworldのメソッドを呼ぶ&lt;/h3&gt;
&lt;h4 id=&#34;基本周波数の推定-dio&#34;&gt;基本周波数の推定: Dio&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;timeAxis, f0 := w.Dio(input, w.NewDioOption()) // default option is used
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;スペクトル包絡の推定-star&#34;&gt;スペクトル包絡の推定: Star&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;spectrogram := w.Star(input, timeAxis, f0)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;励起信号の推定-platinum&#34;&gt;励起信号の推定: Platinum&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;residual := w.Platinum(input, timeAxis, f0, spectrogram)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;パラメータから音声の再合成-synthesis&#34;&gt;パラメータから音声の再合成: Synthesis&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;synthesized := w.Synthesis(f0, spectrogram, residual, len(input))
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;使い方例&#34;&gt;使い方例.&lt;/h2&gt;
&lt;p&gt;音声（wavファイル）を分析して、パラメータから音声を再合成する例を紹介します。80行弱と少し長いですがはっつけておきます&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main

import (
	&amp;quot;flag&amp;quot;
	&amp;quot;fmt&amp;quot;
	&amp;quot;github.com/mjibson/go-dsp/wav&amp;quot;
	&amp;quot;github.com/r9y9/go-world&amp;quot;
	&amp;quot;log&amp;quot;
	&amp;quot;os&amp;quot;
)

var defaultDioOption = world.DioOption{
	F0Floor:          80.0,
	F0Ceil:           640.0,
	FramePeriod:      5,
	ChannelsInOctave: 4.0,
	Speed:            6,
}

// 音声を基本周波数、スペクトル包絡、励起信号の三つに分解したあと、再合成します
func worldExample(input []float64, sampleRate int) []float64 {
	w := world.New(sampleRate, defaultDioOption.FramePeriod)

	// 1. Fundamental frequency
	timeAxis, f0 := w.Dio(input, defaultDioOption)

	// 2. Spectral envelope
	spectrogram := w.Star(input, timeAxis, f0)

	// 3. Excitation spectrum
	residual := w.Platinum(input, timeAxis, f0, spectrogram)

	// 4. Synthesis
	return w.Synthesis(f0, spectrogram, residual, len(input))
}

// 音声を基本周波数、スペクトル包絡、非周期成分の三つに分解したあと、再合成します
func worldExampleAp(input []float64, sampleRate int) []float64 {
	w := world.New(sampleRate, defaultDioOption.FramePeriod)

	// 1. Fundamental frequency
	timeAxis, f0 := w.Dio(input, defaultDioOption)

	// 2. Spectral envelope
	spectrogram := w.Star(input, timeAxis, f0)

	// 3. Apiriodiciy
	apiriodicity, targetF0 := w.AperiodicityRatio(input, f0)

	// 4. Synthesis
	return w.SynthesisFromAperiodicity(f0, spectrogram, apiriodicity, targetF0, len(input))
}

func GetMonoDataFromWavData(data [][]int) []float64 {
	y := make([]float64, len(data))
	for i, val := range data {
		y[i] = float64(val[0])
	}
	return y
}

func main() {
	ifilename := flag.String(&amp;quot;i&amp;quot;, &amp;quot;default.wav&amp;quot;, &amp;quot;Input filename&amp;quot;)
	flag.Parse()

	// Read wav data
	file, err := os.Open(*ifilename)
	if err != nil {
		log.Fatal(err)
	}
	defer file.Close()

	w, werr := wav.ReadWav(file)
	if werr != nil {
		log.Fatal(werr)
	}
	input := GetMonoDataFromWavData(w.Data)
	sampleRate := int(w.SampleRate)

	synthesized := worldExample(input, sampleRate)
	// synthesized := worldExampleAp(input, sampleRate)

	for i, val := range synthesized {
		fmt.Println(i, val)
	}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Goだとメモリ管理きにしなくていいしそこそこ速いし読みやすいし書きやすいし楽でいいですね（信者&lt;/p&gt;
&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;GoはC++ほど速くはないですが、C++の何倍も書きやすいし読みやすい（メンテしやすい）ので、個人的にオススメです（パフォーマンスが厳しく要求される場合には、C++の方がいいかもしれません）&lt;/li&gt;
&lt;li&gt;WORLD良いソフトウェアなので使いましょう&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ちなみに&#34;&gt;ちなみに&lt;/h2&gt;
&lt;p&gt;元はと言えば、オレオレ基本周波数推定（YINもどき）が微妙に精度悪くて代替を探していたとき、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SPTKのRAPTかSWIPE使おうかな…&lt;/li&gt;
&lt;li&gt;RAPTもSWIPEもSPTK.hにインタフェースがない…&lt;/li&gt;
&lt;li&gt;うわRAPTのコード意味わからん&lt;/li&gt;
&lt;li&gt;SWIPEのコードまじ謎&lt;/li&gt;
&lt;li&gt;後藤さんのPreFest実装しよう&lt;/li&gt;
&lt;li&gt;あれ上手くいかない…orz&lt;/li&gt;
&lt;li&gt;どうしようかな…&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;となっていたときに、森勢先生が書いたと思われる以下の文献を見つけて、&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://crestmuse.jp/handbookMI/pdf/2_2_PitchExtraction_Morise.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2-2 基本周波数推定（歌声研究に関する視点から）&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本方法は，低域に雑音が存在する音声に対する推定は困難であるが，低域の雑音が存在しない音声の場合，SWIPE′ や NDF と実質的に同等の性能を達成しつつ，計算時間を SWIPE′の 1/42, NDF の 1/80 にまで低減可能である．&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;あぁworld使おう（白目&lt;/p&gt;
&lt;p&gt;となり、ラッパーを書くにいたりましたとさ、おしまい&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Goで音声信号処理をしたいのでSPTKのGoラッパーを書く</title>
      <link>https://r9y9.github.io/blog/2014/02/10/sptk-go-wrapper/</link>
      <pubDate>Mon, 10 Feb 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/02/10/sptk-go-wrapper/</guid>
      <description>&lt;p&gt;2014/07/22 追記：
パッケージの一部として書きました（&lt;a href=&#34;http://r9y9.github.io/blog/2014/06/08/gossp-speech-signal-processing-for-go/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GOSSP - Go言語で音声信号処理 - LESS IS MORE&lt;/a&gt;を参照）。
SPTKのラップも含め、いくつかGoで信号処理アルゴリズムを実装したので、お求めの方はどうぞ&lt;/p&gt;
&lt;p&gt;&amp;ndash;&lt;/p&gt;
&lt;p&gt;Goが最近オススメです（n度目&lt;/p&gt;
&lt;p&gt;Goで音声信号処理をしたいけど、全部一から書くのは大変だし、既存の資産は出来るだけ再利用したい。というわけで、C言語製の&lt;a href=&#34;http://sp-tk.sourceforge.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SPTK&lt;/a&gt; をGoから使えるようにする&lt;/p&gt;
&lt;h2 id=&#34;cgo&#34;&gt;cgo&lt;/h2&gt;
&lt;p&gt;GoにはC言語のライブラリを使うには、cgoというパッケージを使えばできる。使い方は、公式のページ等を見るといいと思う &lt;a href=&#34;https://golang.org/cmd/cgo/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://golang.org/cmd/cgo/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Cの関数や変数などには、 &lt;code&gt;C.&lt;/code&gt; でアクセスできる&lt;/p&gt;
&lt;h2 id=&#34;ラッパー&#34;&gt;ラッパー&lt;/h2&gt;
&lt;p&gt;例えば以下のように書く。MFCCの計算を例に上げる。必要に応じで&lt;code&gt;SPTK.h&lt;/code&gt;に定義されている関数をラップする&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package sptk

// #cgo pkg-config: SPTK
// #include &amp;lt;stdio.h&amp;gt;
// #include &amp;lt;SPTK/SPTK.h&amp;gt;
import &amp;quot;C&amp;quot;

func MFCC(audioBuffer []float64, sampleRate int, alpha, eps float64, wlng, flng, m, n, ceplift int, dftmode, usehamming bool) []float64 {
	// Convert go bool to C.Boolean (so annoying..
	var dftmodeInGo, usehammingInGo C.Boolean
	if dftmode {
		dftmodeInGo = 1
	} else {
		dftmodeInGo = 0
	}
	if usehamming {
		usehammingInGo = 1
	} else {
		usehammingInGo = 0
	}

	resultBuffer := make([]float64, m)
	C.mfcc((*_Ctype_double)(&amp;amp;audioBuffer[0]), (*_Ctype_double)(&amp;amp;resultBuffer[0]), C.double(sampleRate), C.double(alpha), C.double(eps), C.int(wlng), C.int(flng), C.int(m), C.int(n), C.int(ceplift), dftmodeInGo, usehammingInGo)
	return resultBuffer
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;このパッケージを使う前に、 &lt;a href=&#34;https://github.com/r9y9/SPTK&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/SPTK&lt;/a&gt; を使ってSPTKをインストールする。本家のを使ってもいいですが、その場合は #cgo の設定が変わると思います。公式のSPTK、pkg-configに対応してくれんかな…&lt;/p&gt;
&lt;p&gt;最初は、LDFLAGS つけ忘れてて、symbol not foundってなってつらまった。次回から気をつけよう&lt;/p&gt;
&lt;p&gt;SPTKの、特に（メル）ケプストラム分析当たりは本当に難しいので、論文読んで実装するのも大変だし中身がわからなくてもラップする方が合理的、という結論に至りました。簡単なもの（例えば、メルケプからMLSA filterの係数への変換とか）は、依存関係を少なくするためにもGo nativeで書きなおした方がいいです&lt;/p&gt;
&lt;p&gt;コードは気が向いたら上げる&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Goで信号処理</title>
      <link>https://r9y9.github.io/blog/2014/01/27/start-coding-go-msptools/</link>
      <pubDate>Mon, 27 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2014/01/27/start-coding-go-msptools/</guid>
      <description>&lt;p&gt;最近Go言語を触っていて、これがなかなかいい感じ。そこそこ速いので、信号処理や機械学習も行けると思う&lt;/p&gt;
&lt;h2 id=&#34;goの良い所&#34;&gt;Goの良い所&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;デフォでたくさん便利なパッケージがある。http, json, os, &amp;hellip;&lt;/li&gt;
&lt;li&gt;パッケージのインストールはとても簡単。go getするだけ&lt;/li&gt;
&lt;li&gt;デフォでテストの枠組みがある&lt;/li&gt;
&lt;li&gt;gofmtでコードのformattingしてくれるので書き方で迷わなくて良い&lt;/li&gt;
&lt;li&gt;使わないパッケージをimportするとコンパイルエラーになるし自然と疎結合なコードを書くようになる&lt;/li&gt;
&lt;li&gt;並列処理を言語レベルでサポート&lt;/li&gt;
&lt;li&gt;GCあるのでメモリ管理なんてしなくていい&lt;/li&gt;
&lt;li&gt;全般的にC++より書きやすい（ここ重要）&lt;/li&gt;
&lt;li&gt;そこそこ速い（C++よりは遅いけど）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ホントはPythonでさくっと書きたいけどパフォーマンスもほしいからC++で書くかー（嫌だけど）。と思ってた自分にはちょうどいい&lt;/p&gt;
&lt;h2 id=&#34;goの悪い所主にcと比べて&#34;&gt;Goの悪い所（主にC++と比べて）&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ちょっと遅い。さっと試したウェーブレット変換は、1.5倍くらい遅かった気がする（うろ覚え）&lt;/li&gt;
&lt;li&gt;C++やpythonに比べるとライブラリは少ない&lt;/li&gt;
&lt;li&gt;言語仕様とかそのへんが優れてるかどうかは判断つきませんごめんなさい&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;go-msptools&#34;&gt;Go-msptools&lt;/h1&gt;
&lt;p&gt;2014/07/22 追記：
Go-msptoolsはGOSSPに吸収されました。（&lt;a href=&#34;https://r9y9.github.io/blog/2014/06/08/gossp-speech-signal-processing-for-go/&#34;&gt;GOSSP - Go言語で音声信号処理 - LESS IS MORE&lt;/a&gt;を参照）&lt;/p&gt;
&lt;h2 id=&#34;おまけ音の信号処理に役立ちそうなライブラリ&#34;&gt;おまけ：音の信号処理に役立ちそうなライブラリ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mjibson/go-dsp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;go-dsp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://code.google.com/p/portaudio-go/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;portaudio-go&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>MLSA digital filter のC&#43;&#43;実装</title>
      <link>https://r9y9.github.io/blog/2013/12/01/mlsa-filter-with-c-plus-plus/</link>
      <pubDate>Sun, 01 Dec 2013 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2013/12/01/mlsa-filter-with-c-plus-plus/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://r9y9.github.io/blog/2013/09/23/mlsa-filter-wakaran/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MLSAフィルタわからん&lt;/a&gt;という記事を書いて早2ヶ月、ようやく出来た。&lt;/p&gt;
&lt;p&gt;Mel-log spectrum approximate (MLSA) filterというのは、対数振幅スペクトルを近似するようにメルケプストラムから直接音声を合成するデジタルフィルタです。&lt;a href=&#34;http://sp-tk.sourceforge.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SPTK&lt;/a&gt;のmlsa filterと比較して完全に計算結果が一致したので、間違ってはないはず。MLSAフィルタを使ってメルケプから音声合成するプログラムをC++で自分で書きたいという稀有な人であれば、役に立つと思います。基本的に、SPTKのmlsa filterの再実装です。&lt;/p&gt;
&lt;h1 id=&#34;mlsa_filterh&#34;&gt;mlsa_filter.h&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://gist.github.com/r9y9/7735120&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gist.github.com/r9y9/7735120&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;#pragma once

#include &amp;lt;cmath&amp;gt;
#include &amp;lt;memory&amp;gt;
#include &amp;lt;vector&amp;gt;
#include &amp;lt;cassert&amp;gt;

namespace sp {

/**
 * MLSA BASE digital filter (Mel-log Spectrum Approximate digital filter)
 */
class mlsa_base_filter {
public:
  mlsa_base_filter(const int order, const double alpha);

  template &amp;lt;class Vector&amp;gt;
  double filter(const double x, const Vector&amp;amp; b);

 private:
  mlsa_base_filter();

  double alpha_;
  std::vector&amp;lt;double&amp;gt; delay_;
};

mlsa_base_filter::mlsa_base_filter(const int order, const double alpha)
: alpha_(alpha),
  delay_(order+1)
{
}

template &amp;lt;class Vector&amp;gt;
double mlsa_base_filter::filter(const double x, const Vector&amp;amp; b)
{
  double result = 0.0;

  delay_[0] = x;
  delay_[1] = (1.0-alpha_*alpha_)*delay_[0] + alpha_*delay_[1];

  for (size_t i = 2; i &amp;lt; b.size(); ++i) {
    delay_[i] = delay_[i] + alpha_*(delay_[i+1]-delay_[i-1]);
    result += delay_[i] * b[i];
  }

  // special case
  // TODO: other solution?
  if (b.size() == 2) {
    result += delay_[1] * b[1];
  }

  // t &amp;lt;- t+1 in time
  for (size_t i = delay_.size()-1; i &amp;gt; 1; --i) {
    delay_[i] = delay_[i-1];
  }

  return result;
}

/**
 * MLSA digital filter cascaded
 */
class mlsa_base_cascaded_filter {
 public:
  mlsa_base_cascaded_filter(const int order,
			    const double alpha,
			    const int n_pade);

  template &amp;lt;class Vector&amp;gt;
  double filter(const double x, const Vector&amp;amp; b);

 private:
  mlsa_base_cascaded_filter();

  std::vector&amp;lt;std::unique_ptr&amp;lt;mlsa_base_filter&amp;gt;&amp;gt; base_f_; // cascadad filters
  std::vector&amp;lt;double&amp;gt; delay_;
  std::vector&amp;lt;double&amp;gt; pade_coef_;
};

mlsa_base_cascaded_filter::mlsa_base_cascaded_filter(const int order,
						     const double alpha,
						     const int n_pade)
  : delay_(n_pade + 1),
  pade_coef_(n_pade + 1)
{
  using std::unique_ptr;

  if (n_pade != 4 &amp;amp;&amp;amp; n_pade != 5) {
    std::cerr &amp;lt;&amp;lt; &amp;quot;The number of pade approximations must be 4 or 5.&amp;quot;
	      &amp;lt;&amp;lt; std::endl;
  }
  assert(n_pade == 4 || n_pade == 5);

  for (int i = 0; i &amp;lt;= n_pade; ++i) {
    mlsa_base_filter* p = new mlsa_base_filter(order, alpha);
    base_f_.push_back(unique_ptr&amp;lt;mlsa_base_filter&amp;gt;(p));
  }

  if (n_pade == 4) {
    pade_coef_[0] = 1.0;
    pade_coef_[1] = 4.999273e-1;
    pade_coef_[2] = 1.067005e-1;
    pade_coef_[3] = 1.170221e-2;
    pade_coef_[4] = 5.656279e-4;
  }

  if (n_pade == 5) {
    pade_coef_[0] = 1.0;
    pade_coef_[1] = 4.999391e-1;
    pade_coef_[2] = 1.107098e-1;
    pade_coef_[3] = 1.369984e-2;
    pade_coef_[4] = 9.564853e-4;
    pade_coef_[5] = 3.041721e-5;
  }
}

template &amp;lt;class Vector&amp;gt;
double mlsa_base_cascaded_filter::filter(const double x, const Vector&amp;amp; b)
{
  double result = 0.0;
  double feed_back = 0.0;

  for (size_t i = pade_coef_.size()-1; i &amp;gt;= 1; --i) {
    delay_[i] = base_f_[i]-&amp;gt;filter(delay_[i-1], b);
    double v = delay_[i] * pade_coef_[i];
    if (i % 2 == 1) {
      feed_back += v;
    } else {
      feed_back -= v;
    }
    result += v;
  }

  delay_[0] = feed_back + x;
  result += delay_[0];

  return result;
}

/**
 * MLSA digital filter (Mel-log Spectrum Approximate digital filter)
 * The filter consists of two stage cascade filters
 */
class mlsa_filter {
 public:
  mlsa_filter(const int order, const double alpha, const int n_pade);
 ~mlsa_filter();

 template &amp;lt;class Vector&amp;gt;
 double filter(const double x, const Vector&amp;amp; b);

 private:
 mlsa_filter();

  double alpha_;
  std::unique_ptr&amp;lt;mlsa_base_cascaded_filter&amp;gt; f1_; // first stage
  std::unique_ptr&amp;lt;mlsa_base_cascaded_filter&amp;gt; f2_; // second stage
};

mlsa_filter::mlsa_filter(const int order,
			 const double alpha,
			 const int n_pade)
  : alpha_(alpha),
  f1_(new mlsa_base_cascaded_filter(2, alpha, n_pade)),
  f2_(new mlsa_base_cascaded_filter(order, alpha, n_pade))
{
}

mlsa_filter::~mlsa_filter()
{
}

template &amp;lt;class Vector&amp;gt;
double mlsa_filter::filter(const double x, const Vector&amp;amp; b)
{
  // 1. First stage filtering
  Vector b1 = {0, b[1]};
  double y = f1_-&amp;gt;filter(x, b1);

  // 2. Second stage filtering
  double result = f2_-&amp;gt;filter(y, b);

  return result;
}

} // end namespace sp
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;使い方&#34;&gt;使い方&lt;/h1&gt;
&lt;p&gt;mlsa_filter.hをインクルードすればおｋ&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#include &amp;quot;mlsa_filter.h&amp;quot;

// セットアップ
const double alpha = 0.42;
const int order = 30;
const int n_pade = 5;
sp::mlsa_filter mlsa_f(order, alpha, n_pade);

...
// MLSA フィルタリング
出力一サンプル = mlsa_f.filter(入力一サンプル, フィルタ係数);
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;何で再実装したのか&#34;&gt;何で再実装したのか&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;mlsa filterをC++的なインタフェースで使いたかった&lt;/li&gt;
&lt;li&gt;コード見たらまったく意味がわからなくて、意地でも理解してやろうと思った&lt;/li&gt;
&lt;li&gt;反省はしている&lt;/li&gt;
&lt;li&gt;知り合いの声質変換やってる方がMLSAフィルタを波形合成に使ってるっていうし、ちょっとやってみようかなって&lt;/li&gt;
&lt;li&gt;あと最近音声合成の低レベルに手をつけようとと思ってたし勉強にもなるかなって&lt;/li&gt;
&lt;li&gt;思ったんだ……んだ…だ…&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;車輪の再開発はあんま良くないと思ってるけど許して。
誰かがリファクタせないかんのだ&lt;/p&gt;
&lt;h1 id=&#34;感想&#34;&gt;感想&lt;/h1&gt;
&lt;p&gt;SPTKのmlsa filterは、正直に言うとこれまで読んできたコードの中で一二を争うほど難解でした（いうてC言語はあまり読んできてないので、Cだとこれが普通なのかもしれないけど）。特に、元コードの d: delayという変数の使われ方が複雑過ぎて、とても読みにくくございました。MLSAフィルタは複数のbase filterのcascade接続で表されるわけだけど、それぞれの遅延が一つのdという変数で管理されていたのです。つまり、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;d[1] ~ d[5] までは、あるフィルタの遅延&lt;/li&gt;
&lt;li&gt;d[6] ~ d[11] までは、別のフィルタの遅延&lt;/li&gt;
&lt;li&gt;d[12] ~ にはまた別のフィルタの遅延&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;という感じです。&lt;/p&gt;
&lt;p&gt;改善しようと思って、base filterというクラスを作ってそのクラスの状態として各フィルタの遅延を持たせて、見通しを良くしました&lt;/p&gt;
&lt;h2 id=&#34;さいごに&#34;&gt;さいごに&lt;/h2&gt;
&lt;p&gt;MLSAフィルタ、難しいですね（小並感&lt;/p&gt;
&lt;p&gt;いつかリアルタイム声質変換がやってみたいので、それに使う予定（worldを使うことになるかもしれんけど）。戸田先生当たりがやってる声質変換を一回真似してみたいと思ってる&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SPTKをC&#43;&#43;から使えるようにする</title>
      <link>https://r9y9.github.io/blog/2013/12/01/sptk-with-waf/</link>
      <pubDate>Sun, 01 Dec 2013 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2013/12/01/sptk-with-waf/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://sp-tk.sourceforge.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;音声信号処理ツールキットSPTK&lt;/a&gt;をC++から使おうと思ったら意外とハマってしまったので、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C++から使えるようにC++コンパイラでコンパイルできるようにした&lt;/li&gt;
&lt;li&gt;使いやすいようにwafを組み込みんだ&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;リポジトリ: &lt;a href=&#34;https://github.com/r9y9/SPTK&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/r9y9/SPTK&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;というわけで、使い方について簡単に書いておく&lt;/p&gt;
&lt;h1 id=&#34;sptk-について&#34;&gt;SPTK について&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;SPTKを使うと何ができるか: &lt;a href=&#34;http://aidiary.hatenablog.com/entry/20120701/1341126474&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SPTKの使い方 (1) インストール・波形描画・音声再生 | 人工知能に関する断創録&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SPTKとは: &lt;a href=&#34;[http://sp-tk.sourceforge.net/]&#34;&gt;Speech Signal Processing Toolkit (SPTK)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;sptk-with-waf&#34;&gt;SPTK with waf&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/r9y9/SPTK&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SPTK with waf&lt;/a&gt;は、SPTKをwafでビルド管理できるようにしたものです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SPTKを共有ライブラリとしてインストールできます。&lt;/li&gt;
&lt;li&gt;C、C++の好きな方でコンパイルできます。&lt;/li&gt;
&lt;li&gt;wafが使えます（速い、出力がキレイ）&lt;/li&gt;
&lt;li&gt;自分のC、C++コードからSPTKのメソッドを呼べます。&lt;/li&gt;
&lt;li&gt;コマンドラインツールはインストールされません。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;コマンドラインツールを使いたい人は、元のconfigure scriptを使えば十分です。&lt;/p&gt;
&lt;h1 id=&#34;環境&#34;&gt;環境&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Unix系&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ubuntu 12.04 LTS 64 bitとMac OS X 10.9では確認済み&lt;/p&gt;
&lt;h1 id=&#34;sptkのインストール&#34;&gt;SPTKのインストール&lt;/h1&gt;
&lt;p&gt;リポジトリをクローンしたあと、&lt;/p&gt;
&lt;h2 id=&#34;build&#34;&gt;Build&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt; ./waf configure &amp;amp;&amp;amp; ./waf
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;build-with-clang&#34;&gt;Build with clang++&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt; CXX=clang++ ./waf configure &amp;amp;&amp;amp; ./waf
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;build-with-gcc&#34;&gt;Build with gcc&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt; git checkout c
 ./waf configure &amp;amp;&amp;amp; ./waf
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;build-with-clang-1&#34;&gt;Build with clang&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt; git checkout c
 CC=clang ./waf configure &amp;amp;&amp;amp; ./waf
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;install&#34;&gt;Install&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt; sudo ./waf install
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Include files: &lt;code&gt;/usr/local/include/SPTK&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Library: &lt;code&gt;/usr/local/lib/SPTK&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Pkg-config: &lt;code&gt;/usr/local/lib/pkgconfig&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;オリジナルのSPTKとはインストール場所が異なります（オリジナルは、&lt;code&gt;/usr/local/SPTK&lt;/code&gt;）&lt;/p&gt;
&lt;h1 id=&#34;sptkを使ってコードを書く&#34;&gt;SPTKを使ってコードを書く&lt;/h1&gt;
&lt;p&gt;&lt;code&gt;&amp;lt;SPTK/SPTK.h&amp;gt;&lt;/code&gt; をインクルードして、好きな関数を呼ぶ&lt;/p&gt;
&lt;p&gt;コンパイルは、例えば以下のようにする&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; g++ test.cpp `pkg-config SPTK --cflags --libs`
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;面倒なので、example/ 内のコードを修正して使う（wafを使おう）のがおすすめです。&lt;/p&gt;
&lt;br/&gt;
&lt;h1 id=&#34;きっかけ&#34;&gt;きっかけ&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;SPTKはコマンドラインツールだと思ってたけど、どうやらSPTK.hをインクルードすれば一通りのツールを使えるらしい&lt;/li&gt;
&lt;li&gt;SPTK.hをインクルードして使う方法のマニュアルが見つからない…&lt;/li&gt;
&lt;li&gt;SPTKはC言語で書かれてるし、C++から使うの地味にめんどくさい&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;cから簡単に使いたかった&#34;&gt;C++から簡単に使いたかった&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;gccやclangだけじゃなくg++やclang++でコンパイルできるようにしよう&lt;/li&gt;
&lt;li&gt;自分のコードのビルド管理にはwafを使ってるし、wafで管理できるようにしてしまおう&lt;/li&gt;
&lt;li&gt;waf素晴らしいしな （参考: &lt;a href=&#34;http://d.hatena.ne.jp/tanakh/20100212&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;waf チュートリアル | 純粋関数型雑記帳 &lt;/a&gt;）&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;最後に&#34;&gt;最後に&lt;/h1&gt;
&lt;p&gt;SPTKもwafも素晴らしいので積極的に使おう＾＾&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MFCCの計算方法についてメモ</title>
      <link>https://r9y9.github.io/blog/2013/11/24/mfcc-calculation-memo/</link>
      <pubDate>Sun, 24 Nov 2013 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2013/11/24/mfcc-calculation-memo/</guid>
      <description>&lt;h2 id=&#34;mfcc-とは&#34;&gt;MFCC とは&lt;/h2&gt;
&lt;p&gt;Mel-Frequency Cepstral Coefficients (MFCCs) のこと。音声認識でよく使われる、音声の特徴表現の代表的なもの。&lt;/p&gt;
&lt;h3 id=&#34;算出手順&#34;&gt;算出手順&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;音声信号を適当な長さのフレームで切り出し&lt;/li&gt;
&lt;li&gt;窓がけ&lt;/li&gt;
&lt;li&gt;フーリエ変換して対数振幅スペクトルを求める&lt;/li&gt;
&lt;li&gt;メルフィルタバンクを掛けて、メル周波数スペクトルを求める&lt;/li&gt;
&lt;li&gt;離散コサイン変換により、MFCCを求める&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以上。SPTKのmfccコマンドのソースもだいたいそうなってた。&lt;/p&gt;
&lt;h3 id=&#34;さて&#34;&gt;さて&lt;/h3&gt;
&lt;h4 id=&#34;ここに音声波形があるじゃろ&#34;&gt;ここに音声波形があるじゃろ？？&lt;/h4&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/speech-signal.png &#34;音声信号を適当な長さのフレームで切り出し&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;h4 id=&#34;音声波形を窓がけして&#34;&gt;音声波形を窓がけして…&lt;/h4&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/windowed-signal.png &#34;窓がけ&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;h4 id=&#34;さらにフーリエ変換して対数取って&#34;&gt;さらにフーリエ変換して対数取って…&lt;/h4&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/log-amplitude.png &#34;フーリエ変換して振幅スペクトルを求める&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;h4 id=&#34;ここでメルフィルタバンクの出番じゃ&#34;&gt;ここでメルフィルタバンクの出番じゃ&lt;/h4&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/after-mel-filterbank.png &#34;メルフィルタバンクを掛けて、メル周波数スペクトルを求める&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;h4 id=&#34;最後に離散コサイン変換で完成じゃ&#34;&gt;最後に離散コサイン変換で完成じゃ&lt;/h4&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://r9y9.github.io/images/MFCC.png &#34;離散コサイン変換により、MFCCを求める&#34;&#34; class=&#34;image&#34;&gt;&lt;/div&gt;
&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;MFCC求めたかったら、普通はHTKかSPTK使えばいいんじゃないですかね。自分で書くと面倒くさいです&lt;/li&gt;
&lt;li&gt;正規化はどうするのがいいのか、まだよくわかってない。単純にDCT（IIを使った）を最後に掛けると、かなり大きい値になって使いにくい。ので、 &lt;a href=&#34;http://research.cs.tamu.edu/prism/lectures/sp/l9.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://research.cs.tamu.edu/prism/lectures/sp/l9.pdf&lt;/a&gt; にもあるとおり、mel-filterbankの数（今回の場合は64）で割った。&lt;/li&gt;
&lt;li&gt;間違ってるかもしれないけどご愛嬌&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://research.cs.tamu.edu/prism/lectures/sp/l9.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;L9: Cepstral analysis [PDF]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://shower.human.waseda.ac.jp/~m-kouki/pukiwiki_public/66.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;メル周波数ケプストラム（MFCC） | Miyazawa’s Pukiwiki 公開版&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://aidiary.hatenablog.com/entry/20120225/1330179868&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;メル周波数ケプストラム係数（MFCC） | 人工知能に関する断創録&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>MLSA フィルタの実装</title>
      <link>https://r9y9.github.io/blog/2013/09/23/mlsa-filter-wakaran/</link>
      <pubDate>Mon, 23 Sep 2013 00:00:00 +0000</pubDate>
      <guid>https://r9y9.github.io/blog/2013/09/23/mlsa-filter-wakaran/</guid>
      <description>&lt;p&gt;音声合成に使われるMLSA（Mel-Log Spectrum Approximatation）フィルタを実装したいんだが、なにぶんわからん。SPTKにコードはあるけれど、正直理解できない。デジタル信号処理を小学一年生から勉強しなおしたいレベルだ&lt;/p&gt;
&lt;p&gt;と、前置きはさておき、MLSAフィルタの実装を見つけたのでメモ。ここ最近ちょくちょく調べているが、SPTK以外で初めて見つけた。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://simple4all.org/wp-content/uploads/2013/05/Jiunn.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Realisation and Simulation of the Mel Log Spectrum Approximation Filter | Simple4All Internship Report&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Simple4Allという音声技術系のコミュニティの、学生さんのインターンの成果らしい。ちらっと調べてたら山岸先生も参加してる（た？）っぽい。&lt;/p&gt;
&lt;p&gt;上のreportで引用されているように、MLSA filterの実現方法については、益子さんのD論に詳しく書いてあることがわかった。今井先生の論文と併せて読んでみようと思う。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.109.3623&amp;amp;rep=rep1&amp;amp;type=pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;T. Masuko, &amp;ldquo;HMM-Based Speech Synthesis and Its Applications&amp;rdquo;, Ph.D Thesis, 2002.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;もう正直わからんしブラックボックスでもいいから既存のツール使うかーと諦めかけていたところで割りと丁寧な実装付き解説を見つけたので、もう一度勉強して実装してみようと思い直した。&lt;/p&gt;
&lt;p&gt;機械学習にかまけて信号処理をちゃんと勉強していなかったつけがきている。LMA filterもMLSA filterも、本当にわからなくてツライ……&lt;/p&gt;
&lt;p&gt;(実装だけであれば、実はそんなに難しくなかった 2013/09後半)&lt;/p&gt;
&lt;h3 id=&#34;追記-20150225&#34;&gt;追記 2015/02/25&lt;/h3&gt;
&lt;p&gt;誤解を生む表現があったので、直しました&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
