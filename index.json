[{"authors":null,"categories":null,"content":"I am a software engineer / researcher at LINE Corporation. I am also a Ph.D. student at at Nagoya University, working with my advisor Prof. Tomoki Toda at the Toda Laboratory.\nMy research interests include statistical speech synthesis, voice conversion, singing voice synthesis, and machine learning. Before joining LINE Corporation, I worked in music signal processsing, music information retrieval, and computer vision.\nGitHub Google Scholar Download my CV ","date":1694995200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1694995200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a software engineer / researcher at LINE Corporation. I am also a Ph.D. student at at Nagoya University, working with my advisor Prof. Tomoki Toda at the Toda Laboratory.","tags":null,"title":"Ryuichi Yamamoto","type":"authors"},{"authors":["Reo Shimizu","Ryuichi Yamamoto","Masaya Kawamura","Yuma Shirahata","Hironori Doi","Tatsuya Komatsu","Kentaro Tachibana"],"categories":null,"content":"","date":1694995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694995200,"objectID":"d7aec9a7c5c3b8f7f903cc482954d99b","permalink":"https://r9y9.github.io/projects/promptttspp/","publishdate":"2023-09-18T09:00:00+09:00","relpermalink":"/projects/promptttspp/","section":"project","summary":"Submitted to [ICASSP 2024](https://2024.ieeeicassp.org/)","tags":["Deep Learning","TTS"],"title":"PromptTTS++: Controlling Speaker Identity in Prompt-based Text-to-Speech using Natural Language Descriptions","type":"project"},{"authors":["Ryuichi Yamamoto","Reo Yoneyama","Tomoki Toda"],"categories":[],"content":"Preprint: arXiv:2210.15987 (Submitted to ICASSP 2023)\nUpdates Authors Abstract Systems Samples SVS A/S Mixed demo Sample 1 Sample 2 Sample 3 Bonus samples Japanese Mandarin Error cases Unstable pitch of DiffSinger Acknowledgments Appendix Note pitch distribution References Updates 2023/02/27: Added error cases to address reviewer’s comments. See Error cases. 2022/11/27: Added samples of diffusion-based acoustic models. See Bonus samples. 2022/10/18: Created the demo page. Authors Ryuichi Yamamoto (LINE Corp., Nagoya University) Reo Yoneyama (Nagoya University) Tomoki Toda (Nagoya University) Abstract This paper describes the design of NNSVS, an open-source software for neural network-based singing voice synthesis research. NNSVS is inspired by Sinsy, an open-source pioneer in singing voice synthesis research, and provides many additional features such as multi-stream models, autoregressive fundamental frequency models, and neural vocoders. Furthermore, NNSVS provides extensive documentation and numerous scripts to build complete singing voice synthesis systems. Experimental results demonstrate that our best system significantly outperforms our reproduction of Sinsy and other baseline systems. The toolkit is available at https://github.com/nnsvs/nnsvs.\nSystems The following table summarizes the systems used in our experiments. All the models were trained on Namine Ritsu’s database [1].\nSystem Acoustic Features Multi-stream Architecture Autoregressive streams Vocoder Sinsy [2] MGC, LF0, VUV, BAP No - hn-uSFGAN [5] Sinsy (w/ pitch correction) MGC, LF0, VUV, BAP No - hn-uSFGAN Sinsy (w/ vibrato modeling) MGC, LF0, VUV, BAP No - hn-uSFGAN Muskits RNN [3] MEL No - HiFi-GAN [6] DiffSinger [4] MEL, LF0, VUV Yes - hn-HiFi-GAN NNSVS-Mel v1 MEL, LF0, VUV Yes - hn-uSFGAN NNSVS-Mel v2 MEL, LF0, VUV Yes LF0 hn-uSFGAN NNSVS-Mel v3 MEL, LF0, VUV Yes MEL, LF0 hn-uSFGAN NNSVS-WORLD v0 [1] MGC, LF0, VUV, BAP No - WORLD NNSVS-WORLD v1 MGC, LF0, VUV, BAP Yes - hn-uSFGAN NNSVS-WORLD v2 MGC, LF0, VUV, BAP Yes LF0 hn-uSFGAN NNSVS-WORLD v3 MGC, LF0, VUV, BAP Yes MGC, LF0 hn-uSFGAN NNSVS-WORLD v4 MGC, LF0, VUV, BAP Yes MGC, LF0, BAP hn-uSFGAN hn-HiFI-GAN (A/S) MEL, LF0, VUV - - hn-HiFi-GAN hn-uSFGAN-Mel (A/S) MEL, LF0, VUV - - hn-uSFGAN hn-uSFGAN-WORLD (A/S) MGC, LF0, VUV, BAP - - hn-uSFGAN Notes on baselines\nMuskits and DiffSinger baseline systems were trained with thier offical code (Muskits and DiffSinger). hn-HiFi-GAN is the vocoder used by DiffSinger. The detail of its implementation can be found here. Sinsy systems are based on NNSVS’s implementation. NNSVS-WORLD v0 uses the model trained with the earlier version of NNSVS (as of Nov. 2021) [1]. See here for details. Notes on NNSVS systems\nThe code for reproducing experiments are available here. Samples The following samples are vocal only. Mixed demo can be found here.\nSVS Samples generated from musical score.\nSample 1: 1st color\nRecordingSinsySinsy (with pitch correction) Sinsy (with vibrato modeling)Muskits RNNDiffSinger NNSVS-Mel v1NNSVS-Mel v2NNSVS-Mel v3 NNSVS-WORLD v0NNSVS-WORLD v1NNSVS-WORLD v2 NNSVS-WORLD v3NNSVS-WORLD v4 Sample 2: ARROW\nRecordingSinsySinsy (with pitch correction) Sinsy (with vibrato modeling)Muskits RNNDiffSinger NNSVS-Mel v1NNSVS-Mel v2NNSVS-Mel v3 NNSVS-WORLD v0NNSVS-WORLD v1NNSVS-WORLD v2 NNSVS-WORLD v3NNSVS-WORLD v4 Sample 3: BC\nRecordingSinsySinsy (with pitch correction) Sinsy (with vibrato modeling)Muskits RNNDiffSinger NNSVS-Mel v1NNSVS-Mel v2NNSVS-Mel v3 NNSVS-WORLD v0NNSVS-WORLD v1NNSVS-WORLD v2 NNSVS-WORLD v3NNSVS-WORLD v4 Sample 4: Close to you\nRecordingSinsySinsy (with pitch correction) Sinsy (with vibrato modeling)Muskits RNNDiffSinger NNSVS-Mel v1NNSVS-Mel v2NNSVS-Mel v3 NNSVS-WORLD v0NNSVS-WORLD v1NNSVS-WORLD v2 NNSVS-WORLD v3NNSVS-WORLD v4 Sample 5: ERROR\nRecordingSinsySinsy (with pitch correction) Sinsy (with vibrato modeling)Muskits RNNDiffSinger NNSVS-Mel v1NNSVS-Mel v2NNSVS-Mel v3 NNSVS-WORLD v0NNSVS-WORLD v1NNSVS-WORLD v2 NNSVS-WORLD v3NNSVS-WORLD v4 A/S Samples generated from extracted features (i.e., analysis-by-synthesis).\nSample 1: 1st color\nRecordinghn-HiFi-GANhn-USFGAN-Mel hn-USFGAN-WORLD Sample 2: ARROW\nRecordinghn-HiFi-GANhn-USFGAN-Mel hn-USFGAN-WORLD Sample 3: BC\nRecordinghn-HiFi-GANhn-USFGAN-Mel hn-USFGAN-WORLD Sample 4: Close to you\nRecordinghn-HiFi-GANhn-USFGAN-Mel hn-USFGAN-WORLD Sample 5: ERROR\nRecordinghn-HiFi-GANhn-USFGAN-Mel hn-USFGAN-WORLD Mixed demo System: NNSVS-WORLD v4\nSample 1 ERROR (from test data)\nSample 2 ARROW (from test data)\nSample 3 WAVE (from training data)\nBonus samples We integrated the diffusion model for SVS [4] to improve naturalness of synthetic voice. The following table summarizes the systems for bonus samples.\nSystem Acoustic Features Autoregressive streams Diffusion streams Vocoder NNSVS-WORLD v4* MGC, LF0, VUV, BAP LF0, MGC, BAP - SiFi-GAN [7] NNSVS-Mel v5 MEL, lF0, VUV LF0 MEL SiFi-GAN NNSVS-WORLD v5 MGC, LF0, VUV, BAP LF0 MGC, BAP SiFi-GAN NNSVS-WORLD v4* is the best model (as of …","date":1666099064,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666099064,"objectID":"63f5e7c4761a4ae179df9326f97f16a5","permalink":"https://r9y9.github.io/projects/nnsvs/","publishdate":"2022-10-18T22:17:44+09:00","relpermalink":"/projects/nnsvs/","section":"project","summary":"Accepted to [ICASSP 2023](https://2023.ieeeicassp.org/)","tags":["SVS","Python","Deep Learning","Open-Source","ICASSP"],"title":"NNSVS: A Neural Network-Based Singing Voice Synthesis Toolkit","type":"project"},{"authors":["Reo Yoneyama","Ryuichi Yamamoto","Kentaro Tachibana"],"categories":null,"content":"","date":1666095464,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666095464,"objectID":"9b8ec30b4fc0f9075de1d156a2aee0a1","permalink":"https://r9y9.github.io/projects/dualcyclegan/","publishdate":"2022-10-18T21:17:44+09:00","relpermalink":"/projects/dualcyclegan/","section":"project","summary":"Accepted to [ICASSP 2023](https://2023.ieeeicassp.org/)","tags":["Super resolution","Python","Deep Learning","Open-Source","ICASSP"],"title":"Non-parallel High-Quality Audio Super Resolution with Domain Adaptation and Resampling CycleGANs","type":"project"},{"authors":["Yuma Shirahata","Ryuichi Yamamoto","Eunwoo Song","Ryo Terashima","Jae-Min Kim","Kentaro Tachibana"],"categories":[],"content":"","date":1666072079,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666072079,"objectID":"cfff1782c9679a10493ca0c894de0f6d","permalink":"https://r9y9.github.io/projects/period-vits/","publishdate":"2022-10-18T14:47:59+09:00","relpermalink":"/projects/period-vits/","section":"project","summary":"Accepted to [ICASSP 2023](https://2023.ieeeicassp.org/)","tags":["Deep Learning","End-to-end","TTS","ICASSP"],"title":"Period VITS: Variational Inference With Explicit Pitch Modeling For End-to-End Emotional Speech Synthesis","type":"project"},{"authors":["Masaya Kawamura1","Yuma Shirahata","Ryuichi Yamamoto","Kentaro Tachibana"],"categories":[],"content":"","date":1666002150,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666002150,"objectID":"42e85f2fa53bc7cba2b0b70cc47a5ad2","permalink":"https://r9y9.github.io/projects/lvits/","publishdate":"2022-10-17T19:22:30+09:00","relpermalink":"/projects/lvits/","section":"project","summary":"Accepted to [ICASSP 2023](https://2023.ieeeicassp.org/)","tags":["TTS","Python","Deep Learning","Open-Source"],"title":"Lightweight and High-Fidelity End-to-End Text-to-Speech with Multi-Band Generation and Inverse Short-Time Fourier Transform","type":"project"},{"authors":["Takaaki Saeki","Kentaro Tachibana","Ryuichi Yamamoto"],"categories":[],"content":"","date":1649041867,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649041867,"objectID":"8721f1290b20558b778cec281cb5ebba","permalink":"https://r9y9.github.io/projects/drspeech/","publishdate":"2022-04-04T12:11:07+09:00","relpermalink":"/projects/drspeech/","section":"project","summary":"Accepted to [Interspeech 2022](https://interspeech2022.org/)","tags":["Deep Learning","TTS","Interspeech"],"title":"DRSpeech: Degradation-Robust Text-to-Speech Synthesis with Frame-Level and Utterance-Level Acoustic Representation Learning","type":"project"},{"authors":["Eunwoo Song","Ryuichi Yamamoto","Ohsung Kwon","Chan-Ho Song","Min-Jae Hwang","Suhyeon Oh","Hyun-Wook Yoon","Jin-Seob Kim","Jae-Min Kim"],"categories":[],"content":"","date":1649041865,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649041865,"objectID":"ddc245641a8438a95510b664b91513c2","permalink":"https://r9y9.github.io/projects/tts-by-tts2/","publishdate":"2022-04-04T12:11:05+09:00","relpermalink":"/projects/tts-by-tts2/","section":"project","summary":"Accepted to [Interspeech 2022](https://interspeech2022.org/)","tags":["Deep Learning","TTS","VAE","Interspeech"],"title":"TTS-by-TTS 2: Data-selective Augmentation for Neural Speech Synthesis Using Ranking Support Vector Machine with Variational Autoencoder","type":"project"},{"authors":["Ryo Terashima","Ryuichi Yamamoto","Eunwoo Song","Yuma Shirahata","Hyun-Wook Yoon","Jae-Min Kim","Kentaro Tachibana"],"categories":[],"content":"","date":1649041864,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649041864,"objectID":"fe2fb70f965cc4b978884fc883b828ad","permalink":"https://r9y9.github.io/projects/vc-tts-ps/","publishdate":"2022-04-04T12:11:04+09:00","relpermalink":"/projects/vc-tts-ps/","section":"project","summary":"Accepted to [Interspeech 2022](https://interspeech2022.org/)","tags":["Deep Learning","VC","TTS","Interspeech"],"title":"Cross-Speaker Emotion Transfer for Low-Resource Text-to-Speech Using Non-Parallel Voice Conversion with Pitch-Shift Data Augmentation","type":"project"},{"authors":["Byeongseon Park","Ryuichi Yamamoto","Kentaro Tachibana"],"categories":[],"content":"","date":1649041863,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649041863,"objectID":"d875e89114555b84ede41b5e2cfef2f7","permalink":"https://r9y9.github.io/projects/mtl_accent/","publishdate":"2022-04-04T12:11:03+09:00","relpermalink":"/projects/mtl_accent/","section":"project","summary":"Accepted to [Interspeech 2022](https://interspeech2022.org/)","tags":["Deep Learning","NLP","Interspeech"],"title":"A Unified Accent Estimation Method Based on Multi-Task Learning for Japanese Text-to-Speech","type":"project"},{"authors":["Hyunwook Yoon","Ohsung Kwon","Hoyeon Lee","Ryuichi Yamamoto","Eunwoo Song","Jae-Min Kim","Min-Jae Hwang"],"categories":[],"content":"","date":1649041861,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649041861,"objectID":"2a8496714fdc0fac1bab3408ffbef865","permalink":"https://r9y9.github.io/projects/lmemotiontts/","publishdate":"2022-04-04T12:11:01+09:00","relpermalink":"/projects/lmemotiontts/","section":"project","summary":"Accepted to [Interspeech 2022](https://interspeech2022.org/)","tags":["Deep Learning","VC","TTS","Interspeech"],"title":"Language Model-Based Emotion Prediction Methods for Emotional Speech Synthesis Systems","type":"project"},{"authors":["Ryuichi Yamamoto"],"categories":null,"content":"恩師である酒向先生 (http://sakoweb.net/joomla3/) にお声がけいただき、母校の名古屋工業大学で講義をさせていただきました。\n","date":1643074200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643074200,"objectID":"d80a267217c0f58c4836e277244a2432","permalink":"https://r9y9.github.io/talk/202201nit-lecture/","publishdate":"2022-01-25T10:30:00+09:00","relpermalink":"/talk/202201nit-lecture/","section":"event","summary":" ","tags":["Presentation","TTS","Speech Synthesis","Research"],"title":"企業における音声合成の研究開発 / Research and development for TTS in industry @名古屋工業大学","type":"event"},{"authors":["Ryuichi Yamamoto"],"categories":[],"content":"Summary 新: https://github.com/r9y9/website トップページ ブログ一覧 デモページ一覧 旧: https://github.com/r9y9/blog + https://github.com/r9y9/demos-src これまで、ブログとデモページ1をそれぞれ個別に Hugo で管理していましたが、それらを統合して一つの Hugo site として管理するように変更しました。 内部的に色々変わっていますが、URL はほぼ変わらないようにしています。\nはじめに 2年振りくらいにブログを書いています。大したことを書くわけではないのですが、ウェブサイト（ブログ含む）を大幅にアップデートしたので、その記録を残しておきます。\nWhy なぜ大幅にアップデートをしたのか、理由は以下のとおりです。\n経歴をまとめたプロフィールページなどコンテンツを追加したかった。ただし、これまでは、ブログとデモページをそれらに特化した個別の Hugo site として管理しており、それ以外の情報を追加することが容易ではなかった。プロフィール用に別の Hugo site を作ることも検討したが、管理のし易さの観点から一つの Hugo site にまとめたかった。 Hugo の theme を修正してレイアウトを調整するのが大変だったので、いい感じレスポンシブにしてくれる、スマホで見てもレイアウトが崩れない Hugo theme を使いたくなった。 これまでは、minimal な theme をベースに自分で修正して使っていたが、限界がきた。 後者のレイアウトの問題は、theme を変えれば済む話とも言えますが、前者の問題を解決するためには、theme 変更に加えて複数のsiteを統合する必要がありました。\nアップデートに関してやったこと 基本方針 Static site generator として Hugo を使う。速い。 Hugo theme には https://github.com/wowchemy/starter-hugo-academic を使う。デザインがシンプルで好み、かつカスタマイズが柔軟にできそうだったので。 具体的な手順 starter-hugo-academic をベースに、全体のレイアウトを作る。僕の場合、Home (トップページ) に Profile, Posts, Projects, Talks の4つのコンテンツを配置しました。このうち、Profile には、CV2のリンクを貼るようにしました。 Publications コンテンツを作ることも考えましたが、Google Scholarで良くない？と思って作っていません。 ブログ のコンテンツを Posts に移行する。一覧はこちら https://wowchemy.com/docs/content/front-matter/ にも書かれている通り、Hugo academic ならではの front matter がありますが、基本的には markdown + 必要なstatic filesをコピーするだけで移行はできました。 Hugo academic に summary を表示する機能があったので、すべてのブログ記事に summary を設定しました。 Tableのレイアウトが崩れる問題があったので、微妙にcssを修正しました。 デモページ のコンテンツを Projects に移行する。一覧はこちら blogを移行するのとまったく同じように移行できました 論文に記載したデモページのURLが404にならないように、URLが変わらないようにする、あるいは リダイレクトを設定しました。Hugoだと、aliases というパラメータを front matter に書くことでリダイレクトを実現できます。 これまでの取り組みを整理するいい機会だと思って、共著論文のデモページもコンテンツに追加しました。 Talks のコンテンツをおまけで新しく追加しました。LINE DEV DAYでの過去の発表とか 雑感 良かったこと ウェブサイトはいい感じになった。プロフィールの追加ができたことはもちろん、モバイル端末で見てもレイアウトが崩れなくなったのは地味に嬉しい ブログとデモサイトを一つの Hugo site として管理することで、相互のコンテンツを行き来しやすくなった。具体的には、記事の終わりに、Related というセクションに関連するブログ記事やデモページのリンクが表示されるようになった。また、ページのヘッダーから、相互のコンテンツを行き来することもできる。 管理するリポジトリの数が4から2に減った。 旧: blog, r9y9.github.io, demos-src, demos 新: website, r9y9.github.io 悪かったこと https://github.com/wowchemy/starter-hugo-academic はカスタマイズの自由度は高い一方で、適切に設定するのが難しかった。ドキュメントは豊富だが量が多く、全部読んで使いこなすのは大変に感じた。 大変だった・・・もうしばらくウェブサイトを大きく更新したくないなと思いました。 おわりです。ただの備忘録ですが、ここまで読んで頂きありがとうございました。\n音声合成の論文を書くときに、読者や査読者が音声サンプルを聴取できるようにデモページを作ることがあります。例えばこちらです。 ↩︎\nCVの提出を求められることがたまにあるので、これまでの実績を整理するいい機会かなと思って作りました https://github.com/r9y9/cv ↩︎\n","date":1642507168,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642507168,"objectID":"a8644f77dfe14c8e6853fb17c2f8ecae","permalink":"https://r9y9.github.io/blog/2022/01/18/hugo-academic/","publishdate":"2022-01-18T20:59:28+09:00","relpermalink":"/blog/2022/01/18/hugo-academic/","section":"post","summary":"New: https://github.com/r9y9/website, old: https://github.com/r9y9/blog","tags":["Hugo"],"title":"Hugo Academic を使ってウェブサイトをアップデートしました","type":"post"},{"authors":["Tomohiro Tanaka","Ryuichi Yamamoto"],"categories":null,"content":"Abstract (ja) 2021 年 8 月 30 日から 9 月 3 日にかけてチェコ・ブルノおよびオンラインのハイブリッド形式で Interspeech2021 が開催された．ここでは，会議概要や最新の技術動向，注目の発表について報告する．\n","date":1638504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638504000,"objectID":"4208077e651e51597bf74d64218f6890","permalink":"https://r9y9.github.io/talk/sp-interspeech2021report/","publishdate":"2021-12-03T13:00:00+09:00","relpermalink":"/talk/sp-interspeech2021report/","section":"event","summary":" ","tags":["Presentation","Interspeech"],"title":"国際会議Interspeech2021参加報告 / Report on Participation in Interspeech2021 @SLP研究会","type":"event"},{"authors":["Tomoki Hayashi","Ryuichi Yamamoto","Takenori Yoshimura","Peter Wu","Jiatong Shi","Takaaki Saeki","Yooncheol Ju","Yusuke Yasuda","Shinnosuke Takamichi","Shinji Watanabe"],"categories":[],"content":"","date":1633520557,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633520557,"objectID":"bdbdfa2b7b137d5d9abdf0824804aba3","permalink":"https://r9y9.github.io/projects/espnet2-tts/","publishdate":"2021-10-06T20:42:37+09:00","relpermalink":"/projects/espnet2-tts/","section":"project","summary":"Preprint: [arXiv:2110.07840](https://arxiv.org/abs/2110.07840) (submitted to [ICASSP 2022](https://2022.ieeeicassp.org/))","tags":["Deep Learning","TTS","ICASSP","Open-Source"],"title":"ESPnet2-TTS: Extending the Edge of TTS Research","type":"project"},{"authors":["Ryuichi Yamamoto","Shinnosuke Takamichi"],"categories":[],"content":"","date":1628666842,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628666842,"objectID":"ed95e4e8da22c49f423c1b9769f5ef88","permalink":"https://r9y9.github.io/projects/ttslearn/","publishdate":"2021-08-11T16:27:22+09:00","relpermalink":"/projects/ttslearn/","section":"project","summary":"","tags":["TTS","Python","Deep Learning","Open-Source"],"title":"ttslearn: Library for Pythonで学ぶ音声合成 (Text-to-speech with Python)","type":"project"},{"authors":["Ryuichi Yamamoto","Eunwoo Song","Min-Jae Hwang"],"categories":[],"content":"Submitted to IEEE signal processing letters\nAuthors Abstract TTS samples M1 (male) M2 (male) F1 (female) F2 (female) Acknowledgements Authors Ryuichi Yamamoto (LINE Corp.) Min-Jae Hwang (Search Solutions Inc.) Eunwoo Song (NAVER Corp.) Abstract This letter proposes a voicing-aware Parallel Wave- GAN (VA-PWG) vocoder for a neural text-to-speech (TTS) system. To generate a high-quality speech waveform, it is important to reflect the distinct characteristics of voiced and unvoiced speech signals well. However, it is difficult for the conventional PWG model to accurately represent this condition, since the single unified architectures of the generator and discriminator are insufficient to capture those characteristics. In the proposed method, both the generator and discriminator are divided into their subnetworks to individually model the voicing state-dependent characteristics of a speech signal. In particular, a VA-generator consisting of two sub-WaveNets generates the harmonic and noise components of a speech signal by inputting pitch-dependent sine wave and Gaussian noise sources, respectively. Likewise, a VA-discriminator consisting of two sub-discriminators learns the distinct characteristics of harmonic and noise components by feeding the voiced and unvoiced waveforms, respectively. Subjective evaluation results verified the effectiveness of the proposed VA-PWG vocoder by achieving a 4.25 mean opinion score from a speaker-independent training scenario that was 11% higher than that of a conventional PWG vocoder.\nTTS samples M1 (male) Sample 1: “鹿児島県で最大震度三を観測しています。”\nRecordingWaveNetPWG VA-PWG-GVA-PWG-DVA-PWG-GD (proposed) Sample 2: “葉加瀬太郎の情熱大陸です。”\nRecordingWaveNetPWG VA-PWG-GVA-PWG-DVA-PWG-GD (proposed) Sample 3: “それでうちの部は半分に減らされる。”\nRecordingWaveNetPWG VA-PWG-GVA-PWG-DVA-PWG-GD (proposed) M2 (male) Sample 1: “ヨメの、レオンティーンさんですね。”\nRecordingWaveNetPWG VA-PWG-GVA-PWG-DVA-PWG-GD (proposed) Sample 2: “御予約は、二泊三日ですね。”\nRecordingWaveNetPWG VA-PWG-GVA-PWG-DVA-PWG-GD (proposed) Sample 3: “わたさちの、ローリーさんですね。”\nRecordingWaveNetPWG VA-PWG-GVA-PWG-DVA-PWG-GD (proposed) F1 (female) Sample 1: “かわいそうに、助けてやらなくてはと、家に連れて帰りましたとさ。”\nRecordingWaveNetPWG VA-PWG-GVA-PWG-DVA-PWG-GD (proposed) Sample 2: “失礼のないよう、笑顔で挨拶して。”\nRecordingWaveNetPWG VA-PWG-GVA-PWG-DVA-PWG-GD (proposed) Sample 3: “照れていたので、ちょっと意外な気がしましたー。”\nRecordingWaveNetPWG VA-PWG-GVA-PWG-DVA-PWG-GD (proposed) F2 (female) Sample 1: “そして目に留まったのは、お気に入りの居酒屋の前にあるゴミの山。”\nRecordingWaveNetPWG VA-PWG-GVA-PWG-DVA-PWG-GD (proposed) Sample 2: “今ひとつ、時間が足りず。”\nRecordingWaveNetPWG VA-PWG-GVA-PWG-DVA-PWG-GD (proposed) Sample 3: “実はこの道の先に、高い山があってね。”\nRecordingWaveNetPWG VA-PWG-GVA-PWG-DVA-PWG-GD (proposed) Acknowledgements Work performed with nVoice, Clova Voice, Naver Corp.\n","date":1627614663,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627614663,"objectID":"26e60560c59fdbad145e1188ba19e3eb","permalink":"https://r9y9.github.io/projects/va-pwg/","publishdate":"2021-07-30T12:11:03+09:00","relpermalink":"/projects/va-pwg/","section":"project","summary":"Submitted to [IEEE signal processing letters](https://signalprocessingsociety.org/publications-resources/ieee-signal-processing-letters) (*rejected*)","tags":["Deep Learning","TTS","IEEE SPL"],"title":"Voicing-Aware Parallel WaveGAN for High-Quality Speech Synthesis","type":"project"},{"authors":["Min-Jae Hwang","Ryuichi Yamamoto","Eunwoo Song","Jae-Min Kim"],"categories":[],"content":"","date":1617363276,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617363276,"objectID":"ef0bc624105607a4779b6f942f85ee3e","permalink":"https://r9y9.github.io/projects/mbhnpwg/","publishdate":"2021-04-02T20:34:36+09:00","relpermalink":"/projects/mbhnpwg/","section":"project","summary":"Published version: [ISCA Archive Interspeech 2021](https://www.isca-speech.org/archive/interspeech_2021/hwang21_interspeech.html)","tags":["Deep Learning","TTS","Interspeech"],"title":"High-fidelity Parallel WaveGAN with Multi-band Harmonic-plus-Noise Model","type":"project"},{"authors":["Kosuke Futamata","Byeongseon Park","Ryuichi Yamamoto","Kentaro Tachibana"],"categories":[],"content":"","date":1617363276,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617363276,"objectID":"1cc34c00de758d4c074db8771a6f1642","permalink":"https://r9y9.github.io/projects/pbp_bert/","publishdate":"2021-04-02T20:34:36+09:00","relpermalink":"/projects/pbp_bert/","section":"project","summary":"Preprint: [arXiv:2104.12395](https://arxiv.org/abs/2104.12395), Published version: [ISCA Archive Interspeech 2021](https://www.isca-speech.org/archive/interspeech_2021/futamata21_interspeech.html)","tags":["Deep Learning","NLP","Interspeech"],"title":"Phrase break prediction with bidirectional encoder representations in Japanese text-to-speech synthesis","type":"project"},{"authors":["Togami Masahito","Yusuke Kida","Ryuichi Yamamoto","Keisuke Imoto"],"categories":null,"content":"Abstract (ja) 人の音声をテキストに変換する音声認識技術、テキストから人の音声を生成する音声合成技術をはじめとした音声処理技術が目覚ましい速度で進歩を続けている。さらに、音声に限らないドアの開け閉めの音など一般の音を識別する音響シーン・イベント検出技術などの新しい技術分野が拓けつつある。本セッションでは、LINEから2名のエンジニア（木田祐介・山本龍一）がパネリストとして登壇し、音声認識・音声合成の現状を語る。さらに、同志社大学の井本桂右准教授に登壇いただき、今年国際会議(DCASE)を日本に誘致するなど、日本の研究者の活躍が目覚ましい音響シーン・イベント検出技術の分野の現状を語っていただく。これらの技術分野の進歩には深層学習の進歩が強い影響を与えているが、音声処理特有の要素がどのようにして深層学習と絡み合い技術進化につながっているか掘り下げていきたい。また、様々な音声処理分野で、分野間で共通要素として進展が進む技術要素と特有の要素の分析を通し、各技術分野の特性を明らかにしていきたい。そして、今後どのような方向性で技術が進化していくか、将来の展望について議論していきたい。\n","date":1606290000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606290000,"objectID":"32606dfb86c165e9d63d4f83e46b0930","permalink":"https://r9y9.github.io/talk/linedevday2020panel/","publishdate":"2020-11-25T16:40:00+09:00","relpermalink":"/talk/linedevday2020panel/","section":"event","summary":" ","tags":["Presentation","Speech Processing","Deep Learning"],"title":"ここまで来た音声技術・今後の展望 / Current progress on speech technologies and its future prospects @ LINE DEV DAY 2020","type":"event"},{"authors":["Ryuichi Yamamoto"],"categories":null,"content":"Abstract (ja) コンピュータによってテキストから人間の声を合成する技術は、テキスト音声合成と呼ばれます。LINE CLOVAのスマートスピーカーを初めとするユーザとのリアルタイムのインタラクションが必要なサービスでは、音声合成システムには合成品質が高いことだけでなく、高速に音声を生成できることが求められます。本セッションでは、高速かつ高品質な音声合成を実現するために、NAVERとLINEで共同で開発したGPUベースの音声合成の研究成果について発表します。従来の方法では、品質が良くても合成速度が遅い、合成速度は速い一方でモデルの学習に多大な時間がかかるなどの問題がありました。我々はそのような問題に対してどのようにアプローチしたのか、音声信号処理のトップカンファレンスICASSP 2020に採択された論文の内容を元に、近年の関連分野の発展を交えて紹介します。\n","date":1606281600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606281600,"objectID":"b3642072404fe387583c04ecd2a85380","permalink":"https://r9y9.github.io/talk/linedevday2020pwg/","publishdate":"2020-11-25T14:20:00+09:00","relpermalink":"/talk/linedevday2020pwg/","section":"event","summary":" ","tags":["Presentation","TTS","Deep Learning"],"title":"Parallel WaveGAN: GPUを利用した高速かつ高品質な音声合成 / Parallel WaveGAN: Fast and High-Quality GPU Text-to-Speech @ LINE DEV DAY 2020","type":"event"},{"authors":["Eunwoo Song","Ryuichi Yamamoto","Min-Jae Hwang","Jin-Seob Kim","Ohsung Kwon","Jae-Min Kim"],"categories":[],"content":"","date":1604648624,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604648624,"objectID":"d8b89c358e08834856d85944c5e38050","permalink":"https://r9y9.github.io/projects/pwg-pwsl/","publishdate":"2020-11-06T16:43:44+09:00","relpermalink":"/projects/pwg-pwsl/","section":"project","summary":"Preprint: [arXiv:2101.07412](https://arxiv.org/abs/2101.07412) (accepted to [SLT 2021](http://2021.ieeeslt.org/))","tags":["Deep Learning","TTS","SLT"],"title":"Improved Parallel WaveGAN with perceptually weighted spectrogram loss","type":"project"},{"authors":["Min-Jae Hwang","Ryuichi Yamamoto","Eunwoo Song","Jae-Min Kim"],"categories":[],"content":"","date":1603697832,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603697832,"objectID":"529a4af775d62f66ae6c0341c2428f2d","permalink":"https://r9y9.github.io/projects/tts-by-tts/","publishdate":"2020-10-26T16:37:12+09:00","relpermalink":"/projects/tts-by-tts/","section":"project","summary":"Preprint: [arXiv:2010.13421](https://arxiv.org/abs/2010.13421) (accepted to [ICASSP 2021](https://2021.ieeeicassp.org/))","tags":["Deep Learning","TTS","ICASSP"],"title":"TTS-by-TTS: TTS-driven Data Augmentation for Fast and High-Quality Speech Synthesis","type":"project"},{"authors":["Ryuichi Yamamoto","Eunwoo Song","Min-Jae Hwang","Jae-Min Kim"],"categories":[],"content":"Preprint: arXiv:2010.14151 (accepted to ICASSP 2021)\nTable of contents Analysis/synthesis samples (Japanese) Text-to-speech samples (Japanese) Bonus: analysis/synthesis samples for CMU ARCTIC (English) Authors Ryuichi Yamamoto (LINE Corp.) Eunwoo Song (NAVER Corp.) Min-Jae Hwang (Search Solutions Inc.) Jae-Min Kim (NAVER Corp.) Abstract This paper proposes voicing-aware conditional discriminators for Parallel WaveGAN-based waveform synthesis systems. In this framework, we adopt a projection-based conditioning method that can significantly improve the discriminator’s performance. Furthermore, the conventional discriminator is separated into two waveform discriminators for modeling voiced and unvoiced speech. As each discriminator learns the distinctive characteristics of the harmonic and noise components, respectively, the adversarial training process becomes more efficient, allowing the generator to produce more realistic speech waveforms. Subjective test results demonstrate the superiority of the proposed method over the conventional Parallel WaveGAN and WaveNet systems. In particular, our speaker-independently trained model within a FastSpeech 2 based text-to-speech framework achieves the mean opinion scores of 4.20, 4.18, 4.21, and 4.31 for four Japanese speakers, respectively.\nSystems for comparision System Voiced segments Unvoiced segments Discriminator conditioning S1-WaveNet [1] - - - S2-PWG [2] - - - S3-PWG-cGAN-D - - Yes S4-PWG-V/UV-D $D^{\\mathrm{{v}}}$ $D^{\\mathrm{{v}}}$ Yes S5-PWG-V/UV-D $D^{\\mathrm{{uv}}}$ $D^{\\mathrm{{v}}}$ Yes S6-PWG-V/UV-D $D^{\\mathrm{{uv}}}$ $D^{\\mathrm{{uv}}}$ Yes S7-PWG-V/UV-D (proposed) $D^{\\mathrm{{v}}}$ $D^{\\mathrm{{uv}}}$ Yes Recordings - - - $D^{\\mathrm{{v}}}$: 1-D dilated CNN discrimiantor with the reseptive field size of 127. $D^{\\mathrm{{uv}}}$: 1-D CNN discrimiantor with the reseptive field size of 13. PWG denotes Parallel WaveGAN for short. Systems S2-PWG and S3-PWG-cGAN-D used $D^{\\mathrm{{v}}}$ as the primary discriminator. Note that all the Parallel WaveGAN systems used the same generator architecture and training configurations; they only differed in the discriminator settings.\nAnalysis/synthesis F1 (female) Sample 1\nRecordingS1-WaveNetS2-PWGS7-PWG-V/UV-D (ours) S3-PWG-cGAN-DS4-PWG-V/UV-DS5-PWG-V/UV-DS6-PWG-V/UV-D F2 (female) Sample 1\nRecordingS1-WaveNetS2-PWGS7-PWG-V/UV-D (ours) S3-PWG-cGAN-DS4-PWG-V/UV-DS5-PWG-V/UV-DS6-PWG-V/UV-D M1 (male) Sample 1\nRecordingS1-WaveNetS2-PWGS7-PWG-V/UV-D (ours) S3-PWG-cGAN-DS4-PWG-V/UV-DS5-PWG-V/UV-DS6-PWG-V/UV-D M2 (male) Sample 1\nRecordingS1-WaveNetS2-PWGS7-PWG-V/UV-D (ours) S3-PWG-cGAN-DS4-PWG-V/UV-DS5-PWG-V/UV-DS6-PWG-V/UV-D Text-to-speech FastSpeech 2 ([3]) based acoustic models were used for text-to-speech experiments.\nF1 (female) Sample 1\nRecordingFastSpeech 2 + WaveNetFastSpeech 2 + PWGFastSpeech 2 + PWG-V/UV-D (ours) FastSpeech 2 + PWG-cGAN-D Sample 2\nRecordingFastSpeech 2 + WaveNetFastSpeech 2 + PWGFastSpeech 2 + PWG-V/UV-D (ours) FastSpeech 2 + PWG-cGAN-D Sample 3\nRecordingFastSpeech 2 + WaveNetFastSpeech 2 + PWGFastSpeech 2 + PWG-V/UV-D (ours) FastSpeech 2 + PWG-cGAN-D F2 (female) Sample 1\nRecordingFastSpeech 2 + WaveNetFastSpeech 2 + PWGFastSpeech 2 + PWG-V/UV-D (ours) FastSpeech 2 + PWG-cGAN-D Sample 2\nRecordingFastSpeech 2 + WaveNetFastSpeech 2 + PWGFastSpeech 2 + PWG-V/UV-D (ours) FastSpeech 2 + PWG-cGAN-D Sample 3\nRecordingFastSpeech 2 + WaveNetFastSpeech 2 + PWGFastSpeech 2 + PWG-V/UV-D (ours) FastSpeech 2 + PWG-cGAN-D M1 (male) Sample 1\nRecordingFastSpeech 2 + WaveNetFastSpeech 2 + PWGFastSpeech 2 + PWG-V/UV-D (ours) FastSpeech 2 + PWG-cGAN-D Sample 2\nRecordingFastSpeech 2 + WaveNetFastSpeech 2 + PWGFastSpeech 2 + PWG-V/UV-D (ours) FastSpeech 2 + PWG-cGAN-D Sample 3\nRecordingFastSpeech 2 + WaveNetFastSpeech 2 + PWGFastSpeech 2 + PWG-V/UV-D (ours) FastSpeech 2 + PWG-cGAN-D M2 (male) Sample 1\nRecordingFastSpeech 2 + WaveNetFastSpeech 2 + PWGFastSpeech 2 + PWG-V/UV-D (ours) FastSpeech 2 + PWG-cGAN-D Sample 2\nRecordingFastSpeech 2 + WaveNetFastSpeech 2 + PWGFastSpeech 2 + PWG-V/UV-D (ours) FastSpeech 2 + PWG-cGAN-D Sample 3\nRecordingFastSpeech 2 + WaveNetFastSpeech 2 + PWGFastSpeech 2 + PWG-V/UV-D (ours) FastSpeech 2 + PWG-cGAN-D Bonus: Analysis/synthesis (English) Samples for CMU ARCTIC database are provided as follows. The models were trained using total six speakers (clb, slt, bdl, rms, jmk, and ksp) in a speaker-independent way. The models were similary configured as the above experiments.\nclb (female) Sample 1\nRecordingPWGPWG-cGAN-DPWG-V/UV-D (ours) Sample 2\nRecordingPWGPWG-cGAN-DPWG-V/UV-D (ours) Sample 3\nRecordingPWGPWG-cGAN-DPWG-V/UV-D (ours) slt (female) Sample 1\nRecordingPWGPWG-cGAN-DPWG-V/UV-D (ours) Sample 2\nRecordingPWGPWG-cGAN-DPWG-V/UV-D (ours) Sample 3\nRecordingPWGPWG-cGAN-DPWG-V/UV-D (ours) bdl (male) Sample 1\nRecordingPWGPWG-cGAN-DPWG-V/UV-D (ours) Sample 2\nRecordingPWGPWG-cGAN-DPWG-V/UV-D (ours) Sample 3\nRecordingPWGPWG-cGAN-DPWG-V/UV-D (ours) rms (male) Sample 1 …","date":1603291128,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603291128,"objectID":"3576955af26aa4e4c9525dd09ca93aa5","permalink":"https://r9y9.github.io/projects/vuvd-pwg/","publishdate":"2020-10-21T23:38:48+09:00","relpermalink":"/projects/vuvd-pwg/","section":"project","summary":"Preprint: [arXiv:2010.14151](https://arxiv.org/abs/2010.14151) (accepted to [ICASSP 2021](https://2021.ieeeicassp.org/))","tags":["Deep Learning","TTS","ICASSP"],"title":"Parallel waveform synthesis based on generative adversarial networks with voicing-aware conditional discriminators","type":"project"},{"authors":null,"categories":null,"content":"Summary コード: https://github.com/r9y9/nnsvs Discussion: https://github.com/r9y9/nnsvs/issues/1 Demo on Google colab 春が来た　春が来た　どこに来た。　山に来た　里に来た、野にも来た。花がさく　花がさく　どこにさく。山にさく　里にさく、野にもさく。\nYour browser does not support the audio element.\nNNSVS はなに？ Neural network-based singing voice synthesis library for research\n研究用途を目的とした、歌声合成エンジンを作るためのオープンソースのライブラリを作ることを目指したプロジェクトです。このプロジェクトについて、考えていることをまとめておこうと思います。\nなぜやるか？ NEUTRINO レベルの品質の歌声合成エンジンが作れるのかやってみたかった オープンソースのツールがほぼない分野なので、ツールを作ると誰かの役にも立っていいかなと思った。研究分野が盛り上がると良いですね というのが理由です。前者の割合が大きく、後者は建前の要素が強いです。要は、できるかどうかがどうしても気になって、気がづいたら熱中していた、という感じです。\n研究用途 機械学習や信号処理にある程度明るい人を想定しています。歌声合成技術を使って創作したい人ではなく、どのようにすればより良い歌声合成を作ることができるのか？といった興味を持つ人が主な対象です。\n創作活動のために歌声合成の技術を使う場合には、すでに優れたツールがあると思いますので、そちらを使っていただくのがよいと思います1。NEUTRINO、Synthesizer V、CeVIO など\nオープンソース オープンソースであることを重視します。歌声合成ソフトウェアは多くありますが、オープンソースのものは多くありません2。このプロジェクトは僕が趣味として始めたもので、ビジネスにする気はまったくないので3、誰でも自由に使えるようにしたいと思っています。オープンなソフトウェアが、研究分野の一助になることを期待しています。\nPytorchベース 過去に nnmnkwiiという音声合成のためのライブラリを作りました。その際には、任意の数値微分ライブラリと使えるようにと考えて設計しましたが、nnsvsはあえてpytorchに依存した形で作ります。\nPytorchと切り離して設計すると汎用的にしやすい一方で、Kaldi やESPnet のようなプロジェクトで成功しているレシピというものが作りずらいです。ESPnetに多少関わって、再現性の担保の重要性を身にしみて感じつつあるので、Pytorchベースの学習、推論など、歌声合成のモデルを構築するために必要なすべてをひっくるめたソフトウェアを目指したいと思います。\nレシピの提供 再現性を重視します。そのために、KaldiやESPnetの成功に習って、レシピという実験を再現するのに必要なすべてのステップが含まれたスクリプトを提供します。レシピは、データの前処理、特徴量抽出、モデル学習、推論、波形の合成などを含みます。\n例えば、このブログのトップに貼った音声サンプルを合成するのに使われたモデルは、公開されているレシピで再現することが可能です。歌声合成エンジンを作るためのありとあらゆるものを透明な形で提供します。\nプロジェクトの進め方について 完全に完成してから公開する、というアプローチとは正反対で、構想のみで実態はまったくできていない状態から始めて、進捗を含めてすべてオープンで確認できるような状態で進めます。進捗は https://github.com/r9y9/nnsvs/issues/1 から確認できます。\n過去にwavenet vocoderをつくったときにも同じような方法ではじめました。突然知らない人がコメントをくれたりするのがオープンソースの面白いところの一つだと思っているので、この方式で進めます。\n現時点の状況 きりたんデータベースを使って、parametric SVS（Sinsyの中身に近いもの）が一通り作れるところまでできました。MusicXMLを入力として、音声波形を出力します。作った歌声合成システムは、time-lagモデル、音素継続長モデル、音響モデルの3つのtrainableなモデルで成り立っています。音楽/言語的特徴量はsinsyで抽出して、音声分析合成にはWORLDを使います。仕組みは、以下の論文の内容に近いです。\nY. Hono et al, “Recent Development of the DNN-based Singing Voice Synthesis System — Sinsy,” Proc. of APSIPA, 2017. (PDF) Mixture density networkは使っていない、ビブラートパラメータを推定していない等、違いはたくさんあります。現時点では劣化sinsyといったところですね T.T\n開発履歴 2020/04/08 (初期版) 一番最初につくったものです。見事な音痴歌声合成になりました。TTSの仕組みを使うだけでは当然だめでした、というオチです。音響モデルでは対数lf0を予測するようにしました。このころはtime-lagモデルを作っていなくて、phonetic timeingはアノテーションされたデータのものを使っています。\n2020/04/26 (本ブログ執筆時点での最新版) Time-lag, duration, acoustic modelのすべてを一旦実装し終わったバージョンです。lf0の絶対値を予測するのではなく、relativeなlf0を予測するように変えました。phonetic timing はすべて予測されたものを使っています。ひととおりできたにはいいですが、完成度はいまいちというのが正直なところですね\n今後の予定 https://github.com/r9y9/nnsvs/issues/1 を随時更新しますが、重要なものをいくつかピップアップします。\n音響モデルの強化：特にF0のモデル化が難しい印象で、改善を考えています。いまは本当に適当なCNNをつかっていますが、autoreggresive modelに変えたいと思っています。いくつか選択肢がありますが、WaveNetのようなモデルにする予定です。https://mtg.github.io/singing-synthesis-demos/ 彼らの論文を大いに参考にする予定です。NIIのWangさんのshallow ARモデルを使うもよし。最重要課題で、目下やることリストに入っています 離散F0モデリング: NIIのWangさんの論文が大変参考になりました。音声合成では広く連続F0が使われている印象ですが、離散F0モデリングを試したいと思っています。 Transformerなどの強力なモデル: 今年の ICASSP 2020 で Feed-forward Transformerを使った歌声合成の研究発表がありましたが、近年のnon-autoregressiveモデルの発展はすごいので、同様のアプローチを試してみたいと思っています。製品化は考えないし、どんなにデカくて遅いモデルを使ってもよし ニューラルボコーダ: 音響モデルの改善がある程度できれば、ニューラルボコーダを入れて高品質にできるといいですね。 音楽/言語特徴量の簡略化: 今は450次元くらいの特徴量を使っていますが、https://mtg.github.io/singing-synthesis-demos/ 彼らのグループの研究を見ると、もっとシンプルにできそうに思えてきています。音楽/言語特徴量の抽出は今はsinsyに頼りっきりですが、どこかのタイミングでシンプルにしたいと思っています。 Time-lag/duration modelの改善: 現時点ではめっちゃ雑なつくりなので、https://mtg.github.io/singing-synthesis-demos/ 彼らの研究を見習って細部まで詰めたい 音素アライメントツール: きりたんDBの音素アライメントが微妙に不正確なのがあったりします。今のところある程度手修正していますが、自動でやったほうがいいのではと思えてきました。 その他データセット: JVSなど。きりたんDBである程度できてからですかね これまで歌声合成をやってみての所感 歌声合成クッソムズすぎワロタ\n新しいことにチャレンジするのはとても楽しいですが、やっぱり難しいですね。離散化F0、autoregressive modelの導入でそれなりの品質に持っていけるという淡い期待をしていますが、さてどうなることやら。地道に頑張って改善していきます。\n参考 きりたんデータベース: https://zunko.jp/kiridev/login.php NEUTRINO: https://n3utrino.work/ NNSVS: https://github.com/r9y9/nnsvs NNSVS 進捗: https://github.com/r9y9/nnsvs/issues/1 sinsy: http://sinsy.sourceforge.net/ My fork of sinsy: https://github.com/r9y9/sinsy nnmnkwii: https://github.com/r9y9/nnmnkwii WORLD: https://github.com/mmorise/World Y. Hono et al, “Recent Development of the DNN-based Singing Voice Synthesis System — Sinsy,” Proc. of APSIPA, 2017. (PDF) NEUTRINO並の品質の歌声合成エンジンが作れたらいいなとは思っていますが、まだまだ道のりは長そうです。 ↩︎\nhttp://sinsy.sourceforge.net/ 有名なものにsinsyがありますが、DNNモデルの学習など、すべてがオープンソースなわけではありません ↩︎\n万が一の場合は、察してください… ↩︎\n","date":1589089345,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589089345,"objectID":"2ae80897e0dc3358cb8a8a06e64689c7","permalink":"https://r9y9.github.io/blog/2020/05/10/nnsvs/","publishdate":"2020-05-10T14:42:25+09:00","relpermalink":"/blog/2020/05/10/nnsvs/","section":"post","summary":"Neural network based singing voice synthesis: https://github.com/r9y9/nnsvs","tags":["Singing Voice synthesis","Deep Learning","Open-Source","Research"],"title":"NNSVS: Pytorchベースの研究用歌声合成ライブラリ","type":"post"},{"authors":["Eunwoo Song","Min-Jae Hwang","Ryuichi Yamamoto","Jin-Seob Kim","Ohsung Kwon","Jae-Min Kim"],"categories":[],"content":"","date":1587541869,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587541869,"objectID":"e02a9428401488304fe330e8d9f38bbb","permalink":"https://r9y9.github.io/projects/mbg_excitnet/","publishdate":"2020-04-22T16:51:09+09:00","relpermalink":"/projects/mbg_excitnet/","section":"project","summary":"Preprint: [arXiv:2008.00132](https://arxiv.org/abs/2008.00132), Published version: [ISCA Archive Interspeech 2020](https://www.isca-speech.org/archive_v0/Interspeech_2020/abstracts/2116.html)","tags":["TTS","Deep Learning","Interspeech"],"title":"Neural text-to-speech with a modeling-by-generation excitation vocoder","type":"project"},{"authors":["Tomoki Hayashi","Ryuichi Yamamoto","Katsuki Inoue","Takenori Yoshimura","Kazuya Takemura","Tomoki Toda","Shinji Watanabe"],"categories":null,"content":"","date":1584331200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584331200,"objectID":"c58dccbe505257683c4e59ec3214f745","permalink":"https://r9y9.github.io/talk/asj-espnet2-tutorial/","publishdate":"2020-03-16T13:00:00+09:00","relpermalink":"/talk/asj-espnet2-tutorial/","section":"event","summary":" ","tags":["Presentation","Invited Talk","TTS","ASR","Deep Learning","Open-Source"],"title":"End-to-End 音声合成の研究を加速させるツールキット ESPnet-TTS / ESPnet-TTS: A toolkit to accelerate research on end-to-end speech synthesis @ ASJ 2020s","type":"event"},{"authors":["Tomoki Hayashi","Ryuichi Yamamoto","Katsuki Inoue","Takenori Yoshimura","Shinji Watanabe","Tomoki Toda","Kazuya Takeda","Yu Zhang","Xu Tan"],"categories":[],"content":"","date":1571901687,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571901687,"objectID":"6cf90ab6af95b49be435fb3171323de2","permalink":"https://r9y9.github.io/projects/espnet-tts/","publishdate":"2019-10-24T16:21:27+09:00","relpermalink":"/projects/espnet-tts/","section":"project","summary":"Preprint: [arXiv:1910.10909](https://arxiv.org/abs/1910.10909) (submitted to [ICASSP 2020](https://2020.ieeeicassp.org/))","tags":["Deep Learning","TTS","ICASSP","Open-Source"],"title":"ESPnet-TTS: Unified, Reproducible, and Integratable Open Source End-to-End Text-to-Speech Toolkit","type":"project"},{"authors":["Ryuichi Yamamoto","Eunwoo Song","Jae-Min Kim"],"categories":[],"content":"Preprint: arXiv:1910.11480 (accepted to ICASSP 2020)\nAudio samples (Japanese) Audio samples (English) Japanese samples were used in the subjective evaluations reported in our paper.\nAuthors Ryuichi Yamamoto (LINE Corp.) Eunwoo Song (NAVER Corp.) Jae-Min Kim (NAVER Corp.) Abstract We propose Parallel WaveGAN1, a distillation-free, fast, and small-footprint waveform generation method using a generative adversarial network. In the proposed method, a non-autoregressive WaveNet is trained by jointly optimizing multi-resolution spectrogram and adversarial loss functions, which can effectively capture the time-frequency distribution of the realistic speech waveform. As our method does not require density distillation used in the conventional teacher-student framework, the entire model can be easily trained even with a small number of parameters. In particular, the proposed Parallel WaveGAN has only 1.44 M parameters and can generate 24 kHz speech waveform 28.68 times faster than real-time on a single GPU environment. Perceptual listening test results verify that our proposed method achieves 4.16 mean opinion score within a Transformer-based text-to-speech framework, which is comparative to the best distillation-based Parallel WaveNet system.\nSystems used for comparision Ground truth: Recorded speech. WaveNet: Gaussian WaveNet [1] ClariNet-$L^{(1)}$: ClariNet [1] with the single STFT auxiliary loss ClariNet-$L^{(1,2,3)}$: ClariNet with the multi-resolution STFT loss ClariNet-GAN-$L^{(1,2,3)}$: ClariNet with the multi-resolution STFT and adversarial losses [2] Parallel WaveGAN-$L^{(1)}$: Parallel WaveGAN with th single STFT loss Parallel WaveGAN-$L^{(1,2,3)}$: Parallel WaveGAN with the multi-resolution STFT loss Audio samples (Japanese) Analysis/synthesis Text-to-speech (Transformer TTS + vocoder models) Analysis/synthesis Sample 1\nGround truthWaveNetClariNet-$L^{(1)}$ ClariNet-$L^{(1,2,3)}$ClariNet-GAN-$L^{(1,2,3)}$Parallel WaveGAN-$L^{(1)}$ Parallel WaveGAN-$L^{(1,2,3)}$ (ours) Sample 2\nGround truthWaveNetClariNet-$L^{(1)}$ ClariNet-$L^{(1,2,3)}$ClariNet-GAN-$L^{(1,2,3)}$Parallel WaveGAN-$L^{(1)}$ Parallel WaveGAN-$L^{(1,2,3)}$ (ours) Sample 3\nGround truthWaveNetClariNet-$L^{(1)}$ ClariNet-$L^{(1,2,3)}$ClariNet-GAN-$L^{(1,2,3)}$Parallel WaveGAN-$L^{(1)}$ Parallel WaveGAN-$L^{(1,2,3)}$ (ours) Sample 4\nGround truthWaveNetClariNet-$L^{(1)}$ ClariNet-$L^{(1,2,3)}$ClariNet-GAN-$L^{(1,2,3)}$Parallel WaveGAN-$L^{(1)}$ Parallel WaveGAN-$L^{(1,2,3)}$ (ours) Sample 5\nGround truthWaveNetClariNet-$L^{(1)}$ ClariNet-$L^{(1,2,3)}$ClariNet-GAN-$L^{(1,2,3)}$Parallel WaveGAN-$L^{(1)}$ Parallel WaveGAN-$L^{(1,2,3)}$ (ours) Text-to-speech Sample 1\nGround truthTransformer + WaveNetTransformer + ClariNet-$L^{(1,2,3)}$ Transformer + ClariNet-GAN-$L^{(1,2,3)}$Transformer + Parallel WaveGAN--$L^{(1,2,3)}$ (ours) Sample 2\nGround truthTransformer + WaveNetTransformer + ClariNet-$L^{(1,2,3)}$ Transformer + ClariNet-GAN-$L^{(1,2,3)}$Transformer + Parallel WaveGAN--$L^{(1,2,3)}$ (ours) Sample 3\nGround truthTransformer + WaveNetTransformer + ClariNet-$L^{(1,2,3)}$ Transformer + ClariNet-GAN-$L^{(1,2,3)}$Transformer + Parallel WaveGAN--$L^{(1,2,3)}$ (ours) Sample 4\nGround truthTransformer + WaveNetTransformer + ClariNet-$L^{(1,2,3)}$ Transformer + ClariNet-GAN-$L^{(1,2,3)}$Transformer + Parallel WaveGAN--$L^{(1,2,3)}$ (ours) Sample 5\nGround truthTransformer + WaveNetTransformer + ClariNet-$L^{(1,2,3)}$ Transformer + ClariNet-GAN-$L^{(1,2,3)}$Transformer + Parallel WaveGAN--$L^{(1,2,3)}$ (ours) Audio samples (English) Analysis/synthesis Text-to-speech (ESPnet-TTS + Our Parallel WaveGAN) LJSpeech dataset is used for the test. Mel-spectrograms (with the range of 70 - 7600 Hz2) were used for local conditioning.\nPlease note that the English samples were not used in the subjective evaluations reported in our paper.\nAnalysis/synthesis That is reflected in definite and comprehensive operating procedures.\nGround truthWaveNetClariNet-$L^{(1)}$ ClariNet-$L^{(1,2,3)}$ClariNet-GAN-$L^{(1,2,3)}$Parallel WaveGAN-$L^{(1)}$ Parallel WaveGAN-$L^{(1,2,3)}$ (ours) The commission also recommends.\nGround truthWaveNetClariNet-$L^{(1)}$ ClariNet-$L^{(1,2,3)}$ClariNet-GAN-$L^{(1,2,3)}$Parallel WaveGAN-$L^{(1)}$ Parallel WaveGAN-$L^{(1,2,3)}$ (ours) That the secret service consciously set about the task of inculcating and maintaining the highest standard of excellence and esprit, for all of its personnel.\nGround truthWaveNetClariNet-$L^{(1)}$ ClariNet-$L^{(1,2,3)}$ClariNet-GAN-$L^{(1,2,3)}$Parallel WaveGAN-$L^{(1)}$ Parallel WaveGAN-$L^{(1,2,3)}$ (ours) This involves tight and unswerving discipline as well as the promotion of an outstanding degree of dedication and loyalty to duty.\nGround truthWaveNetClariNet-$L^{(1)}$ ClariNet-$L^{(1,2,3)}$ClariNet-GAN-$L^{(1,2,3)}$Parallel WaveGAN-$L^{(1)}$ Parallel WaveGAN-$L^{(1,2,3)}$ (ours) The commission emphasizes that it finds no causal connection between the assassination.\nGround truthWaveNetClariNet-$L^{(1)}$ …","date":1571656707,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571656707,"objectID":"952662edb50f57b07704e535ca032113","permalink":"https://r9y9.github.io/projects/pwg/","publishdate":"2019-10-21T20:18:27+09:00","relpermalink":"/projects/pwg/","section":"project","summary":"Preprint: [arXiv:1910.11480](https://arxiv.org/abs/1910.11480) (accepted to [ICASSP 2020](https://2020.ieeeicassp.org/))","tags":["Deep Learning","TTS","ICASSP"],"title":"Parallel WaveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram","type":"project"},{"authors":["Ryuichi Yamamoto","Eunwoo Song","Jae-Min Kim"],"categories":[],"content":"Preprint: arXiv:1904.04472, Published version: ISCA Archive Interspeech 2019\nAuthors Ryuichi Yamamoto (LINE Corp.) Eunwoo Song (NAVER Corp.) Jae-Min Kim (NAVER Corp.) Abstract This paper proposes an effective probability density distillation (PDD) algorithm for WaveNet-based parallel waveform generation (PWG) systems. Recently proposed teacher-student frameworks in the PWG system have successfully achieved a real-time generation of speech signals. However, the difficulties optimizing the PDD criteria without auxiliary losses result in quality degradation of synthesized speech. To generate more natural speech signals within the teacher-student framework, we propose a novel optimization criterion based on generative adversarial networks (GANs). In the proposed method, the inverse autoregressive flow-based student model is incorporated as a generator in the GAN framework, and jointly optimized by the PDD mechanism with the proposed adversarial learning method. As this process encourages the student to model the distribution of realistic speech waveform, the perceptual quality of the synthesized speech becomes much more natural. Our experimental results verify that the PWG systems with the proposed method outperform both those using conventional approaches, and also autoregressive generation systems with a well-trained teacher WaveNet.\nAudio samples There are 8 different systems, that include 6 parallel waveform generation systems (Student-*) trained by different optimization criteria as follows:\nGround truth: Recorded speech. Teacher: Teacher Gaussian WaveNet [1]. Student-AX: STFT auxiliary loss. Student-AXAD: STFT and adversarial losses. Student-KL: KLD loss (Ablation study; not used for subjective evaluations). Student-KLAX: KLD and STFT auxiliary losses. Student-KLAXAD: KLD, STFT, and adversarial losses (proposed). Student-KLAXAD*: Weights optimized version of the above (proposed). Copy-synthesis Japanese female speaker Sample 1\nGround truthTeacherStudent-AX Student-AXAVStudent-KLStudent-KLAX Student-KLAXADStudent-KLAXAD* Sample 2\nGround truthTeacherStudent-AX Student-AXAVStudent-KLStudent-KLAX Student-KLAXADStudent-KLAXAD* Sample 3\nGround truthTeacherStudent-AX Student-AXAVStudent-KLStudent-KLAX Student-KLAXADStudent-KLAXAD* Sample 4\nGround truthTeacherStudent-AX Student-AXAVStudent-KLStudent-KLAX Student-KLAXADStudent-KLAXAD* Sample 5\nGround truthTeacherStudent-AX Student-AXAVStudent-KLStudent-KLAX Student-KLAXADStudent-KLAXAD* References [1]: W. Ping, K. Peng, and J. Chen, “ClariNet: Parallel wave generation in end-to-end text-to-speech,” in Proc. ICLR, 2019 (arXiv). Acknowledgements Work performed with nVoice, Clova Voice, Naver Corp.\nCitation @inproceedings{Yamamoto2019, author={Ryuichi Yamamoto and Eunwoo Song and Jae-Min Kim}, title={{Probability Density Distillation with Generative Adversarial Networks for High-Quality Parallel Waveform Generation}}, year=2019, booktitle={Proc. Interspeech 2019}, pages={699--703}, doi={10.21437/Interspeech.2019-1965}, url={http://dx.doi.org/10.21437/Interspeech.2019-1965} } ","date":1561450829,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561450829,"objectID":"580e1ba5bd688e0e2276ebf5b6e98595","permalink":"https://r9y9.github.io/projects/gan-pwn/","publishdate":"2019-06-25T17:20:29+09:00","relpermalink":"/projects/gan-pwn/","section":"project","summary":"Preprint: [arXiv:1904.04472](https://arxiv.org/abs/1904.04472), Published version: [ISCA Archive Interspeech 2019](https://www.isca-speech.org/archive_v0/Interspeech_2019/abstracts/1965.html)","tags":["Deep Learning","TTS","Interspeech"],"title":"Probability Density Distillation with Generative Adversarial Networks for High-Quality Parallel Waveform Generation","type":"project"},{"authors":null,"categories":null,"content":"LJSpeech Dataset: https://keithito.com/LJ-Speech-Dataset/\nまとめ 最近いろんな研究で LJSpeech が使われていますが、合成音の品質を比べるならクリーンなデータセットを使ったほうがいいですね。でないと、合成音声に含まれるノイズがモデルの限界からくるノイズなのかコーパスの音声が含むノイズ（LJSpeechの場合リバーブっぽい音）なのか区別できなくて、公平に比較するのが難しいと思います。\n例えば、LJSpeechを使うと、ぶっちゃけ WaveGlow がWaveNetと比べて品質がいいかどうかわかんないですよね…1. 例えば最近のNICT岡本さんの研究 (基本周波数とメルケプストラムを用いたリアルタイムニューラルボコーダに関する検討) を引用すると、実際にクリーンなデータで実験すれば（Noise shaping なしで）MOS は WaveNet (4.19) \u0026gt; WaveGlow (3.27) と、結構な差が出たりします。LJSpeechを使った場合の WaveGlow (3.961) \u0026gt; WaveNet (3.885) と比べると大きな差ですね。\nとはいえ、End-to-end音声合成を試すにはとてもいいデータセットであると思うので、積極的に活用しましょう。最近 LibriTTS が公開されたので、そちらも合わせてチェックするといいですね。\nWhy LJSpeech LJSpeech は、keithito さんによって2017年に公開された、単一女性話者によって録音された24時間程度の英語音声コーパスです。なぜ近年よく使われて始めているのかと言うと（2019年6月時点でGoogle scholarで27件の引用）、End-to-end 音声合成の研究に用いるデータセットとして、LJSpeechは最もといっていいほど手軽に手に入るからだと考えています。LJSpeech は public domainで配布されており、利用に制限もありませんし、企業、教育機関、個人など様々な立場から自由に使用することができます。End-to-end 音声合成（厳密にはseq2seq モデルの学習）は一般に大量のデータが必要なことが知られていますが、その要件も満たしていることから、特にEnd-to-end音声合成の研究で用いられている印象を受けます。最近だと、FastSpeech: Fast, Robust and Controllable Text to Speech にも使われていましたね。\n個人的な経験 個人的には、過去に以下のブログ記事の内容で使用してきました。\nTacotron: Towards End-to-End Speech Synthesis / arXiv:1703.10135 [cs.CL] 【単一話者編】Deep Voice 3: 2000-Speaker Neural Text-to-Speech / arXiv:1710.07654 [cs.SD] 【108 話者編】Deep Voice 3: 2000-Speaker Neural Text-to-Speech / arXiv:1710.07654 [cs.SD] Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention. [arXiv:1710.08969] WaveNet vocoder をやってみましたので、その記録です / WaveNet: A Generative Model for Raw Audio [arXiv:1609.03499] WN-based TTSやりました / Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions [arXiv:1712.05884] この記事を書くにあたって整理してみて、ずいぶんとたくさんお世話になっていることが改めてわかりました。keithitoさん本当にありがとうございます。\n2017年、僕がTacotronで遊び始めた当時、End-to-end音声合成が流行ってきていたのですが、フリーで手に入って、End-to-end 音声合成にも使えるような程々に大きな（\u0026gt; 20時間）コーパスって、あんまりなかったんですよね。今でこそ M-AILABS 、LibriTTS、日本語なら JSUT もありますが、当時は選択肢は少なかったと記憶しています。今はいい時代になってきていますね。\n最後に 久しぶりに短いですがブログを書きました。LJSpeechは良いデータセットですので、積極的に活用しましょう。ただ、データセットの特徴として、録音データが若干リバーブがかかったような音になっていることから、ニューラルボコーダの品質比較には（例えば WaveGlow vs WaveNet）あんまり向かないかなと思っています。\n2017年に、End-to-end音声合成を気軽に試そうと思った時にはLJSpeechは最有力候補でしたが、現在は他にもいろいろ選択肢がある気がします。以下、僕がぱっと思いつくものをまとめておきますので、参考までにどうぞ。\nEnd-to-end音声合成に使える手軽に手に入るデータセット LJSpeech Dataset: https://keithito.com/LJ-Speech-Dataset/ LibriTTS: https://arxiv.org/abs/1904.02882 JSUT: https://sites.google.com/site/shinnosuketakamichi/publication/jsut M-AILABS: https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/ VCTK: https://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html 参考 WaveNet: https://deepmind.com/blog/wavenet-generative-model-raw-audio/ WaveGlow: https://nv-adlr.github.io/WaveGlow FastSpeech: https://speechresearch.github.io/fastspeech/ 岡本拓磨，戸田智基，志賀芳則，河井恒，“基本周波数とメルケプストラムを用いたリアルタイムニューラルボコーダに関する検討”，日本音響学会講演論文集，2019年春季, pp. 1057–1060, Mar. 2019. slides 僕の実装 をbest publicly availableWaveNet implementation として比較に使っていただいて恐縮ですが…。 ↩︎\n","date":1560178830,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560178830,"objectID":"0815c2fb7b6c2d9f65949b82f81f39bc","permalink":"https://r9y9.github.io/blog/2019/06/11/ljspeech/","publishdate":"2019-06-11T00:00:30+09:00","relpermalink":"/blog/2019/06/11/ljspeech/","section":"post","summary":"LJSpeech Dataset: https://keithito.com/LJ-Speech-Dataset/\n","tags":["Speech Synthesis","Corpus"],"title":"LJSpeech は価値のあるデータセットですが、ニューラルボコーダの品質比較には向かないと思います","type":"post"},{"authors":null,"categories":null,"content":"Summary Thank you for coming to see my blog post about WaveNet text-to-speech.\nYour browser does not support the audio element. 論文リンク: https://arxiv.org/abs/1712.05884 オンラインデモ: Tacotron2: WaveNet-based text-to-speech demo コード r9y9/wavenet_vocoder, Rayhane-mamah/Tacotron-2 音声サンプル: https://r9y9.github.io/wavenet_vocoder/ 三行まとめ 自作WaveNet (WN) と既存実装Tacotron 2 (WNを除く) を組み合わせて、英語TTSを作りました LJSpeechを学習データとした場合、自分史上 最高品質 のTTSができたと思います Tacotron 2と Deep Voice 3 のabstractを読ませた音声サンプルを貼っておきますので、興味のある方はどうぞ なお、Tacotron 2 の解説はしません。申し訳ありません（なぜなら僕がまだ十分に読み込んでいないため）\n背景 過去に、WaveNetを実装しました（参考: WaveNet vocoder をやってみましたので、その記録です / WaveNet: A Generative Model for Raw Audio [arXiv:1609.03499]）。過去記事から引用します。\nTacotron2 は、あとはやればほぼできる感じですが、直近では僕の中で優先度が低めのため、しばらく実験をする予定はありません。興味のある方はやってみてください。\nやりたいことの一つとしてあったとはいえ、当初の予定通り、スクラッチでTacotron 2を実装する時間は取れなかったのですが、既存実装を使ってみたところ十分に上手く動いているように思えたので、ありがたく使わせていただき、WaveNet TTSを実現することができました。というわけで、結果をここにカジュアルに残しておこうという趣旨の記事になります。\nオープンなデータセット、コードを使って、実際どの程度の品質が得られるのか？学習/推論にどのくらい時間がかかるのか？いうのが気になる方には、参考になるかもしれませんので、よろしければ続きをどうぞ。\n実験条件 細かい内容はコードに譲るとして、重要な点だけリストアップします\nPre-trained models、hyper parameters へのリンク Tacotron2 (mel-spectrogram prediction part): trained 189k steps on LJSpeech dataset (Pre-trained model, Hyper params). WaveNet: trained over 1000k steps on LJSpeech dataset (Pre-trained model, Hyper params) WaveNet 1000k step以上訓練されたモデル (2018/1/27に作ったもの、10日くらい1学習した）をベースに、さらに 320k step学習（約3日）しました。再学習したのは、以前のコードには wavenet_vocoder/issues/33 こんなバグがあったからです。 評価には、exponential moving averagingされたパラメータを使いました。decay パラメータはTaco2論文と同じ 0.9999 学習には、Mel-spectrogram prediction networkにより出力される Ground-truth-aligned (GTA) なメルスペクトログラムではなく、生音声から計算されるメルスペクトログラムを使いました。時間の都合上そうしましたが、GTAを使うとより品質が向上すると考えられます Tacotron 2 (mel-spectrogram prediction) https://github.com/Rayhane-mamah/Tacotron-2 にはWaveNet実装も含まれていますが、mel-spectrogram prediction の部分だけ使用しました2 https://github.com/Rayhane-mamah/Tacotron-2/issues/30#issue-317360759 で公開されている 182k step学習されたモデルを、さらに7k stepほど（数時間くらい）学習させました。再学習させた理由は、自分の実装とRayhane氏の実装で想定するメルスペクトログラムのレンジが異なっていたためです（僕: [0, 1], Rayhane: [-4, 4]）。そういう経緯から、[-4, 4] のレンジであったところ，[0, 4] にして学習しなおしました。直接 [0, 1] にして学習しなかったのは（それでも動く、と僕は思っているのですが）、mel-spectrogram のレンジを大きく取った方が良い、という報告がいくつかあったからです（例えば https://github.com/Rayhane-mamah/Tacotron-2/issues/4#issuecomment-377728945 )。Attention seq2seq は経験上学習が難しいので、僕の直感よりも先人の知恵を優先することにした次第です。WNに入力するときには、 Taco2が出力するメルスペクトログラムを c = np.interp(c, (0, 4), (0, 1)) とレンジを変換して与えました デモ音声 https://r9y9.github.io/wavenet_vocoder/ にサンプルはたくさんあります。が、ここでは違うサンプルをと思い、Tacotron 2 と Deep Voice 3の abstract を読ませてみました。 学習データに若干残響が乗っているので（ノイズっぽい）それが反映されてしまっているのですが、個人的にはまぁまぁよい結果が得られたと思っています。興味がある方は、DeepVoice3など僕の過去記事で触れているTTS結果と比べてみてください。\nなお、推論の計算速度は,、僕のローカル環境（GTX 1080Ti, i7-7700K）でざっと 170 timesteps / second といった感じでした。これは、Parallel WaveNet の論文で触れられている数字とおおまかに一致します。\nThis paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text.\nYour browser does not support the audio element. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize timedomain waveforms from those spectrograms.\nYour browser does not support the audio element. Our model achieves a mean opinion score of 4.53 comparable to a MOS of 4.58 for professionally recorded speech.\nYour browser does not support the audio element. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the input to WaveNet instead of linguistic, duration, and F0 features.\nYour browser does not support the audio element. We further demonstrate that using a compact acoustic intermediate representation enables significant simplification of the WaveNet architecture.\nYour browser does not support the audio element. We present Deep Voice 3, a fully-convolutional attention-based neural text-to-speech system.\nYour browser does not support the audio element. Deep Voice 3 matches state-of-the-art neural speech synthesis systems in naturalness while training ten times faster.\nYour browser does not support the audio element. We scale Deep Voice 3 to data set sizes unprecedented for TTS, training on more than eight hundred hours of audio from over two thousand speakers.\nYour browser does not support the audio element. In addition, we identify common error modes of attention-based speech synthesis networks, demonstrate how to mitigate them, and compare several different waveform synthesis methods.\nYour browser does not support the audio element. We also describe how to scale inference to ten million queries per day on one single-GPU server.\nYour browser does not support the audio element. オンラインデモ Tacotron2: WaveNet-based text-to-speech demo\nGoogle Colabで動かせるようにデモノートブックを作りました。環境構築が不要なので、手軽にお試しできるかと思います。\n雑記 WaveNetを学習するときに、Mel-spectrogram precition networkのGTAな出力でなく、生メルスペクトログラムをそのまま使っても品質の良い音声合成ができるのは個人的に驚きでした。これは …","date":1526793690,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526793690,"objectID":"279fce5533de353975a97d15138d8478","permalink":"https://r9y9.github.io/blog/2018/05/20/tacotron2/","publishdate":"2018-05-20T14:21:30+09:00","relpermalink":"/blog/2018/05/20/tacotron2/","section":"post","summary":"Audio samples: https://r9y9.github.io/wavenet_vocoder/","tags":["Speech Synthesis","Deep Learning","End-to-End","Python","WaveNet"],"title":" WN-based TTSやりました / Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions [arXiv:1712.05884]","type":"post"},{"authors":null,"categories":null,"content":"Summary コード: https://github.com/r9y9/wavenet_vocoder 音声サンプル: https://r9y9.github.io/wavenet_vocoder/ 三行まとめ Local / global conditioning を最低要件と考えて、WaveNet を実装しました DeepVoice3 / Tacotron2 の一部として使えることを目標に作りました PixelCNN++ の旨味を少し拝借し、16-bit linear PCMのscalarを入力として、（まぁまぁ）良い22.5kHzの音声を生成させるところまでできました Tacotron2 は、あとはやればほぼできる感じですが、直近では僕の中で優先度が低めのため、しばらく実験をする予定はありません。興味のある方はやってみてください。\n音声サンプル 左右どちらかが合成音声です^^\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. 自分で書いた背景 WaveNetが発表されたのは、一年以上前 (記事) のことです。発表後すぐに、いくつかオープンソースの実装が出ていたように記憶しています。 一方で、僕が確認していた限りでは、local / global conditioningを十分にサポートした実装がなかったように思います。 例えば、Githubで一番スターが付いている ibab/tensorflow-wavenet では、いまだに十分にサポートされていません（#112）。 これはつまり、生成モデルとしては使えても、TTSには使えない、ということで、僕の要望を満たしてくれるものではありませんでした。また、ちょうど最近、Parallel WaveNetが発表されたのもあり、勉強も兼ねて、local / global conditioningを最低要件として置いて、自分で実装してみようと思った次第です。\n実装を通して僕が一番知りたかった（体感したかった）のは、WaveNetで本当に自然音声並みの品質の音声を生成できるのか？ということなので、Parallel WaveNetで提案されているような推論を高速化するための工夫に関しては手を付けていませんので、あしからず。\n実験を通して得た知見 Dropoutの有無については、WaveNetの論文に書いていませんが、僕は5%をゼロにする形で使いました。問題なく動いていそうです。PixelCNN++にはDropoutを使う旨が書かれていたので、WaveNetでも使われているのかなと推測しています。 Gradient clippingの有無は、両方試しましたが、なくてもあっても学習は安定していました。 条件付けする特徴量と音声サンプルの時間解像度を合わせるのには、（少なくともLJSpeechを使う場合には）同じ値をduplicateするのではなく、Transposed convolutionを使うほうが良さそうです。 ref: r9y9/wavenet_vocoder/#1 初期のWaveNetでは、音声サンプルを256階調にmu-law quantizeして入力します。僕もはじめそうしていたのですが、22.5kHzのLJSpeechのデータを扱っていた時、そもそもmulaw / inv-mulaw で明らかに品質が劣化していることに気づきました。512階調にすればまだましになりましたが、どうせならと思ってPixelCNN++で提案されているMixture of logistic distributionsを使った次第です。 Mixture of logistic distributionsを使う場合は、分散の下限を小さくするのが重要な気がしました (PixelCNN++でいうpixel_cnn_pp/nn.py#L54 の部分)。でないと、生成される音声がノイジーになりやすい印象を受けました。直感的には、external featureで条件付けする場合は特に、logistic distributionがかなりピーキー（分散がすごく小さく）なり得るので、そのピーキーな分布を十分表現できる必要があるのかなと思っています。生成時には確率分布からサンプリングすることになるので、分散の下限値を大きくとってしまった場合、ノイジーになりえるのは想像がつきます。 ref: r9y9/wavenet_vocoder/#7 WaveNetの実装は（比較的）簡単だったので、人のコード読むのツライ…という方は、（僕のコードを再利用なんてせずに）自分で実装するのも良いかなと思いました。勉強にもなりました。 WaveNetが発表された当時は、個人レベルの計算環境でやるのは無理なんじゃないかと思って手を出していなかったのですが、最近はそれが疑問に思えてきたので、実際にやってみました。僕のPCには1台しかGPUがついていませんが (GTX 1080 Ti)、個人でも可能だと示せたかと思います。 実験をはじめた当初、バッチサイズ1でもGPUメモリ (12GB) を使いきってしまう…とつらまっていたのですが、Parallel WaveNetの論文でも言及されている通り、音声の一部を短く（7680サンプルとか）切り取って使っても、品質には影響しなさそうなことを確認しました。参考までに、この記事に貼ったサンプルは、バッチサイズ2、一音声あたりの長さ8000に制限して、実験して得たものです。学習時間は、パラメータを変えながら重ね重ねファインチューニングしていたので正確なことは言えないのですが、トータルでいえば10日くらい学習したかもしれません。ただ、1日くらいで、それなりにまともな音声はでます。 おわりに WaveNetのすごさを実際に体感することができました。まだやりたいことは残っていますが、僕はそこそこ満足しました。 今後のTODO及び過去/現在の進捗は、 r9y9/wavenet_vocoder/#1 にまとめています。海外の方との議論も見つかるので、興味のある方は見てください。 実装をはじめた当初からコードを公開していたのですが、どうやら興味を持った方が複数いたようで、上記issueにて有益なコメントをたくさんもらいました。感謝感謝 参考にした論文 Aaron van den Oord, Sander Dieleman, Heiga Zen, et al, “WaveNet: A Generative Model for Raw Audio”, arXiv:1609.03499, Sep 2016. Aaron van den Oord, Yazhe Li, Igor Babuschkin, et al, “Parallel WaveNet: Fast High-Fidelity Speech Synthesis”, arXiv:1711.10433, Nov 2017. Tamamori, Akira, et al. “Speaker-dependent WaveNet vocoder.” Proceedings of Interspeech. 2017. Jonathan Shen, Ruoming Pang, Ron J. Weiss, et al, “Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions”, arXiv:1712.05884, Dec 2017. Wei Ping, Kainan Peng, Andrew Gibiansky, et al, “Deep Voice 3: 2000-Speaker Neural Text-to-Speech”, arXiv:1710.07654, Oct. 2017. Tim Salimans, Andrej Karpathy, Xi Chen, Diederik P. Kingma, “PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications”, arXiv:1701.05517, Jan. 2017. 参考になったコード tensorflow/magenta/nsynth/wavenet musyoku/wavenet コードはもちろん、こちら #4 のイシューも役に立ちました。 ibab/tensorflow-wavenet openai/pixel-cnn PixelCNN++の公式実装です pclucas14/pixel-cnn-pp PixelCNN++のPyTorch実装です 参考になりそうなコード ※僕は参考にしませんでしたが、役に立つかもしれません\nhttps://github.com/kan-bayashi/PytorchWaveNetVocoder https://github.com/tomlepaine/fast-wavenet https://github.com/vincentherrmann/pytorch-wavenet https://github.com/dhpollack/fast-wavenet.pytorch ","date":1517066075,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517066075,"objectID":"9d63be46886393e969e0a89bb0a64af5","permalink":"https://r9y9.github.io/blog/2018/01/28/wavenet_vocoder/","publishdate":"2018-01-28T00:14:35+09:00","relpermalink":"/blog/2018/01/28/wavenet_vocoder/","section":"post","summary":"Audio samples: https://r9y9.github.io/wavenet_vocoder/","tags":["Speech Synthesis","Deep Learninng","WaveNet"],"title":"WaveNet vocoder をやってみましたので、その記録です / WaveNet: A Generative Model for Raw Audio [arXiv:1609.03499]","type":"post"},{"authors":["Ryuichi Yamamoto"],"categories":[],"content":"","date":1515568373,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515568373,"objectID":"9d3ffbebab160d64c03b1bb54ffac30d","permalink":"https://r9y9.github.io/projects/wavenet/","publishdate":"2018-01-10T16:12:53+09:00","relpermalink":"/projects/wavenet/","section":"project","summary":"","tags":["Open-Source","TTS"],"title":"An open-source implementation of WaveNet vocoder","type":"project"},{"authors":["Ryuichi Yamamoto"],"categories":[],"content":"","date":1514359062,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514359062,"objectID":"cb635d233e155491d4565866c81d9c8c","permalink":"https://r9y9.github.io/projects/deepvoice3/","publishdate":"2017-12-27T16:17:42+09:00","relpermalink":"/projects/deepvoice3/","section":"project","summary":"","tags":["Open-Source","TTS"],"title":"An open-source implementation of Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning","type":"project"},{"authors":null,"categories":null,"content":"Summary 論文リンク: arXiv:1710.07654 コード: https://github.com/r9y9/deepvoice3_pytorch VCTK: https://datashare.ed.ac.uk/handle/10283/2950 音声サンプルまとめ: https://r9y9.github.io/deepvoice3_pytorch/ 三行まとめ arXiv:1710.07654: Deep Voice 3: 2000-Speaker Neural Text-to-Speech を読んで、複数話者の場合のモデルを実装しました 論文のタイトル通りの2000話者とはいきませんが、VCTK を使って、108 話者対応の英語TTSモデルを作りました（学習時間1日くらい） 入力する話者IDを変えることで、一つのモデルでバリエーションに富んだ音声サンプルを生成できることを確認しました 概要 【単一話者編】Deep Voice 3: 2000-Speaker Neural Text-to-Speech / arXiv:1710.07654 [cs.SD] の続編です。\n論文概要は前回紹介したものと同じなので、話者の条件付けの部分についてのみ簡単に述べます。なお、話者の条件付けに関しては、DeepVoice2の論文 (arXiv:1705.08947 [cs.CL]) の方が詳しいです。\nまず基本的に、話者の情報は trainable embedding としてモデルに組み込みます。text embeddingのうようにネットワークの入力の一箇所に入れるような設計では学習が上手くかない（話者情報を無視するようになってしまうのだと思います）ため、ネットワークのあらゆるところに入れるのがポイントのようです。具体的には、Encoder, Decoder (+ Attention), Converterのすべてに入れます。さらに具体的には、ネットワークの基本要素である Gated linear unit + Conv1d のすべてに入れます。詳細は論文に記載のarchitectureの図を参照してください。\n話者の条件付けに関して、一つ注意を加えるとすれば、本論文には明示的に書かれていませんが、 speaker embeddingは各時間stepすべてにexpandして用いるのだと思います（でないと実装するときに困る）。DeepVoice2の論文にはその旨が明示的に書かれています。\nVCTK の前処理 実験に入る前に、VCTKの前処理について、簡単にまとめたいと思います。VCTKの音声データには、数秒に渡る無音区間がそれなりに入っているので、それを取り除く必要があります。以前、日本語 End-to-end 音声合成に使えるコーパス JSUT の前処理 で書いた内容と同じように、音素アライメントを取って無音区間を除去します。僕は以下の二つの方法をためしました。\nGentle (Kaldiベース) Merlin 付属のアライメントツール (festvoxベース) (便利スクリプト) 論文中には、（無音除去のため、という文脈ではないのですが1）Gentleを使った旨が書かれています。しかし、試したところアライメントが失敗するケースがそれなりにあり、loop は後者の方法を用いており良い結果も出ていることから、結論としては僕は後者を採用しました。なお、両方のコードは残してあるので、気になる方は両方ためしてみてください。\n実験 VCTK の108話者分のすべて2を使用して、20時間くらい（30万ステップ x 2）学習しました。30万ステップ学習した後できたモデルをベースに、さらに30万ステップ学習しました3。モデルは、単一話者の場合とほとんど同じですが、変更を加えた点を以下にまとめます。\n共通: Speaker embedding を追加しました。 共通: Speaker embeddingをすべての時間ステップにexpandしたあと、Dropoutを適用するようにしました（論文には書いていませんが、結論から言えば重要でした…） Decoder: アテンションのレイヤー数を2から1に減らしました 計算速度は、バッチサイズ16で、8.6 step/sec くらいでした。GPUメモリの使用量は9GB程度でした。Convolution BlockごとにLinearレイヤーが追加されるので、それなりにメモリ使用量が増えます。PyTorch v0.3.0を使いました。\n学習に使用したコマンドは以下です。\npython train.py --data-root=./data/vctk --checkpoint-dir=checkpoints_vctk \\ --hparams=\u0026#34;preset=deepvoice3_vctk,builder=deepvoice3_multispeaker\u0026#34; \\ --log-event-path=log/deepvoice3_multispeaker_vctk_preset \\ --load-embedding=20171221_deepvoice3_checkpoint_step000300000.pth # \u0026lt;\u0026lt; 30万ステップで一旦打ち切り \u0026gt;\u0026gt; # もう一度0から30万ステップまで学習しなおし python train.py --data-root=./data/vctk --checkpoint-dir=checkpoints_vctk_fineturn \\ --hparams=\u0026#34;preset=deepvoice3_vctk,builder=deepvoice3_multispeaker\u0026#34; \\ --log-event-path=log/deepvoice3_multispeaker_vctk_preset_fine \\ --restore-parts=./checkpoints_vctk/checkpont_step000300000.pth 学習を高速化するため、LJSpeechで30万ステップ学習したモデルのembeddingの部分を再利用しました。また、cyclic annealingのような効果が得られることを期待して、一度学習を打ち切って、さらに0stepからファインチューニングしてみました。\nコードのコミットハッシュは 0421749 です。正確なハイパーパラメータが知りたい場合は、ここから辿れると思います。\nアライメントの学習過程 (~30万ステップ) 学習された Speaker embedding の可視化 論文のappendixに書かれているのと同じように、学習されたEmbeddingに対してPCAをかけて可視化しました。論文の図とは少々異なりますが、期待通り、男女はほぼ線形分離できるようになっていることは確認できました。\n音声サンプル 最初に僕の感想を述べておくと、LJSpeechで単一話者モデルを学習した場合と比べると、汎化しにくい印象がありました。文字がスキップされるといったエラーケースも比較して多いように思いました。 たくさんサンプルを貼るのは大変なので、興味のある方は自分で適当な未知テキストを与えて合成してみてください。学習済みモデルは deepvoice3_pytorch#pretrained-models からダウンロードできるようにしてあります。\nLoop と同じ文章 Some have accepted this as a miracle without any physical explanation\n(69 chars, 11 words)\nspeaker IDが若い順に12サンプルの話者ID を与えて、合成した結果を貼っておきます。\n225, 23, F, English, Southern, England (ID, AGE, GENDER, ACCENTS, REGION)\nYour browser does not support the audio element. 226, 22, M, English, Surrey\nYour browser does not support the audio element. 227, 38, M, English, Cumbria\nYour browser does not support the audio element. 228, 22, F, English, Southern England\nYour browser does not support the audio element. 229, 23, F, English, Southern England\nYour browser does not support the audio element. 230, 22, F, English, Stockton-on-tees\nYour browser does not support the audio element. 231, 23, F, English, Southern England\nYour browser does not support the audio element. 232, 23, M, English, Southern England\nYour browser does not support the audio element. 233, 23, F, English, Staffordshire\nYour browser does not support the audio element. 234, 22, F, Scottish, West Dumfries\nYour browser does not support the audio element. 236, 23, F, English, Manchester\nYour browser does not support the audio element. 237, 22, M, Scottish, Fife\nYour browser does not support the audio element. 声質だけでなく、話速にもバリエーションが出ているのがわかります。231 の最初で一部音が消えています（こういったエラーケースはよくあります）。\nkeithito/tacotron のサンプル と同じ文章 簡単に汎化性能をチェックするために、未知文章でテストします。\n男性 (292, 23, M, NorthernIrish, Belfast) 女性 (288, 22, F, Irish, Dublin) の二つのサンプルを貼っておきます。\nScientists at the CERN laboratory say they have discovered a new particle.\n(74 chars, 13 words)\nYour browser does not support the audio element. Your browser does not support the audio element. There’s a way to measure the acute emotional intelligence that has never gone out of style.\n(91 chars, 18 words)\nYour browser does not support the audio element. Your browser does not support the audio element. President Trump met with other leaders at the Group of 20 conference.\n(69 …","date":1513924200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513924200,"objectID":"eab1b2f00688d185620743406d81b7c5","permalink":"https://r9y9.github.io/blog/2017/12/22/deepvoice3_multispeaker/","publishdate":"2017-12-22T15:30:00+09:00","relpermalink":"/blog/2017/12/22/deepvoice3_multispeaker/","section":"post","summary":"Audio samples: https://r9y9.github.io/deepvoice3_pytorch/","tags":["Speech Synthesis","Python","Deep Learning"],"title":"【108 話者編】Deep Voice 3: 2000-Speaker Neural Text-to-Speech / arXiv:1710.07654 [cs.SD]","type":"post"},{"authors":null,"categories":null,"content":" Jupyter Advent Calendar 2017 21日目の記事です。\nC++をJupyterで使う方法はいくつかあります。この記事では、僕が試したことのある以下の4つの方法について、比較しつつ紹介したいと思います。\nroot/cling 付属のカーネル root/root 付属のカーネル xeus-cling Keno/Cxx.jl をIJuliaで使う まとめとして、簡単に特徴などを表にまとめておきますので、選ぶ際の参考にしてください。詳細な説明は後に続きます。\ncling ROOT xeus-cling Cxx.jl + IJulia C++インタプリタ実装 C++ C++ C++ Julia + C++ (Tab) Code completion ○ ○ ○ x Cインタプリタ △1 △ △ ○ %magics x %%cpp, %%jsroot, その他 x △2 他言語との連携 x Python, R 3 x Julia バイナリ配布 公式リンク 公式リンク (python2系向け） condaで提供 △4 オンラインデモ x rootdemo binderリンク x 共通事項\nすべて、clang/llvmをC++インタプリタのベースにしています。Cxx.jl以外は、C++インタプリタであるclingをベースに、さらにその上にjupyterカーネルを実装しています。\n1. cling clingは、いわずとしれた（？）C++インタプリタ実装です。後述するROOTという data analysis framework の一部として、CERNによって開発されています。(20年くらい前の) 古くからあったCINTというC++インタプリタ実装が、clangを使って書き直された、という歴史があります。clingプロジェクトの一環としてJupyterカーネルが開発されています。\n良いところ\nclingの起動が速いのでストレスが少ない 5 イマイチなところ\nIPythonだと使える %time のようなマジックはない cling本体で良いのでは？感が否めない。cling本体のREPLを使えば、Ctrl+Rによるヒストリ検索も使えるし… 使ってみた感想まとめ\n個人的には、Jupyterは可視化と組み合わせてこそ良さがあると思っているのもありますが、あえてJupyterで使う必要性を僕は感じませんでした。cling自体はとても素晴らしいのと、ノートブックとして実行結果ごとコードを保存したい、といった目的でjupyterを使う場合には、良いと思いました。\n注意\n#include \u0026lt;iostream\u0026gt; のあとにcode completionをしようとするとclingが落ちる、というバグがあります。Jupyterの場合はカーネルがリスタートします。\nhttps://github.com/vgvassilev/cling/issues/152 参考リンク\n公式web: https://cdn.rawgit.com/root-project/cling/master/www/index.html Github: https://github.com/root-project/cling 紹介スライド: LLVM Developers’ Meeting, “Creating cling, an interactive interpreter interface for clang”, Axel Naumann, Philippe Canal, Paul Russo, Vassil Vassilev, 04.11.2010, San Jose, CA, United States 2. ROOT ROOTの説明を公式ページから引用します：\nA modular scientific software framework. It provides all the functionalities needed to deal with big data processing, statistical analysis, visualisation and storage. It is mainly written in C++ but integrated with other languages such as Python and R.\n日本語の情報が少ない印象ですが、ROOT 講習会 2017 | 高エネルギー宇宙物理学のための ROOT 入門 – ROOT for High-Energy Astrophysics (RHEA) によると、実験系素粒子物理学では標準的なデータ解析ソフトウェア・ライブラリ群のようです。\nROOTプロジェクト自体にclingを含みますが、clingが提供するjupyterカーネルとは別で、JupyROOT というカーネルが開発されています。\n良いところ\nPythonとC++をミックスできる。%%cpp magicでC++関数を定義して、ホスト（python) 側から呼び出せる %%jsroot magic により、グラフをインタラクティブに動かせる IPythonで使えるmagicが使える（%timeit, %time, %load_ext等） イマイチなところ\nNumpyやmatplotlibなど、Pythonを用いた数値計算ではデファクトに近いツールとの連携は微妙に思いました 6。cythonのように、手軽にnumpy arrayをC++に渡す、といった方法はなさそう・・・？（あったら教えてください） ROOTの（でかい）APIを覚えないと使えなさそうで、ハードルが高い 公式のbinderのデモノートブック、ちょいちょいカーネルが落ちる… 使ってみた感想まとめ\nJupyterカーネルはclingのものよりも良いと思いました。PythonとC++をミックスできるのが特に良いと思います。個人的には、ROOTが機能もりもりのデカイソフトウェアなことがあまり好きになれず、使い込んでいないのですが、ROOTのAPIに慣れた人、あるいは好きになれる人には、良いと思います。\nclingだと #include \u0026lt;iostream\u0026gt;のあとにcode completionで落ちる、というバグがありまたが、ROOT付属のcling (ROOT 6.10/08 をソースからビルドして使いました) ではそのバグはありませんでした。\n参考リンク\n公式ページ: https://root.cern.ch/ Github: https://github.com/root-project/root オンラインデモ: https://swanserver.web.cern.ch/swanserver/rootdemo/ 3. xeus-cling 先月 11月30日に、Jupyter blog で紹介 されたカーネルです。名前の通りclingをベースにしています。C++インタプリタとしては機能的にcling付属カーネルと同じですが、xeus というJupyter kernel protocolのC++実装をベースにしている点が異なります。\n良いところ\ncondaでパッケージとして提供されているので、インストールが楽。clang/clingも併せてインストールしてくれます 同じ開発元が、xplot という可視化ライブラリを提供している（ただしalphaバージョン） 標準ライブラリのヘルプが ? コマンドで確認できます (例. ?std::vector) イマイチなところ\n外部ライブラリをロードしようとしたら動きませんでした（なので プルリク 投げました（が、いい方法ではなかったようでcloseされました %timeit の実装があったので試してみましたが、エラーが出て動きませんでした 使ってみた感想まとめ\n少しalphaバージョンの印象を受けました。xplotなど周辺ツールへの期待がありますが、個人的にはmatplotlib等pythonの可視化ツールでいいのでは…という気持ちになりました。\n参考リンク\nGithub: https://github.com/QuantStack/xeus-cling 紹介記事: https://blog.jupyter.org/interactive-workflows-for-c-with-jupyter-fe9b54227d92 4. Cxx.jl + IJulia.jl Cxx.jlは、clangをベースにしたJuliaのC++インタフェースです。JuliaにはIJuliaというJupyterカーネルの実装があるので、IJuliaとCxx.jlを使えば、Jupyter上でC++を使うことができます。過去にCxx.jlに関する記事をいくつか書きましたので、そのリンクを貼っておきます。\nCxx.jlを用いてJulia expression/value をC++に埋め込む実験 | LESS IS MORE Cxx.jl を使ってみた感想 + OpenCV.jl, Libfreenect2.jl の紹介 | LESS IS MORE 良いところ\nJuliaとC++をミックスできます。過去記事に書きましたが、例えばC++関数内でJuliaのプログレスバーを使ったりできます C++インタプリタとCインタプリタを切り替えられます icxx と cxx マクロで、それぞれローカル/グローバルスコープを切り替えられます。 Juliaの配列をC++に渡すのは非常に簡単にできます。例を以下に示します C++ \u0026gt; #include \u0026lt;iostream\u0026gt; true julia\u0026gt; cxx\u0026#34;\u0026#34;\u0026#34; template \u0026lt;class T\u0026gt; void f(T x, int n) { for (int i = 0; i \u0026lt; n; ++i) { std::cout \u0026lt;\u0026lt; x[i] \u0026lt;\u0026lt; std::endl; } }\u0026#34;\u0026#34;\u0026#34;; julia\u0026gt; x = rand(5) 10-element Array{Float64,1}: 0.593086 0.736548 0.344246 0.390799 0.226175 julia\u0026gt; icxx\u0026#34;f($(pointer(x)), $(length(x)));\u0026#34; 0.593086 0.736548 0.344246 0.390799 0.226175 イマイチなところ\nCxxパッケージを読み込むのに多少時間がかかります。僕の環境では（プリコンパイルされた状態で）2.5秒程度でした (Tab) Code completionは実装されていません #61 icxx or cxx で囲まないといけず、syntax highlightはされません 使ってみた感想まとめ\n僕は一年以上Cxx.jlを使っているので、バイアスも入っていると思いますが、かなり使いやすいと思います。パッケージのロードに時間がかかるのは、何度もカーネルやjuliaを再起動したりしなければ、まったく気になりません。IJuliaの設計上の理由 により、magicはありませんが、例えば %time は @time マクロで十分であり、不便に感じません。\n参考リンク\nIJulia: https://github.com/JuliaLang/IJulia.jl Cxx : https://github.com/Keno/Cxx.jl まとめ C++と他言語のやりとりのスムースさの観点から、やはり僕は対話環境でC++を使うならCxx.jlが最高だと思いました。Cxx + JuliaのREPLも便利ですが、Cxx + IJuliaも良いと思います。 ただし、C++単体でしか使わない、ということであれば、cling or xeus-clingが良いと思います。ただし xeus-clingは、前述の通り外部ライブラリを読みこもうとするとエラーになる問 …","date":1513782000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513782000,"objectID":"de1b5d16f8b2aa2282c5bcbb56b6b1b0","permalink":"https://r9y9.github.io/blog/2017/12/21/jupyter-cxx/","publishdate":"2017-12-21T00:00:00+09:00","relpermalink":"/blog/2017/12/21/jupyter-cxx/","section":"post","summary":"[Jupyter Advent Calendar 2017](https://qiita.com/advent-calendar/2017/jupyter)","tags":["Jupyter","C++","Julia","cling","Advent Calendar"],"title":"Interactive C++: Jupyter上で対話的にC++を使う方法の紹介 [Jupyter Advent Calendar 2017]","type":"post"},{"authors":null,"categories":null,"content":"Line https://bokeh.pydata.org/en/latest/docs/reference/models/glyphs/line.html\nVBar https://bokeh.pydata.org/en/latest/docs/reference/models/glyphs/vbar.html\nHBar https://bokeh.pydata.org/en/latest/docs/reference/models/glyphs/hbar.html\nImageRGBA https://bokeh.pydata.org/en/latest/docs/reference/models/glyphs/image_rgba.html\nImageRGBA https://bokeh.pydata.org/en/latest/docs/reference/models/glyphs/image_rgba.html\n前置き Jupyter Advent Calendar 2017 14日目の記事です。この記事は、Jupyter notebookで作成したものをnbconvertでmarkdownに変換し、手で少し修正して作りました。読み物としてはこの記事を、実行するにはノートブックの方を参照していただくのが良いかと思います。\nノートブック (gist) nbviewer 概要 適当なニューラルネットの学習過程の可視化（ロス、正解率の遷移等）を題材にして、Bokehを使って動的にグラフを更新していくことによる可視化の実用例を紹介します。このノートブックの冒頭に、最後まで実行すると得られるグラフ一覧をまとめました。どうやってグラフを作るのか知りたい方は続きを読んでもらえればと思います。Bokehの詳細な使い方は、公式ドキュメントを参考にしてください。\nなお、ここで紹介する例は、僕が過去に出た機械学習のコンペ (https://deepanalytics.jp/compe/36?tab=compedetail) で実際に使用したコードからほぼ取ってきました（8/218位でした）。グラフを動的に更新する方法は 公式ドキュメント に記述されていますが、そのサンプルの内容は「円を描画して色を変える」といった実用性皆無のものであること、またググっても例が多く見つからないことから、このような紹介記事を書くことにしました。参考になれば幸いです。\n余談ではありますが、こと機械学習に関しては、tensorboardを使ったほうが簡単で良いと思います。僕は最近そうしています。 https://qiita.com/r9y9/items/d54162d37ec4f110f4b4. 色なり位置なり大きさなりを柔軟にカスタマイズしたい、あるいはノートブックで処理を完結させたい、と言った場合には、ここで紹介する方法も良いかもしれません。\n%pylab inline Populating the interactive namespace from numpy and matplotlib from IPython.display import HTML, Image import IPython from os.path import exists def summary(): baseurl = \u0026#34;https://bokeh.pydata.org/en/latest/docs/reference/models/glyphs/\u0026#34; for (name, figname, url) in [ (\u0026#34;Line\u0026#34;, \u0026#34;line\u0026#34;, \u0026#34;line.html\u0026#34;), (\u0026#34;VBar\u0026#34;, \u0026#34;vbar\u0026#34;, \u0026#34;vbar.html\u0026#34;), (\u0026#34;HBar\u0026#34;, \u0026#34;hbar\u0026#34;, \u0026#34;hbar.html\u0026#34;), (\u0026#34;ImageRGBA\u0026#34;, \u0026#34;gray_image\u0026#34;, \u0026#34;image_rgba.html\u0026#34;), (\u0026#34;ImageRGBA\u0026#34;, \u0026#34;inferno_image\u0026#34;, \u0026#34;image_rgba.html\u0026#34;), ]: gif = \u0026#34;./fig/{}.gif\u0026#34;.format(figname) print(\u0026#34;\\n\u0026#34;,name, baseurl + url) if exists(gif): with open(gif, \u0026#39;rb\u0026#39;) as f: IPython.display.display(Image(data=f.read()), format=\u0026#34;gif\u0026#34;) summary() (※ブログ先頭に貼ったので省略します)\n# True にしてノートブックを実行すると、上記gifの元となる画像を保存し、最後にgifを生成する save_img = False if save_img: import os from os.path import exists if not exists(\u0026#34;./fig\u0026#34;): os.makedirs(\u0026#34;./fig\u0026#34;) toolbar_location = None else: toolbar_location = \u0026#34;above\u0026#34; # bokehで描画したグラフはnotebookに残らないので、Trueの場合は代わりに事前に保存してあるgifを描画する # もしローカルで実行するときは、Falseにしてください show_static = True 準備 先述の通り、ニューラルネットの学習過程の可視化を題材として、Jupyter上でのBokehの使い方を紹介していきたいと思います。今回は、PyTorch (v0.3.0) を使ってニューラルネットの学習のコードを書きました。\nhttps://github.com/pytorch/examples/tree/master/mnist をベースに、可視化しやすいように少しいじりました。\nimport torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torchvision import datasets, transforms from torch.autograd import Variable import numpy as np Data use_cuda = torch.cuda.is_available() batch_size = 128 kwargs = {\u0026#39;num_workers\u0026#39;: 1, \u0026#39;pin_memory\u0026#39;: True} if use_cuda else {} train_loader = torch.utils.data.DataLoader( datasets.MNIST(\u0026#39;./data\u0026#39;, train=True, download=True, transform=transforms.Compose([ transforms.ToTensor() ])), batch_size=batch_size, shuffle=True, **kwargs) test_loader = torch.utils.data.DataLoader( datasets.MNIST(\u0026#39;./data\u0026#39;, train=False, transform=transforms.Compose([ transforms.ToTensor(), ])), batch_size=batch_size, shuffle=False, **kwargs) data_loaders = {\u0026#34;train\u0026#34;: train_loader, \u0026#34;test\u0026#34;:test_loader} Model 簡単な畳み込みニューラルネットです。\nclass Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(1, 10, kernel_size=5) self.conv2 = nn.Conv2d(10, 20, kernel_size=5) self.conv2_drop = nn.Dropout2d() self.fc1 = nn.Linear(320, 50) self.fc2 = nn.Linear(50, 10) def forward(self, x): x = F.relu(F.max_pool2d(self.conv1(x), 2)) x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2)) x = x.view(-1, 320) x = F.relu(self.fc1(x)) x = F.dropout(x, training=self.training) x = self.fc2(x) return F.log_softmax(x, dim=-1) Train loop from tqdm import tnrange epochs = 20 def __train_loop(model, data_loaders, optimizer, epoch, phase): model = model.train() if phase == \u0026#34;train\u0026#34; else model.eval() running_loss = 0 running_corrects = 0 corrects = [0]*10 counts = [0]*10 for batch_idx, (x, y) in enumerate(data_loaders[phase]): x = x.cuda() if use_cuda else x y = y.cuda() if use_cuda else y x, y = Variable(x), Variable(y) optimizer.zero_grad() y_hat = model(x) # loss loss = F.nll_loss(y_hat, y) # update if phase == \u0026#34;train\u0026#34;: loss.backward() optimizer.step() running_loss += loss.data[0] # predict preds = torch.max(y_hat.data, 1)[1] match = (preds == y.data).cpu() running_corrects += match.sum() # カテゴリごとの正解率を出すのにほしい for i in range(len(match)): if match.view(-1)[i] \u0026gt; 0: corrects[y.data.view(-1)[i]] += 1 for i in range(len(match)): counts[y.data.view(-1)[i]] += 1 # epoch-wise metrics l = running_loss / len(data_loaders[phase]) acc = running_corrects / len(data_loaders[phase].dataset) return {\u0026#34;loss\u0026#34;: l, \u0026#34;acc\u0026#34;: acc, \u0026#34;corrects\u0026#34;: corrects, \u0026#34;counts\u0026#34;: counts} def train_loop(model, data_loaders, optimizer, epochs=12, callback=None): history = {\u0026#34;train\u0026#34;: {}, \u0026#34;test\u0026#34;: {}} for epoch in tnrange(epochs): for phase in [\u0026#34;train\u0026#34;, \u0026#34;test\u0026#34;]: d = __train_loop(model, data_loaders, optimizer, …","date":1513177230,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513177230,"objectID":"d7614ded2e099a24132d6d1d84bb80ec","permalink":"https://r9y9.github.io/blog/2017/12/14/jupyter-bokeh/","publishdate":"2017-12-14T00:00:30+09:00","relpermalink":"/blog/2017/12/14/jupyter-bokeh/","section":"post","summary":"[Jupyter Advent Calendar 2017](https://qiita.com/advent-calendar/2017/jupyter)","tags":["Jupyter","Python","Bokeh","PyTorch","Visualization","Advent Calendar"],"title":"ニューラルネットの学習過程の可視化を題材に、Jupyter + Bokeh で動的な描画を行う方法の紹介 [Jupyter Advent Calendar 2017]","type":"post"},{"authors":null,"categories":null,"content":"Summary 論文リンク: arXiv:1710.07654 コード: https://github.com/r9y9/deepvoice3_pytorch 三行まとめ arXiv:1710.07654: Deep Voice 3: 2000-Speaker Neural Text-to-Speech を読んで、単一話者の場合のモデルを実装しました（複数話者の場合は、今実験中です (deepvoice3_pytorch/#6) arXiv:1710.08969 と同じく、RNNではなくCNNを使うのが肝です 例によって LJSpeech Dataset を使って、英語TTSモデルを作りました（学習時間半日くらい）。論文に記載のハイパーパラメータでは良い結果が得られなかったのですが、arXiv:1710.08969 のアイデアをいくつか借りることで、良い結果を得ることができました。 概要 Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention. [arXiv:1710.08969] で紹介した方法と、モチベーション、基本的な方法論はまったく同じのため省略します。モデルのアーキテクチャが異なりますが、その点についても前回述べたので、そちらを参照ください。 今回の記事では、DeepVoice3のアーキテクチャをベースにした方法での実験結果をまとめます。\n予備実験 はじめに、可能な限り論文に忠実に、論文に記載のモデルアーキテクチャ、ハイパーパラメータで、レイヤー数やConvレイヤーのカーネル数を若干増やしたモデルで試しました。（増やさないと、LJSpeechではイントネーションが怪しい音声が生成されてしまいました）。しかし、どうもビブラートがかかったような音声が生成される傾向にありました。色々試行錯誤して改良したのですが、詳細は後述するとして、改良前/改良後の音声サンプルを以下に示します。\nGenerative adversarial network or variational auto-encoder.\n(59 chars, 7 words)\n改良前：\nYour browser does not support the audio element. 改良後：\nYour browser does not support the audio element. いかがでしょうか。結構違いますよね。なお、改良前のモデルは53万イテレーション、改良後は21万イテレーション学習しました。回数を増やせばいいというものではないようです（当たり前ですが）。結論からいうと、モデルの自由度が足りなかったのが品質が向上しにくかった原因ではないかと考えています。\n2017/12/21 追記：すいません、21万イテレーションのモデルは、何かしら別の事前学習したモデルから、さらに学習したような気がしてきました…。ただ、合計で53万もイテレーションしていないのは間違いないと思います申し訳ございません\n実験 前回と同じく LJSpeech Dataset を使って、11時間くらい（21万ステップ）学習しました。モデルは、DeepVoice3で提案されているものを少しいじりました。どのような変更をしたのか、以下にまとめます。\nEncoder: レイヤー数を増やし、チャンネル数を大きくしました。代わりにカーネル数は7から3に減らしました Decoder: メル周波数スペクトログラムの複数フレームをDecoderの1-stepで予測するのではなく、arXiv:1710.08969 で述べられているように、1-stepで（粗い）1フレームを予測して、ConvTransposed1d により元の時間解像度までアップサンプリングする（要は時間方向のアップサンプリングをモデルで学習する）ようにしました Decoder: アテンションの前に、いくつかConv1d + ReLUを足しました Converter: ConvTransposed1dを二つ入れて、時間解像度を4倍にアップサンプリングするようにしました Converter: チャンネル数を大きくしました Decoder/Converter: レイヤーの最後にSigmoidを追加しました Loss: Guided attention lossを加えました Loss: Binary divergenceを加えました 共通: Linearを1x1 convolutionに変えました。Dilationを大きくとりました 上記変更点について、本来ならば、Extensiveに実験して、どれがどの程度有効か調べるのが一番良いのですが、計算資源の都合により、部分的にしかやっていません（すいません）。部分的とはいえ、わかったことは最後にまとめておきます。\n計算速度は、バッチサイズ16で、5.3 step/sec くらいの計算速度でした。arXiv:1710.08969 よりは若干速いくらいです。GPUメモリの使用量は5 ~ 6GB程度でした。PyTorch v0.3.0を使いました。\n学習に使用したコマンドは以下です。\npython train.py --checkpoint-dir=checkpoints_deepvoice3 \\ --hparams=\u0026#34;use_preset=True,builder=deepvoice3\u0026#34; \\ --log-event-path=log/deepvoice3_preset コードのコミットハッシュは 7bcf1d0704 です。正確なハイパーパラメータが知りたい場合は、ここから辿れると思います。\nアライメントの学習過程 今回の実験ではアテンションレイヤーは二つ（最初と最後）ありますが、以下に平均を取ったものを示します。\n各種ロスの遷移 音声サンプル 前回の記事 で貼ったサンプルとまったく同じ文章を用いました。興味のある方は聴き比べてみてください。\nhttps://tachi-hi.github.io/tts_samples/ より icassp stands for the international conference on acoustics, speech and signal processing.\n(90 chars, 14 words)\nYour browser does not support the audio element. a matrix is positive definite, if all eigenvalues are positive.\n(63 chars, 12 words)\nYour browser does not support the audio element. a spectrogram is obtained by applying es-tee-ef-tee to a signal.\n(64 chars, 11 words)\nYour browser does not support the audio element. keithito/tacotron のサンプル と同じ文章 Scientists at the CERN laboratory say they have discovered a new particle.\n(74 chars, 13 words)\nYour browser does not support the audio element. There’s a way to measure the acute emotional intelligence that has never gone out of style.\n(91 chars, 18 words)\nYour browser does not support the audio element. President Trump met with other leaders at the Group of 20 conference.\n(69 chars, 13 words)\nYour browser does not support the audio element. The Senate’s bill to repeal and replace the Affordable Care Act is now imperiled.\n(81 chars, 16 words)\nYour browser does not support the audio element. Generative adversarial network or variational auto-encoder.\n(59 chars, 7 words)\nYour browser does not support the audio element. The buses aren’t the problem, they actually provide a solution.\n(63 chars, 13 words)\nYour browser does not support the audio element. まとめ 以下、知見をまとめますが、あくまでその傾向がある、という程度に受け止めてください。\nTacotron, DeepVoice3で述べられているようにメル周波数スペクトログラムの複数フレームをDecoderの1-stepで予測するよりも、arXiv:1710.08969 で述べられているように、1-stepで（粗い）1フレームを予測して、ConvTransposed1d により元の時間解像度までアップサンプリングする方が良い。生成された音声のビブラートのような現象が緩和されるように感じた Dilationを大きくしても、大きな品質の変化はないように感じた Guided-attentionは、アテンションが早くmonotonicになるという意味で良い。ただし、品質に大きな影響はなさそうに感じた Encoderのレイヤー数を大きくするのは効果あり Converterのチャンネル数を大きくするのは効果あり Binary divergence lossは、学習を安定させるために、DeepVoice3風のアーキテクチャでも有効だった Encoder/Converterは arXiv:1710.08969 のものを、DecoderはDeepVoice3のものを、というパターンで試したことがありますが、arXiv:1710.08969に比べて若干品質が落ちたように感じたものの、ほぼ同等と言えるような品質が得られました。arXiv:1710.08969 ではDecoderに20レイヤー以上使っていますが、10未満でもそれなりの品質になったように思います（上で貼った音声サンプルがまさにその例です） 品質を改良するために、arXiv:1710.08969 から色々アイデアを借りましたが、逆にDeepVoice3のアイデアで良かったと思えるものに、Decoderの入力に、(メル周波数の次元まで小さくして、Sigmoidを通して得られる）メル周波数スペクトログラムを使うのではなくその前のhidden stateを使う、といったことがありました。勾配がサチりやすいSigmoidをかまないからか、スペクトログラムに対するL1 Lossの減少が確実に速くなりました (22a6748)。 この記事に貼った音声サンプルにおいて、先頭のaが抜けている例が目立ちますが、過去にやった実験ではこういう例は稀だったので、何かハイパーパラメータを誤っていじったんだと思います（闇 参考 Wei Ping, Kainan Peng, Andrew …","date":1513134900,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513134900,"objectID":"39fece761f120a2c8706825af163c691","permalink":"https://r9y9.github.io/blog/2017/12/13/deepvoice3/","publishdate":"2017-12-13T12:15:00+09:00","relpermalink":"/blog/2017/12/13/deepvoice3/","section":"post","summary":"Audio samples: https://r9y9.github.io/deepvoice3_pytorch/","tags":["Speech Synthesis","Python","Deep Learning"],"title":"【単一話者編】Deep Voice 3: 2000-Speaker Neural Text-to-Speech / arXiv:1710.07654 [cs.SD]","type":"post"},{"authors":null,"categories":null,"content":"Summary 論文リンク: arXiv:1710.08969 コード: https://github.com/r9y9/deepvoice3_pytorch 三行まとめ arXiv:1710.08969: Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention. を読んで、実装しました RNNではなくCNNを使うのが肝で、オープンソースTacotronと同等以上の品質でありながら、高速に (一日程度で) 学習できる のが売りのようです。 LJSpeech Dataset を使って、英語TTSモデルを作りました（学習時間一日くらい）。完全再現とまではいきませんが、大まかに論文の主張を確認できました。 前置き 本当は DeepVoice3 の実装をしていたのですが、どうも上手くいかなかったので気分を変えてやってみました。 以前 Tacotronに関する長いブログ記事 (リンク) を書いてしまったのですが、読む方も書く方もつらいので、簡潔にまとめることにしました。興味のある人は続きもどうぞ。\n概要 End-to-endテキスト音声合成 (Text-to-speech synthesis; TTS) のための Attention付き畳み込みニューラルネット (CNN) が提案されています。SampleRNN, Char2Wav, Tacotronなどの従来提案されてきたRNNをベースとする方法では、モデルの構造上計算が並列化しにくく、 学習/推論に時間がかかることが問題としてありました。本論文では、主に以下の二つのアイデアによって、従来法より速く学習できるモデルを提案しています。\nRNNではなくCNNを使うこと (参考論文: arXiv:1705.03122) Attentionがmotonicになりやすくする効果を持つLossを考えること (Guided attention) 実験では、オープンソースTacotron (keithito/tacotron) の12日学習されたモデルと比較し、主観評価により同等以上の品質が得られたことが示されています。\nDeepVoice3 との違い ほぼ同時期に発表されたDeepVoice3も同じく、CNNをベースとするものです。論文を読みましたが、モチベーションとアプローチの基本は DeepVoice3 と同じに思いました。しかし、ネットワーク構造は DeepVoice3とは大きく異なります。いくつか提案法の特徴を挙げると、以下のとおりです。\nネットワークが深い（DeepVoice3だとEncoder, Decoder, Converter それぞれ10未満ですが、この論文ではDecoderだけで20以上）。すべてにおいて深いです。カーネルサイズは3と小さいです1 Fully-connected layer ではなく1x1 convolutionを使っています チャンネル数が大きい（256とか512とか、さらにネットワーク内で二倍になっていたりする）。DeepVoice3だとEncoderは64です レイヤーの深さに対して指数上に大きくなるDilationを使っています（DeepVoiceではすべてdilation=1） アテンションレイヤーは一つ（DeepVoice3は複数 DeepVoice3は、arXiv:1705.03122 のモデル構造とかなり似通っている一方で、本論文では（参考文献としてあげられていますが）影も形もないくらい変わっている、という印象を受けます。\nロスに関しては、Guided attentionに関するロスが加わるのに加えて、TacotronやDeepVoice3とは異なり、スペクトログラム/メルスペクトログラムに関して binary divergence (定義は論文参照) をロスに加えているという違いがあります。\n実験 LJSpeech Dataset を使って、17時間くらい（26.5万ステップ）学習しました。計算資源の都合上、SSRNのチャンネル数は512ではなくその半分の256にしました。\nなお、実装するにあたっては、厳密に再現しようとはせず、色々雰囲気でごまかしました。もともとDeepVoice3の実装をしていたのもあり、アイデアをいくつか借りています。例えば、デコーダの出力をいつ止めるか、というdone flag predictionをネットワークに入れています。Dropoutについて言及がありませんが、ないと汎化しにくい印象があったので2、足しました。\n計算速度は、バッチサイズ16で、4.3 step/sec くらいの計算速度でした。僕のマシンのGPUはGTX 1080Ti です。使用したハイパーパラメータはこちらです。学習に使用したコマンドは以下です（メモ）。\npython train.py --data-root=./data/ljspeech --checkpoint-dir=checkpoints_nyanko \\ --hparams=\u0026#34;use_preset=True,builder=nyanko\u0026#34; \\ --log-event-path=log/nyanko_preset アライメントの学習過程 数万ステップで、綺麗にmonotonicになりました。GIFは、同じ音声に対するアライメントではなく、毎度違う（ランダムな）音声サンプルに対するアライメントを計算して、くっつけたものです（わかりずらくすいません\n各種ロスの遷移 見づらくて申し訳ありませんという感じですが、僕のための簡易ログということで貼っておきます。binary divergenceは、すぐに収束したようでした。\n音声サンプル 公式音声サンプル と同じ文章（抜粋） 公式サンプルとの比較です。11/23時点で、公式のサンプル数が15個と多いので、適当に3つ選びました。公式と比べると少し異なっている印象を受けますが、まぁまぁ良いかなと思いました（曖昧ですが\nicassp stands for the international conference on acoustics, speech and signal processing.\n(90 chars, 14 words)\nYour browser does not support the audio element. a matrix is positive definite, if all eigenvalues are positive.\n(63 chars, 12 words)\nYour browser does not support the audio element. a spectrogram is obtained by applying es-tee-ef-tee to a signal.\n(64 chars, 11 words)\nYour browser does not support the audio element. keithito/tacotron のサンプル と同じ文章 Scientists at the CERN laboratory say they have discovered a new particle.\n(74 chars, 13 words)\nYour browser does not support the audio element. There’s a way to measure the acute emotional intelligence that has never gone out of style.\n(91 chars, 18 words)\nYour browser does not support the audio element. President Trump met with other leaders at the Group of 20 conference.\n(69 chars, 13 words)\nYour browser does not support the audio element. The Senate’s bill to repeal and replace the Affordable Care Act is now imperiled.\n(81 chars, 16 words)\nYour browser does not support the audio element. Generative adversarial network or variational auto-encoder.\n(59 chars, 7 words)\nYour browser does not support the audio element. The buses aren’t the problem, they actually provide a solution.\n(63 chars, 13 words)\nYour browser does not support the audio element. まとめ \u0026amp; わかったことなど Tacotronでは学習に何日もかかっていましたが（計算も遅く1日で10万step程度）、1日でそれなりの品質になりました。 Guided atetntionがあると、確かに速くattentionがmonotonicになりました。 2時間程度の学習では ここ にあるのと同程度の品質にはなりませんでした… DeepVoice3のモデルアーキテクチャで学習した場合と比べると、品質は向上しました DeepVoice3と比べると、深いせいなのか学習が難しいように思いました。重みの初期化のパラメータをちょっといじると、sigmoidの出力が0 or 1になって学習が止まる、といったことがありました。重みの初期化はとても重要でした 上記にも関連して、勾配のノルムが爆発的に大きくなることがしばしばあり、クリッピングを入れました（重要でした） Binary divergenceをロスにいれても品質には影響がないように感じました。ただしないと学習初期に勾配が爆発しやすかったです 提案法は色々なアイデアが盛り込まれているのですが、実際のところどれが重要な要素なのか、といった点に関しては、論文では明らかにされていなかったように思います。今後その辺りを明らかにする論文があってもいいのではないかと思いました。 学習に使うGPUメモリ量、Tacotronより多い（SSRNのチャンネル数512, バッチサイズ16で 8GBくらい 5~6GB くらいでした）……厳しい……3 2017/12/19追記: Dropoutなしだと、入力テキストとは無縁の英語らしき何かが生成されるようになってしまいました。Dropoutはやはり重要でした 一番の学びは、ネットワークの重みの初期化方法は重要、ということでした。おしまい\n参考 Hideyuki Tachibana, Katsuya Uenoyama, Shunsuke Aihara, “Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention”. arXiv:1710.08969, Oct 2017. Wei Ping, Kainan Peng, Andrew Gibiansky, et al, “Deep Voice 3: 2000-Speaker Neural Text-to-Speech”, arXiv:1710.07654, Oct. 2017. Jonas Gehring, Michael …","date":1511433000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1511433000,"objectID":"6ee5d42e9702a54f1e695ddaf0c6367e","permalink":"https://r9y9.github.io/blog/2017/11/23/dctts/","publishdate":"2017-11-23T19:30:00+09:00","relpermalink":"/blog/2017/11/23/dctts/","section":"post","summary":"GitHub: https://github.com/r9y9/deepvoice3_pytorch","tags":["Speech Synthesis","Python","Deep Learning"],"title":"Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention. [arXiv:1710.08969]","type":"post"},{"authors":null,"categories":null,"content":"Summary コーパス配布先リンク: JSUT (Japanese speech corpus of Saruwatari Lab, University of Tokyo) - Shinnosuke Takamichi (高道 慎之介) 論文リンク: arXiv:1711.00354 三行まとめ 日本語End-to-end音声合成に使えるコーパスは神、ありがとうございます クリーンな音声であるとはいえ、冒頭/末尾の無音区間は削除されていない、またボタンポチッみたいな音も稀に入っているので注意 僕が行った無音区間除去の方法（Juliusで音素アライメントを取って云々）を記録しておくので、必要になった方は参考にどうぞ。ラベルファイルだけほしい人は連絡ください JSUT とは ツイート引用：\nフリーの日本語音声コーパス（単一話者による10時間データ）を公開しました．音声研究等にお役立てください．https://t.co/94ShJY44mA pic.twitter.com/T0etDwD7cS\n— Shinnosuke Takamichi (高道 慎之介) (@forthshinji) October 26, 2017 つい先月、JSUT という、日本語 End-to-end 音声合成の研究に使えることを前提に作られた、フリーの大規模音声コーパスが公開されました。詳細は上記リンク先を見てもらうとして、簡単に特徴をまとめると、以下のとおりです。\n単一日本語女性話者の音声10時間 無響室で収録されている、クリーンな音声コーパス 1 非営利目的で無料で使える 僕の知る限り、日本語 End-to-end 音声合成に関する研究はまだあまり発展していないように感じていたのですが、その理由の一つに誰でも自由に使えるコーパスがなかったことがあったように思います。このデータセットはとても貴重なので、ぜひ使っていきたいところです。 高道氏およびコーパスを整備してくださった方、本当にありがとうございます。\nこの記事では、僕が実際に日本語End-to-end音声合成の実験をしようと思った時に、必要になった前処理（最初と最後の無音区間の除去）について書きたいと思います。\n問題 まずはじめに、最初と最後の無音区間を除去したい理由には、以下の二つがありました。\nTacotronのようなattention付きseq2seqモデルにおいて、アライメントを学習するのに不都合なこと。句読点に起因する無音区間ならともかく、最初/最後の無音区間は、テキスト情報からはわからないので、直感的には不要であると思われます。参考までに、DeepVoice2の論文のsection 4.2 では、無音区間をトリミングするのがよかったと書かれています。 発話の前、発話の後に、微妙にノイズがある（息を大きく吸う音、ボタンをポチッ、みたいな機械音等）データがあり、そのノイズが不都合なこと。例えばTacotronのようなモデルでは、テキスト情報とスペクトログラムの関係性を学習したいので、テキストに関係のないノイズは可能な限り除去しておきたいところです。参考までに、ボタンポチノイズは 例えば basic5000/wav/BASIC5000_0008.wav に入っています 最初何も考えずに（ダメですが）データを入れたら、アライメントが上手く学習されないなーと思い、データを見ていたところ、後者に気づいた次第です。\n方法 さて、無音区間を除去する一番簡単な方法は、適当にパワーで閾値処理をすることです。しかし、前述の通りボタンをポチッと押したようなノイズは、この方法では難しそうでした。というわけで、少し手間はかかりますが、Juliusで音素アライメントを取って、無音区間を推定することにしました。 以下、Juliusを使ってアライメントファイル（.lab) を作る方法です。コードは、 https://github.com/r9y9/segmentation-kit/tree/jsut にあります。\n自分で準備するのが面倒だから結果のラベルファイルだけほしいという方がいれば、連絡をいただければお渡しします。Linux環境での実行を想定しています。僕はUbuntu 16.04で作業しています。\n準備 Julius をインストールする。/usr/local/bin/julius にバイナリがあるとします MeCabをインストールする mecab-ipadic-neologd をインストールする nnmnkwii のmasterブランチを入れる pip install mecab-python3 jaconv sudo apt-get install sox Juliusの音素セグメンテーションツールキットのフォーク (jsutブランチ) をクローンする。クローン先を作業ディレクトリとします コーパスの場所を設定 params.py というファイルに、コーパスの場所を指定する変数 (in_dir) があるので、設定します。僕の場合、以下のようになっています。\n# coding: utf-8 in_dir = \u0026#34;/home/ryuichi/data/jsut_ver1\u0026#34; dst_dir = \u0026#34;jsut\u0026#34; 音素アライメントの実行 bash run.sh でまるっと実行できるようにしました。MeCabで読みを推定するなどの処理は、この記事を書いている時点では a.py, b.py, c.py, d.pyというファイルに書かれています。 適当なファイル名で申し訳ありませんという気持ちですが、自分のための書いたコードはこうなってしまいがちです、申し訳ありません。\n7000ファイル以上処理するので、三十分くらいかかります。./jsut というディレクトリに、labファイルができていれば正常に実行完了です。最後に、\nFailed number of utterances: 87 のように、アライメントに失敗したファイル数が表示されるようになっています。失敗の理由には、MeCabでの読みの推定に失敗した（特に数字）などがあります。手で直すことも可能なのですが（実際に一度はやろうとした）非常に大変なので、多少失敗してもよいので大雑把にアライメントを取ることを目的として、スクリプトを作りました。\nなお、juliusはwavesurferのフォーマットでラベルファイルを吐きますが、HTKのラベルフォーマットの方が僕には都合がよかったので、変換するようにしました。\nコーパスにパッチ 便宜上、下記のようにwavディレクトリと同じ階層にラベルファイルがあると都合がよいので、僕はそのようにします。\ntree ~/data/jsut_ver1/ -d -L 2 /home/ryuichi/data/jsut_ver1/ ├── basic5000 │ ├── lab │ └── wav ├── countersuffix26 │ ├── lab │ └── wav ├── loanword128 │ ├── lab │ └── wav ├── onomatopee300 │ ├── lab │ └── wav ├── precedent130 │ ├── lab │ └── wav ├── repeat500 │ ├── lab │ └── wav ├── travel1000 │ ├── lab │ └── wav ├── utparaphrase512 │ ├── lab │ └── wav └── voiceactress100 ├── lab └── wav 以下のコマンドにより、生成されたラベルファイルをコーパス配下にコピーします。この処理は、run.sh では実行しないようになっているので、必要であれば自己責任でおこなってください。\npython d.py ラベル活用例 https://gist.github.com/r9y9/db6b5484a6a5deca24e81e76cb17e046 のようなコードを書いて、ボタンポチ音が末尾に入っている basic5000/wav/BASIC5000_0008.wav に対して無音区間削除を行ってみると、結果は以下のようになります。\nパワーベースの閾値処理では上手くいかない一方で2、音素アライメントを使った方法では上手く無音区間除去ができています。その他、数十サンプルを目視で確認しましたが、僕の期待どおり上手くいっているようでした。めでたし。\nおわり 以上です。End-to-end系のモデルにとってはデータは命であり、このコーパスは神であります。このコーパスを使って、同じように前処理をしたい人の参考になれば幸いです。\nいま僕はこのコーパスを使って、日本語end-to-end音声合成の実験も少しやっているので、まとまったら報告しようと思っています。\n","date":1510423200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510423200,"objectID":"f123a5bb4f52580a50d0add16ba8d024","permalink":"https://r9y9.github.io/blog/2017/11/12/jsut_ver1/","publishdate":"2017-11-12T03:00:00+09:00","relpermalink":"/blog/2017/11/12/jsut_ver1/","section":"post","summary":"JSUT: https://sites.google.com/site/shinnosuketakamichi/publication/jsut","tags":["Speech Synthesis","Corpus","JSUT","End-to-End"],"title":"日本語 End-to-end 音声合成に使えるコーパス JSUT の前処理 [arXiv:1711.00354]","type":"post"},{"authors":null,"categories":null,"content":"Googleが2017年4月に発表したEnd-to-Endの音声合成モデル Tacotron: Towards End-to-End Speech Synthesis / arXiv:1703.10135 [cs.CL] に興味があったので、自分でも同様のモデルを実装して実験してみました。結果わかったことなどをまとめておこうと思います。\nGoogleによるTacotronの音声サンプルは、 https://google.github.io/tacotron/ から聴けます。僕の実装による音声サンプルはこの記事の真ん中くらいから、あるいは Test Tacotron.ipynb | nbviewer1 から聴くことができます。\nとても長い記事になってしまったので、結論のみ知りたい方は、一番最後まで飛ばしてください。最後の方のまとめセクションに、実験した上で僕が得た知見がまとまっています。\n概要 論文のタイトルにもある通り、End-to-Endを目指しています。典型的な（複雑にあなりがちな）音声合成システムの構成要素である、\n言語依存のテキスト処理フロントエンド 言語特徴量と音響特徴量のマッピング (HMMなりDNNなり) 波形合成のバックエンド を一つのモデルで達成しようとする、attention付きseq2seqモデル を提案しています。ただし、Toward とあるように、完全にEnd-to-Endではなく、ネットワークは波形ではなく 振幅スペクトログラム を出力し、Griffin limの方法によって位相を復元し、逆短時間フーリエ変換をすることによって、最終的な波形を得ます。根本にあるアイデア自体はシンプルですが、そのようなEnd-to-Endに近いモデルで高品質な音声合成を実現するのは困難であるため、論文では学習を上手くいくようするためのいくつかのテクニックを提案する、といった主張です。以下にいくつかピックアップします。\nエンコーダに CBFG (1-D convolution bank + highway network + bidirectional GRU) というモジュールを使う デコーダの出力をスペクトログラムではなく（より低次元の）メル周波数スペクトログラム にする。スペクトログラムはアライメントを学習するには冗長なため。 スペクトログラムは、メル周波数スペクトログラムに対して CBFG を通して得る その他、BatchNormalizationを入れたり、Dropoutを入れたり、GRUをスタックしたり、と色々ありますが、正直なところ、どれがどのくらい効果があるのかはわかっていません（調べるには、途方もない時間がかかります）が、論文の主張によると、これらが有効なようです。\n既存実装 Googleは実装を公開していませんが、オープンソース実装がいくつかあります。\nhttps://github.com/Kyubyong/tacotron https://github.com/barronalex/Tacotron https://github.com/keithito/tacotron 自分で実装する前に、上記をすべてを簡単に試したり、生成される音声サンプルを比較した上で、僕は keithito/tacotron が一番良いように思いました。最も良いと思った点は、keithito さんは、LJ Speech Dataset という単一話者の英語読み上げ音声 約24時間のデータセットを構築 し、それを public domainで公開 していることです。このデータセットは貴重です。デモ音声サンプルは、そのデータセットを使った結果でもあり、他と比べてとても高品質に感じました。自分でも試してみて、1時間程度で英語らしき音声が生成できるようになったのと、さらに数時間でアライメントも学習されることを確認しました。\nなお、上記3つすべてで学習スクリプトを回して音声サンプルを得る、程度のことは試しましたが、僕がコードレベルで読んだのは keithito/tacotron のみです。読んだコードは、TensorFlowに詳しくない僕でも読めるもので、とても構造化されていて読みやすかったです。\n自前実装 勉強も兼ねて、PyTorchでスクラッチから書きました。その結果が https://github.com/r9y9/tacotron_pytorch です。\n先にいくつか結論を書いておくと、\n音の品質は、keithito/tacotron の方が良く感じました（同じモデルの実装を心がけたのに…つらい…）。ただ、データセットの音声には残響が乗っていて、生成された音声が元音声に近いのかというのは、僕には判断がつきにくいです。記事の後半に比較できるようにサンプルを貼っておきますので、気になる方はチェックしてみてください keithito/tacotron では長い入力だと合成に失敗する一方で2、僕の実装では比較的長くてもある程度合成できるようです。なぜのかを突き詰めるには、TensorFlowのseq2seq APIの コード (APIは抽象化されすぎていてdocstringからではよくわからないので…) を読みとく必要があるかなと思っています（やっていませんすいません 実験 基本的には keithito/tacotron の学習スクリプトと同じで、LJ Speech Dataset を使って学習させました。テキスト処理、音声処理 (Griffin lim等) には既存のコードをそのまま使用し、モデル部分のみ自分で置き換えました。実験では、\nattention付きseq2seqの肝である、アライメントがどのように学習されていくのか 学習が進むにつれて、生成される音声はどのように変わっていくのか 学習されたモデルは、汎化性能はどの程度なのか（未知文章、長い文章、スペルミスに対してパフォーマンスはどう変わるのか、等） を探っていきました。\nアライメントの学習過程の可視化 通常のseq2seqは、エンコーダRNNによって得た最後のタイムステップにおける隠れ層の状態を、デコーダのRNNの初期状態として渡します。一方attentiont付きのseq2seqモデルでは、デコーダRNNは各タイムステップで、エンコーダRNNの各タイムステップにおける隠れ層の状態を重みづけて使用し、その重みも学習します。attention付きのseq2seqでは、アライメントがきちんと（曖昧な表現ですが）学習されているかを可視化してチェックするのが、学習がきちんと進んでいるのか確認するのに便利です。\n以下に、47000 step (epochではありません。僕の計算環境 GTX 1080Ti で半日かからないくらい) iterationしたときのアライメント結果と、47000 stepの時点での予測された音声サンプルを示します。なお、gifにおける各画像は、データセットをランダムにサンプルした際のアライメントであり、ある同じ音声に対するアライメントではありません。Tacotron論文には、Bahdanau Attentionを使用したとありますが、keithito/tacotron #24 Try Monotonic Attention によると、Tacotron論文の第一著者は新しいバージョンのTacotronでは Monotonic attentionを使用しているらしいということから、Monotonic Attentionでも試してみました。あとでわかったのですが、長文（200文字、数文とか）を合成しようとすると途中でアライメントがスキップすることが多々見受けられたので、そういった場合に、monotonicという制約が上手く働くのだと思います。\n以下の順でgifを貼ります。\nkeithito/tacotron, Bahdanau attention keithito/tacotron, Bahdanau-style monotonic attention r9y9/tacotron_pytorch, Bahdanau attention keithito: Bahdanau Attention\nYour browser does not support the audio element. keithito: (Bahdanau-style) Monotonic Attention\nYour browser does not support the audio element. 自前実装: Bahdanau Attention\nYour browser does not support the audio element. Monotonicかどうかで比較すると、Monotonic attentionの方がアライメントがかなり安定しているように見えます。しかし、Githubのスレッドにあった音声サンプルを聴くと、音質的な意味では大きな違いがないように思ったのと、収束速度（簡単に試したところ、アライメントがまともになりだすstepは20000くらいで、ほぼ同じでした）も同じに思えました。一方で自前実装は、アライメントがまともになるstepが10000くらいとやや早く、またシャープに見えます。\n音声サンプルの方ですが、既存実装は両者ともそれなりにまともです。一方自前実装では、まだかなりノイジーです。できるだけtf実装と同じようにつくり、実験条件も同じにしたつもりですが、何か間違っているかもしれません。が、イテレーションを十分に回すと、一応音声はそれなりに出るようになります。\n音声サンプルに関する注意点としては、これはデコードの際に教師データを使っているので、この時点でのモデル使って、同等の音質の音声を生成できるとは限りません。学習時には、デコーダの各タイムステップで教師データのスペクトログラム（正確には、デコーダの出力はメル周波数スペクトログラム）を入力とする一方で、評価時には、デコーダ自身が出力したスペクトログラムを次のタイムステップの入力に用います。評価時には、一度変なスペクトログラムを出力してしまったら、エラーが蓄積していってどんどん変な出力をするようになってしまうことは想像に難しくないと思います。seq2seqモデルのデコードにはビームサーチが代表的なものとしてありますが、Tacotronでは単純にgreedy decodingをします。\n学習が進むにつれて、生成される音声はどのように変わっていくのか さて、ここからは自前実装のみでの実験結果です。約10日、70万step程度学習させましたので、5000, 10000, 50000, そのあとは10万から10万ステップごとに70万ステップまでそれぞれで音声を生成して、どのようになっているのかを見ていきます。\n例文1 Hi, my name is Tacotron. I’m still learning a lot from data.\n(56 chars, 14 words)\nstep 5000\nYour browser does not support the audio element. step 10000\nYour browser does not support the audio element. step 50000\nYour browser does not support the audio element. step 100000\nYour browser does not support the audio element. step 200000\nYour browser does not support the audio element. step 300000\nYour browser does not support the audio element. …","date":1508043600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1508043600,"objectID":"dd118bfd42bf8be82d82a16809c8c111","permalink":"https://r9y9.github.io/blog/2017/10/15/tacotron/","publishdate":"2017-10-15T14:00:00+09:00","relpermalink":"/blog/2017/10/15/tacotron/","section":"post","summary":"GitHub: https://github.com/r9y9/tacotron_pytorch","tags":["Speech Synthesis","Deep Learning","End-to-End","Python"],"title":"Tacotron: Towards End-to-End Speech Synthesis / arXiv:1703.10135 [cs.CL]","type":"post"},{"authors":null,"categories":null,"content":"10/11 追記: IEEE TASLPのペーパー (Open access) が公開されたようなので、リンクを貼っておきます: https://ieeexplore.ieee.org/document/8063435/\narXiv論文リンク: arXiv:1709.08041\n前回の記事 の続きです。これでこのシリーズは終わりの予定です。\n前回は英語音声合成でしたが、以前書いた DNN日本語音声合成の記事 で使ったデータと同じものを使い、日本語音声合成をやってみましたので、結果を残しておきます。\n実験 実験条件 HTSのNIT-ATR503のデモデータ (ライセンス) から、wavデータ503発話を用います。442を学習用、56を評価用、残り5をテスト用にします（※英語音声とtrain/evalの比率は同じです）。継続長モデルは、state-levelではなくphone-levelです。サンプリング周波数が48kHzなので、mgcの次元を25から60に増やしました。モデル構造は、すべて英語音声合成の場合と同じです。ADV loss は0次を除くmgcを用いて計算しました。F0は入れていません。\ngantts の jpブランチ をチェックアウトして、以下のシェルを実行すると、ここに貼った結果が得られます。\n./jp_tts_demo.sh jp_tts_order59 ただし、シェル中に、HTS_ROOT という変数があり、シェル実行前に、環境に合わせてディレクトリを指定する必要があります。\ndiff --git a/jp_tts_demo.sh b/jp_tts_demo.sh index 7a8f12c..b18e604 100755 --- a/jp_tts_demo.sh +++ b/jp_tts_demo.sh @@ -8,7 +8,7 @@ experiment_id=$1 fs=48000 # Needs adjastment -HTS_DEMO_ROOT=~/local/HTS-demo_NIT-ATR503-M001 +HTS_DEMO_ROOT=HTS日本語デモの場所を指定してください # Flags run_duration_training=1 変換音声の比較 音響モデルのみ適用 自然音声 ベースライン GAN の順に音声を貼ります。聴きやすいように、soxで音量を正規化しています。\nnitech_jp_atr503_m001_j49\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. nitech_jp_atr503_m001_j50\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. nitech_jp_atr503_m001_j51\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. nitech_jp_atr503_m001_j52\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. nitech_jp_atr503_m001_j53\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. 音響モデル＋継続長モデルを適用 nitech_jp_atr503_m001_j49\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. nitech_jp_atr503_m001_j50\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. nitech_jp_atr503_m001_j51\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. nitech_jp_atr503_m001_j52\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. nitech_jp_atr503_m001_j53\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. どうでしょうか。ちょっと早口になってしまっている箇所もありますが、全体的には明瞭性が上がって、品質が改善されたような感じがします。若干ノイジーな感じは、音響モデルにRNNを使えば改善されるのですが、今回は計算リソースの都合上、Feed-forward型のサンプルとなっています。\nGV nitech_jp_atr503_m001_j49 に対して計算した結果です。\n英語音声合成の実験でも確認しているのですが、mgcの次元を大きく取ると、高次元でGVが若干落ちる傾向にあります。ただし、一週間前の僕のツイート によると、なぜかそんなこともなく（当時ばりばりのプロトタイピングの時期だったので、コードが残っておらず、いまは再現できないという、、すいません）、僕が何かミスをしている可能性もあります。ただ、品質はそんなに悪くないように思います。\n変調スペクトル 評価用セットで平均を取ったものです。\n特徴量の分布 nitech_jp_atr503_m001_j49 に対して計算した結果です。\nおまけ: HTSデモと聴き比べ HTSデモを実行すると生成されるサンプルとの聴き比べです。注意事項ですが、実験条件がまったく異なります。あくまで参考程度にどうぞ。\nHTSデモ ベースライン GAN の順に音声を貼ります。\n1 こんにちは\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. 2 それではさようなら\nHTS\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. 3 はじめまして\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. 4 ようこそ名古屋工業大学へ\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. 5 今夜の名古屋の天気は雨です\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. おわりに アイデアはシンプル、効果は素晴らしいという、僕の好きな（試したくなる）研究の紹介でした。ありがとうございました。\nGANシリーズのその他記事へのリンクは以下の通りです。\nGAN 声質変換 (en) 編はこちら GAN 音声合成 (en) 編はこちら 追記: 図を作るのに使ったノートブックは こちら においておきました。\n参考 Yuki Saito, Shinnosuke Takamichi, Hiroshi Saruwatari, “Statistical Parametric Speech Synthesis Incorporating Generative Adversarial Networks”, arXiv:1709.08041 [cs.SD], Sep. 2017 ","date":1507603532,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1507603532,"objectID":"f86e9e3be255db9a24370851eb66b2c1","permalink":"https://r9y9.github.io/blog/2017/10/10/gantts-jp/","publishdate":"2017-10-10T11:45:32+09:00","relpermalink":"/blog/2017/10/10/gantts-jp/","section":"post","summary":"IEEE TASLP: https://ieeexplore.ieee.org/document/8063435/","tags":["Speech synthesis","Python","Deep Learning","GAN"],"title":"GAN 日本語音声合成 [arXiv:1709.08041]","type":"post"},{"authors":null,"categories":null,"content":"10/11 追記: IEEE TASLPのペーパー (Open access) が公開されたようなので、リンクを貼っておきます: https://ieeexplore.ieee.org/document/8063435/\narXiv論文リンク: arXiv:1709.08041\n前回の記事 の続きです。音響モデルの学習にGANを使うというアイデアは、声質変換だけでなく音声合成にも応用できます。CMU ARCTIC を使った英語音声合成の実験を行って、ある程度良い結果がでたので、まとめようと思います。音声サンプルだけ聴きたい方は真ん中の方まで読み飛ばしてください。\nコードはこちら: r9y9/gantts | PyTorch implementation of GAN-based text-to-speech and voice conversion (VC) (VCのコードも一緒です) 音声サンプル付きデモノートブックはこちら: The effects of adversarial training in text-to-speech synthesis | nbviewer 前回の記事でも書いた注意書きですが、厳密に同じ結果を再現しようとは思っていません。同様のアイデアを試す、といったことに主眼を置いています。\n実験 実験条件 CMU ARCTIC から、話者 slt のwavデータそれぞれ1131発話すべてを用います。 Merlin の slt デモの条件と同様に、1000を学習用、126を評価用、残り5をテスト用にします。継続長モデル（state-level）には Bidirectional-LSTM RNN を、音響モデルには Feed-forward型 のニューラルネットを使用しました1。継続長モデル、音響モデルの両方にGANを取り入れました。論文の肝である ADV loss についてですが、mgcのみ（0次は除く）を使って計算するパターンと、mgc + lf0で計算するパターンとで比較しました2。\n実験の結果 (ADV loss: mgcのみ) は、 a5ec247 をチェックアウトして、下記のシェルを実行すると再現できます。\n./tts_demo.sh tts_test データのダウンロード、特徴抽出、モデル学習、音声サンプル合成まで一通り行われます。tts_test の部分は何でもよいです。tensorboard用に吐くログイベント名、モデル出力先、音声サンプル出力先の決定に使われます。詳細はコードを参照ください。 (ADV loss: mgc + lf0) の結果は、ハイパーパラメータを下記のように変更してシェルを実行すると再現できます。\ndiff --git a/hparams.py b/hparams.py index d82296c..e73dc57 100644 --- a/hparams.py +++ b/hparams.py @@ -175,7 +175,7 @@ tts_acoustic = tf.contrib.training.HParams( # Streams used for computing adversarial loss # NOTE: you should probably change discriminator\u0026#39;s `in_dim` # if you change the adv_streams - adversarial_streams=[True, False, False, False], + adversarial_streams=[True, True, False, False], # Don\u0026#39;t switch this on unless you are sure what you are doing # If True, you will need to adjast `in_dim` for discriminator. # Rationale for this is that power coefficients are less meaningful @@ -202,7 +202,7 @@ tts_acoustic = tf.contrib.training.HParams( # Discriminator discriminator=\u0026#34;MLP\u0026#34;, discriminator_params={ - \u0026#34;in_dim\u0026#34;: 24, + \u0026#34;in_dim\u0026#34;: 25, \u0026#34;out_dim\u0026#34;: 1, \u0026#34;num_hidden\u0026#34;: 2, \u0026#34;hidden_dim\u0026#34;: 256, 変換音声の比較 音響モデルのみ適用 (ADV loss: mgcのみ) 継続長モデルを適用しない、かつ ADV lossにmgcのみを用いる場合です。\n自然音声 ベースライン GAN の順に音声を貼ります。聴きやすいように、soxで音量を正規化しています。\narctic_b0535\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. arctic_b0536\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. arctic_b0537\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. arctic_b0538\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. arctic_b0539\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. VCの場合と同じように、音声の明瞭性が上がったように思います。\n音響モデル＋継続長モデルを適用 (ADV loss: mgcのみ) arctic_b0535\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. arctic_b0536\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. arctic_b0537\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. arctic_b0538\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. arctic_b0539\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. 音声の明瞭性が上がっているとは思いますが、継続長に関しては、ベースライン/GANで差異がほとんどないように感じられると思います。これは、（僕が実験した範囲では少なくとも）DiscriminatorがGeneartorに勝ちやすくて (音響モデルの場合は、そんなことはない)、 ADV lossが下がるどころか上がってしまい、結果 MGE lossを最小化する場合とほとんど変わっていない、という結果になっています。論文に記載の内容とは異なり、state-levelの継続長モデルではあるものの、ハイパーパラメータなどなどいろいろ変えて試したのですが、上手くいきませんでした。\nADV loss: mgc vs mgc + lf0 次に、ロスの比較です。F0の変化に着目しやすいように、継続長モデルを使わず、音響モデルのみを適用します。\n自然音声 ADV loss (mgcのみ, 24次元) ADV loss (mgc + lf0, 25次元) の順に音声を貼ります。また、WORLD (dio + stonemask) で分析したF0の可視化結果も併せて貼っておきます。\narctic_b0535\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. arctic_b0536\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. arctic_b0537\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. arctic_b0538\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. arctic_b0539\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser …","date":1507482000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1507482000,"objectID":"af320c4a037a8c8ade25d3e40edfadbc","permalink":"https://r9y9.github.io/blog/2017/10/09/gantts/","publishdate":"2017-10-09T02:00:00+09:00","relpermalink":"/blog/2017/10/09/gantts/","section":"post","summary":"IEEE TASLP: https://ieeexplore.ieee.org/document/8063435/","tags":["Speech synthesis","Python","Deep Learning","GAN"],"title":"【音声合成編】Statistical Parametric Speech Synthesis Incorporating Generative Adversarial Networks [arXiv:1709.08041]","type":"post"},{"authors":null,"categories":null,"content":"10/11 追記: IEEE TASLPのペーパー (Open access) が公開されたようなので、リンクを貼っておきます: https://ieeexplore.ieee.org/document/8063435/\narXiv論文リンク: arXiv:1709.08041\n2017年9月末に、表題の 論文 が公開されたのと、nnmnkwii という designed for easy and fast prototyping を目指すライブラリを作ったのもあるので、実装してみました。僕が実験した限りでは、声質変換 (Voice conversion; VC) では安定して良くなりました（音声合成ではまだ実験中です）。この記事では、声質変換について僕が実験した結果をまとめようと思います。音声合成については、また後日まとめます\nコードはこちら: r9y9/gantts | PyTorch implementation of GAN-based text-to-speech and voice conversion (VC) (TTSのコードも一緒です) 音声サンプルを聴きたい方はこちら: The effects of adversarial training in voice conversion | nbviewer (※解説はまったくありませんのであしからず) なお、厳密に同じ結果を再現しようとは思っていません。同様のアイデアを試す、といったことに主眼を置いています。コードに関しては、ここに貼った結果を再現できるように気をつけました。\n概要 一言でいえば、音響モデルの学習に Generative Adversarial Net (GAN) を導入する、といったものです。少し具体的には、\n音響モデル（生成モデル）が生成した音響特徴量を偽物か本物かを見分けようとする識別モデルと、 生成誤差を小さくしつつ (Minimum Generation Error loss; MGE loss の最小化) 、生成した特徴量を識別モデルに本物だと誤認識させようとする (Adversarial loss; ADV loss の最小化) 生成モデル を交互に学習することで、自然音声の特徴量と生成した特徴量の分布を近づけるような、より良い音響モデルを獲得する、といった方法です。\nベースライン ベースラインとしては、 MGE training が挙げられています。DNN音声合成でよくあるロス関数として、音響特徴量 (静的特徴量 + 動的特徴量) に対する Mean Squared Error (MSE loss) というものがあります。これは、特徴量の各次元毎に誤差に正規分布を考えて、その対数尤度を最大化することを意味します。 しかし、\n静的特徴量と動的特徴量の間には本来 deterministic な関係があることが無視されていること ロスがフレーム単位で計算されるので、 (動的特徴量が含まれているとはいえ) 時間構造が無視されてしまっていること から、それらの問題を解決するために、系列単位で、かつパラメータ生成後の静的特徴量の領域でロスを計算する方法、MGE training が提案されています。1\n実験 実験条件 CMU ARCTIC から、話者 clb と slt のwavデータそれぞれ500発話を用います。439を学習用、56を評価用、残り5をテスト用にします。音響特徴量には、WORLDを使って59次のメルケプストラムを抽出し、0次を除く59次元のベクトルを各フレーム毎の特徴量とします。F0、非周期性指標に関しては、元話者のものをそのまま使い、差分スペクトル法を用いて波形合成を行いました。F0の変換はしていません。音響モデルには、\nYuki Saito, Shinnosuke Takamichi, and Hiroshi Saruwatari, “Voice conversion using input-to-output highway networks,” IEICE Transactions on Information and Systems, Vol.E100-D, No.8, pp.1925–1928, Aug. 2017 で述べられている highway network を用います。ただし、活性化関数をReLUからLeakyReLUにしたり、Dropoutを入れたり、アーキテクチャは微妙に変えています。前者は、調べたら勾配が消えにくくて学習の不安定なGANに良いと書いてある記事があったので（ちゃんと理解しておらず安直ですが、実験したところ悪影響はなさそうでしたので様子見）、後者は、GANの学習の安定化につながった気がします（少なくともTTSでは）。Discriminatorには、Dropout付きの多層ニューラルネットを使いました。MGE loss と ADV loss をバランスする重み w_d は、 1.0 にしました。層の数、ニューロンの数等、その他詳細が知りたい方は、コードを参照してください。実験に使用したコードの正確なバージョンは ccbb51b です。ハイパーパラメータは こちら です。\nここで示す結果を再現したい場合は、\nコードをチェックアウト パッケージと依存関係をインストール clb と slt のデータをダウンロード（僕の場合は、 ~/data/cmu_arctic にあります そして、以下のスクリプトを実行すればOKです。\n./vc_demo.sh ~/data/cmu_arctic なお実行には、GPUメモリが4GBくらいは必要です（バッチサイズ32の場合）。GTX 1080Ti + i7-7700K の計算環境で、約1時間半くらいで終わります。スクリプト実行が完了すれば、generated ディレクトリに、ベースライン/GAN それぞれで変換した音声が出力されます。以下に順に示す図については、デモノートブック を実行すると作ることができます。\n変換音声の比較 テストセットの5つのデータに対しての変換音声、およびその元音声、ターゲット音声を比較できるように貼っておきます。下記の順番です。\n元話者の音声 ターゲット話者の音声 MGE Loss を最小化して得られたモデルによる変換音声 MGE loss + ADV loss を最小化して得られたモデルによる変換音声 比較しやすいように、音量はsoxで正規化しました。\narctic_a0496\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. arctic_a0497\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. arctic_a0498\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. arctic_a0499\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. arctic_a0500\nYour browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. clb, slt は違いがわかりにくいと以前誰かから指摘されたのですが、これに慣れてしまいました。わかりづらかったらすいません。僕の耳では、明瞭性が上がって、良くなっているように思います。\nGlobal variance は補償されているのか？ 統計ベースの手法では、変換音声の Global variance (GV) が落ちてしまい、品質が劣化してしまう問題がよく知られています。GANベースの手法によって、この問題に対処できているのかどうか、実際に確認しました。以下に、データセット中の一サンプルを適当にピックアップして、GVを計算したものを示します。縦軸は対数、横軸はメルケプストラムの次元です。\nおおおまか、論文で示されているのと同等の結果を得ることができました。\nModulation spectrum (変調スペクトル) は補償されているのか？ GVをより一般化ものとして、変調スペクトルという概念があります。端的に言えば、パラメータ系列の時間方向に対する離散フーリエ変換の二乗（の対数※定義によるかもですが、ここでは対数をとったもの）です。統計処理によって劣化した変換音声は、変調スペクトルが自然音声と比べて小さくなっていることが知られています。というわけで、GANベースの方法によって、変調スペクトルは補償されているのか？ということを調べてみました。これは、論文には書いていません（が、きっとされていると思います）。以下に、評価用の音声56発話それぞれで変調スペクトルを計算し、それらの平均を取り、適当な特徴量の次元をピックアップしたものを示します。横軸は変調周波数です。一番右端が50Hzです。\n特に高次元の変調スペクトルに対して、ベースラインは大きく落ちている一方で、GANベースでは比較的自然音声と近いことがわかります。しかし、高次元になるほど、自然音声とGANベースでも違いが出ているのがわかります。改善の余地はありそうですね。\n特徴量の分布 論文で示されているscatter plotですが、同じことをやってみました。\n概ね、論文通りの結果となっています。\n詐称率について w_d を変化させて、詐称率がどうなるかは実験していないのですが、w_d = 1.0 の場合に、だいたい0.7 ~ 0.9 くらいに収まることを確認しました。TTSでは0.99くらいの、論文と同様の結果が出ました。くらい、というのは、どのくらい Discriminator を学習させるか、初期化としてのMGE学習（例えば25epochくらい）のあと生成された特徴量に対して学習させるのか、それとも初期化とは別でベースライン用のモデル（100epochとか）を使って生成された特徴量に対して学習させるのか、によって変わってくるのと、その辺りが論文からではあまりわからなかったのと、学習率や最適化アルゴリズムやデータによっても変わってくるのと、詐称率の計算は品質に …","date":1507213536,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1507213536,"objectID":"cec0fc48715bce8ae58838a36eadcf72","permalink":"https://r9y9.github.io/blog/2017/10/05/ganvc/","publishdate":"2017-10-05T23:25:36+09:00","relpermalink":"/blog/2017/10/05/ganvc/","section":"post","summary":"IEEE TASLP: https://ieeexplore.ieee.org/document/8063435/","tags":["Speech synthesis","Python","Deep Learning","GAN","Voice Conversion"],"title":"【声質変換編】Statistical Parametric Speech Synthesis Incorporating Generative Adversarial Networks [arXiv:1709.08041]","type":"post"},{"authors":null,"categories":null,"content":"nnmnkwii というDNN音声合成のためのライブラリを公開しましたので、その紹介をします。\nhttps://t.co/p8MnOxkVoH Library to build speech synthesis systems designed for easy and fast prototyping. Open sourced:)\n— 山本りゅういち (@r9y9) August 14, 2017 ドキュメントの最新版は https://r9y9.github.io/nnmnkwii/latest/ です。以下に、いくつかリンクを貼っておきます。\nなぜ作ったのか、その背景の説明と設計 (日本語) クイックガイド DNN英語音声合成のチュートリアル よろしければご覧ください1。\nドキュメントは、だいたい英語でお硬い雰囲気で書いたので、この記事では、日本語でカジュアルに背景などを説明しようと思うのと、（ドキュメントには英語音声合成の例しかないので）HTSのデモに同梱のATR503文のデータセットを使って、DNN日本語音声合成 を実装する例を示したいと思います。結果だけ知りたい方は、音声サンプルが下の方にあるので、適当に読み飛ばしてください。\nなぜ作ったのか 一番大きな理由は、僕が 対話環境（Jupyter, IPython等） で使えるツールがほしかったからです2。 僕は結構前からREPL (Read-Eval-Print-Loop) 信者で、プログラミングのそれなりの時間をREPLで過ごします。 IDEも好きですし、emacsも好きなのですが、同じくらいJupyterやJuliaのREPLが好きです。 用途に応じて使い分けますが、特に何かデータを分析する必要があるような時に、即座にデータを可視化できるJupyter notebookは、僕にとってプログラミングに欠かせないものになっています。\nところが、HTSの後継として生まれたDNN音声合成ツールである Merlin は、コマンドラインツールとして使われる想定のもので、僕の要望を満たしてくれるものではありませんでした。 とはいえ、Merlinは優秀な音声研究者たちの産物であり、当然役に立つ部分も多く、使っていました。しかし、ことプロトタイピングにおいては、やはり対話環境でやりたいなあという思いが強まっていきました。\n新しく作るのではなく、Merlinを使い続ける、Merlinを改善する方針も考えました。僕がMerlinを使い始めた頃、Merlinはpython3で動かなかったので、動くように プルリク を出したこともあるのですが、まぁレビューに数カ月もかかってしまったので、これは新しいものを作った方がいいな、と思うに至りました。\n以上が、僕が新しくツール作ろうと思った理由です。\n特徴 さて、Merlinに対する敬意と不満から生まれたツールでありますが、その特徴を簡単にまとめます。\n対話環境 での使用を前提に、設計されています。コマンドラインツールはありません。ユーザが必要に応じて作ればよい、という考えです。 DNN音声合成のデモをノートブック形式で提供しています。 大規模データでも扱えるように、データセットとデータセットのイテレーション（フレーム毎、発話毎の両方）のユーティリティが提供されています Merlinとは異なり、音響モデルは提供しません。自分で実装する必要があります（が、今の時代簡単ですよね、lstmでも数行で書けるので 任意の深層学習フレームワークと併せて使えるように、設計されています3（autogradパッケージのみ、今のところPyTorch依存です 言語特徴量の抽出の部分は、Merlinのコードをリファクタして用いています。そのせいもあって、Merlinのデモと同等のパフォーマンスを簡単に実現できます。 対象ユーザ まずはじめに、大雑把にいって、音声合成の研究（or その真似事）をしてみたい人が主な対象です。 自前のデータを元に、ブラックボックスでいいので音声合成エンジンを作りたい、という人には厳しいかもしれません。その前提を元に、少し整理します。\nこんな人におすすめです Jupyter notebookが好きな人 REPLが好きな人 Pythonで処理を完結させたい人 オープンソースの文化に寛容な人4 音声合成の研究を始めてみたい人 こんな人には向かないかも コマンドラインツールこそ至高な人 パイプライン処理こそ至高な人 SPTKのコマンドラインツール至高な人 信頼のある機関が作ったツールしか使わない人5 音声研究者ガチ勢で、自前のツールで満足している人 DNN日本語音声合成の実装例 さて、前置きはこのくらいにして、日本語音声合成の実装例を示します。シンプルなFeed forwardなネットワークと、Bi-directional LSTM RNNの2パターンを、ノートブック形式で作成しました。\nソースコードは、 https://github.com/r9y9/nnmnkwii_gallery にあります。以下に、現状点での直リンク（gitのコミットハッシュがURLに入っています）を貼っておきます。nbviewerに飛びます。\nFeed forwardなネットワークを使った音声合成のノートブックへの直リンク Bi-directional LSTM RNNを使った音声合成のノートブックへの直リンク 興味のある人は、ローカルに落として実行してみてください。CUDA環境があることが前提ですが、通常のFeed forwardのネットワークを用いたデモは、 特徴抽出の時間（初回実行時に必要）を除けば、5分で学習\u0026amp;波形生成が終わります。Bi-directional LSTMのデモは、僕の環境 (i7-7700K, GTX 1080Ti) では、約2時間で終わります。GPUメモリが少ない場合は、バッチサイズを小さくしなければならず、より時間がかかるかもしれません。\nデータセット 今回は、HTSのNIT-ATR503のデモデータ (ライセンス) を拝借します。ライブラリを使って音声合成を実現するためのデータとして、最低限以下が必要です。\n(state or phone level) フルコンテキストラベル Wavファイル 質問ファイル（言語特徴量の抽出に必要） 上2つは、今回はHTSのデモスクリプトからまるまるそのまま使います（※HTSのデモスクリプトを回す必要はありません）。質問ファイルは、コンテキストクラスタリングに使われる質問ファイルを元に、質問数を（本当に）適当に減らして、Merlinのデモの質問ファイルからCQSに該当する質問を加えて、作成しました。 フルコンテキストラベルには、phone-levelでアライメントされたものを使いますが、 state-levelでアライメントされたものを使えば、性能は上がると思います。今回は簡単のためにphone-levelのアライメントを使います。\n質問の選定には、改善の余地があることがわかっていますが、あくまでデモということで、ご了承ください。\n音声合成の結果 全体の処理に興味がある場合は別途ノートブックを見てもらうとして、ここでは結果だけ貼っておきます。 HTSのデモからとってきた例文5つに対して、それぞれ\nFeed forward neural networks (MyNetとします) で生成したもの Bi-directional LSTM recurrent neural networks (MyRNNとします)で生成したもの HTSデモで生成したもの (HTSとします) の順番に、音声ファイルを添付しておきます。聴きやすいように、soxで正規化しています。それではどうぞ。\n1 こんにちは\nMyNet\nYour browser does not support the audio element. MyRNN\nYour browser does not support the audio element. HTS\nYour browser does not support the audio element. 2 それではさようなら\nMyNet\nYour browser does not support the audio element. MyRNN\nYour browser does not support the audio element. HTS\nYour browser does not support the audio element. 3 はじめまして\nMyNet\nYour browser does not support the audio element. MyRNN\nYour browser does not support the audio element. HTS\nYour browser does not support the audio element. 4 ようこそ名古屋工業大学へ\nMyNet\nYour browser does not support the audio element. MyRNN\nYour browser does not support the audio element. HTS\nYour browser does not support the audio element. 5 今夜の名古屋の天気は雨です\nMyNet\nYour browser does not support the audio element. MyRNN\nYour browser does not support the audio element. HTS\nYour browser does not support the audio element. 一応HTSで生成された音声も貼りましたが、そもそも実験条件が違いすぎる6ので、単純に比較することはできません。 せめて HTS ＋ STRAIGHTと比較したかったところですが、僕はSTRAIGHTを持っていないので、残念ながらできません、悲しみ。\nしかし、それなりにまともな音声が出ているような気がします。\nおわりに いままでさんざん、汎用性とは程遠いクソコードを書いてきましたが、今回こそは少しはマシなものを作ろうと思って作りました。僕以外の人にも役に立てば幸いです。あと、この記事を書いた目的は、いろんな人に使ってみてほしいのと、使ってみた結果のフィードバックがほしい（バグ見つけた、そもそもエラーで動かん、ここがクソ、等）ということなので、フィードバックをくださると助かります。よろしくお願いします。\nちなみに名前ですが、ななみ or しちみと読んでください。何でもいいのですが、常識的に考えてあぁ確かに読めないなぁと思いました（小並感）。ドキュメントにあるロゴは、昔三次元物体追跡の実験をしていたときに撮ったく某モンのポイントクラウドですが、そのうち七味的な画像に変えようと思っています。適当ですいません\nリンク切れが怖いので、v0.0.1のリンクを貼りました。できれば、最新版をご覧ください。 https://r9y9.github.io/nnmnkwii/latest/ こちらからたどれます ↩︎\n知っている人にはまたか、と言われそう ↩︎\n音響モデルの提供をライブラリの範囲外とすることで、間接的に達成されています ↩︎\nバグにエンカウントしたらすぐに使うのをやめてしまう人には、向いていないかもしれません。 ↩︎\nMerlinは、エジンバラ大学の優秀な研究者の方々によって作られています ↩︎\nf0分析、スペクトル包絡抽出、非周期性成分の抽出法がすべてことなる、またポストフィルタの種類も異なる。条件をある程度揃えて比較するのが面倒そうだったので（なにせHTSを使ったモデルの学習は数時間かかるし…）、手を抜きました、すいませ …","date":1502892656,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1502892656,"objectID":"a7845fb75244101b8afe8d5b1c4138e4","permalink":"https://r9y9.github.io/blog/2017/08/16/japanese-dnn-tts/","publishdate":"2017-08-16T23:10:56+09:00","relpermalink":"/blog/2017/08/16/japanese-dnn-tts/","section":"post","summary":"GitHub: https://github.com/r9y9/nnmnkwii","tags":["Speech Synthesis","Python","Deep Learning"],"title":"DNN音声合成のためのライブラリの紹介とDNN日本語音声合成の実装例","type":"post"},{"authors":null,"categories":null,"content":"この記事では、音声合成ツールキットであるMerlinが、具体的に何をしているのか（特徴量の正規化、無音区間の削除、ポストフィルタなど、コードを読まないとわからないこと）、その中身を僕が理解した範囲でまとめます。 なお、HMM音声合成について簡単に理解していること（HMMとは、状態とは、フルコンテキストラベルとは、くらい）を前提とします。\nはじめに Merlinの概要については以下をご覧ください。\nWu, Zhizheng, Oliver Watts, and Simon King. “Merlin: An open source neural network speech synthesis system.” Proc. SSW, Sunnyvale, USA (2016). “A Demonstration of the Merlin Open Source Neural Network Speech Synthesis System” 公式ドキュメント Merlinにはデモスクリプトがついています。基本的にユーザが使うインタフェースはrun_merlin.pyというコマンドラインスクリプトで、 デモスクリプトではrun_merlin.pyに用途に応じた設定ファイルを与えることで、継続長モデルの学習/音響モデルの学習/パラメータ生成など、音声合成に必要なステップを実現しています。\nデモスクリプトを実行すると、音声データ (wav) と言語特徴量（HTSのフルコンテキストラベル）から、変換音声が合成されるところまでまるっとやってくれるのですが、それだけでは内部で何をやっているのか、理解することはできません。 ツールキットを使う目的が、自分が用意したデータセットで音声合成器を作りたい、といった場合には、特に内部を知る必要はありません。 また、設定ファイルをちょこっといじるだけでこと済むのであれば、知る必要はないかもしれません。 しかし、モデル構造を変えたい、学習アルゴリズムを変えたい、ポストフィルタを入れたい、といったように、少し進んだ使い方をしようとすれば、内部構造を理解しないとできないことも多いと思います。\nrun_merlin.py はあらゆる処理 (具体的にはあとで述べます) のエントリーポイントになっているがゆえに、コードはなかなかに複雑になっています1。この記事では、run_merlin.pyがいったい何をしているのかを読み解いた結果をまとめます。\nMerlinでは提供しないこと Merlinが何を提供してくれるのかを理解する前に、何を提供しないのか、をざっくりと整理します。以下のとおりです。\nText-processing (Frontend) Speech analysis/synthesis (Backend) HTSと同様に、frontend, backendといった部分は提供していません。Merlinの論文にもあるように、HTSの影響を受けているようです。\nFrontendには、英語ならFestival、BackendにはWORLDやSTRAIGHTを使ってよろしくやってね、というスタンスです。 Backendに関しては、Merlinのインストールガイドにあるように、WOLRDをインストールするように促されます。\nデモスクリプトでは、Frontendによって生成されたフルコンテキストラベル（HTS書式）が事前に同梱されているので、Festivalをインストールする必要はありません。 misc以下に、Festivalを使ってフルコンテキストラベルを作るスクリプト (make_labels) があるので、デモデータ以外のデータセットを使う場合は、それを使います。\nSteps 本編です。slt_arcticのデモスクリプトに従い、いくらかのステップに分けて、詳細に見ていきます。なお、以下デモスクリプトと書いた際には、slt_arcticのデモスクリプトを指すものとします。\n継続長モデルの学習 音響モデルの学習 変換音声の合成 なお、Merlinのスクリプトによってはかれるデータは、基本的に\nx.astype(np.float32).tofile(\u0026#34;foobar.bin\u0026#34;) といった感じで、32bit浮動小数点のnumpyの配列がヘッダなしのバイナリフォーマットで保存されています。デバッグ時には、\nnp.fromfile(\u0026#34;foobar.bin\u0026#34;, dtype=np.float32) として、ファイルを読み込んでインスペクトするのが便利です。注意事項として、ややこしいことに、拡張子は信頼できません。.lab という拡張子であっても、フルコンテキストラベルのテキストファイルである場合もあれば、上述のようにバイナリフォーマットである可能性もあります。つらいですね！\n継続長モデルの学習 継続長モデルとは、言語特徴量から、継続長を予測するモデルです。Merlinでは、phone-level / state-level のどちらかを選択可能です。Merlinの提供するDNN音声合成では、継続長の予測→音響特徴量の予測→合成、といったスタイルをとります。 デフォルトでは、state-levelで継続長（具体的には一状態当たりの継続フレーム数）を予測します。状態レベルのアライメントのほうが、時間解像度の高いコンテキストを得られ、結果音声合成の品質が良くなるので、デフォルトになっているのだと思います。 https://github.com/CSTR-Edinburgh/merlin/issues/18 に少し議論があります。\nデモスクリプトを実行すると、 experiments/slt_arctic_demo/duration_model/ 以下に継続長モデル用のデータがは出力されます。いくつか重要なものについて、以下に示します。\ndata label_phone_align: 音素レベルでのフルコンテキストラベルです dur: 状態別継続長です。正確には、T をフルコンテキストラベル中の音素数として、(T, 5) の配列が発話ごとに保存されます。5は音素あたりのHMMの状態数で、慣例的に？5が使用されているような気がします。 inter_module 中間結果のファイル群です\nbinary_label_416/: HTS形式の質問ファイルを元に生成した、言語特徴量行列です。デモスクリプトでは、416個の質問があるので、一状態あたり416次元の特徴ベクトルになります。binaryな特徴量（母音か否か）と連続的な特徴量（単語中のsylalbleの数等）があります。(T, 416) の行列が、発話ごとに保存されています。 label_norm_HTS_416.dat: 416次元の特徴ベクトルの正規化に必要な情報です。デモスクリプトでは、言語特徴量に関してはmin/max正規化が行われるので、minおよびmaxの416次元のベクトル（計416*2次元）が保存されています。 nn_dur_5/: 無音区間が除去された、状態別継続長です。フォルダ名からは察することは難しいですが、無音区間が除去されています。 nn_no_silence_lab_416/: 無音区間が除去された、言語特徴量行列です。 nn_no_silence_lab_norm_416/: 無音区間が除去された、min/max正規化された言語特徴量行列です。 nn_norm_dur_5/ 無音区間が除去された、mean/variance正規化された状態別継続長です。 norm_info_dur_5_MVN.dat: 継続長の正規化に必要な情報です。具体的には、Mean-variance正規化（N(0, 1)になるようにする）が行われるので、平均と標準偏差（not 分散）が入っています。状態レベルでのアライメントを使用する場合は、5*2で計10次元のベクトルです。 ref_data/: RMSEなどの評価基準を計算する際に使われる継続長のテストデータです。data/dur ディレクトリの継続長データを元に、無音区間が除去されたものです var/: 継続長の分散（not 標準偏差）です。パラメータ生成 (MLPG) に使われる想定のデータです けっこうたくさんありますね。これだけでも、いかに多くのことがrun_merlin.pyによってなされているか、わかるかと思います。\n入力/出力 中間ファイルがたくさんあってややこしいですが、整理すると、ネットワーク学習に用いる入力と出力は以下になります。\n入力: nn_no_silence_lab_norm_416, 一発話あたりの特徴量のshape: (T, 416) 出力: nn_norm_dur_5, 一発話あたりの特徴量のshape: (T, 5) 学習されたモデルは、 nnets_modelというフォルダに保存されます。\n音響モデルの学習 音響モデルとは、言語特徴量からメルケプストラム、F0、非周期性成分などの音響特徴量を予測するモデルです。Merlinのデモスクリプトでは、\nメルケプストラム: 60次元（動的特徴量を合わせると、180次元) 対数F0: 1次元（動的特徴量を合わせると、3次元) 有声 or 無声フラグ (voiced/unvoiced; vuv): 1次元 非周期性成分: 1次元（動的特徴量を合わせると、3次元) の計187次元の音響特徴量を予測するモデルを考えます。継続長モデルのときと同様に、出力されるファイルについていくらか説明します。\ndata bap: 発話毎に計算された非周期性成分が入っています。bapはband averaged aperiodicityの略です（専門家の人にとっては当たり前かと思いますが、一応 label_phone_align: phone-levelでアライメントされたHTSのコンテキストラベルが入っています。デフォルトの設定では使いません。 label_state_align: state-levelでアライメントされたHTSのコンテキストラベルが入っています lf0: 対数F0です。なお、WORLDではかれるF0は無声区間で0を取りますが、無声区間の部分を線形補間することによって、非ゼロの値で補完しています。 mgc: メルケプストラムです（フォルダ名は、慣習的にメル一般化ケプストラムを表す mgcとなっていますが、デモスクリプトでは実際にはメルケプストラムです） inter_module binary_label_425/: 言語特徴量の行列です。継続長モデルの場合と違って、フレーム単位で生成されているのと、フレーム単位ならではの特徴量（音素中の何フレーム目なのか、等）が追加されています。フレーム数を T として、 (T, 425) の配列が発話ごとに保存されています。 label_norm_HTS_425.dat: 言語特徴量のmin/max正規化に必要なmin/maxのベクトルです。 nn_mgc_lf0_vuv_bap_187/: mgc, lf0, vuv, bapを結合した音響特徴量です。よくcmp (composed featureから来ていると思われる) と表されるものです。ディレクトリ名からは判別が付きませんが、無音区間は削除されています。ややこしい nn_no_silence_lab_425/: binary_label_425 の言語特徴量から無音区間を削除したものです nn_no_silence_lab_norm_425/: それをさらにmin/max正規化したものです nn_norm_mgc_lf0_vuv_bap_187/: nn_mgc_lf0_vuv_bap_187/の音響特徴量をN(0, 1)になるようにmean/variance …","date":1502820000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1502820000,"objectID":"cbffc668d75cf1c5f3c2f257ed1808d7","permalink":"https://r9y9.github.io/blog/2017/08/16/trying-to-understand-merlin/","publishdate":"2017-08-16T03:00:00+09:00","relpermalink":"/blog/2017/08/16/trying-to-understand-merlin/","section":"post","summary":"GitHub: https://cstr-edinburgh.github.io/merlin/","tags":["Speech Synthesis","Deep Learning","Python","Merlin"],"title":"DNN統計的音声合成ツールキット Merlin の中身を理解をする","type":"post"},{"authors":null,"categories":null,"content":" 本家サイト: http://www.cl.ecei.tohoku.ac.jp/nlp100/ 僕が書いたコード: https://github.com/r9y9/nlp100 最近、自然言語処理(NLP)を勉強しようという熱が出ました。ある自然言語処理の問題を解きたかったのですが、 無知のためにか直感がまったく働かず、これはまずいと感じたので、 入門的なのに手を出そうと思った次第です。 結果、毎日やりつづけて、12日かかりました（上図は、横軸が日付、縦軸が達成した問題数です。図はseabornで適当に作りました）。 速度重視1で問題を解きましたが、思ったよりうまく進まず大変だった、というのが正直な感想です。以下、雑多な感想です。\nmecab, cabocha, CoreNLPの解析結果をパースするコードを書くのは、ただただ面倒に感じた NER実装しろ、みたいな問題があったらより楽しかったかなと思った 正規表現をまったく使いこなせていなかったことがわかったので、勉強し直せてよかった 全体を通して、第9章のword embeddingを自前で作る部分が一番楽しかった うろ覚えですが2、問題文中に表現が正確でない（と感じる）部分があって、困惑したことがあった 9割python、1割juliaで書きましたが、sklearn, numpy, scipyなどを使わなくてよい、かつ速度が重要な場合は、簡単に速くできるのでjulia良い python、ライブラリが充実しすぎていて本当に楽 素人の言語処理100本ノック:まとめ - Qiita がとても丁寧で、解いたはいいものの自信がないときなどに、ちょくちょく見ていました。参考になりました 今後 深層学習による自然言語処理を買ったので3、それを読んで、自然言語処理の勉強を続けようと思います。\nナイーブな実装多し、コピペ多し、descriptiveでない変数名多し、等 ↩︎\n掘り返して探す気力がない・・・ ↩︎\nAmazonによると、僕は5/29に買っている模様。なお現在の進捗は0 ↩︎\n","date":1497013130,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1497013130,"objectID":"a9580dbac88fd45c93d941a2a8ca348a","permalink":"https://r9y9.github.io/blog/2017/06/09/nlp100/","publishdate":"2017-06-09T21:58:50+09:00","relpermalink":"/blog/2017/06/09/nlp100/","section":"post","summary":"GitHub: https://github.com/r9y9/nlp100","tags":["NLP","Python","Julia"],"title":"言語処理100本ノック 2015 をすべてやりました","type":"post"},{"authors":null,"categories":null,"content":"Julia advent calendar 2016 23日目の記事です。\nはじめに Juilaを最も簡単にインストールする方法は、公式のダウンロードページからバイナリ or インストーラを使用することだと思います。多くの人は、処理系をソースからビルドして使用することはめったにないと思いますが1、自分好みにビルドをカスタマイズしてJuliaを使いたいというコアな方向けに、僕がよく使うビルド時のオプションや便利そうなオプション、ビルド時のTipsなどを紹介しようと思います。\n僕がソースからビルドすることになった主な理由は、ソースからビルドしないと使えないパッケージがあったからです2。\n下準備 git clone https://github.com/JuliaLang/julia \u0026amp;\u0026amp; cd julia ビルドのカスタマイズ方法 Juliaのビルドシステムでは、Make.userというファイルで、ユーザがいくらかカスタマイズすることを許可しています。プロジェクトトップにMake.userを作成し、そこに override LLVM_VER=3.7.1 のような書き方で記述することで、カスタマイズ可能です（詳細は公式の説明をご覧ください）。例えば僕の場合、主な開発環境であるmacOSではMake.userを以下のように記述しています（項目の説明は後ほどします）。\noverride LLVM_VER=3.7.1 override LLVM_ASSERTIONS=1 override BUILD_LLVM_CLANG=1 override USE_LLVM_SHLIB=1 あとは、通常通りmakeコマンドを走らせることで、ビルドを行います。\nmake -j4 コア数は適当に指定します。llvm, openblasあたりのビルドが結構重いので、並列ビルドがオススメです。\n僕がよく使うオプション ここから、僕がよく使うオプションをいくつか解説します。\nLLVM_VER llvmのバージョンを表します。Julia上でC++を使いたいというcrazyな人に激推しの Keno/Cxx.jl というパッケージがあるのですが、このパッケージはclangとllvmの3.7.1以上を必要とします（Cxx.jlについては、過去に何度か記事を書いたので、例えば Cxx.jlを用いてJulia expression/value をC++に埋め込む実験 をご覧ください）。llvm 3.3がデフォルトだったJulia v0.4時代では、明示的に3.7.1と指定する必要がありました。いまは、\nJulia v0.5の公式配布バイナリでも、Pkg.add(\u0026#34;Cxx\u0026#34;)でインストールできるとされている（Keno/Cxx.jl/#287） かつ現状のデフォルトバージョンが3.7.1 (もうすぐ3.9.1になりそうですが JuliaLang/julia/#19768) なので、僕の場合は明示的にLLVM_VERを指定する必要はなくなってきましたが、例えば、LLVMのNVPTX backendを使ってJuliaでCUDAカーネルを書けるようにする JuliaGPU/CUDAnative.jl （要 llvm 3.9）のような、experimentalなパッケージを試したい場合など、LLVM_VERを指定したくなる場合もあるかと思います。\nLLVM_ASSERTIONS LLVMをassert付きでビルドするかどうかを表します。ONにするとビルドかかる時間が長くなり、LLVMのパフォーマンスが若干落ちますが、デバッグには便利です。Juliaのコード生成周りでエラーを起こしやすいようなコードを書くときには、ONにしておくと便利です。\nBUILD_LLVM_CLANG llvmとあわせて、clangをビルドするかどうか、というオプションです。Cxx.jlに必要なので、僕はそのためにONにしています。その他必要なケースとしては、clangのaddress/memory sanitizerを使いたい場合が考えられます。詳細はdevdocs/sanitizers をご覧ください。\nCC, CXX コンパイラの指定です。僕の場合 ubuntu 14.04では、（Cxx.jlのために）以下のように設定しています。\noverride CC=gcc-6 override CXX=g++-6 参考: https://github.com/r9y9/julia-cxx\nmacOS では特に設定していませんが、Julia以外のプロジェクトをビルドするときに、たまに\nCXX=usr/local/bin/clang++ cmake ${path_to_project} のように、xcode付属のclangではなく、自前でビルドしたclangを使いたい場合などに、CC, CXXを指定したりします。\nUSE_CLANG clangを使ってビルドするかどうかを表します。gccを使いたくない、というときにオンにします。\nUSE_LLVM_SHLIB llvmを共有ライブラリとしてビルドするかどうかを表します。v0.4ではデフォルトがオフで、v0.5からはオンになっています。llvmの共有ライブラリをdlopenして色々いじりたい場合（何度もアレですが、Cxx.jlを使いたい場合とか）は、オンにする必要があります。\nUSE_SYSTEM_${LIB_NAME} Juliaでは、デフォルトで依存ライブラリをソースからビルドします。システムにインストールされたライブラリを使用したい場合、USE_SYSTEM_XXX （e.g. USE_SYSTEM_BLAS）をオンにします。ビルド時間を短縮することが可能です。\nUSE_SYSTEM_xxx にどのようなものがあるのかは、Make.inc をご覧ください。\n便利そうなオプション USE_INTEL_MKL MKLを使うかどうかを表します。MKLを持っている場合にオンにすれば、一部パフォーマンスが向上しそうですね。\nUSE_GPL_LIBS GPLのライブラリ（FFTWなど）を使用するかどうかを表します。使ったことはありませんが、Juliaを組み込みで使用したい場合に、便利かもしれません。\nビルド時のTips Juliaは依存関係が多く、cloneした直後の状態からのビルドには一時間以上かかることもあります3。また、masterを追いかけている場合は、途中でビルドにこけてしまうことも珍しくありません。個人的な経験で言えば、\nllvm openblas libgit2 mbettls libunwind あたりの依存関係のビルドで、何度も失敗しています。僕がソースビルドをし始めたころ、よく調べずに make clean \u0026amp;\u0026amp; make をして、案の定駄目で、よくわからずに make distcleannall してしまうこともありました（distcleanallが必要なケースは稀であり、そうでない場合は非常に時間を無駄にします）4。過去の失敗から、僕が学んできたTipsを紹介します。\nプロジェクトトップMakefileのcleanコマンドを適切に使い分ける cleanコマンドにはさまざまなものがあります。ビルドのし直しが不要なものまでcleanして、無駄に時間を消費しないように、正しく使い分けましょう。以下、基本的なcleanコマンドを簡単にまとめます。\nコマンド 説明 clean Julia本体のclean cleanall Julia本体、flisp、libuvのclean distclean binary配布用の成果物をclean distcleanall deps以下の依存関係をすべてclean よほどのことがない限り、 make distcleanall を使わないようにしましょう。make distclean はほとんど使う必要はないと思います。\nコマンドの詳細、その他コマンドについては、julia/Makefile をご覧ください\nサブディレクトリのMakefileを使いわける deps: 依存関係 src: コンパイラ (C, C++, flisp) base: 標準ライブラリ (Julia) doc: ドキュメント など（一部です）、Makefileは復数のサブディレクトリにわかれています。依存関係のビルドに失敗した場合には、depsディレクトリ以下のMakefileが使えます。\ndepsディレクトリ以下、依存関係のcleanコマンドには、大きく以下の二種類があります。\nコマンド 説明 clean-xxx xxx の clean distclean-xxx xxx の clean と rm -rf ビルドディレクトリ 例えばlibgit2のバージョンが変わってエラーが出たからといって、すべてをビルドし直す必要は基本的にはありません。まずは、\nmake -C deps clean-libgit2 \u0026amp;\u0026amp; make としてビルドし直し、それでも駄目な場合は、\nmake -C deps distclean-libgit2 \u0026amp;\u0026amp; make といった具合に、軽いcleanコマンドから順に試しましょう。\nまとめ Make.user をプロジェクトトップに配置することで、ビルドをカスタマイズできます ビルドに失敗したとき、良く考えずに make distcleanall するのをやめましょう（自戒 サブディレクトリの Makefile を使い分けて、rebuildは最小限にしましょう 大変ですよね ↩︎\nhttps://github.com/Keno/Cxx.jl です ↩︎\n環境によります ↩︎\n僕があほなだけの可能性が大いにあります ↩︎\n","date":1482483968,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1482483968,"objectID":"f2c337c54e9bc5df6891b00745d4758d","permalink":"https://r9y9.github.io/blog/2016/12/23/julia-advent-calender-2016-customize-source-build/","publishdate":"2016-12-23T18:06:08+09:00","relpermalink":"/blog/2016/12/23/julia-advent-calender-2016-customize-source-build/","section":"post","summary":"[Julia advent calendar 2016](https://qiita.com/advent-calendar/2016/julialang)","tags":["Julia","Advent Calendar"],"title":"Juliaをソースからビルドする / Building Julia from source","type":"post"},{"authors":null,"categories":null,"content":"Keno氏によるJuliaCon 2015 の発表 Keno Fischer: Shaving the Yak でタイトルの内容が一部紹介されていて、便利そうだなと思い、色々試してみました。\n発表の内容は大まかに、Keno氏がなぜCxx.jlを作ったのか、なぜJuliaを始めたのか、といったモチベーションの話から、Cxx.jlでできることについてlive programmingを交えての紹介、といった話になっています。50分とけっこう長いですが、面白いので興味のある方はどうぞ。この記事は、上の動画を見たあと、Cxx.jlと戯れた結果をまとめたものです。\n以下、この記事の目次です。\n前置き：C++をJulia上で使う 本編：JuliaのexpressionやvalueをC++に埋め込む 前置きが若干長いので、タイトルの内容が知りたい方は、飛ばして下さい。\n前置き：C++をJulia上で使う Cxx.jlを使えば、C++をJulia上で非常にスムーズに扱うことができます。例えば、C++のstd::vector\u0026lt;T\u0026gt;を使いたい、さらにはJuliaのfilter関数をstd::vector\u0026lt;T\u0026gt;に対して使えるようにしたい、といった場合は、以下に示すように、ほんのすこしのコードを書くだけでできます。\n準備：\nusing Cxx import CxxStd: StdVector filter関数：\nfunction Base.filter{T}(f, v::StdVector{T}) r = icxx\u0026#34;std::vector\u0026lt;$T\u0026gt;();\u0026#34; for i in 0:length(v)-1 if f(T(v[i])) push!(r, v[i]) end end r end なお、filter関数に出てくる、length, getindex, push! は、Cxx..jlにそれぞれ以下のように定義されています。\nBase.getindex(it::StdVector,i) = icxx\u0026#34;($(it))[$i];\u0026#34; Base.length(it::StdVector) = icxx\u0026#34;$(it).size();\u0026#34; Base.push!(v::StdVector,i) = icxx\u0026#34;$v.push_back($i);\u0026#34; 計算結果を見やすくするために、show 関数も定義しておきます。\nfunction Base.show{T}(io::IO, v::StdVector{T}) println(io, \u0026#34;$(length(v))-element StdVector{$T}:\u0026#34;) for i = 0:length(v)-1 println(io, T(v[i])) end end 実行結果：\njulia\u0026gt; v = icxx\u0026#34;std::vector\u0026lt;double\u0026gt;{1,2,3,4,5,6,7,8,9,10};\u0026#34; 10-element StdVector{Float64}: 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 julia\u0026gt; filter(x -\u0026gt; x \u0026gt; 5, v) 5-element StdVector{Float64}: 6.0 7.0 8.0 9.0 10.0 簡単にできました。とても強力です。\nさて、以降本編に入りたいと思いますが、Julia上でC++を使うのは簡単かつ、Cxx.jlの主な用途だとは思うのですが（少なくとも自分がそうでした）、逆はどうなのでしょうか？実は、limitationはあるものの、かなり面白いことができます。\nJuliaのexpressionやvalueをC++に埋め込む まず簡単に、基本的な使い方を整理します。\nvalueを埋める $(some_value) という書き方をします\njulia\u0026gt; cxx\u0026#34;\u0026#34;\u0026#34; int getRandom() { return $(rand(1:10)); } \u0026#34;\u0026#34;\u0026#34; true julia\u0026gt; @cxx getRandom() 2 julia\u0026gt; @cxx getRandom() 2 julia\u0026gt; @cxx getRandom() 2 rand関数を評価したvalueを埋め込んでいるので、何度getRandomを呼び出しても結果は同じになります。\nexpressionを埋める $:(some_expression) という書き方をします。\njulia\u0026gt; cxx\u0026#34;\u0026#34;\u0026#34; int getReallyRandom() { return $:(rand(1:10)); } \u0026#34;\u0026#34;\u0026#34; true julia\u0026gt; @cxx getReallyRandom() 1 julia\u0026gt; @cxx getReallyRandom() 9 julia\u0026gt; @cxx getReallyRandom() 2 期待した通りの動作になっていますね。\n発展例 さて、以下、もう少し発展的な例です。\nC++ expressionの中にJuila expressionを埋めて、さらにその中にC++ expressionを埋める (1) 言葉にするとややこしいですが、例を見ればすぐにわかると思います。\njulia\u0026gt; cxx\u0026#34;\u0026#34;\u0026#34; void test4(int N) { for (int i = 0; i \u0026lt; N; ++i) { $:(println(icxx\u0026#34;return i;\u0026#34;); nothing); } } \u0026#34;\u0026#34;\u0026#34; true julia\u0026gt; @cxx test4(10) 0 1 2 3 4 5 6 7 8 9 簡単に説明すると、C++のfor分の中で、Juliaのprintln関数を読んでいて、さらにprintlnの引数に、C++ expressionが渡されています。icxx\u0026#34;return i;\u0026#34;という部分が重要で、これは C++ lambda[\u0026amp;](){return i;)} に相当しています。中々キモい表記ですが、こんなこともできるようです。\nC++ expressionの中にJuila expressionを埋めて、さらにその中にC++ expressionを埋める (2) もう少し実用的な例です。C++関数の中で、Juliaのプログレスバーを使ってみます。\njulia\u0026gt; using ProgressMeter julia\u0026gt; cxx\u0026#34;\u0026#34;\u0026#34; #include \u0026lt;iostream\u0026gt; #include \u0026lt;cmath\u0026gt; double FooBar(size_t N) { double result = 0.0; $:(global progress_meter = Progress(icxx\u0026#34;return N;\u0026#34;, 1); nothing); for (size_t i = 0; i \u0026lt; N; ++i) { result = log(1+i) + log(2+i); $:(next!(progress_meter); nothing); } return result; } \u0026#34;\u0026#34;\u0026#34; true julia\u0026gt; @cxx FooBar(100000000) Progress: 100% Time: 0:00:18 36.84136149790473 プログレスバーについては、Juliaでプログレスバーの表示をする | qiitq を参考にどうぞ。このコードもなかなかきもいですが、期待した通りに、プログレスバーが表示されます。\nさて、この例からは、Cxx.jlの（現在の）limitationが垣間見えるのですが、\nJuliaのexpressionで定義したローカル変数は、C++的には同じ関数スコープであっても、Julia expressionからはアクセス不可（上記例では、progress_meterをglobalにしないと、for文内のjulia expressionからはprogress_meter にアクセスできません） 随所にあるnothingにお気づきの人もいると思うのですが、C++ expression内のJulia expressionにさらにC++ expressionを埋め込む場合（※そういったexpressionのことを、nested expressions と呼ぶんだと思います）、返り値はVoid型しか受け付けられません（nothing をJulia expressionの末尾に置くことで、Julia expressionの返り値をVoidにしています） 後者について、簡単に例をあげておきます。\nネストしていないからOK julia\u0026gt; cxx\u0026#34;\u0026#34;\u0026#34; int getRandom2() { int r = $:(rand(1:10)); return r; } \u0026#34;\u0026#34;\u0026#34; true julia\u0026gt; @cxx getRandom2() 2 ネストしているからダメ julia\u0026gt; cxx\u0026#34;\u0026#34;\u0026#34; int getRandom3(int hi) { int r = $:(rand(1:icxx\u0026#34;return hi;\u0026#34;)); return r; } \u0026#34;\u0026#34;\u0026#34; In file included from :1: __cxxjl_10.cpp:2:9: error: cannot initialize a variable of type \u0026#39;int\u0026#39; with an rvalue of type \u0026#39;void\u0026#39; int r = __julia::call2([\u0026amp;](){ return hi; }); ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ERROR: Currently only `Void` is supported for nested expressions in InstantiateSpecializations at /Users/ryuyamamoto/.julia/v0.5/Cxx/src/cxxstr.jl:268 [inlined code] from /Users/ryuyamamoto/.julia/v0.5/Cxx/src/cxxstr.jl:723 in anonymous at /Users/ryuyamamoto/.julia/v0.5/Cxx/src/cxxstr.jl:759 in eval at ./boot.jl:265 nested expressionsで、返り値がVoid以外も取れるようになると、嬉しいなーと思います。\nC++ lambda に Julia expressionを埋める これは現在、間接的にしかできませんでした。以下に例をあげます。\nulia\u0026gt; for f in [\u0026#34;iostream\u0026#34;, \u0026#34;thread\u0026#34;] cxxinclude(f) end julia\u0026gt; cxx\u0026#34;\u0026#34;\u0026#34; int getRandom() { return $:(rand(1:10)); } \u0026#34;\u0026#34;\u0026#34; true julia\u0026gt; th = icxx\u0026#34;\u0026#34;\u0026#34; std::thread([]{ for (size_t i = 0; i \u0026lt; 10; ++i) { std::cout \u0026lt;\u0026lt; getRandom() \u0026lt;\u0026lt; std::endl; } }); \u0026#34;\u0026#34;\u0026#34; 6 10 5 6 5 3 7 2 6 9 (class std::__1::thread) { } julia\u0026gt; @cxx th-\u0026gt;join() threadである必要はない例ですが、lambdaの例ということで。間接的にというのは、一度Julia関数をC++関数に埋め込んで、そのC++関数をlambdaの中で呼び出す、という意味です。\n以下のようにJulia expressionを直接埋めようとすると、assertion failureで落ちるてしまうので、注意\njulia\u0026gt; th = icxx\u0026#34;\u0026#34;\u0026#34; std::thread([]{ for (size_t i = 0; i \u0026lt; 10; ++i) { std::cout \u0026lt;\u0026lt; $:(rand(1:10)) \u0026lt;\u0026lt; std::endl; } }); …","date":1453642328,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1453642328,"objectID":"4798b2ecb61d4ef42e168c8481e7fda4","permalink":"https://r9y9.github.io/blog/2016/01/24/passing-julia-expressions-to-cxx/","publishdate":"2016-01-24T22:32:08+09:00","relpermalink":"/blog/2016/01/24/passing-julia-expressions-to-cxx/","section":"post","summary":"[Keno Fischer: Shaving the Yak](https://www.youtube.com/watch?v=OB8BclL_Tmo)","tags":["Julia","C++"],"title":"Cxx.jlを用いてJulia expression/value をC++に埋め込む実験","type":"post"},{"authors":null,"categories":null,"content":"新年はじめての記事ということで、少し遅いですが、あけましておめでとうございます。PCLを対話環境で使いたかったので、お正月の間にPCLのラッパーを作りました1。なぜ作ったのか、どうやって作ったのか、少し整理して書いてみようと思います。\nPoint Cloud Library (PCL) とは http://www.pointclouds.org/\n問題 PCL はboost、Eigenに依存している、かつtemplateを多く使用しているため、PCLを使用したプロジェクトのコンパイル時間は非常に長くなるという問題があります。twitterで [PCL コンパイル] として検索すると、例えば以下の様なツイートが見つかりますが、完全に同意です。\nPCLリンクしてるコードのコンパイルに一分半くらいかかる。つらい\n— がらえもん(プログラム書く (@garaemon_coder) August 14, 2015 PCLはC++だしコンパイル遅いしで色々めんどくさい\n— 動かないで点P (@initial_D_0601) August 25, 2015 PCLを使うプロジェクトのコンパイル時間かかりすぎて辛いわ\n— kato tetsuro (@tkato_) November 6, 2015 boostへの依存関係が必須かどうかについては疑問が残りますが、点群処理ではパフォーマンスが求められることが多いと思われるので、C++で書かれていることは合理的に思います。とはいえ、コンパイル時間が長いのは試行錯誤するにはつらいです。\nではどうするか 試行錯誤のサイクルを速く回せるようにすることは僕にとって非常に重要だったのと、 C++で書かなければいけないという制約もなかった（※組み込み用途ではない）ので、対話的にPCLを使うために、僕は動的型付け言語でラッパーを作ることにしました。\n参考までに、対話環境を使うことによるメリットは、下記スライドが参考になります。PCLの紹介もされています2。\nコンピュータビジョンの最新ソフトウェア開発環境 SSII2015 チュートリアル hayashi from Masaki Hayashi 何で書くか 世の中には色んなプログラミング言語があります。C++ライブラリのラッパー作るぞとなったとき、僕にとって選択肢は、\nPython Julia の二択でした。それぞれ、以下のプロジェクトに頼れば templateを多用したライブラリのラップができそうだと思いました。\nCython Cxx.jl pythonに関しては、すでに cythonで書かれた strawlab/python-pcl というラッパーがあります。しかし、\n現在あまりメンテされていない サポートされている機能も多くはない templateを多用したライブラリのラップをcythonで十分にできるかどうか自信がなかった 3 Juliaは関数や型がパラメータを持てるため、templateを多用したライブラリのラップが簡単にできそうだと思った（i.e. pcl::PointCloud\u0026lt;T\u0026gt; は PointCloud{T} と書ける4） Cxx.jl を使えば JITライクに C++ を使える（試行錯誤できる）し、Juliaのほうがいいかな といった理由から、Juliaで書くことにしました。\n成果物 https://github.com/r9y9/PCL.jl\nStatisticalOutlierRemovalのデモ | nbviewer こんな感じで、jupyter上で試行錯誤できるようになりましたとさ5。strawlab/python-pcl よりも多くのことができると思います。6\nPCLは非常に大きなライブラリのため、全ての機能をラップするつもりはありませんが、今後必要に応じて機能を追加するかもしれません。\n適当なスクショ PCL.jl で、少なくとも最低限以下はできますということで。ソースコードは r9y9/PCL.jl/examples にあります。\nPCLVisualizer 3D Object Recognition based on Correspondence Grouping Hypothesis Verification for 3D Object Recognition Extracting indices from a PointCloud Kinect v2で遊ぶ 画質低い \u0026amp; クロップが適当で一部しか見えませんが、諸々の処理を含めて fpsは15くらいでしょうか。depthとrgb imageのregistration、その結果の点群への変換に関しては、20~30fps程度でした 測りなおしたら平均40fpsくらいはでてました。real-timeで点群を処理するようなアプリケーションを書く場合は、現実的にはC++で書くことになるかと思います。\n余談 Kinect v2 から得たデータを点群に変換するのに、Juliaではパフォーマンスを出すのに苦労したのですが、結果面白い（キモい？）コードができたので、少し話はそれますが簡単に紹介しておきたいと思います。\nDepthとcolorを点群に変換する関数 まず、コードを以下に示します。\nfunction getPointCloudXYZRGB(registration, undistorted, registered) w = width(undistorted) h = height(undistorted) cloud = pcl.PointCloud{pcl.PointXYZRGB}(w, h) icxx\u0026#34;$(cloud.handle)-\u0026gt;is_dense = false;\u0026#34; pointsptr = icxx\u0026#34;\u0026amp;$(cloud.handle)-\u0026gt;points[0];\u0026#34; icxx\u0026#34;\u0026#34;\u0026#34; for (size_t ri = 0; ri \u0026lt; $h; ++ri) { for (size_t ci = 0; ci \u0026lt; $w; ++ci) { auto p = $(pointsptr) + $w * ri + ci; $(registration)-\u0026gt;getPointXYZRGB($(undistorted.handle), $(registered.handle), ri, ci, p-\u0026gt;x, p-\u0026gt;y, p-\u0026gt;z, p-\u0026gt;rgb); } } \u0026#34;\u0026#34;\u0026#34; cloud end r9y9/PCL.jl/examples/libfreenect2_grabbar.jl#L12-L29\nsyntax highlightとは何だったのか、と言いたくなるようなコードですが、performance heavy な部分は icxx\u0026#34;\u0026#34;\u0026#34;...\u0026#34;\u0026#34;\u0026#34; という形で、C++ で記述しています。Juliaのコード中で、こんなに自由にC++を使えるなんて、何というかキモいけど書いていて楽しいです。\nなお、最初に書いたコードは、以下の様な感じでした。\nfunction getPointCloudXYZRGB(registration, undistorted, registered) w = width(undistorted) h = height(undistorted) cloud = pcl.PointCloud{pcl.PointXYZRGB}(w, h) icxx\u0026#34;$(cloud.handle)-\u0026gt;is_dense = true;\u0026#34; pointsptr = icxx\u0026#34;\u0026amp;$(cloud.handle)-\u0026gt;points[0];\u0026#34; for ri in 0:h-1 for ci in 0:w-1 p = icxx\u0026#34;$(pointsptr) + $w * $ri + $ci;\u0026#34; x,y,z,r,g,b = getPointXYZRGB(registration, undistorted, registered, ri, ci) isnan(z) \u0026amp;\u0026amp; icxx\u0026#34;$(cloud.handle)-\u0026gt;is_dense = false;\u0026#34; icxx\u0026#34;\u0026#34;\u0026#34; $p-\u0026gt;x = $x; $p-\u0026gt;y = $y; $p-\u0026gt;z = $z; $p-\u0026gt;r = $r; $p-\u0026gt;g = $g; $p-\u0026gt;b = $b; \u0026#34;\u0026#34;\u0026#34; end end cloud end r9y9/PCL.jl/examples/libfreenect2_grabbar.jl#L12-L29\nこのコードだと、forループの中でJulia関数の呼びだしが発生するため、実は重たい処理になっています。このコードだと、確かfps 3 とかそのくらいでした。関数呼び出しがボトルネックだと気づいて、icxx\u0026#34;\u0026#34;\u0026#34;...\u0026#34;\u0026#34;\u0026#34; でくるんで（一つの関数にすることで）高速化を図った次第です。\n雑記 以下、僕のmacbook proで tic(); using PCL; toc() をした結果：\njulia\u0026gt; tic(); using PCL; toc() INFO: vtk include directory found: /usr/local/include/vtk-6.3 INFO: Loading Cxx.jl... INFO: dlopen... INFO: vtk version: 6.3.0 INFO: Including headers from system path: /usr/local/include INFO: pcl_version: 1.8 INFO: Include pcl top-level headers 1.053026 seconds (91 allocations: 4.266 KB) INFO: Include pcl::common headers 5.433219 seconds (91 allocations: 4.078 KB) INFO: adding vtk and visualization module headers INFO: Include pcl::io headers 0.389614 seconds (195 allocations: 11.034 KB) INFO: Include pcl::registration headers 1.428106 seconds (195 allocations: 11.065 KB) INFO: Include pcl::recognition headers 1.154518 seconds (136 allocations: 6.141 KB) INFO: Include pcl::features headers 0.033937 seconds (181 allocations: 8.094 KB) INFO: Include pcl::filters headers 0.070545 seconds (316 allocations: 14.125 KB) INFO: Include pcl::kdtree headers 0.022809 seconds (91 allocations: 4.078 KB) INFO: Include pcl::sample_consensus headers 0.014600 seconds (91 allocations: 4.141 KB) INFO: Include pcl::segmentation headers 0.010710 seconds (46 allocations: 2.094 KB) INFO: FLANN version: 1.8.4 elapsed time: 39.194405845 seconds 39.194405845 r9y9/PCL.jl/src/PCL.jl#L90-L101 pcl/pcl_base.h. pcl/common/common_headers.h 当たりのパース …","date":1453045486,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1453045486,"objectID":"c2306d4775bb757172d165859ecb1a4a","permalink":"https://r9y9.github.io/blog/2016/01/18/trying-to-use-pcl-in-dynamic-language/","publishdate":"2016-01-18T00:44:46+09:00","relpermalink":"/blog/2016/01/18/trying-to-use-pcl-in-dynamic-language/","section":"post","summary":"http://www.pointclouds.org/","tags":["Julia","PCL"],"title":"対話環境でPoint Cloud Library (PCL) を使いたい","type":"post"},{"authors":null,"categories":null,"content":"","date":1450742400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1450742400,"objectID":"50aafce84dcdc989cf2aeb8ec01da520","permalink":"https://r9y9.github.io/blog/2015/12/22/cxx-jl/","publishdate":"2015-12-22T00:00:00Z","relpermalink":"/blog/2015/12/22/cxx-jl/","section":"post","summary":"[Julia Advent Calendar 2015](https://qiita.com/advent-calendar/2015/julialang)","tags":["Julia","Advent Calendar"],"title":"Cxx.jl を使ってみた感想 + OpenCV.jl, Libfreenect2.jl の紹介","type":"post"},{"authors":null,"categories":null,"content":"はじめに Julia Advent Calendar 2015 8日目の記事です。\nこの記事では、値 (value) と変数 (variable) に対する type annotation の違いを、問題とそれに対する解答を用意する形式で説明しようと思います。そんなの知ってるぜ！という方は、問題だけ解いてみて自分の理解度を試してもらえればと思います。\n記事に出てくるJuliaコードは、Julia 0.5-dev, 0.4.0 で動作確認しました。\n問題 新規REPLセッションを開いて、A、B それぞれを実行したときの挙動はどうなるでしょうか？エラーの発生の有無と、エラーが発生しない場合は返り値の値、型を答えてください。\nA function f() x = (1.0 + 2.0)::Int return x end f() B function g() x::Int = (1.0 + 2.0) return x end g() なお、一方ではエラーが起き、もう一方はエラー無く実行されます。一見似たような書き方ですが、二つは異なる意味を持ちます。この記事ではそれぞれを解説しようと思います。\nこの問題の答えがわからなかった方は、この記事を読むと正解がわかるはずなので、続きをご覧ください。下の方に、簡潔な問題の解答とおまけ問題を書いておきました。\nA: 値に対する type annotation Aの2行目では、値に対して type annotation をしています。これは typeassert とも呼びます。Aで使った type annotation を日本語で説明してみると、「(1.0 + 2.0) という式を評価した値は、Int 型であることを保証する」となります。\n(1.0 + 2.0) を評価した値は 3.0 であり、 Float64の型を持ちます。したがって Float64 != Int であるため、\nERROR: TypeError: typeassert: expected Int64, got Float64 のような typeassert のエラーが吐かれます。\n(1.0 + 2.0)を評価した値の型は一見して明らかため、実用的な例ではありませんが、例えば関数の返り値の型は一見してわからないことがあるので、例えば以下のような書き方は有用な場合もあると思います。\nx = f(y)::Int B: 変数に対する type annotation Bの2行目では、変数に対して type annotation をしています。同じく日本語で説明すると、「xという変数に入る値は、Int 型であることを保証する」となります。また、値に対する annotation とは異なりスコープを持ちます。\n前述したとおり、(1.0 + 2.0) を評価した値は 3.0 であり、Float64の型を持ちます。一方で、x は Int型の値を持つ変数として宣言されているため、この場合、Float64型である (1.0 + 2.0) を、Int 型に変換するような処理が暗黙的に行われます。したがって、変換可能な場合には（B の例がそうです）、エラーは起きません。暗黙的に処理が行われるというのは、知らないと予期せぬバグに遭遇することになるため、気をつける必要があります。\nでは、変数に対する type annotation はどのような場合に使うかというと、あるスコープの範囲で、代入によって変数の型が変わってしまうのを防ぐために使います。ある変数の型がスコープの範囲で不変というのはコンパイラにとっては嬉しい事で、パフォーマンスの向上に繋がります。Performance tips にもありますね（参考: Performance tips / Avoid changing the type of a variable）\n違いまとめ ここまでの話から、違いをまとめると、以下のようになります。\nType annotation の種類 typeassert error　暗黙的な型変換　スコープ　値に対する type annotation あり なし なし 変数に対する type annotation なし あり あり 最後に type annotation を使うときは、値と変数に対する annotation の違いを意識して、使い分けましょう\n問題の解答 A: typeassert に引っかかり、TypeError が吐かれる B: Int 型の 3 が返り値として得られる おまけ問題 1 function h() x::UInt8 = UInt8(0) x = Float64(0.0) x end # なんと表示されるでしょうか？ println(typeof(h())) 2 function s() x::Int = Float64(0) x = UInt8(0) x = Float32(0.5) x end # なんと表示されるでしょうか？ s() 解答は、各自REPLで実行して確認してみてください。長々と読んでくださりありがとうございました。\n参考 公式ドキュメント / Type Declarations ","date":1449532800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1449532800,"objectID":"fdfcfc7a8a04d136adbf7cf218205efc","permalink":"https://r9y9.github.io/blog/2015/12/08/julia-type-annotations/","publishdate":"2015-12-08T00:00:00Z","relpermalink":"/blog/2015/12/08/julia-type-annotations/","section":"post","summary":"[Julia Advent Calendar 2015](https://qiita.com/advent-calendar/2015/julialang)","tags":["Julia","Advent Calendar"],"title":"Julia: 値と変数に対する Type Annotation の違い","type":"post"},{"authors":null,"categories":null,"content":"2015/09/05:\nhttps://t.co/WFBmYEIVce SPTKのpythonラッパー（マシなやつ）完成\nドキュメント http://t.co/jYhw1y3Bzg\npip install pysptk でインストールできるようになりました。pypi童貞捨てれた\n— 山本 龍一 / Ryuichi Yamamoto (@r9y9) September 4, 2015 ずいぶん前に、swig遊びをしがてらpythonのラッパーを書いていたんですが、cythonを使って新しく作りなおしました。かなりパワーアップしました。\npip install pysptk でインストールできるので、よろしければどうぞ\nなぜ作ったのか cythonとsphinxで遊んでたらできた 使い方 以下のデモを参考にどうぞ\nIntroduction to pysptk: メル一般化ケプストラム分析とか Speech analysis and re-synthesis: 音声の分析・再合成のデモ。合成音声はnotebook上で再生できます ドキュメント http://pysptk.readthedocs.org\nぼやき SPTKの関数、変な値入れるとexitしたりセグフォったりするので、ちゃんとテスト書いてほしいなあ\n関連 SPTKのPythonラッパーを書いた - LESS IS MORE ","date":1441497600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441497600,"objectID":"786e6df75b6d87ad73721d36eb30922c","permalink":"https://r9y9.github.io/blog/2015/09/06/pysptk/","publishdate":"2015-09-06T00:00:00Z","relpermalink":"/blog/2015/09/06/pysptk/","section":"post","summary":"pysptk: https://github.com/r9y9/pysptk","tags":["Speech Signal Processing","Python","SPTK"],"title":"pysptk: SPTKのpythonラッパーを作った (part 2)","type":"post"},{"authors":null,"categories":null,"content":"hello 遡ればもう約一年まえになるでしょうか、統計的声質遊びをしたいと思い、理論の勉強を始めたり、（特にJuliaで）コードを色々書いていました（お前ほんといろんな言語で遊んでるな、というツッコミはさておき）。統計的声質変換クッソムズすぎワロタ（チュートリアル編） - LESS IS MORE を書いていた当初は、当然自分のためだけに書いていて、まぁアレな出来でしたが、最近気を取り直して多少マシに仕上げましたので、何となくブログに書いてみようかなーと思った次第です。というわけで、最近公式に登録したいくつかのパッケージを、まとめて簡単に紹介します。\n主な改善点は、windowsもちゃんとサポートするようにしたこと（誰得？）と、テストをきちんと書いたことと、julia的なインタフェースを意識するようにしたことですかね。3つ目はかなり曖昧ですが、まぁ気持ち使いやすくなったと思います。\nパッケージ MelGeneralizedCepstrums.jl: メル一般化ケプストラム分析 SynthesisFilters.jl: メル一般化ケプストラムからの音声波形合成 SPTK.jl: SPTKのラッパー 車輪の再発明はできるだけしたくなかったので、最初のほうはCライブラリのラッパーを書くことが多く、windowsとかめんどくさいしunix環境でしか動作確認してませんでしたが、WindowsのJuliaから呼べるようなCライブラリの共有ライブラリ（DLL）を作る | qiita 重い腰を上げてwindowsでも動くように頑張ったことがあり（めんどくさいとか言って手を動かさないのホント良くないですね）、登録したパッケージはすべてwindowsでも動くようになりました。めでたし。WORLD.jl もwindowsで動くようにしました。\nMelGeneralizedCepstrums.jl メルケプストラムの推定とか。いくつか例を載せておきます\n詳細はこちらのノートブックへ\nメルケプストラム分析、メル一般化ケプストラム分析に関しては、SPTKの実装をjuliaで再実装してみました。結果、速度は1.0 ~ 1.5倍程度でおさまって、かつ数値的な安定性は増しています（メモリ使用量はお察し）。まぁ僕が頑張ったからというわけでなく、単にJuliaの線形方程式ソルバーがSPTKのものより安定しているというのが理由です。\nSynthesisFilters.jl メルケプストラムからの波形合成とか。\n詳細はこちらのノートブックへ。いくつかの音声合成フィルタの合成音をノートブック上で比較することができます。\nmixed excitation（っぽいの）を使ったバージョンのノートブック: 実装に自信がないので、そのうち消すかも。聴覚的にはこっちのほうが良いです。\nSPTK.jl 公式のSPTKではなく、僕が少しいじったSPTK（windowsで動くようにしたり、APIとして使いやすいように関数内でexitしてた部分を適切なreturn code返すようにしたり、swipeというF0抽出のインタフェースをexposeしたり、など）をベースにしています。\nデモ用のノートブック\nMelGeneralizedCepstrums.jl と SynthesiFilters.jl は、ほとんどSPTK.jlで成り立っています。本質的に SPTK.jl にできて MelGeneralizedCepstrums.jl と SynthesiFilters.jlにできないことは基本的にないのですが、後者の方が、より簡単な、Julia的なインタフェースになっています。\n例えば、メルケプストラム、ケプストラム、LPCなど、スペクトルパラメータの型に応じて、適切なフィルタ係数に変換する、合成フィルタを選択するなど、multiple dispatchを有効に活用して、よりシンプルなインタフェースを提供するようにしました（というか自分がミスりたくなかったからそうしました）。\nおわり かなり適当に書きましたが、最近の進捗は、Juliaで書いていたパッケージ多少改善して、公式に登録したくらいでした。進捗まじ少なめ。あと些細なことですが、ipython（ijulia）に音埋め込むのクッソ簡単にできてびっくりしました（なんで今までやらなかったんだろう）。@jfsantos に感謝\n","date":1440288000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1440288000,"objectID":"70b0657d3f42be6439d24316144d97d5","permalink":"https://r9y9.github.io/blog/2015/08/23/speech-analysis-and-synthesis-in-julia/","publishdate":"2015-08-23T00:00:00Z","relpermalink":"/blog/2015/08/23/speech-analysis-and-synthesis-in-julia/","section":"post","summary":" ","tags":["Speech Signal Processing","Speech Synthesis","Julia"],"title":"最近の音声信号処理遊びの進捗","type":"post"},{"authors":null,"categories":null,"content":"JuliaTokyo #3でLT発表してきました。前回のJuliaTokyo #2でも発表したので、二回目でした。\nスライド JuliaTokyo #3 Speech Signal Processing in Julia from Ryuichi YAMAMOTO コード https://github.com/r9y9/JuliaTokyo3\n三行まとめ 発表の内容を三行でまとめると、\n音声ファイルの読み込み（or 書き込み）は[WAV.jl]((https://github.com/dancasimiro/WAV.jl)を使おう 基本的なデジタル信号処理は JuliaDSP/DSP.jl をチェック（※JuliaDSPにはウェーブレットとかもあるよ） 音声に特化した信号処理は、r9y9/WORLD.jl がオススメです という感じです。\n応用例として、歌声を分離する話（デモコード）、統計的声質変換（統計的声質変換クッソムズすぎワロタ（チュートリアル編） - LESS IS MORE）、画像をスペクトログラムに足しこむ話とか、さっと紹介しました。\n補足 僕が使う/作ったパッケージを、あとで見返せるように最後のスライドにまとめておいたのですが、改めてここで整理しておきます。\ndancasimiro/WAV WAVファイルの読み込み JuliaDSP/DSP 窓関数、スペクトログラム、デジタルフィルタ r9y9/WORLD 音声分析・合成フレームワーク r9y9/MelGeneralizedCepstrums メル一般化ケプストラム分析 r9y9/SynthesisFilters メル一般化ケプストラムからの波形合成 r9y9/SPTK 音声信号処理ツールキット r9y9/RobustPCA ロバスト主成分分析(歌声分離へ応用) r9y9/REAPER 基本周波数推定 r9y9/VoiceConversion 統計的声質変換 上から順に、汎用的かなーと思います1。僕が書いたパッケージの中では、WORLDのみ公式パッケージにしています。理由は単純で、その他のパッケージはあまりユーザがいないだろうなーと思ったからです。かなりマニアックであったり、今後の方針が決まってなかったり（ごめんなさい）、応用的過ぎて全然汎用的でなかったり。WORLDは自信を持ってオススメできますので、Juliaで音声信号処理をやってみようかなと思った方は、ぜひお試しください。\nざっくり感想 ＃Juliaわからん 本当に素晴らしいと思うので、僕も積極的に #Juliaわからん とつぶやいていこうと思います（詳しくは @chezou さんの記事をどうぞ #JuliaTokyo で #juliaわからん という雑なレポジトリを立てた話をしたら julia.tokyo ができてた - once upon a time,）。僕は、Julia に Theano が欲しいです。T.grad 強力すぎる ccall かんたんとか言いましたが、ミスった書き方をしたときのエラーメッセージはあまり親切ではないので、つまずきやすいかも。僕は気合で何とかしています。 Julia遅いんだけど？？？と言われたら、@bicycle1885 さんの What’s wrong with this Julia? を投げつけようと思います。 かなり聴衆が限定的になってしまう話をしてしまったので、次発表するならJulia 言語自体の話をしようかなと思いました 最後に @soramiさんを筆頭とする運営の方々、本当にありがとうございました！楽しかったです。\nとスライドに書いたけど、考えなおすと、僕が思う品質の高さ順、の方が正確です、失礼しました。MelGeneneralizedCepstrumsは一番気合入れて書いたけど、ユーザーがいるかといったらいないし、RobustPCAはさっと書いただけだけど、アルゴリズムとしては汎用的だし。またRobustPCAだけ毛色が違いますが、応用例で紹介したのでリストに入れておきました。 ↩︎\n","date":1430006400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1430006400,"objectID":"c64f4c4cd26d9d7f853ed299f5cf1a83","permalink":"https://r9y9.github.io/blog/2015/04/26/juliatokyo3-speech-signal-processing-in-julia/","publishdate":"2015-04-26T00:00:00Z","relpermalink":"/blog/2015/04/26/juliatokyo3-speech-signal-processing-in-julia/","section":"post","summary":"[JuliaTokyo #3](https://juliatokyo.connpass.com/event/13218/)","tags":["Julia","Speech Signal Processing","Machine Learning","Presentation","JuliaTokyo"],"title":"JuliaTokyo #3 Speech Signal Processing in Julia","type":"post"},{"authors":null,"categories":null,"content":"Julia Advent Calendar 2014 9日目の記事です。\nはじめに CやFortranの関数をJuliaから呼ぶために使用するccallにおいて、ポインタに関係するハマりどころとその解決法を紹介します。純粋なJuliaを使っている場合にはポインタを意識することはめったにないと思うので、ccall を使う人（計算が重いボトルネック部分をCで書いてJuliaから呼びたい人、Cのライブラリのラッパーを書きたい/書いてる人）を主な読者と想定して記事を書きました（限定的でごめんなさい）。\n困った時は、公式ドキュメントの Calling C and Fortran Code を参考にしましょう。\n注意: 最新版の公式ドキュメントをいくつか引用していますが、ドキュメントは日々更新されていますので、この記事を読んで頂いた時点とは異なる可能性があることにご注意ください。\nこんなとき ccall を使う際に、ポインタに関する以下のような疑問を持つことがあります。\nポインタを引数に持つ（例. double*）関数のラッピングはどうすればいいのか？ 構造体のポインタを引数に持つ関数のラッピングはどうすれば？ ポインタのポインタを引数に持つ（例. double**）関数のラッピングは？ 一つ目は非常に簡単で、Array（Cの関数がdouble*を取るならばArray{Float64,1}）をそのまま渡せばよいだけです。ドキュメントのArray Conversionsにも書かれています。が、残りの二つに関してはハマりどころがあります。順に説明します。\n構造体のポインタを引数に持つ関数のラッピングはどうすれば？ 現状のドキュメントは少し不親切なので、引用した上で、整理します。\nCalling C and Fortran Code より引用:\nCurrently, it is not possible to pass structs and other non-primitive types from Julia to C libraries. However, C functions that generate and use opaque struct types by passing pointers to them can return such values to Julia as Ptr{Void}, which can then be passed to other C functions as Ptr{Void}. Memory allocation and deallocation of such objects must be handled by calls to the appropriate cleanup routines in the libraries being used, just like in any C program.\n冒頭に it is not possible とあります。が、決して不可能なわけではありません。上記文章の要点をまとめると、\nopaqueな構造体はCからJuliaへポインタとして渡すことができる そのポインタは Ptr{Void} としてCの関数に渡すことができる と書かれています。つまり、一般には構造体は渡せないけどポインタ渡しはできるよ、ということです。\nじゃあnon-opaqueな構造体についてはどうなんだ？Juliaの型を渡せないのか？という疑問が出てきます。結論からいえば、non-opaqueな構造体についてもポインタ渡しは可能です。つまり、Cの構造体に相当するimmutableな型1をjuliaで宣言してあげれば、juliaの型をCに渡すことが可能です（値渡しはできません）\n例を示します。\nCコード typedef struct { double a; int b; } Foo; # 構造体のポインタを引数にとる関数1 void print(Foo* foo) { printf(\u0026#34;a=%lf\\n\u0026#34;, foo-\u0026gt;a); printf(\u0026#34;b=%d\\n\u0026#34;, foo-\u0026gt;b); } # 構造体のポインタを引数にとる関数2 void reset(Foo* foo) { foo-\u0026gt;a = 0.0; foo-\u0026gt;b = 0; } Juliaコード # Cの構造体 Foo に相当する型を宣言します immutable Foo a::Float64 b::Int32 # cのintはjuliaのInt32に対応します end foo = Foo(10.0, 2) # Cの関数に、ポインタとしてJuliaの型を渡すことができます ccall(:print, \u0026#34;libfoo\u0026#34;, Void, (Ptr{Foo},), \u0026amp;foo) # ポインタで渡す場合、Cで変更した結果はJuliaにも反映されます ccall(:reset, \u0026#34;libfoo\u0026#34;, Void, (Ptr{Foo},), \u0026amp;foo) # foo(0.0, 0) と表示される println(foo) ちなみにJuliaからCへ値渡しをしてもエラーにならないので、お気をつけください（ハマりました）。\n公式ドキュメントは不親切と言いましたが、 プルリクエスト update documentation for passing struct pointers to C #8948（まだマージはされていない）で改善されているので、もしかするとこの記事が読まれる頃には改善されているかもしれません。\nまた、値渡しを可能にしようとする動きもあります（RFC: Make struct passing work properly #3466, WIP: types as C-structs #2818 マージ待ち）。\n構造体渡しのまとめ Cの構造体に相当するJuliaの型を定義して、ポインタで渡せばOK 値渡しは現状できない ポインタを受けることはできる（Ptr{Void}として受ける） ポインタのポインタを引数に持つ（例. double**）関数のラッピングは？ さて、これはドキュメントにまったく書かれておらず、かつハマりやすいと僕は思っています。例を交えつつ解説します。以下のような関数のラッピングを考えます。\nvoid fooo(double** input, int w, int h, double** output); inputは入力の行列、outputは計算結果が格納される行列、行列のサイズは共に 列数w、行数h だと思ってください。Juliaからは input::Array{Float64,2} を入力として、output::Array{Float64,2} を得たいとします。\ndouble*を引数にとる場合はArray{Float64,1}を渡せばよかったのに対して、double**を引数に取る関数に Array{Float64,2}やArray{Array{Float64,1},1}を単純に渡すだけでは、残念ながらコンパイルエラーになります。はい、すでに若干面倒ですね。。さて、どうすればいいかですが、\nどんな型で渡せばいいか どのように型を変換するか 変換した型をどのように元に戻すか という三点に分けて説明します。\n1. どんな型で渡せばいいか Array{Ptr{Float64}} で渡せばよいです。外側のArrayは、ccall がポインタに変換してくれるので、Juliaの型でいえばPtr{Ptr{Float64}}、Cの型で言えばdouble**になるわけです。\n2. どのように型を変換するか ここがハマりどころです。今回の例では、Array{Float64,2} を Array{Ptr{Float64},1} に変換すればよいので、例えば以下のような実装が思いつきます。\n# Array{T,2} -\u0026gt; Array{Ptr{T}} function ptrarray2d{T\u0026lt;:Real}(src::Array{T,2}) dst = Array{Ptr{T}, size(src, 2)) for i=1:size(src, 2) dst[i] = pointer(src[:,i], 1) # 先頭要素のポインタを取り出す end dst end 実はこの実装はバグを含んでいます。バグがあるとしたら一行しか該当する部分はないですが、\ndst[i] = pointer(src[:,i], 1) ここが間違っています。何が間違っているかというと、pointer(src[:,i], 1)は一見srcのi列目の先頭要素のポインタを指しているような気がしますが、src[:,1]で getindexという関数が走って内部データのコピーを行っているので、そのコピーに対するポインタを指している（元データのi列目のポインタを指していない）点が間違っています2。これは、JuliaのArray実装ついて多少知らないとわからないと思うので、ハマりどころと書きました。\nArray Aに対する syntax X = A[I_1, I_2, ..., I_n] は X = getindex(A, I_1, I_2, ..., I_n) と等価です。詳細は、Multi-dimensional Arraysや標準ライブラリのドキュメント を参考にしてください\nさて、正解を示します。\n# Array{T,2} -\u0026gt; Array{Ptr{T}} function ptrarray2d{T\u0026lt;:Real}(src::Array{T,2}) dst = Array{Ptr{T}, size(src, 2)) for i=1:size(src, 2) dst[i] = pointer(sub(src, 1:size(src,1), i), 1) end dst end 違いは SubArrayを使うようになった点です。SubArrayは、indexingを行うときにコピーを作らないので、期待した通りにi列目の先頭要素のポインタを取得することができます。SubArrayについて、以下引用しておきます3。\nSubArray is a specialization of AbstractArray that performs indexing by reference rather than by copying. A SubArray is created with the sub() function, which is called the same way as getindex() (with an array and a series of index arguments). The result of sub() looks the same as the result of getindex(), except the data is left in place. sub() stores the input index vectors in a SubArray object, which can later be used to index the original array indirectly.\n引用元: Multi-dimensional Arrays\n3. 変換した型をどのように元に戻すか Juliaで計算結果（上の例でいう double** output）を受け取りたい場合、ポインタに変換した値をJuliaのArrayに戻す必要があります（必ずしもそうではないですが、まぁほぼそうでしょう）。つまり、Array(Ptr{Float64},1)をArray{Float64,2}したいわけです。幸いにも、これはpointer_to_arrayを使うと簡単にできます。コードを以下に示します。\n# ccall …","date":1418083200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1418083200,"objectID":"bba5e5885ad2f9c713ab10763b68e92f","permalink":"https://r9y9.github.io/blog/2014/12/09/julia-advent-calender-2014-poiner-tips/","publishdate":"2014-12-09T00:00:00Z","relpermalink":"/blog/2014/12/09/julia-advent-calender-2014-poiner-tips/","section":"post","summary":"[Julia Advent Calendar 2014](https://qiita.com/advent-calendar/2014/julialang)","tags":["Julia","Advent Calendar"],"title":"ccallにおけるポインタ周りのハマりどころとその解決法","type":"post"},{"authors":null,"categories":null,"content":"はじめに こんばんは。統計的声質変換（以降、簡単に声質変換と書きます）って面白いなーと思っているのですが、興味を持つ人が増えたらいいなと思い、今回は簡単なチュートリアルを書いてみます。間違っている箇所があれば、指摘してもらえると助かります。よろしくどうぞ。\n前回の記事（統計的声質変換クッソムズすぎワロタ（実装の話） - LESS IS MORE）では変換部分のコードのみを貼りましたが、今回はすべてのコードを公開します。なので、記事内で示す声質変換の結果を、この記事を読んでいる方が再現することも可能です。対象読者は、特に初学者の方で、声質変換を始めたいけれど論文からコードに落とすにはハードルが高いし、コードを動かしながら仕組みを理解していきたい、という方を想定しています。役に立てば幸いです。\nコード https://github.com/r9y9/VoiceConversion.jl\nJulia という言語で書かれています。Juliaがどんな言語かをさっと知るのには、以下のスライドがお勧めです。人それぞれ好きな言語で書けばいいと思いますが、個人的にJuliaで書くことになった経緯は、最後の方に簡単にまとめました。\nプログラミング言語 Julia の紹介 from Kentaro Iizuka サードパーティライブラリ 声質変換は多くのコンポーネントによって成り立っていますが、すべてを自分で書くのは現実的ではありません。僕は、主に以下のライブラリを活用しています。\nWORLD - 音声分析合成のフレームワークとして、あるいは単にスペクトル包絡を抽出するツールとして使っています。Juliaラッパーを書きました。 SPTK - メル対数スペクトル近似（Mel-Log Spectrum Approximation; MLSA）フィルタを変換処理に使っています。これもJuliaラッパーを書きました。 sklearn - sklearn.mixture をGMMの学習に使っています。pythonのライブラリは、juliaから簡単に呼べます。 音声分析合成に関しては、アカデミック界隈ではよく使われているSTRAIGHTがありますが、WORLDの方がライセンスもゆるくソースも公開されていて、かつ性能も劣らない（正確な話は、森勢先生の論文を参照してください）ので、おすすめです。\nVoiceConversion.jl でできること 追記 2015/01/07 この記事を書いた段階のv0.0.1は、依存ライブラリの変更のため、現在は動きません。すみません。何のためのタグだ、という気がしてきますが、、最低限masterは動作するようにしますので、そちらをお試しください（基本的には、新しいコードの方が改善されています）。それでも動かないときは、issueを投げてください。\n2014/11/10現在（v0.0.1のタグを付けました）、できることは以下の通りです（外部ライブラリを叩いているものを含む）。\n音声波形からのメルケプストラムの抽出 DPマッチングによるパラレルデータの作成 GMMの学習 GMMベースのframe-by-frame特徴量変換 GMMベースのtrajectory特徴量変換 GMMベースのtrajectory特徴量変換（GV考慮版） 音声分析合成系WORLDを使った声質変換 MLSAフィルタを使った差分スペクトルに基づく声質変換 これらのうち、trajectory変換以外を紹介します。\nチュートリアル：CMU_ARCTICを使ったGMMベースの声質変換（特徴抽出からパラレルデータの作成、GMMの学習、変換・合成処理まで） データセットにCMU_ARCTICを使って、GMMベースの声質変換（clb -\u0026gt; slt）を行う方法を説明します。なお、VoiceConversion.jl のv0.0.1を使います。ubuntuで主に動作確認をしていますが、macでも動くと思います。\n0. 前準備 0.1. データセットのダウンロード Festvox: CMU_ARCTIC Databases を使います。コマンド一発ですべてダウンロードするスクリプトを書いたので、ご自由にどうぞ。\n0.2. juliaのインストール 公式サイトからバイナリをダウンロードするか、githubのリポジトリをクローンしてビルドしてください。バージョンは、現在の最新安定版のv0.3.2を使います。\n記事内では、juliaの基本的な使い方については解説しないので、前もってある程度調べておいてもらえると、スムーズに読み進められるかと思います。\n0.3. VoiceConversion.jl のインストール juliaを起動して、以下のコマンドを実行してください。\njulia\u0026gt; Pkg.clone(\u0026#34;https://github.com/r9y9/VoiceConversion.jl\u0026#34;) julia\u0026gt; Pkg.build(\u0026#34;VoiceConversion\u0026#34;) サードパーティライブラリは、sklearnを除いてすべて自動でインストールされます。sklearnは、例えば以下のようにしてインストールしておいてください。\nsudo pip install sklearn これで準備は完了です！\n1. 音声波形からのメルケプストラムの抽出 まずは、音声から声質変換に用いる特徴量を抽出します。特徴量としては、声質変換や音声合成の分野で広く使われているメルケプストラムを使います。メルケプストラムの抽出は、scripts/mcep.jl を使うことでできます。\n2014/11/15 追記 実行前に、julia\u0026gt; Pkg.add(\u0026#34;WAV\u0026#34;) として、WAVパッケージをインストールしておいてください。(2014/11/15時点のmasterでは自動でインストールされますが、v0.0.1ではインストールされません、すいません）。また、メルケプストラムの出力先ディレクトリは事前に作成しておいてください（最新のスクリプトでは自動で作成されます）。\n以下のようにして、2話者分の特徴量を抽出しましょう。以下のスクリプトでは、 ~/data/cmu_arctic/ にデータがあることを前提としています。\n# clb julia mcep.jl ~/data/cmu_arctic/cmu_us_clb_arctic/wav/ ~/data/cmu_arctic_jld/speakers/clb/ # slt julia mcep.jl ~/data/cmu_arctic/cmu_us_slt_arctic/wav/ ~/data/cmu_arctic_jld/speakers/slt/ 基本的な使い方は、mcep.jl \u0026lt;wavファイルがあるディレクトリ\u0026gt; \u0026lt;メルケプストラムが出力されるディレクトリ\u0026gt; になっています。オプションについては、 mcep.jl -h としてヘルプを見るか、コードを直接見てください。\n抽出されたメルケプストラムは、HDF5フォーマットで保存されます。メルケプストラムの中身を見てみると、以下のような感じです。可視化には、PyPlotパッケージが必要です。Juliaを開いて、julia\u0026gt; Pkg.add(\u0026#34;PyPlot\u0026#34;) とすればOKです。IJuliaを使いたい場合（僕は使っています）は、julia\u0026gt; Pkg.add(\u0026#34;IJulia\u0026#34;) としてIJuliaもインストールしておきましょう。\n# メルケプストラムの可視化 using HDF5, JLD, PyPlot x = load(\u0026#34;clb/arctic_a0028.jld\u0026#34;) figure(figsize=(16, 6), dpi=80, facecolor=\u0026#34;w\u0026#34;, edgecolor=\u0026#34;k\u0026#34;) imshow(x[\u0026#34;feature_matrix\u0026#34;], origin=\u0026#34;lower\u0026#34;, aspect=\u0026#34;auto\u0026#34;) colorbar() ","date":1415750400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1415750400,"objectID":"c07f1742771ea7c712182a11daad6965","permalink":"https://r9y9.github.io/blog/2014/11/12/statistical-voice-conversion-code/","publishdate":"2014-11-12T00:00:00Z","relpermalink":"/blog/2014/11/12/statistical-voice-conversion-code/","section":"post","summary":"https://github.com/r9y9/VoiceConversion.jl","tags":["Voice Conversion","Machine-Learning","Speech Signal Processing","Tutorial"],"title":"統計的声質変換クッソムズすぎワロタ（チュートリアル編）","type":"post"},{"authors":null,"categories":null,"content":"ずーーっと前に、 NMFアルゴリズムの導出（ユークリッド距離版） - LESS IS MORE で実際に実装してみてやってみると書いていたのに、まったくやっていなかったことに気づいたのでやりました。\n音楽に対してやってみたのですが、簡単な曲だったら、まぁぼちぼち期待通りに動いたかなぁという印象です。コードとノートを挙げたので、興味のある方はどうぞ。\nGithub https://github.com/r9y9/julia-nmf-ss-toy\nノート NMF-based Music Source Separation Demo.ipynb | nbviewer\nNMFのコード (Julia) NMFの実装の部分だけ抜き出しておきます。\nfunction nmf_euc(Y::AbstractMatrix, K::Int=4; maxiter::Int=100) H = rand(size(Y, 1), K) U = rand(K, size(Y, 2)) const ϵ = 1.0e-21 for i=1:maxiter H = H .* (Y*U\u0026#39;) ./ (H*U*U\u0026#39; + ϵ) U = U .* (H\u0026#39;*Y) ./ (H\u0026#39;*H*U + ϵ) U = U ./ maximum(U) end return H, U end いやー簡単ですねー。NMFアルゴリズムの導出（ユークリッド距離版） - LESS IS MORE で導出した更新式ほぼそのままになってます（異なる点としては、ゼロ除算回避をしているのと、Uをイテレーション毎に正規化していることくらい）。\nB3, B4くらいの人にとっては参考になるかもしれないと思ってあげてみた。\nおわり\n","date":1413676800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1413676800,"objectID":"76480dfd2538bc0cee0b8335361c2221","permalink":"https://r9y9.github.io/blog/2014/10/19/nmf-music-source-separation/","publishdate":"2014-10-19T00:00:00Z","relpermalink":"/blog/2014/10/19/nmf-music-source-separation/","section":"post","summary":"https://github.com/r9y9/julia-nmf-ss-toy","tags":["Machine Learning","Matrix Factorization"],"title":"NMFで音源分離を試してみる","type":"post"},{"authors":null,"categories":null,"content":"JuliaTokyo #2 - connpass\n発表概要 C/C++ライブラリのラッパー（C++は現状のJuliaでは難しいけど）を作るときに、どうやってライブラリの依存関係を管理するか？という話です。結論としては、方法はいくつかありますが　BinDeps.jl というパッケージを使うのが楽で良いですよ、ということです。Githubのいろんなリポジトリをあさった僕の経験上、BinDeps.jl はバイナリの依存関係管理におけるデファクトスタンダードな気がしています。BinDeps.jl の使い方は、既存のパッケージのコードを読みまくって学ぶのがおすすめです。\nさて、途中で書くのに疲れてしまったのですが、自作のJuliaパッケージで、Cライブラリとの依存性を記述する - Qiita に以前似たような内容をまとめたので、併せてどうぞ。qiitaにも書きましたが、最適化関係のプロジェクトを集めた JuliaOpt コミュニティでは、バイナリの依存関係管理にBinDeps.jlを使用することを推奨しています。\n雑感 勉強会にはデータ分析界隈の人が多い印象。音声系の人はとても少なかった。 R人気だった Go使ってる！って人と合わなかった（つらい） @show マクロ最高 unicode最高 懇親会では、なぜか途中から深層学習やベイズの話をしていた… いい忘れたけど僕もnightly build勢でした。毎日あたたかみのある手動pull \u0026amp; make をしています。 Julia の話ができて楽しかったので、また参加したいなー LTで MeCab.jl について話をしてくれたchezouさんが、ちょうどBinDeps.jl に興味を持たれているようだったので、勉強会のあとに BinDeps.jl を使ってバイナリの管理を実装して、プルリクをしてみました。参考になればうれしいなーと思います。\nおしまい。\n","date":1412035200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1412035200,"objectID":"9ffc3cc20c33b0678d18aa3305d36ea6","permalink":"https://r9y9.github.io/blog/2014/09/30/juliatokyo2/","publishdate":"2014-09-30T00:00:00Z","relpermalink":"/blog/2014/09/30/juliatokyo2/","section":"post","summary":"[JuliaTokyo #2 - connpass](juliatokyo.connpass.com/event/8010/)","tags":["Julia","JuliaTokyo","BinDeps","Presentation","JuliaTokyo"],"title":"JuliaTokyo #2でBinDeps.jl についてLTしてきた","type":"post"},{"authors":null,"categories":null,"content":"","date":1410739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1410739200,"objectID":"0cc0bbcc05d0d2c50d5d762f74fcf50b","permalink":"https://r9y9.github.io/blog/2014/09/15/sptk-for-julia/","publishdate":"2014-09-15T00:00:00Z","relpermalink":"/blog/2014/09/15/sptk-for-julia/","section":"post","summary":"https://github.com/r9y9/SPTK.jl","tags":["Speech Signal Processing","Julia","SPTK"],"title":"SPTKのJuliaラッパーも書いた","type":"post"},{"authors":null,"categories":null,"content":"最近 Julia で遊んでいて、その過程で非負値行列因子分解（NMF）のノンパラ版の一つであるGamma Process Non-negative Matrix Factorization (GaP-NMF) を書いてみました。（まぁmatlabコードの写経なんですが）\nhttps://github.com/r9y9/BNMF.jl\n元論文: Bayesian Nonparametric Matrix Factorization for Recorded Music by Matthew D. Hoffman et al. in ICML 2010.\nデモ http://nbviewer.ipython.org/github/r9y9/BNMF.jl/blob/master/notebook/GaP-NMF.ipynb\n適当な音声（音楽じゃなくてごめんなさい）に対して、GaP-NMFをfittingしてみた結果のメモです。$K=100$ で始めて、100回ほどイテレーションを回すと適度な数（12くらい）にtruncateしているのがわかると思います。予めモデルの複雑度を指定しなくても、データから適当な数を自動決定してくれる、ノンパラベイズの良いところですね。\nハマったところ GIGの期待値を求めるのに必要な第二種変形ベッセル関数は、exponentially scaled versionを使いましょう。じゃないとInf地獄を見ることになると思います（つらい）。Juliaで言うなら besselkx で、scipyで言うなら scipy.special.kve です。 雑感 MatlabのコードをJuliaに書き直すのは簡単。ところどころ作法が違うけど（例えば配列の要素へのアクセスはmatlabはA(i,j)でJuliaはA[i,j]）、だいたい一緒 というかJuliaがMatlabに似すぎ？ Gamma分布に従う乱数は、Distributions,jl を使えばめっちゃ簡単に生成できた。素晴らしすぎる 行列演算がシンプルにかけてホント楽。pythonでもmatlabでもそうだけど（Goだとこれができないんですよ…） 第二種変形ベッセル関数とか、scipy.special にあるような特殊関数が標準である。素晴らしい。 Python版と速度比較 bp_nmf/code/gap_nmf と比較します。matlabはもってないので比較対象からはずします、ごめんなさい\nGistにベンチマークに使ったスクリプトと実行結果のメモを置いときました https://gist.github.com/r9y9/3d0c6a90dd155801c4c1\n結果だけ書いておくと、あらゆる現実を（ry の音声にGaP-NMFをepochs=100でfittingするのにかかった時間は、\nJulia: Mean elapsed time: 21.92968243 [sec] Python: Mean elapsed time: 18.3550617 [sec] という結果になりました。つまりJuliaのほうが1.2倍くらい遅かった（僕の実装が悪い可能性は十分ありますが）。どこがボトルネックになっているのか調べていないので、気が向いたら調べます。Juliaの方が速くなったらいいなー\nおわりに GaP-NMFの実装チャレンジは二回目でした。（たぶん）一昨年、年末に実家に帰るときに、何を思ったのか急に実装したくなって、電車の中で論文を読んで家に着くなり実装するというエクストリームわけわからんことをしていましたが、その時はNaN and Inf地獄に負けてしまいました。Pythonで書いていましたが、今見るとそのコードマジクソでした。\nそして二回目である今回、最初はmatlabコードを見ずに自力で書いていたんですが、またもやInf地獄に合いもうだめだと思って、matlabコードを写経しました。あんま成長していないようです（つらい）\nJulia歴二週間くらいですが、良い感じなので使い続けて見ようと思います。\n","date":1408492800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1408492800,"objectID":"0632534e92dc793cfc9500cbc16a67be","permalink":"https://r9y9.github.io/blog/2014/08/20/gap-nmf-julia/","publishdate":"2014-08-20T00:00:00Z","relpermalink":"/blog/2014/08/20/gap-nmf-julia/","section":"post","summary":"https://github.com/r9y9/BNMF.jl","tags":["Julia","Machine Learning","Non-parametric Bayes","Matrix Factorization"],"title":"Gamma Process Non-negative Matrix Factorization (GaP-NMF) in Julia","type":"post"},{"authors":null,"categories":null,"content":"2015/09/06 追記 ましなpythonラッパーを新しく作りました: Pysptk: SPTKのpythonラッパーを作った (Part 2)\n2014/08/10 追記 ipython notebookによる簡単なチュートリアルを貼っておきます\nSPTK を Pythonから呼ぶ | nbviewer\n2014/11/09 タイポ修正しました…\nscipy.mixture -\u0026gt; sklearn.mixture\nSPTKの中で最も価値がある（と僕が思っている）メルケプストラム分析、メルケプストラムからの波形合成（MLSA filter）がpythonから可能になります。\nご自由にどうぞ\nSpeech Signal Processing Toolkit (SPTK) for API use with python | Github\n注意ですが、SPTK.hにある関数を全部ラップしているわけではないです。僕が必要なものしか、現状はラップしていません（例えば、GMMとかラップする必要ないですよね？sklearn.mixture使えばいいし）。ただ、大方有用なものはラップしたと思います。\n参考 Goで音声信号処理をしたいのでSPTKのGoラッパーを書く - LESS IS MORE Goでも書いたのにPythonでも書いてしまった。\n一年くらい前に元指導教員の先生と「Pythonから使えたらいいですよね」と話をしていました。先生、ようやく書きました。\n","date":1407628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1407628800,"objectID":"9c5a61620cd25a90c5cc9b417394d574","permalink":"https://r9y9.github.io/blog/2014/08/10/sptk-from-python/","publishdate":"2014-08-10T00:00:00Z","relpermalink":"/blog/2014/08/10/sptk-from-python/","section":"post","summary":"https://github.com/r9y9/pysptk","tags":["Speech Signal Processing","Python","SPTK"],"title":"SPTKのPythonラッパーを書いた","type":"post"},{"authors":null,"categories":null,"content":"Tokyo.SciPy ハッシュタグ: #tokyoscipy\nTokyo.Scipy は科学技術計算で Python を利用するための勉強会です．\nとのことです。最近、python/numpy/scipyによくお世話になっているので、参加してみました。雑感をメモしておきます。\nTokyo.Scipy 006 第6回のようでした。プログラムだけさっとまとめると、\nそこそこ大規模Python並列/パイプライン処理入門 w/o MapReduceレジーム (柏野雄太 @yutakashino) 45分 初心者が陥るN個の罠。いざ進めNumpy/Scipyの道 (@nezuq) 15分 Making computations reproducible (@fuzzysphere) 30分 IPython Notebookで始めるデータ分析と可視化 (杜世橋 @lucidfrontier45) 30分 PyMCがあれば，ベイズ推定でもう泣いたりなんかしない (神嶌敏弘 @shima__shima) 45分 という感じ。僕的には、@shima__shima 先生の発表が目当てだった\n雑感 今回（？）はscipyの話はほとんどなかった。pythonを使った科学技術計算に関する幅広いトピックを扱ってる印象。 ipython はやっぱ便利ですね。僕も良く使います @shima__shima 先生の発表がとてもわかりやすかったので、本当に参考にしたい 正直もっとコアな話もあっていいのでは、と思った 懇親会で気づいたが、意外と音声信号処理やってる（た）人がいてびっくりした scikit-learn を初期の頃に作られてた方 @cournape がいてびっくり。開発当初はGMMとSVMくらいしかなくて全然ユーザーがつかなかったなどなど、裏話を色々聞けた フランス人の「たぶん大丈夫」は絶対無理の意（わろた Rust, juliaがいいと教えてもらった。うちjuliaは今やってみてるがなかなかいい 発表でも話題に上がったけど、Pandasがいいという話を聞いたので、試してみたい 運営の方々、発表された方々、ありがとうございました。僕も機会が合えば何か発表したい\n","date":1407196800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1407196800,"objectID":"8463d9b6353d9a375c03154027836f0d","permalink":"https://r9y9.github.io/blog/2014/08/05/tokyo-scipy/","publishdate":"2014-08-05T00:00:00Z","relpermalink":"/blog/2014/08/05/tokyo-scipy/","section":"post","summary":"[Tokyo.SciPy](https://github.com/tokyo-scipy/archive)","tags":["Python","SciPy"],"title":"Tokyo.Scipyに参加してきた","type":"post"},{"authors":null,"categories":null,"content":"いまいち成果出ないので気分転換にブログをだらだら書いてみるテストです。\nまえがき 半年くらい前に、某深層学習に興味を持ってやってみようかなーと思っていた時期があって、その時にGoでいくつかニューラルネットを書きました（参考：Restricted Boltzmann Machines with MNIST - LESS IS MORE、githubに上げたコード）。なぜGoだったかというと、僕がGoに興味を持ち始めていたからというのが大きいです。Goを知る前は、たくさん計算するようなコードを書くときはC++だったけれど、C++は色々つらいものがあるし、GoはC++には速度面で劣るもののそこそこ速く、かつスクリプト的な書きやすさもあります。C++のデバッグやメンテに費やす膨大な時間に比べれば、計算時間が1.5~2倍に増えるくらい気にしないというスタンスで、僕はC++のかわりGoを使おうとしていました（※今でも間違っているとは思いませんが、とはいえ、厳しいパフォーマンスを求められる場合や既存の資産を有効活用したい場合など、必要な場面ではC++を書いています）。\nGoで機械学習 僕は機械学習がけっこう好きなので、Goでコード書くかーと思っていたのですが、結果としてまったく捗りませんでした。ニューラルネットをてきとーに書いたくらいです。\n検索するとわかりますが、現状、他の主流な言語に比べて圧倒的に数値計算のライブラリが少ないです。特に、線形代数、行列演算のデファクト的なライブラリがないのはつらいです。いくつか代表的なものをあげます。\nskelterjohn/go.matrix - もうまったくメンテされていないし、たぶんするつもりはないと思います。使い勝手は、僕にとってはそんなに悪くなかった（試しにNMFを書いてみた）ですが、実装は純粋なGoで書かれていて、GPUを使って計算するのが流行りな時代では、例えば大きなニューラルネットをパラメータを変えながら何度も学習するのにはしんどいと思いました。 gonum/matrix - 比較的最近出てきたライブラリで、biogo から行列演算に関する部分を切り出して作られたもののようです。行列演算の内部でblasを使っていて、かつ将来的にはcublasにも対応したい、みたいな投稿をGoogle Groupsで見たのもあって、半年くらい前にはgoで行列演算を行うならこのライブラリを使うべきだと判断しました（以前けっこう調べました：gonum/matrix のデザインコンセプトに関するメモ - Qiita）。しかし、それほど頻繁にアップデートされていませんし、機能もまだ少ないです。 自分で作るかー、という考えも生まれなかったことはないですが、端的に言えばそれを行うだけのやる気がありませんでした。まぁ本当に必要だったら多少難しくてもやるのですが、ほら、僕達にはpythonがあるじゃないですか…\nPythonで機械学習 python 機械学習 - Google 検索 約 119,000 件（2014/07/29現在）\nもうみんなやってますよね。\nGolang 機械学習 - Google 検索 約 9,130 件（2014/07/29現在）\nいつかpythonのように増えるんでしょうか。正直に言って、わかりません（正確には、あんま考えていませんごめんなさい）\nさて、僕もよくpython使います。機械学習のコードを書くときは、だいたいpythonを使うようになりました（昔はC++で書いていました）。なぜかって、numpy, scipyのおかげで、とても簡潔に、かつ上手く書けばそこそこ速く書けるからです。加えて、ライブラリがとても豊富なんですよね、機械学習にかかわらず。numpy, scipyに加えて、matplotlibという優秀な描画ライブラリがあるのが、僕がpythonを使う大きな理由になっています。\npythonの機械学習ライブラリは、scikit-learn が特に有名でしょうか。僕もちょいちょい使います。使っていて最近おどろいたのは、scipy.mixtureには通常のGMMだけでなく変分GMM、無限混合GMMも入っていることですよね。自分で実装しようとしたら、たぶんとても大変です。昔変分GMMの更新式を導出したことがありますが、何度も心が折れそうになりました。いやー、いい時代になったもんですよ…（遠い目\nPythonでニューラルネット（pylearn2を使おう） Deep何とかを含め流行りのニューラルネットが使える機械学習のライブラリでは、僕は pylearn2 がよさ気だなーと思っています。理由は、高速かつ拡張性が高いからです。pylearn2は、数学的な記号表現からGPUコード（GPUがなければCPU向けのコード）を生成するmathコンパイラ Theano で書かれているためpythonでありながら高速で、かつ機械学習に置いて重要なコンポーネントであるデータ、モデル、アルゴリズムが上手く分離されて設計されているのがいいところかなと思います（全部ごっちゃに書いていませんか？僕はそうですごめんなさい。データはともかくモデルと学習を上手く切り分けるの難しい）。A Machine Learning library based on Theanoとのことですが、Deep learningで有名な lisa lab 発ということもあり、ニューラルネットのライブラリという印象が少し強いですね。\n一つ重要なこととして、このライブラリはかなり研究者向けです。ブラックボックスとして使うのではなく、中身を読んで必要に応じて自分で拡張を書きたい人に向いているかと思います。\nIan J. Goodfellow, David Warde-Farley, Pascal Lamblin, Vincent Dumoulin, Mehdi Mirza, Razvan Pascanu, James Bergstra, Frédéric Bastien, and Yoshua Bengio. “Pylearn2: a machine learning research library”. arXiv preprint arXiv:1308.4214\n↑の論文のIntroductionの部分に、その旨は明記されています。と、論文のリンクを貼っておいてなんですが、Ian Goodfellow のホームページにもっと簡潔に書いてありました。以下、引用します。\nI wrote most of Pylearn2, a python library designed to make machine learning research convenient. Its mission is to provide a toolbox of interchangeable parts that provide a lot of flexibility for setting up machine learning experiments, providing enough extensibility that pretty much any research idea is feasible within the context of the library. This is in contrast to other machine learning libraries such as scikits-learn that are designed to be black boxes that just work. Think of pylearn2 as user friendly for machine learning researchers and scikits-learn as user friendly for developers that want to apply machine learning.\npylearn2では、Multi-layer Perceptron (MLP)、Deep Bolztmann Machines (DBM)、新しいものでMaxout Network等、手軽に試すことができます（まぁゆうて計算はめっちゃ時間かかるけど）。先述の通りmathコンパイラの Theano を使って実装されているので、GPUがある場合はGPUを使って計算してくれます。環境構築に関しては、今はAWSという便利なサービスがあるので、GPUを持っていなくてもウェブ上でポチポチしてるだけで簡単にGPU環境を構築できます（参考：Pylearn2, theanoをEC2 g2.x2large で動かす方法 - LESS IS MORE）。本当にいい時代になったものですね（二回目\npylearn2、コードやドキュメント、活発なgithubでの開発、議論を見ていて、素晴らしいなーと思いました（まだ使い始めたばかりの僕の意見にあまり信憑性はないのですが…）。僕もこれくらい汎用性、拡張性のあるコードを書きたい人生でした…（自分の書いたニューラルネットのコードを見ながら）\nPylearn2は遅いって？ 本当に速さを求めるなら cuda-convnet2 や Cafee、もしくは直でcudaのAPIをだな…と言いたいところですが、確かにpylearn2は他の深層学習のライブラリに比べて遅いようです。最近、Convolutional Neural Network (CNN) に関するベンチマークがGithubで公開されていました。\nsoumith/convnet-benchmarks\n現時点でまだ work in progressと書いてありますが、参考になると思います。優劣の問題ではなく、必要に応じて使い分ければいいと僕は思っています。\nさてさて、本当はここから僕が書いたGoのニューラルネットのコードがいかにクソかという話を書こうかと思ったのですが、長くなったのでまた今度にします。\nまとめ Goでニューラルネットとか機械学習をやるのは現状しんどいし（golearnとかあるけど、まだまだearly stage）、おとなしくpython使うのが無難 pythonはやっぱり楽。ライブラリ豊富だし。ニューラルネットならpylearn2がおすすめ。ただし自分で拡張まで書きたい人向けです。 散々pythonいいよゆうてますが、どちらかといえば僕はGoの方が好きです。機械学習には現状pythonを使うのがいいんじゃないかなーと思って、Goでニューラルネットを書いていた時を思い出しながらつらつらと書いてみました。\nおわり。\n","date":1406592000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1406592000,"objectID":"68fe0dd2760be25336dab88320c477c6","permalink":"https://r9y9.github.io/blog/2014/07/29/neural-networks-in-go-and-python/","publishdate":"2014-07-29T00:00:00Z","relpermalink":"/blog/2014/07/29/neural-networks-in-go-and-python/","section":"post","summary":"https://github.com/r9y9/nnet","tags":["Go","Python","Machine Learning"],"title":"Goでニューラルネットいくつか書いたけどやっぱPythonが楽でいいですね","type":"post"},{"authors":null,"categories":null,"content":"","date":1405814400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1405814400,"objectID":"ee0d0875963ee503caa89eac2ed459c7","permalink":"https://r9y9.github.io/blog/2014/07/20/pylearn2-on-ec2-g2-2xlarge/","publishdate":"2014-07-20T00:00:00Z","relpermalink":"/blog/2014/07/20/pylearn2-on-ec2-g2-2xlarge/","section":"post","summary":"https://gist.github.com/r9y9/50f13ba28b5b158c25ae","tags":["Machine Learning","GPU","AWS","EC2"],"title":"Pylearn2, theanoをEC2 g2.x2large で動かす方法","type":"post"},{"authors":null,"categories":null,"content":"2014/07/28 追記： 重み行列の構築の部分を改良したのでちょいアップデート。具体的にはdense matrixとして構築してからスパース行列に変換していたのを、はじめからスパース行列として構築するようにして無駄にメモリを使わないようにしました。あとdiffが見やすいようにgistにあげました https://gist.github.com/r9y9/88bda659c97f46f42525\nまえがき 前回、統計的声質変換クッソムズすぎワロタ - LESS IS MORE という記事を書いたら研究者の方々等ちょいちょい反応してくださって嬉しかったです。差分スペクトル補正、その道の人が聴いても音質がいいそう。これはいい情報です。\nTwitter引用:\n統計的声質変換クッソムズすぎワロタ - LESS IS MORE http://t.co/8RkeXIf6Ym @r9y9さんから ムズすぎと言いながら，最後の音はしっかり出ているあたり凄いなぁ．\n— M. Morise (忍者系研究者) (@m_morise) July 5, 2014 @ballforest 従来のパラメータ変換と比較すると、音質は従来よりもよさそうな気はしますがスペクトル包絡の性差ががっつりと影響しそうな気もするんですよね。\n— 縄文人（妖精系研究者なのです） (@dicekicker) July 5, 2014 異性間に関しては、実験が必要ですね。異性間だとF0が結構変わってくると思いますが、差分スペクトル補正の場合そもそもF0をいじらないという前提なので、F0とスペクトル包絡が完全に独立でない（ですよね？）以上、同姓間に比べて音質は劣化する気はします。簡単にやったところ、少なくとも僕の主観的には劣化しました\nところで、結構いい感じにできたぜひゃっはーと思って、先輩に聞かせてみたら違いわかんねと言われて心が折れそうになりました。やはり現実はつらいです。\n実装の話 さて、今回は少し実装のことを書こうと思います。学習\u0026amp;変換部分はPythonで書いています。その他はGo（※Goの話は書きません）。\nトラジェクトリベースのパラメータ変換が遅いのは僕の実装が悪いからでした本当に申し訳ありませんでしたorz 前回トラジェクトリベースは処理が激重だと書きました。なんと、4秒程度の音声（フレームシフト5msで777フレーム）に対して変換部分に600秒ほどかかっていたのですが（重すぎワロタ）、結果から言えばPythonでも12秒くらいまでに高速化されました（混合数64, メルケプの次元数40+デルタ=80次元、分散共分散はfull）。本当にごめんなさい。\n何ヶ月か前、ノリでトラジェクトリベースの変換を実装しようと思ってサクッと書いたのがそのままで、つまりとても効率の悪い実装になっていました。具体的には放置していた問題が二つあって、\nナイーブな逆行列の計算 スパース性の無視 です。特に後者はかなりパフォーマンスに影響していました\nナイーブな逆行列の計算 numpy.linalg.invとnumpy.linalg.solveを用いた逆行列計算 - 睡眠不足？！ (id:sleepy_yoshi)\nnumpy.linalg.invを使っていましたよね。しかもnumpy.linalg.solveのほうが速いことを知っていながら。一ヶ月前の自分を問い詰めたい。numpy.linalg.solveで置き換えたら少し速くなりました。\n600秒 -\u0026gt; 570秒 1.05倍の高速化\nスパース性の無視 T. Toda, A. W. Black, and K. Tokuda, “Voice conversion based on maximum likelihood estimation of spectral parameter trajectory,” IEEE Trans. Audio, Speech, Lang. Process, vol. 15, no. 8, pp. 2222–2235, Nov. 2007. 論文を見ていただければわかるのですが、トラジェクトリベースの変換法における多くの計算は、行列を使って表すことができます。で、論文中の$W$という行列は、サイズがめちゃくちゃでかいのですがほとんどの要素は0です。この性質を使わない理由はないですよね？？\n…残念なことに、僕は密行列として扱って計算していました。ほら、疎行列ってちょっと扱いづらいじゃないですか…めんどくさそう…と思って放置してました。ごめんなさい\npythonで疎行列を扱うなら、scipy.sparseを使えば良さそうです。結果、$W$を疎行列として扱うことで行列演算は大きく高速化されました。\n570秒 -\u0026gt; 12秒くらい 単純に考えると50倍の高速化ですか。本当にアホだった。最初からscipy.sparse使っておけばよかったです。\nscipy.sparseの使い方は以下を参考にしました。みなさんぜひ使いましょう\nPython で疎行列(SciPy) - 唯物是真 @Scaled_Wurm Sparse matrices (scipy.sparse) — SciPy v0.14.0 Reference Guide scipy.sparseで疎行列の行列積 | frontier45 コード メモ的な意味で主要なコードを貼っておきます。 https://gist.github.com/r9y9/88bda659c97f46f42525\n#!/usr/bin/python # coding: utf-8 import numpy as np from numpy import linalg from sklearn.mixture import GMM import scipy.linalg import scipy.sparse import scipy.sparse.linalg class GMMMap: \u0026#34;\u0026#34;\u0026#34;GMM-based frame-by-frame speech parameter mapping. GMMMap represents a class to transform spectral features of a source speaker to that of a target speaker based on Gaussian Mixture Models of source and target joint spectral features. Notation -------- Source speaker\u0026#39;s feature: X = {x_t}, 0 \u0026lt;= t \u0026lt; T Target speaker\u0026#39;s feature: Y = {y_t}, 0 \u0026lt;= t \u0026lt; T where T is the number of time frames. Parameters ---------- gmm : scipy.mixture.GMM Gaussian Mixture Models of source and target joint features swap : bool True: source -\u0026gt; target False target -\u0026gt; source Attributes ---------- num_mixtures : int the number of Gaussian mixtures weights : array, shape (`num_mixtures`) weights for each gaussian src_means : array, shape (`num_mixtures`, `order of spectral feature`) means of GMM for a source speaker tgt_means : array, shape (`num_mixtures`, `order of spectral feature`) means of GMM for a target speaker covarXX : array, shape (`num_mixtures`, `order of spectral feature`, `order of spectral feature`) variance matrix of source speaker\u0026#39;s spectral feature covarXY : array, shape (`num_mixtures`, `order of spectral feature`, `order of spectral feature`) covariance matrix of source and target speaker\u0026#39;s spectral feature covarYX : array, shape (`num_mixtures`, `order of spectral feature`, `order of spectral feature`) covariance matrix of target and source speaker\u0026#39;s spectral feature covarYY : array, shape (`num_mixtures`, `order of spectral feature`, `order of spectral feature`) variance matrix of target speaker\u0026#39;s spectral feature D : array, shape (`num_mixtures`, `order of spectral feature`, `order of spectral feature`) covariance matrices of target static spectral features px : scipy.mixture.GMM Gaussian Mixture Models of source speaker\u0026#39;s features Reference --------- - [Toda 2007] Voice Conversion Based on Maximum Likelihood Estimation of Spectral Parameter Trajectory. http://isw3.naist.jp/~tomoki/Tomoki/Journals/IEEE-Nov-2007_MLVC.pdf \u0026#34;\u0026#34;\u0026#34; def __init__(self, gmm, swap=False): # D is the order of spectral feature for a speaker self.num_mixtures, D = gmm.means_.shape[0], gmm.means_.shape[1]/2 self.weights = gmm.weights_ # Split source and target parameters from joint GMM self.src_means = gmm.means_[:, 0:D] self.tgt_means = gmm.means_[:, D:] self.covarXX = gmm.covars_[:, :D, :D] self.covarXY = gmm.covars_[:, :D, D:] self.covarYX = gmm.covars_[:, D:, :D] self.covarYY = gmm.covars_[:, D:, D:] # swap src and target parameters if swap: self.tgt_means, self.src_means = …","date":1405209600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1405209600,"objectID":"f2c298d7b98546adb9a5fc0a5397a6a0","permalink":"https://r9y9.github.io/blog/2014/07/13/statistical-voice-conversion-wakaran/","publishdate":"2014-07-13T00:00:00Z","relpermalink":"/blog/2014/07/13/statistical-voice-conversion-wakaran/","section":"post","summary":"https://gist.github.com/r9y9/88bda659c97f46f42525","tags":["Voice Conversion","Machine-Learning","Speech Signal Processing"],"title":"統計的声質変換クッソムズすぎワロタ（実装の話）","type":"post"},{"authors":null,"categories":null,"content":"2014/10/12 追記 少なくともGVのコードに致命的なバグがあったことがわかりました。よって、あまりあてにしないでください…（ごめんなさい\nこんにちは。\n最近、統計的声質変換の勉強をしていました。で、メジャーなGMM（混合ガウスモデル）ベースの変換を色々やってみたので、ちょろっと書きます。実は（というほどでもない?）シンプルなGMMベースの方法だと音質クッソ悪くなってしまうんですが、色々試してやっとまともに聞ける音質になったので、試行錯誤の形跡を残しておくとともに、音声サンプルを貼っておきます。ガチ勢の方はゆるりと見守ってください\n基本的に、以下の論文を参考にしています\nT. Toda, A. W. Black, and K. Tokuda, “Voice conversion based on maximum likelihood estimation of spectral parameter trajectory,” IEEE Trans. Audio, Speech, Lang. Process, vol. 15, no. 8, pp. 2222–2235, Nov. 2007. GMMベースの声質変換の基本 シンプルなGMMベースの声質変換は大きく二つのフェーズに分けられます。\n参照話者と目標話者のスペクトル特徴量の結合GMM $P(x,y)$を学習する 入力$x$が与えらたとき、$P(y|x)$が最大となるようにスペクトル特徴量を変換する あらかじめ話者間の関係をデータから学習しておくことで、未知の入力が来た時にも変換が可能になるわけです。\n具体的な変換プロセスとしては、音声を\n基本周波数 非周期性成分 スペクトル包絡 の3つに分解し、スペクトル包絡の部分（≒声質を表す特徴量）に対して変換を行い、最後に波形を再合成するといった方法がよく用いられます。基本周波数や非周期性成分も変換することがありますが、ここではとりあえず扱いません\nシンプルな方法では、フレームごとに独立に変換を行います。\nGMMベースのポイントは、東大の齋藤先生の以下のツイートを引用しておきます。\n@shurabaP GMMベースの声質変換の肝は、入力xが与えられた時の出力yの条件付き確率P(y|x) が最大になるようにyを選ぶという確率的な考えです。私のショボい自作スクリプトですが、HTKを使ったGMMの学習レシピは研究室内部用に作ってあるので、もし必要なら公開しますよ。\n— Daisuke Saito (@dsk_saito) March 17, 2011 ちなみに僕はscipy.mixture.GMMを使いました。HTKヤダー\nやってみる さて、実際にやってみます。データベースには、[CMU_ARCTIC speech synthesis databases](ht tp://www.festvox.org/cmu_arctic/)を使います。今回は、女性話者の二人を使いました。\n音声の分析合成には、WORLDを使います。WORLDで求めたスペクトル包絡からメルケプストラム（今回は32次元）に変換したものを特徴量として使いました。\n学習では、学習サンプル10641フレーム（23フレーズ）、GMMの混合数64、full-covarianceで学習しました。\n変換元となる話者（参照話者） 変換対象となる話者（目標話者） GMMベースのframe-by-frameな声質変換の結果 はぁー、正直聞けたもんじゃないですね。声質は目標話者に近づいている感がありますが、何分音質が悪い。学習条件を色々変えて試行錯誤しましたけどダメでした\nGMMベースの声質変換の弱点 さて、なぜダメかを考えます。もう考えつくされてる感あるけど、大事なところだけ整理します\nフレーム毎に独立な変換処理 まず、音声が時間的に独立なわけないですよね。フレームごとに独立に変換すると、時間的に不連続な点が出てきてしまいます。その結果、ちょっとノイジーな音声になってしまったのではないかと考えられます。\nこれに対する解決法としては、戸田先生の論文にあるように、動的特徴量も併せてGMMを学習して、系列全体の確率が最大となるように変換を考えるトラジェクトリベースのパラメータ生成方法があります。1\nさて、やってみます。参照音声、目標音声は↑で使ったサンプルと同じです。\nトラジェクトリベースの声質変換の結果 あんま変わらないですね。計算量めっちゃ食うのに、本当につらい。実装が間違ってる可能性もあるけど…\n他の方法を考えるとするならば、まぁいっぱいあると思うんですが、スペクトル包絡なんて時間的に不連続にコロコロ変わるようなもんでもない気がするので、確率モデルとしてそういう依存関係を考慮した声質変換があってもいいもんですけどね。あんま見てない気がします。\nちょっと調べたら見つかったもの↓\nKim, E.K., Lee, S., Oh, Y.-H. (1997). “Hidden Markov Model Based Voice Conversion Using Dynamic Characteristics of Speaker”, Proc. of Eurospeech’97, Rhodes, Greece, pp. 2519-2522. 過剰な平滑化 これはGMMに限った話ではないですが、GMMベースのFrame-by-Frameな声質変換の場合でいえば、変換後の特徴量は条件付き期待値を取ることになるので、まぁ常識的に考えて平滑化されますよね。\nこれに対する解法としては、GV（Global Variance）を考慮する方法があります。これは戸田先生が提案されたものですね。\nさて、やってみます。wktk\nGVを考慮したトラジェクトリベースの声質変換の結果 多少ましになった気もしなくもないけど、やっぱり音質はいまいちですね。そして計算量は激マシします。本当につらい。学会で聞いたGVありの音声はもっと改善してた気がするんだけどなー音声合成の話だけど。僕の実装が間違ってるんですかね…\nムズすぎわろた 以上、いくつか試しましたが、統計的声質変換は激ムズだということがわかりました。え、ここで終わるの？という感じですが、最後に一つ別の手法を紹介します。\n差分スペクトル補正に基づく統計的声質変換 これまでは、音声を基本周波数、非周期性成分、スペクトル包絡に分解して、スペクトル包絡を表す特徴量を変換し、変換後の特徴量を元に波形を再合成していました。ただ、よくよく考えると、そもそも基本周波数、非周期性成分をいじる必要がない場合であれば、わざわざ分解して再合成する必要なくね？声質の部分のみ変換するようなフィルタかけてやればよくね？という考えが生まれます。実は、そういったアイデアに基づく素晴らしい手法があります。それが、差分スペクトル補正に基づく声質変換です。\n詳細は、以下の予稿をどうぞ\n小林 和弘, 戸田 智基, Graham Neubig, Sakriani Sakti, 中村 哲. “差分スペクトル補正に基づく統計的歌声声質変換”, 日本音響学会2014年春季研究発表会(ASJ). 東京. 2014年3月.\nでは、やってみます。歌声ではなく話し声ですが。他の声質変換の結果とも聴き比べてみてください。\n差分スペクトル補正に基づく声質変換の結果 かなり音声の自然性は上がりましたね。これはヘタすると騙されるレベル。本当に素晴らしいです。しかも簡単にできるので、お勧めです。↑のは、GMMに基づくframe-by-frameな変換です。計算量も軽いので、リアルタイムでもいけますね。\nおわりに 声質変換であれこれ試行錯誤して、ようやくスタートラインにたてた感があります。今後は新しい方法を考えようかなーと思ってます。\nおわり\nおわび お盆の間に学習ベースの声質変換のプログラム書く（宿題） #宣言\n— 山本りゅういち (@r9y9) August 12, 2013 約1年かかりました……。本当に申し訳ありませんでした(´･_･`)\n追記 Twitterで教えてもらいました。トラジェクトリベースで学習も変換も行う研究もありました\n@r9y9 つ トラジェクトリＧＭＭな特徴量変換 http://t.co/kUn7bp9EUt\n— 縄文人（妖精系研究者なのです） (@dicekicker) July 5, 2014 ただ、これはトラジェクトリベースのパラメータ生成法の提案であって、トラジェクトリモデル自体を学習してるわけではないんだよなー。普通に考えると学習もトラジェクトリで考える方法があっていい気がするが、 まだ見てないですね。 ありました。追記参照 ↩︎\n","date":1404518400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1404518400,"objectID":"c4ef67feb0fed05c6223b275fa80b209","permalink":"https://r9y9.github.io/blog/2014/07/05/statistical-voice-conversion-muzui/","publishdate":"2014-07-05T00:00:00Z","relpermalink":"/blog/2014/07/05/statistical-voice-conversion-muzui/","section":"post","summary":"Voice conversion based on maximum likelihood estimation of spectral parameter trajectory","tags":["Voice Conversion","Machine-Learning","Speech Signal Processing"],"title":"統計的声質変換クッソムズすぎワロタ","type":"post"},{"authors":null,"categories":null,"content":"C++からGoへ みなさん、C++で信号処理のアルゴリズムを書くのはつらいと思ったことはありませんか？C++で書くと速いのはいいけれど、いかんせん書くのが大変、コンパイルエラーは読みづらい、はたまたライブラリをビルドしようとしたら依存関係が上手く解決できず……そんな覚えはないでしょうか？謎のコンパイルエラーに悩みたくない、ガーベジコレクションほしい、Pythonのようにさくっと書きたい、型推論もほしい、でも動作は速い方がいい、そう思ったことはないでしょうか。\nそこでGoです。もちろん、そういった思いに完全に答えてくれるわけではありませんが、厳しいパフォーマンスを要求される場合でなければ、Goの方が良い場合も多いと僕は思っています。 とはいえ、まだ比較的新しい言語のため、ライブラリは少なく信号処理を始めるのも大変です。というわけで、僕がC++をやめてGoに移行したことを思い出し、Goでの信号処理の基礎と、今まで整備してきたGoでの音声信号処理ライブラリを紹介します。\nGoの良いところ/悪いところについては書きません。正直、本当は何の言語でもいいと思っていますが、僕はGoが好きなので、ちょっとでもGoで信号処理したいと思う人が増えるといいなーと思って書いてみます。\nあとで書きますが、僕が書いたコードで使えそうなものは、以下にまとめました。\nhttps://github.com/r9y9/gossp\n基礎 Wavファイルの読み込み/書き込み [wav] ","date":1402185600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1402185600,"objectID":"0f103649e2b273be64619c526ded86be","permalink":"https://r9y9.github.io/blog/2014/06/08/gossp-speech-signal-processing-for-go/","publishdate":"2014-06-08T00:00:00Z","relpermalink":"/blog/2014/06/08/gossp-speech-signal-processing-for-go/","section":"post","summary":"https://github.com/r9y9/gossp","tags":["Speech Signal Processing","Go"],"title":"GOSSP - Go言語で音声信号処理","type":"post"},{"authors":null,"categories":null,"content":"「ウェーブレット変換って難しいんじゃ…マザーウェーブレット？よくわかんない…」\n大丈夫、そんな人には以下の文献がお勧めです\nTorrence, C. and G.P. Compo “A Practical Guide to Wavelet Analysis”, Bull. Am. Meteorol. Soc., 79, 61–78, 1998.\n前置きはさておき、上の文献を参考にMorlet, Paul, DOG (Derivative of Gaussian) の代表的な3つのマザーウェーブレットで音声に対してウェーブレット変換をしてみたので、メモがてら結果を貼っておく\n図の横軸はサンプルで、縦軸は周波数Hz（対数目盛り）にした\nマザーウェーブレットのパラメータは、Morletは $\\omega_{0} = 6.0$、Paulは $M = 4$、DOGは $M = 6$\nスケールは、min=55hzで、25cent毎に8オクターブ分取った※厳密には違うのでごめんなさい\n分析に使った音声は、逆連続ウェーブレット変換による信号の再構成 - LESS IS MORE で使ったのと同じ\nMorlet ","date":1401580800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1401580800,"objectID":"58c71dcfafc5b67829f087d7612bf424","permalink":"https://r9y9.github.io/blog/2014/06/01/continuous-wavelet-transform-types/","publishdate":"2014-06-01T00:00:00Z","relpermalink":"/blog/2014/06/01/continuous-wavelet-transform-types/","section":"post","summary":"[A Practical Guide to Wavelet Analysis](https://paos.colorado.edu/research/wavelets/bams_79_01_0061.pdf)","tags":["Signal Processing","Wavelet Transform","Continuous Wavelet Transform"],"title":"連続ウェーブレット変換に使うマザーウェーブレット色々: Morlet, Paul, DOG","type":"post"},{"authors":null,"categories":null,"content":"1000番煎じだけど、知り合いにニューラルネットを教えていて、その過程で書いたコード。わかりやすさ重視。\nこのために、誤差伝播法をn回導出しました（意訳：何回もメモなくしました）\n#!/usr/bin/python # coding: utf-8 # ニューラルネットワーク(Feed-Forward Neural Networks)の学習、認識の # デモコードです。 # 誤差伝搬法によってニューラルネットを学習します。 # XORの学習、テストの簡単なデモコードもついています # 2014/05/10 Ryuichi Yamamoto import numpy as np def sigmoid(x): return 1.0 / (1.0 + np.exp(-x)) def dsigmoid(y): return y * (1.0 - y) class NeuralNet: def __init__(self, num_input, num_hidden, num_output): \u0026#34;\u0026#34;\u0026#34; パラメータの初期化 \u0026#34;\u0026#34;\u0026#34; # 入力層から隠れ層への重み行列 self.W1 = np.random.uniform(-1.0, 1.0, (num_input, num_hidden)) self.hidden_bias = np.ones(num_hidden, dtype=float) # 隠れ層から出力層への重み行列 self.W2 = np.random.uniform(-1.0, 1.0, (num_hidden, num_output)) self.output_bias = np.ones(num_output, dtype=float) def forward(self, x): \u0026#34;\u0026#34;\u0026#34; 前向き伝搬の計算 \u0026#34;\u0026#34;\u0026#34; h = sigmoid(np.dot(self.W1.T, x) + self.hidden_bias) return sigmoid(np.dot(self.W2.T, h) + self.output_bias) def cost(self, data, target): \u0026#34;\u0026#34;\u0026#34; 最小化したい誤差関数 \u0026#34;\u0026#34;\u0026#34; N = data.shape[0] E = 0.0 for i in range(N): y, t = self.forward(data[i]), target[i] E += np.sum((y - t) * (y - t)) return 0.5 * E / float(N) def train(self, data, target, epoches=30000, learning_rate=0.1,\\ monitor_period=None): \u0026#34;\u0026#34;\u0026#34; Stochastic Gradient Decent (SGD) による学習 \u0026#34;\u0026#34;\u0026#34; for epoch in range(epoches): # 学習データから1サンプルをランダムに選ぶ index = np.random.randint(0, data.shape[0]) x, t = data[index], target[index] # 入力から出力まで前向きに信号を伝搬 h = sigmoid(np.dot(self.W1.T, x) + self.hidden_bias) y = sigmoid(np.dot(self.W2.T, h) + self.output_bias) # 隠れ層-\u0026gt;出力層の重みの修正量を計算 output_delta = (y - t) * dsigmoid(y) grad_W2 = np.dot(np.atleast_2d(h).T, np.atleast_2d(output_delta)) # 隠れ層-\u0026gt;出力層の重みを更新 self.W2 -= learning_rate * grad_W2 self.output_bias -= learning_rate * output_delta # 入力層-\u0026gt;隠れ層の重みの修正量を計算 hidden_delta = np.dot(self.W2, output_delta) * dsigmoid(h) grad_W1 = np.dot(np.atleast_2d(x).T, np.atleast_2d(hidden_delta)) # 入力層-\u0026gt;隠れ層の重みを更新 self.W1 -= learning_rate * grad_W1 self.hidden_bias -= learning_rate * hidden_delta # 現在の目的関数の値を出力 if monitor_period != None and epoch % monitor_period == 0: print \u0026#34;Epoch %d, Cost %f\u0026#34; % (epoch, self.cost(data, target)) print \u0026#34;Training finished.\u0026#34; def predict(self, x): \u0026#34;\u0026#34;\u0026#34; 出力層の最も反応するニューロンの番号を返します \u0026#34;\u0026#34;\u0026#34; return np.argmax(self.forward(x)) if __name__ == \u0026#34;__main__\u0026#34;: import argparse parser = argparse.ArgumentParser(description=\u0026#34;Specify options\u0026#34;) parser.add_argument(\u0026#34;--epoches\u0026#34;, dest=\u0026#34;epoches\u0026#34;, type=int, required=True) parser.add_argument(\u0026#34;--learning_rate\u0026#34;, dest=\u0026#34;learning_rate\u0026#34;,\\ type=float, default=0.1) parser.add_argument(\u0026#34;--hidden\u0026#34;, dest=\u0026#34;hidden\u0026#34;, type=int, default=20) args = parser.parse_args() nn = NeuralNet(2, args.hidden, 1) data = np.array([[0, 0], [0 ,1], [1, 0], [1, 1]]) target = np.array([0, 1, 1, 0]) nn.train(data, target, args.epoches, args.learning_rate,\\ monitor_period=1000) for x in data: print \u0026#34;%s : predicted %s\u0026#34; % (x, nn.forward(x)) #!/usr/bin/python # coding: utf-8 # MNISTを用いたニューラルネットによる手書き数字認識のデモコードです # 学習方法やパラメータによりますが、だいたい 90 ~ 97% くらいの精度出ます。 # 使い方は、コードを読むか、 # python mnist_net.py -h # としてください # 参考までに、 # python mnist_net.py --epoches 50000 --learning_rate 0.1 --hidden 100 # とすると、テストセットに対して、93.2%の正解率です # 僕の環境では、学習、認識合わせて（だいたい）5分くらいかかりました。 # 2014/05/10 Ryuichi Yamamoto import numpy as np from sklearn.externals import joblib import cPickle import gzip import os # 作成したニューラルネットのパッケージ import net def load_mnist_dataset(dataset): \u0026#34;\u0026#34;\u0026#34; MNISTのデータセットをダウンロードします \u0026#34;\u0026#34;\u0026#34; # Download the MNIST dataset if it is not present data_dir, data_file = os.path.split(dataset) if (not os.path.isfile(dataset)) and data_file == \u0026#39;mnist.pkl.gz\u0026#39;: import urllib origin = \u0026#39;http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz\u0026#39; print \u0026#39;Downloading data from %s\u0026#39; % origin urllib.urlretrieve(origin, dataset) f = gzip.open(dataset, \u0026#39;rb\u0026#39;) train_set, valid_set, test_set = cPickle.load(f) f.close() return train_set, valid_set, test_set def augument_labels(labels, order): \u0026#34;\u0026#34;\u0026#34; 1次元のラベルデータを、ラベルの種類数(order)次元に拡張します \u0026#34;\u0026#34;\u0026#34; new_labels = [] for i in range(labels.shape[0]): v = np.zeros(order) v[labels[i]] = 1 new_labels.append(v) return np.array(new_labels).reshape((labels.shape[0], order)) if __name__ == \u0026#34;__main__\u0026#34;: import argparse parser = argparse.ArgumentParser(description=\u0026#34;MNIST手書き数字認識のデモ\u0026#34;) parser.add_argument(\u0026#34;--epoches\u0026#34;, dest=\u0026#34;epoches\u0026#34;, type=int, required=True) parser.add_argument(\u0026#34;--learning_rate\u0026#34;, dest=\u0026#34;learning_rate\u0026#34;,\\ type=float, default=0.1) parser.add_argument(\u0026#34;--hidden\u0026#34;, dest=\u0026#34;hidden\u0026#34;, type=int, default=100) args = parser.parse_args() train_set, valid_set, test_set = load_mnist_dataset(\u0026#34;mnist.pkl.gz\u0026#34;) n_labels = 10 # 0,1,2,3,4,5,6,7,9 n_features = 28*28 # モデルを新しく作る nn = net.NeuralNet(n_features, args.hidden, n_labels) # モデルを読み込む # nn = joblib.load(\u0026#34;./nn_mnist.pkl\u0026#34;) nn.train(train_set[0], augument_labels(train_set[1], n_labels),\\ args.epoches, args.learning_rate, monitor_period=2000) ## テスト test_data, labels = test_set results = np.arange(len(test_data), dtype=np.int) for n in range(len(test_data)): results[n] = nn.predict(test_data[n]) # print \u0026#34;%d : predicted %s, expected %s\u0026#34; % (n, results[n], labels[n]) print …","date":1399766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1399766400,"objectID":"de2ee3dcd0baf308c616396ffd5727af","permalink":"https://r9y9.github.io/blog/2014/05/11/python-feed-forward-neural-network-toy-code/","publishdate":"2014-05-11T00:00:00Z","relpermalink":"/blog/2014/05/11/python-feed-forward-neural-network-toy-code/","section":"post","summary":"https://github.com/r9y9/python-neural-net-toy-codes","tags":["Python","Deep Learning","Machine Learning"],"title":"PythonによるニューラルネットのToyコード","type":"post"},{"authors":null,"categories":null,"content":"C/C++ライブラリのGoラッパーを書くためには、cgoというパッケージを使うのだけど、特にCのポインタ周りにハマりどころが多かったので、少しまとめとく\ncgoの基礎については、以下の二つを読むことを推奨\nhttps://golang.org/cmd/cgo/ https://code.google.com/p/go-wiki/wiki/cgo この記事では、cgo基本的な使い方と、いくつかポインタ絡みのTipsをまとめる。Tipsのみ必要な場合は、最初の方は飛ばして下さい\ncgo Cgo enables the creation of Go packages that call C code.\nhttps://golang.org/cmd/cgo/\ncgoとは、GoからCの関数/型にアクセスするために用いるパッケージのこと。cgoを使えば、GoからCのコードが呼べる。つまり、Cで書かれたライブラリが、Goでも再利用できる。\nなお、go v1.2 から、C++もサポートされている様子 http://golang.org/doc/go1.2#cgo_and_cpp\nただし、C++ライブラリの使用方法については現時点でドキュメントはほぼ無し。僕の経験では、extern “C” を付けておくとC++用のコンパイラでコンパイルされたライブラリでも呼べる\n基本的な使い方 まず、Cの型/関数にアクセスするために、cgoパッケージのimportを行う\nimport \u0026#34;C\u0026#34; import文のすぐ上のコメントにinclude \u0026lt;ヘッダ.h\u0026gt; と書けば、コンパイルする際に自動で読み込まれるので、必要なヘッダを書く\n// #include \u0026lt;stdio.h\u0026gt; // #include \u0026lt;stdlib.h\u0026gt; import \u0026#34;C\u0026#34; これで、C.int, C.float, C.double, *C.char、C.malloc, C.free などのようにして、Cの型や関数にアクセスできる\n外部ライブラリを呼ぶ 通常は、ヘッダファイルをincludeするだけでなく、何かしらのライブラリとリンクして用いることが多いので、そのような場合には、ライブラリの依存関係をgoのコードに記述する\ncgoでは、includeの設定と同様に、CFLAGS、CPPFLAGS、CXXFLAGS、LDFLAGS、pkg-configを記述することができる\npkg-configを使うと 、\n// #cgo pkg-config: png cairo // #include \u0026lt;png.h\u0026gt; import \u0026#34;C\u0026#34; こんな感じ（Goの公式ページから参照）\nTips さて、ここからTips。主に、WORLDのGoラッパーを書いていたときに得た知見です。ラッパーは、Githubにあげた\n1. GoのスライスをCのポインタとして関数の引数に渡す 例えば、[]float64 -\u0026gt; double* のイメージ\nこれは比較的簡単にできる。以前qiitaにも書いた http://qiita.com/r9y9/items/e6d879c9b7d4f2697593\n(*C.double)(\u0026amp;slice[0]) のようにキャストしてやればOK\n2. GoのスライスのスライスをCのポインタのポインタとして関数の引数に渡す [][]float64 -\u0026gt; double** のようなイメージ\n例として、worldから引っ張ってきた以下のようなCの関数を考える\nvoid Star(double *x, int x_length, int fs, double *time_axis, double *f0, int f0_length, double **spectrogram); **spectrogramには処理結果が格納される。もちろん処理結果はGoの型で扱いたいんだが、では****spectrogramにどうやってGoの型を渡すか？**ということが問題になる\ndoubleの二次元配列なので、\ns := [][]float64 というスライスのスライスを考えて、キャストして渡したいところだけど、結論から言うとこれはできない\nではどうするかというと、苦肉の策として、\nwspace := make([]*C.double, len(f0)) というスライスを考えて、\n(**C.double)(\u0026amp;wspace[0]) とすれば、double**として関数の引数に渡すことができる。他にも方法がある気がするが、これでも期待通りの動作をする（あまりハックっぽいことしたくない…\nまとめると、\n[][]float64 -\u0026gt; double**はできないが、 []*C.double -\u0026gt; double**はできる。よって、一応Goの型をdouble**に渡すことはできる です。\n3. ポインタのポインタからスライスのスライスへの変換 double** -\u0026gt; [][]float64 のようなイメージ\nTipsその2の例より、Cの関数の処理が終われば**spectrogramにデータが格納される。もちろん処理結果はGoの型で扱いたいので、[][]float64 にしたい。ただし、先程の例では、Cの関数に渡した型は実際には []*C.doubleで、Cの型を含んでいる。\nそこで、次に問題になるのは、**[]*C.doubleにから[][]float64 に変換するにはどうするか？**ということ。そして、これも面倒です…（※節の頭でdouble** -\u0026gt; [][]float64と書いたけど、正確には []*C.double -\u0026gt; [][]float64）\n結論から言えば、直接の変換は難しいけど中間変数をかませばできる\n[]bytes型でtmp変数を作り、C.GoBytes を使って*C.double -\u0026gt; []bytes にコピー encoding/binaryパッケージを使って、[]bytes -\u0026gt; []float64 に書き込み この処理をsliceOfSlices[0], sliceOfSlices[1], … に対して繰り返す 以上。とても面倒ですね…\nさて、結局上のStarのラッパーは以下のようになった\nfunc Star(x []float64, fs int, timeAxis, f0 []float64) [][]float64 { FFTSize := C.size_t(C.GetFFTSizeForStar(C.int(fs))) numFreqBins := FFTSize/2 + 1 // Create workspace wspace := make([]*C.double, len(f0)) for i := range wspace { wspace[i] = (*C.double)(C.malloc(byteSizeOfFloat64 * numFreqBins)) defer C.free(unsafe.Pointer(wspace[i])) } // Perform star C.Star((*C.double)(\u0026amp;x[0]), C.int(len(x)), C.int(fs), (*C.double)(\u0026amp;timeAxis[0]), (*C.double)(\u0026amp;f0[0]), C.int(len(f0)), (**C.double)(\u0026amp;wspace[0])) // Copy to go slice spectrogram := make([][]float64, len(f0)) for i := range spectrogram { spectrogram[i] = CArrayToGoSlice(wspace[i], C.int(numFreqBins)) } return spectrogram } 上で使っているutility function\nfunc CArrayToGoSlice(array *C.double, length C.int) []float64 { slice := make([]float64, int(length)) b := C.GoBytes(unsafe.Pointer(array), C.int(byteSizeOfFloat64*length)) err := binary.Read(bytes.NewReader(b), binary.LittleEndian, slice) if err != nil { panic(err) } return slice } []*C.double のスライスを作り、作業領域のメモリを確保する（Tips2の内容+メモリ確保） []C.double のスライスをdouble* にキャストして、Cの関数を実行（Tips2の内容） []*C.double から[][]float64に変換する（Tips3の内容） という手順になってます\n※2013/03/27 追記 :もっとシンプルかつ効率的（copyの必要がないように）に書けた。[][]float64で返り値用のスライスを作り、それを[]*double型に変換してCに渡せば、[][]float64に変更が反映されるので、そもそも[]*doubleから[][]float64に変換する必要はなかった。\nfunc Star(x []float64, fs int, timeAxis, f0 []float64) [][]float64 { FFTSize := C.size_t(C.GetFFTSizeForStar(C.int(fs))) numFreqBins := C.size_t(FFTSize/2 + 1) spectrogram := make([][]float64, len(f0)) for i := range spectrogram { spectrogram[i] = make([]float64, numFreqBins) } spectrogramUsedInC := Make2DCArrayAlternative(spectrogram) // Perform star C.Star((*C.double)(\u0026amp;x[0]), C.int(len(x)), C.int(fs), (*C.double)(\u0026amp;timeAxis[0]), (*C.double)(\u0026amp;f0[0]), C.int(len(f0)), (**C.double)(\u0026amp;spectrogramUsedInC[0])) return spectrogram } func Make2DCArrayAlternative(matrix [][]float64) []*C.double { alternative := make([]*C.double, len(matrix)) for i := range alternative { // DO NOT free because the source slice is managed by Go alternative[i] = (*C.double)(\u0026amp;matrix[i][0]) } return alternative } おわりに ポインタのポインタを引数に取る関数のラップはめんどくさい Goは使いやすいのに cgoは使いにくい cgoつらい よりいい方法があれば教えて下さい 2014/08/10 追記 cgo使いにくいと書いたけど、あとから考えてみればcgoさんまじ使いやすかったです（遅い\nJuliaのccallはもっと使いやすい。\n","date":1395446400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1395446400,"objectID":"2b8f43f175f823848ed9948728a8dd81","permalink":"https://r9y9.github.io/blog/2014/03/22/cgo-tips/","publishdate":"2014-03-22T00:00:00Z","relpermalink":"/blog/2014/03/22/cgo-tips/","section":"post","summary":"https://golang.org/cmd/cgo/","tags":["Go","cgo"],"title":"cgo の基本的な使い方とポインタ周りのTips (Go v1.2)","type":"post"},{"authors":null,"categories":null,"content":"音声分析変換合成システムWORLD WORLDとは、山梨大学の森勢先生が作られている高品質な音声分析変換合成システムです。非常に高品質かつ高速に動作するのが良い所です。詳細は以下のURLへ\nhttp://ml.cs.yamanashi.ac.jp/world/\nオリジナルはC+＋で書かれていますが、Goからも使えるようにラッパーを書きました。非常にいいソフトウェアなので、もしよろしければどうぞ\nGO-WORLD https://github.com/r9y9/go-world\n使い方について、ほんの少し解説を書きます\n※ubuntu12.04でのみ動作確認してます。\n準備 1. WORLDのインストール まずWORLDをインストールする必要があります。公式のパッケージではinstallerに相当するものがなかったので、作りました\nhttps://github.com/r9y9/world\n./waf configure \u0026amp;\u0026amp; ./waf sudo ./waf install でインストールできます。\nなお、WORLDは最新版ではなく0.1.2としています。最新版にすると自分の環境でビルドコケてしまったので…\n2. GO-WORLDのインストール go get github.com/r9y9/go-world 簡単ですね！\n使い方 1. インポートする import \u0026#34;github.com/r9y9/go-world\u0026#34; 2. worldのインスタンスを作る w := world.New(sampleRate, framePeriod) // e.g. (44100, 5) 3. 好きなworldのメソッドを呼ぶ 基本周波数の推定: Dio timeAxis, f0 := w.Dio(input, w.NewDioOption()) // default option is used スペクトル包絡の推定: Star spectrogram := w.Star(input, timeAxis, f0) 励起信号の推定: Platinum residual := w.Platinum(input, timeAxis, f0, spectrogram) パラメータから音声の再合成: Synthesis synthesized := w.Synthesis(f0, spectrogram, residual, len(input)) 使い方例. 音声（wavファイル）を分析して、パラメータから音声を再合成する例を紹介します。80行弱と少し長いですがはっつけておきます\npackage main import ( \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/mjibson/go-dsp/wav\u0026#34; \u0026#34;github.com/r9y9/go-world\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; ) var defaultDioOption = world.DioOption{ F0Floor: 80.0, F0Ceil: 640.0, FramePeriod: 5, ChannelsInOctave: 4.0, Speed: 6, } // 音声を基本周波数、スペクトル包絡、励起信号の三つに分解したあと、再合成します func worldExample(input []float64, sampleRate int) []float64 { w := world.New(sampleRate, defaultDioOption.FramePeriod) // 1. Fundamental frequency timeAxis, f0 := w.Dio(input, defaultDioOption) // 2. Spectral envelope spectrogram := w.Star(input, timeAxis, f0) // 3. Excitation spectrum residual := w.Platinum(input, timeAxis, f0, spectrogram) // 4. Synthesis return w.Synthesis(f0, spectrogram, residual, len(input)) } // 音声を基本周波数、スペクトル包絡、非周期成分の三つに分解したあと、再合成します func worldExampleAp(input []float64, sampleRate int) []float64 { w := world.New(sampleRate, defaultDioOption.FramePeriod) // 1. Fundamental frequency timeAxis, f0 := w.Dio(input, defaultDioOption) // 2. Spectral envelope spectrogram := w.Star(input, timeAxis, f0) // 3. Apiriodiciy apiriodicity, targetF0 := w.AperiodicityRatio(input, f0) // 4. Synthesis return w.SynthesisFromAperiodicity(f0, spectrogram, apiriodicity, targetF0, len(input)) } func GetMonoDataFromWavData(data [][]int) []float64 { y := make([]float64, len(data)) for i, val := range data { y[i] = float64(val[0]) } return y } func main() { ifilename := flag.String(\u0026#34;i\u0026#34;, \u0026#34;default.wav\u0026#34;, \u0026#34;Input filename\u0026#34;) flag.Parse() // Read wav data file, err := os.Open(*ifilename) if err != nil { log.Fatal(err) } defer file.Close() w, werr := wav.ReadWav(file) if werr != nil { log.Fatal(werr) } input := GetMonoDataFromWavData(w.Data) sampleRate := int(w.SampleRate) synthesized := worldExample(input, sampleRate) // synthesized := worldExampleAp(input, sampleRate) for i, val := range synthesized { fmt.Println(i, val) } } Goだとメモリ管理きにしなくていいしそこそこ速いし読みやすいし書きやすいし楽でいいですね（信者\nおわりに GoはC++ほど速くはないですが、C++の何倍も書きやすいし読みやすい（メンテしやすい）ので、個人的にオススメです（パフォーマンスが厳しく要求される場合には、C++の方がいいかもしれません） WORLD良いソフトウェアなので使いましょう ちなみに 元はと言えば、オレオレ基本周波数推定（YINもどき）が微妙に精度悪くて代替を探していたとき、\nSPTKのRAPTかSWIPE使おうかな… RAPTもSWIPEもSPTK.hにインタフェースがない… うわRAPTのコード意味わからん SWIPEのコードまじ謎 後藤さんのPreFest実装しよう あれ上手くいかない…orz どうしようかな… となっていたときに、森勢先生が書いたと思われる以下の文献を見つけて、\n2-2 基本周波数推定（歌声研究に関する視点から）\n本方法は，低域に雑音が存在する音声に対する推定は困難であるが，低域の雑音が存在しない音声の場合，SWIPE′ や NDF と実質的に同等の性能を達成しつつ，計算時間を SWIPE′の 1/42, NDF の 1/80 にまで低減可能である．\nあぁworld使おう（白目\nとなり、ラッパーを書くにいたりましたとさ、おしまい\n","date":1395446400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1395446400,"objectID":"c6acd4736a519eeda16353ff14375731","permalink":"https://r9y9.github.io/blog/2014/03/22/go-world/","publishdate":"2014-03-22T00:00:00Z","relpermalink":"/blog/2014/03/22/go-world/","section":"post","summary":"https://github.com/r9y9/go-world","tags":["Speech Signal Processing","Go","WORLD"],"title":"音声分析変換合成システムWORLDのGoラッパーを書いた","type":"post"},{"authors":null,"categories":null,"content":"","date":1394064000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1394064000,"objectID":"724a935aaae00fca36936066c1c9ca38","permalink":"https://r9y9.github.io/blog/2014/03/06/restricted-boltzmann-machines-mnist/","publishdate":"2014-03-06T00:00:00Z","relpermalink":"/blog/2014/03/06/restricted-boltzmann-machines-mnist/","section":"post","summary":"https://github.com/r9y9/nnet","tags":["Deep Learning","RBM","Machine Learning"],"title":"Restricted Boltzmann Machines with MNIST","type":"post"},{"authors":null,"categories":null,"content":"一番の違いは、生成モデルか識別モデルか、ということ。それぞれ、\nMarkov Random Fields (MRF) は生成モデル Conditional Random Fields (CRF) は識別モデル です。\nWhat is exactly the difference between MRF and CRF ここを見ると割とすっきりする。\nただ、少しスムーズに納得できないことがありまして…それは、MRFもCRFもグラフィカルモデルで書くと無向グラフとなること。識別モデルは無向グラフで生成モデルは有向グラフなんじゃ…？と思ってしまう人もいるんじゃないかと思う（いなかったらごめんなさい）。\nグラフィカルモデルとしての表現 一般に、生成モデルは有向グラフの形で記述され、識別モデルは無向グラフとして記述される。例えば、隠れマルコフモデル (HMM) は有向グラフで、条件付き確率場 (CRF) は無向グラフで表される。図を貼っておく\nその道の人には、馴染みのある図だと思う（ｼｭｳﾛﾝから引っ張ってきた）。グレーの○が観測変数、白い○が隠れ変数です\nここで重要なのは、例外もあるということ。具体的には、タイトルにあるMRFは生成モデルだけど無向グラフで書かれる。MRFというと、例えばRestricted Boltzmann Machine とかね！\n単純なことだけど、これを知らないとMRFについて学習するときにつっかかってしまうので注意\nAn Introduction to Conditional Random Fields の2.2 Generative versus Discriminative Models から引用すると、\nBecause a generative model takes the form p(y,x) = p(y)p(x|y), it is often natural to represent a generative model by a directed graph in which in outputs y topologically precede the inputs. Similarly, we will see that it is often natural to represent a discriminative model by a undirected graph. However, this need not always be the case, and both undirected generative models, such as the Markov random ﬁeld (2.32), and directed discriminative models, such as the MEMM (6.2), are sometimes used. It can also be useful to depict discriminative models by directed graphs in which the x precede the y.\nらしいです\n結論 生成モデル＝有向グラフ、識別モデル＝無向グラフで表されるとは限らない ことMRFに関して言えば生成モデルだけど無向グラフで表されるよ ということです\nさらに言えば、MRFとCRFはグラフィカルモデルでは同じように書けてしまうけれど、両者には明確な違いがあることに気をつけましょう、ということです（ちょっと自信ない）\n間違っていたら教えて下さい\n参考 What is exactly the difference between MRF and CRF An Introduction to Conditional Random Fields (PDF) More about Undirected Graphical Models ","date":1393632000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1393632000,"objectID":"cf4320bd48646e0802b277f1b7e63c27","permalink":"https://r9y9.github.io/blog/2014/03/01/difference-between-mrf-and-crf/","publishdate":"2014-03-01T00:00:00Z","relpermalink":"/blog/2014/03/01/difference-between-mrf-and-crf/","section":"post","summary":"[What is exactly the difference between MRF and CRF](http://metaoptimize.com/qa/questions/4021/what-is-exactly-the-difference-between-mrf-and-crf)","tags":["Graphical Model","Random Field"],"title":"マルコフ確率場 (MRF) と条件付き確率場 (CRF) の違い","type":"post"},{"authors":null,"categories":null,"content":"2014/07/22 追記： パッケージの一部として書きました（GOSSP - Go言語で音声信号処理 - LESS IS MOREを参照）。 SPTKのラップも含め、いくつかGoで信号処理アルゴリズムを実装したので、お求めの方はどうぞ\n–\nGoが最近オススメです（n度目\nGoで音声信号処理をしたいけど、全部一から書くのは大変だし、既存の資産は出来るだけ再利用したい。というわけで、C言語製のSPTK をGoから使えるようにする\ncgo GoにはC言語のライブラリを使うには、cgoというパッケージを使えばできる。使い方は、公式のページ等を見るといいと思う https://golang.org/cmd/cgo/\nCの関数や変数などには、 C. でアクセスできる\nラッパー 例えば以下のように書く。MFCCの計算を例に上げる。必要に応じでSPTK.hに定義されている関数をラップする\npackage sptk // #cgo pkg-config: SPTK // #include \u0026lt;stdio.h\u0026gt; // #include \u0026lt;SPTK/SPTK.h\u0026gt; import \u0026#34;C\u0026#34; func MFCC(audioBuffer []float64, sampleRate int, alpha, eps float64, wlng, flng, m, n, ceplift int, dftmode, usehamming bool) []float64 { // Convert go bool to C.Boolean (so annoying.. var dftmodeInGo, usehammingInGo C.Boolean if dftmode { dftmodeInGo = 1 } else { dftmodeInGo = 0 } if usehamming { usehammingInGo = 1 } else { usehammingInGo = 0 } resultBuffer := make([]float64, m) C.mfcc((*_Ctype_double)(\u0026amp;audioBuffer[0]), (*_Ctype_double)(\u0026amp;resultBuffer[0]), C.double(sampleRate), C.double(alpha), C.double(eps), C.int(wlng), C.int(flng), C.int(m), C.int(n), C.int(ceplift), dftmodeInGo, usehammingInGo) return resultBuffer } このパッケージを使う前に、 https://github.com/r9y9/SPTK を使ってSPTKをインストールする。本家のを使ってもいいですが、その場合は #cgo の設定が変わると思います。公式のSPTK、pkg-configに対応してくれんかな…\n最初は、LDFLAGS つけ忘れてて、symbol not foundってなってつらまった。次回から気をつけよう\nSPTKの、特に（メル）ケプストラム分析当たりは本当に難しいので、論文読んで実装するのも大変だし中身がわからなくてもラップする方が合理的、という結論に至りました。簡単なもの（例えば、メルケプからMLSA filterの係数への変換とか）は、依存関係を少なくするためにもGo nativeで書きなおした方がいいです\nコードは気が向いたら上げる\n","date":1391990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1391990400,"objectID":"ec09dde5e0640cfec2f71a919c693441","permalink":"https://r9y9.github.io/blog/2014/02/10/sptk-go-wrapper/","publishdate":"2014-02-10T00:00:00Z","relpermalink":"/blog/2014/02/10/sptk-go-wrapper/","section":"post","summary":"http://sp-tk.sourceforge.net/","tags":["Go","Speech Signal processing","SPTK"],"title":"Goで音声信号処理をしたいのでSPTKのGoラッパーを書く","type":"post"},{"authors":null,"categories":null,"content":"","date":1390867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1390867200,"objectID":"0c8242baea8e10e2030fef661b4ae5b9","permalink":"https://r9y9.github.io/blog/2014/01/28/go-chroma-vector/","publishdate":"2014-01-28T00:00:00Z","relpermalink":"/blog/2014/01/28/go-chroma-vector/","section":"post","summary":"https://github.com/r9y9/go-msptools","tags":["Go","Music Signal Processing"],"title":"Goでクロマベクトルを求める","type":"post"},{"authors":null,"categories":null,"content":"最近Go言語を触っていて、これがなかなかいい感じ。そこそこ速いので、信号処理や機械学習も行けると思う\nGoの良い所 デフォでたくさん便利なパッケージがある。http, json, os, … パッケージのインストールはとても簡単。go getするだけ デフォでテストの枠組みがある gofmtでコードのformattingしてくれるので書き方で迷わなくて良い 使わないパッケージをimportするとコンパイルエラーになるし自然と疎結合なコードを書くようになる 並列処理を言語レベルでサポート GCあるのでメモリ管理なんてしなくていい 全般的にC++より書きやすい（ここ重要） そこそこ速い（C++よりは遅いけど） ホントはPythonでさくっと書きたいけどパフォーマンスもほしいからC++で書くかー（嫌だけど）。と思ってた自分にはちょうどいい\nGoの悪い所（主にC++と比べて） ちょっと遅い。さっと試したウェーブレット変換は、1.5倍くらい遅かった気がする（うろ覚え） C++やpythonに比べるとライブラリは少ない 言語仕様とかそのへんが優れてるかどうかは判断つきませんごめんなさい Go-msptools 2014/07/22 追記： Go-msptoolsはGOSSPに吸収されました。（GOSSP - Go言語で音声信号処理 - LESS IS MOREを参照）\nおまけ：音の信号処理に役立ちそうなライブラリ go-dsp portaudio-go ","date":1390780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1390780800,"objectID":"06902f4bc98344b3bd22cc5b776f770c","permalink":"https://r9y9.github.io/blog/2014/01/27/start-coding-go-msptools/","publishdate":"2014-01-27T00:00:00Z","relpermalink":"/blog/2014/01/27/start-coding-go-msptools/","section":"post","summary":"https://github.com/mjibson/go-dsp/","tags":["Go","Speech Signal Processing"],"title":"Goで信号処理","type":"post"},{"authors":null,"categories":null,"content":"MLSAフィルタわからんという記事を書いて早2ヶ月、ようやく出来た。\nMel-log spectrum approximate (MLSA) filterというのは、対数振幅スペクトルを近似するようにメルケプストラムから直接音声を合成するデジタルフィルタです。SPTKのmlsa filterと比較して完全に計算結果が一致したので、間違ってはないはず。MLSAフィルタを使ってメルケプから音声合成するプログラムをC++で自分で書きたいという稀有な人であれば、役に立つと思います。基本的に、SPTKのmlsa filterの再実装です。\nmlsa_filter.h https://gist.github.com/r9y9/7735120\n#pragma once #include \u0026lt;cmath\u0026gt; #include \u0026lt;memory\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;cassert\u0026gt; namespace sp { /** * MLSA BASE digital filter (Mel-log Spectrum Approximate digital filter) */ class mlsa_base_filter { public: mlsa_base_filter(const int order, const double alpha); template \u0026lt;class Vector\u0026gt; double filter(const double x, const Vector\u0026amp; b); private: mlsa_base_filter(); double alpha_; std::vector\u0026lt;double\u0026gt; delay_; }; mlsa_base_filter::mlsa_base_filter(const int order, const double alpha) : alpha_(alpha), delay_(order+1) { } template \u0026lt;class Vector\u0026gt; double mlsa_base_filter::filter(const double x, const Vector\u0026amp; b) { double result = 0.0; delay_[0] = x; delay_[1] = (1.0-alpha_*alpha_)*delay_[0] + alpha_*delay_[1]; for (size_t i = 2; i \u0026lt; b.size(); ++i) { delay_[i] = delay_[i] + alpha_*(delay_[i+1]-delay_[i-1]); result += delay_[i] * b[i]; } // special case // TODO: other solution? if (b.size() == 2) { result += delay_[1] * b[1]; } // t \u0026lt;- t+1 in time for (size_t i = delay_.size()-1; i \u0026gt; 1; --i) { delay_[i] = delay_[i-1]; } return result; } /** * MLSA digital filter cascaded */ class mlsa_base_cascaded_filter { public: mlsa_base_cascaded_filter(const int order, const double alpha, const int n_pade); template \u0026lt;class Vector\u0026gt; double filter(const double x, const Vector\u0026amp; b); private: mlsa_base_cascaded_filter(); std::vector\u0026lt;std::unique_ptr\u0026lt;mlsa_base_filter\u0026gt;\u0026gt; base_f_; // cascadad filters std::vector\u0026lt;double\u0026gt; delay_; std::vector\u0026lt;double\u0026gt; pade_coef_; }; mlsa_base_cascaded_filter::mlsa_base_cascaded_filter(const int order, const double alpha, const int n_pade) : delay_(n_pade + 1), pade_coef_(n_pade + 1) { using std::unique_ptr; if (n_pade != 4 \u0026amp;\u0026amp; n_pade != 5) { std::cerr \u0026lt;\u0026lt; \u0026#34;The number of pade approximations must be 4 or 5.\u0026#34; \u0026lt;\u0026lt; std::endl; } assert(n_pade == 4 || n_pade == 5); for (int i = 0; i \u0026lt;= n_pade; ++i) { mlsa_base_filter* p = new mlsa_base_filter(order, alpha); base_f_.push_back(unique_ptr\u0026lt;mlsa_base_filter\u0026gt;(p)); } if (n_pade == 4) { pade_coef_[0] = 1.0; pade_coef_[1] = 4.999273e-1; pade_coef_[2] = 1.067005e-1; pade_coef_[3] = 1.170221e-2; pade_coef_[4] = 5.656279e-4; } if (n_pade == 5) { pade_coef_[0] = 1.0; pade_coef_[1] = 4.999391e-1; pade_coef_[2] = 1.107098e-1; pade_coef_[3] = 1.369984e-2; pade_coef_[4] = 9.564853e-4; pade_coef_[5] = 3.041721e-5; } } template \u0026lt;class Vector\u0026gt; double mlsa_base_cascaded_filter::filter(const double x, const Vector\u0026amp; b) { double result = 0.0; double feed_back = 0.0; for (size_t i = pade_coef_.size()-1; i \u0026gt;= 1; --i) { delay_[i] = base_f_[i]-\u0026gt;filter(delay_[i-1], b); double v = delay_[i] * pade_coef_[i]; if (i % 2 == 1) { feed_back += v; } else { feed_back -= v; } result += v; } delay_[0] = feed_back + x; result += delay_[0]; return result; } /** * MLSA digital filter (Mel-log Spectrum Approximate digital filter) * The filter consists of two stage cascade filters */ class mlsa_filter { public: mlsa_filter(const int order, const double alpha, const int n_pade); ~mlsa_filter(); template \u0026lt;class Vector\u0026gt; double filter(const double x, const Vector\u0026amp; b); private: mlsa_filter(); double alpha_; std::unique_ptr\u0026lt;mlsa_base_cascaded_filter\u0026gt; f1_; // first stage std::unique_ptr\u0026lt;mlsa_base_cascaded_filter\u0026gt; f2_; // second stage }; mlsa_filter::mlsa_filter(const int order, const double alpha, const int n_pade) : alpha_(alpha), f1_(new mlsa_base_cascaded_filter(2, alpha, n_pade)), f2_(new mlsa_base_cascaded_filter(order, alpha, n_pade)) { } mlsa_filter::~mlsa_filter() { } template \u0026lt;class Vector\u0026gt; double mlsa_filter::filter(const double x, const Vector\u0026amp; b) { // 1. First stage filtering Vector b1 = {0, b[1]}; double y = f1_-\u0026gt;filter(x, b1); // 2. Second stage filtering double result = f2_-\u0026gt;filter(y, b); return result; } } // end namespace sp 使い方 mlsa_filter.hをインクルードすればおｋ\n#include \u0026#34;mlsa_filter.h\u0026#34; // セットアップ const double alpha = 0.42; const int order = 30; const int n_pade = 5; sp::mlsa_filter mlsa_f(order, alpha, n_pade); ... // MLSA フィルタリング 出力一サンプル = mlsa_f.filter(入力一サンプル, フィルタ係数); 何で再実装したのか mlsa filterをC++的なインタフェースで使いたかった コード見たらまったく意味がわからなくて、意地でも理解してやろうと思った 反省はしている 知り合いの声質変換やってる方がMLSAフィルタを波形合成に使ってるっていうし、ちょっとやってみようかなって あと最近音声合成の低レベルに手をつけようとと思ってたし勉強にもなるかなって 思ったんだ……んだ…だ… 車輪の再開発はあんま良くないと思ってるけど許して。 誰かがリファクタせないかんのだ\n感想 SPTKのmlsa filterは、正直に言うとこれまで読んできたコードの中で一二を争うほど難解でした（いうてC言語はあまり読んできてないので、Cだとこれが普通なのかもしれないけど）。特に、元コードの d: delayという変数の使われ方が複雑過ぎて、とても読みにくくございました。MLSAフィルタは複数のbase filterのcascade接続で表されるわけだけど、それぞれの遅延が一つのdという変数で管理されていたのです。つまり、\nd[1] ~ d[5] までは、あるフィルタの遅延 d[6] ~ d[11] までは、別のフィルタの遅延 d[12] ~ にはまた別のフィルタの遅延 という感じです。\n改善しようと思って、base filterというクラスを作ってそのクラスの状態として各フィルタの遅延を持たせて、見通しを良くしました\nさいごに MLSAフィルタ、難しいですね（小並感\nいつかリアルタイム声質変換がやってみたいので、それに使う予定（worldを使うことになるかもしれんけど）。戸田先生当たりがやってる声質変換を一回真似してみたいと思ってる\n","date":1385856000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1385856000,"objectID":"f9defa1e0462843e3639c94b352a5933","permalink":"https://r9y9.github.io/blog/2013/12/01/mlsa-filter-with-c-plus-plus/","publishdate":"2013-12-01T00:00:00Z","relpermalink":"/blog/2013/12/01/mlsa-filter-with-c-plus-plus/","section":"post","summary":"https://gist.github.com/r9y9/7735120","tags":["Speech Signal Processing","Speech-Synthesis","C++","SPTK"],"title":"MLSA digital filter のC++実装","type":"post"},{"authors":null,"categories":null,"content":"音声信号処理ツールキットSPTKをC++から使おうと思ったら意外とハマってしまったので、\nC++から使えるようにC++コンパイラでコンパイルできるようにした 使いやすいようにwafを組み込みんだ リポジトリ: https://github.com/r9y9/SPTK\nというわけで、使い方について簡単に書いておく\nSPTK について SPTKを使うと何ができるか: SPTKの使い方 (1) インストール・波形描画・音声再生 | 人工知能に関する断創録 SPTKとは: Speech Signal Processing Toolkit (SPTK) SPTK with waf SPTK with wafは、SPTKをwafでビルド管理できるようにしたものです。\nSPTKを共有ライブラリとしてインストールできます。 C、C++の好きな方でコンパイルできます。 wafが使えます（速い、出力がキレイ） 自分のC、C++コードからSPTKのメソッドを呼べます。 コマンドラインツールはインストールされません。 コマンドラインツールを使いたい人は、元のconfigure scriptを使えば十分です。\n環境 Unix系 Ubuntu 12.04 LTS 64 bitとMac OS X 10.9では確認済み\nSPTKのインストール リポジトリをクローンしたあと、\nBuild ./waf configure \u0026amp;\u0026amp; ./waf Build with clang++ CXX=clang++ ./waf configure \u0026amp;\u0026amp; ./waf Build with gcc git checkout c ./waf configure \u0026amp;\u0026amp; ./waf Build with clang git checkout c CC=clang ./waf configure \u0026amp;\u0026amp; ./waf Install sudo ./waf install Include files: /usr/local/include/SPTK Library: /usr/local/lib/SPTK Pkg-config: /usr/local/lib/pkgconfig オリジナルのSPTKとはインストール場所が異なります（オリジナルは、/usr/local/SPTK）\nSPTKを使ってコードを書く \u0026lt;SPTK/SPTK.h\u0026gt; をインクルードして、好きな関数を呼ぶ\nコンパイルは、例えば以下のようにする\ng++ test.cpp `pkg-config SPTK --cflags --libs` 面倒なので、example/ 内のコードを修正して使う（wafを使おう）のがおすすめです。\nきっかけ SPTKはコマンドラインツールだと思ってたけど、どうやらSPTK.hをインクルードすれば一通りのツールを使えるらしい SPTK.hをインクルードして使う方法のマニュアルが見つからない… SPTKはC言語で書かれてるし、C++から使うの地味にめんどくさい C++から簡単に使いたかった gccやclangだけじゃなくg++やclang++でコンパイルできるようにしよう 自分のコードのビルド管理にはwafを使ってるし、wafで管理できるようにしてしまおう waf素晴らしいしな （参考: waf チュートリアル | 純粋関数型雑記帳 ） 最後に SPTKもwafも素晴らしいので積極的に使おう＾＾\n","date":1385856000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1385856000,"objectID":"448da364b62d45e999cc4b68effbd1bd","permalink":"https://r9y9.github.io/blog/2013/12/01/sptk-with-waf/","publishdate":"2013-12-01T00:00:00Z","relpermalink":"/blog/2013/12/01/sptk-with-waf/","section":"post","summary":"https://github.com/r9y9/SPTK","tags":["Speech Signal Processing","SPTK"],"title":"SPTKをC++から使えるようにする","type":"post"},{"authors":null,"categories":null,"content":"MFCC とは Mel-Frequency Cepstral Coefficients (MFCCs) のこと。音声認識でよく使われる、音声の特徴表現の代表的なもの。\n算出手順 音声信号を適当な長さのフレームで切り出し 窓がけ フーリエ変換して対数振幅スペクトルを求める メルフィルタバンクを掛けて、メル周波数スペクトルを求める 離散コサイン変換により、MFCCを求める 以上。SPTKのmfccコマンドのソースもだいたいそうなってた。\nさて ここに音声波形があるじゃろ？？ ","date":1385251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1385251200,"objectID":"c631a6ed0e973bbd1ab5f68c5ad8d265","permalink":"https://r9y9.github.io/blog/2013/11/24/mfcc-calculation-memo/","publishdate":"2013-11-24T00:00:00Z","relpermalink":"/blog/2013/11/24/mfcc-calculation-memo/","section":"post","summary":"Mel-Frequency Cepstral Coefficients (MFCCs)","tags":["Speech Signal Processing","SPTK"],"title":"MFCCの計算方法についてメモ","type":"post"},{"authors":null,"categories":null,"content":"","date":1383436800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1383436800,"objectID":"27dad77497e6d31db53ad92f41b7d662","permalink":"https://r9y9.github.io/blog/2013/11/03/mit-media-lab-talk-participated/","publishdate":"2013-11-03T00:00:00Z","relpermalink":"/blog/2013/11/03/mit-media-lab-talk-participated/","section":"post","summary":"[TOKYO DESIGNES WEEK MIT Media Lab 特別フォーラム](https://www.tdwa.com/tdw/special/forum/mitmedialab.html)","tags":["Daily"],"title":"MIT Media Lab 特別フォーラムに参加してきた","type":"post"},{"authors":null,"categories":null,"content":"やったのでメモ。おそらく正しくできたと思う。結果貼っとく。ウェーブレットの参考は以下の文献\nTorrence, C. and G.P. Compo “A Practical Guide to Wavelet Analysis”, Bull. Am. Meteorol. Soc., 79, 61–78, 1998.\nウェーブレットの条件 マザーウェーブレットはmorletを使う\n\\begin{align} \\psi_{0}(\\eta) = \\pi^{-1/4}e^{i\\omega_{0}\\eta}e^{-\\eta^{2}/2} \\end{align} 文献に従って$\\omega_{0} = 6.0$とした。\n以下にいっぱい図を張る。軸は適当\n元の信号 ","date":1382313600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1382313600,"objectID":"5ca62f4359f298271387c5f986e1a9b6","permalink":"https://r9y9.github.io/blog/2013/10/21/signal-reconstruction-using-invere-cwt/","publishdate":"2013-10-21T00:00:00Z","relpermalink":"/blog/2013/10/21/signal-reconstruction-using-invere-cwt/","section":"post","summary":"[A Practical Guide to Wavelet Analysis](https://paos.colorado.edu/research/wavelets/bams_79_01_0061.pdf)","tags":["Inverse CWT","Wavelet Transform","Signal Processing"],"title":"逆連続ウェーブレット変換による信号の再構成","type":"post"},{"authors":null,"categories":null,"content":"","date":1382227200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1382227200,"objectID":"297b95ae83d9947bba2799716fad2a15","permalink":"https://r9y9.github.io/blog/2013/10/20/continuous-wavelet-tranform/","publishdate":"2013-10-20T00:00:00Z","relpermalink":"/blog/2013/10/20/continuous-wavelet-tranform/","section":"post","summary":"[A Practical Guide to Wavelet Analysis](https://paos.colorado.edu/research/wavelets/bams_79_01_0061.pdf)","tags":["Signal Processing","Wavelett Transform","Continuous Wavelet Transform","FFT"],"title":"FFTを使った連続ウェーブレット変換の高速化","type":"post"},{"authors":null,"categories":null,"content":"音声合成に使われるMLSA（Mel-Log Spectrum Approximatation）フィルタを実装したいんだが、なにぶんわからん。SPTKにコードはあるけれど、正直理解できない。デジタル信号処理を小学一年生から勉強しなおしたいレベルだ\nと、前置きはさておき、MLSAフィルタの実装を見つけたのでメモ。ここ最近ちょくちょく調べているが、SPTK以外で初めて見つけた。\nRealisation and Simulation of the Mel Log Spectrum Approximation Filter | Simple4All Internship Report\nSimple4Allという音声技術系のコミュニティの、学生さんのインターンの成果らしい。ちらっと調べてたら山岸先生も参加してる（た？）っぽい。\n上のreportで引用されているように、MLSA filterの実現方法については、益子さんのD論に詳しく書いてあることがわかった。今井先生の論文と併せて読んでみようと思う。\nT. Masuko, “HMM-Based Speech Synthesis and Its Applications”, Ph.D Thesis, 2002.\nもう正直わからんしブラックボックスでもいいから既存のツール使うかーと諦めかけていたところで割りと丁寧な実装付き解説を見つけたので、もう一度勉強して実装してみようと思い直した。\n機械学習にかまけて信号処理をちゃんと勉強していなかったつけがきている。LMA filterもMLSA filterも、本当にわからなくてツライ……\n(実装だけであれば、実はそんなに難しくなかった 2013/09後半)\n追記 2015/02/25 誤解を生む表現があったので、直しました\n","date":1379894400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1379894400,"objectID":"742443df486171777577d74d969e996e","permalink":"https://r9y9.github.io/blog/2013/09/23/mlsa-filter-wakaran/","publishdate":"2013-09-23T00:00:00Z","relpermalink":"/blog/2013/09/23/mlsa-filter-wakaran/","section":"post","summary":"https://gist.github.com/r9y9/7735120","tags":["Speech Signal Processing","Speech-Synthesis"],"title":"MLSA フィルタの実装","type":"post"},{"authors":null,"categories":null,"content":"HPSSとは（一行説明） HPSS（Harmonic/Percussive Sound Separation）というのは、音源中の調波音/打楽器音が、それぞれ時間方向に滑らか/周波数方向に滑らかという異った性質を持つことを利用して、両者を分離する方法のこと。わからんければ論文へ\nアイデアはシンプル、実装は簡単、効果は素晴らしい。specmurtに似たものを感じる。ということで少し感動したので結果を載せる\n実装 調波音のスペクトログラムを$H$、打楽器音のスペクトログラムを$P$、時間indexをt、周波数indexをkとして、以下の数式をそのまま実装して、適当に反復計算すればおｋ\n\\begin{align} |H_{t, k}| = \\frac{w_{H}^2 (|H_{t+1,k}| + |H_{t-1,k}|)^2 |W_{t,k}|}{w_{H}^2 (|H_{t+1,k}| + |H_{t-1,k}|)^2 + w_{P}^2(|P_{t,k+1}| + |P_{t,k-1}|)^2} \\end{align} \\begin{align} |P_{t, k}| = \\frac{w_{P}^2 (|P_{t,k+1}| + |P_{t,k-1}|)^2 |W_{t,k}|}{w_{H}^2 (|H_{t+1,k}| + |H_{t-1,k}|)^2 + w_{P}^2(|P_{t,k+1}| + |P_{t,k-1}|)^2} \\end{align} ただし\n\\begin{align} |W_{t,k}| = |H_{t,k}| + |P_{t,k}| \\end{align} 絶対値はパワースペクトル。論文中の表記とはけっこう違うので注意。厳密ではないです。$w_{H}, w_{P}$は重み係数で、両方共1.0くらいにしとく。\nHPSSの論文はたくさんあるけど、日本語でかつ丁寧な “スペクトルの時間変化に基づく音楽音響信号からの歌声成分の強調と抑圧” を参考にした。\nH/Pから音源を再合成するときは、位相は元の信号のものを使えばおｋ\n一点だけ、HとPの初期値どうすればいいんかなぁと思って悩んだ。まぁ普通に元音源のスペクトログラムを両方の初期値としてやったけど、うまく動いてるっぽい。\n結果 フリー音源でテストしてみたので、結果を貼っとく。$w_{H}=1.0, w_{P}=1.0$、サンプリング周波数44.1kHz、モノラル、フレーム長512、窓関数はhanning。反復推定の回数は30。音源は、歌もの音楽素材：歌入り素材系のフリー音楽素材一覧 から使わせてもらいました。ありがとうございまっす。元音源だけステレオです。 18秒目くらいからを比較すると効果がわかりやすいです\n元音源 Hのみ取り出して再合成した音源 Pのみ取り出して再合成した音源 それにしても特に泥臭い努力をせずに、このクオリティーが出せるのはすごい。音源に対する事前知識も何もないし。あと、ちょっとノイズが載ってるのはたぶんプログラムミス。つらたーん\nコレ以外にも多重HPSSとかもやったけど、いやーおもしろい手法だなーと思いました（こなみ\n詳しくは論文へ（僕のじゃないけど\n参考 橘 秀幸, 小野 順貴, 嵯峨山 茂樹, “スペクトルの時間変化に基づく音楽音響信号からの歌声成分の強調と抑圧”, 情報処理学会研究報告, vol. 2009-MUS-81(12), pp. 1-6, 2009. ","date":1379116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1379116800,"objectID":"bd09acaa9ce734ba4c8995f37dbd99d7","permalink":"https://r9y9.github.io/blog/2013/09/14/hpss/","publishdate":"2013-09-14T00:00:00Z","relpermalink":"/blog/2013/09/14/hpss/","section":"post","summary":"[スペクトルの時間変化に基づく音楽音響信号からの歌声成分の強調と抑圧](https://ci.nii.ac.jp/naid/110007997346)","tags":["Music Signal Processing","Source Separation","Matrix Factorization"],"title":"調波打楽器音分離（HPSS）を試す","type":"post"},{"authors":null,"categories":null,"content":"前回は学習アルゴリズムを導出したので、今回はそれを実装する。Gaussian Naive Bayesのみやった。例によって、アルゴリズムを書く時間よりも言語の使い方等を調べてる時間などの方が圧倒的に多いという残念感だったけど、とりあえずメモる。python, numpy, scipy, matplotlibすべて忘れてた。どれも便利だから覚えよう…\nそもそもナイーブベイズやろうとしてたのも、MNISTのdigit recognitionがやりたかったからなので、実際にやってみた。\nコードはgithubに置いた https://github.com/r9y9/naive_bayes\n結果だけ知りたい人へ：正解率 76 %くらいでした。まぁこんなもんですね\n手書き数字認識 手書き数字の画像データから、何が書かれているのか当てる。こういうタスクを手書き数字認識と言う。郵便番号の自動認識が有名ですね。\n今回は、MNISTという手書き数字のデータセットを使って、0〜9の数字認識をやる。MNISTについて詳しくは本家へ→THE MNIST DATABASE of handwritten digits ただし、MNISTのデータセットは直接使わず、Deep Learningのチュートリアルで紹介されていた（ここ）、pythonのcPickleから読める形式に変換されているデータを使った。感謝\nとりあえずやってみる $ git clone https://github.com/r9y9/naive_bayes $ cd naive_bayes $ python mnist_digit_recognition.py プログラムの中身は以下のようになってる。\nMNISTデータセットのダウンロード モデルの学習 テスト 実行すると、学習されたGaussianの平均が表示されて、最後に認識結果が表示される。今回は、単純に画像のピクセル毎に独立なGaussianを作ってるので、尤度の計算にめちゃくちゃ時間かかる。実装のせいもあるけど。なので、デフォでは50サンプルのみテストするようにした。\n学習されたGaussianの平均 ","date":1375747200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1375747200,"objectID":"a6cb2d7a04d6bc43c3c41e5a0de0caac","permalink":"https://r9y9.github.io/blog/2013/08/06/naive-bayes-mnist/","publishdate":"2013-08-06T00:00:00Z","relpermalink":"/blog/2013/08/06/naive-bayes-mnist/","section":"post","summary":"https://github.com/r9y9/naive_bayes","tags":["Machine Learning","Digit Recognition","MNIST"],"title":"Naive Bayesの復習（実装編）: MNISTを使って手書き数字認識","type":"post"},{"authors":null,"categories":null,"content":"些細な違いなんだけど調べたのでメモ。Multinomial distributionは多項分布のこと。Categorical distributionは、一般的な日本語表現が見つからなかった（なのでタイトルは英語）。打つのが大変なので、以下カテゴリカル分布と書く。\n結論としては、多項分布のn=1の特殊な場合がカテゴリカル分布ですよってこと。以下少しまとめる。\n分布を仮定する離散変数をカテゴリと呼ぶとして、\n多項分布は、n回試行したときに各カテゴリが何回出るかを表す確率分布 多項分布は、二項分布を多カテゴリに一般化したもの カテゴリカル分布は、多項分布のn=1の場合に相当する カテゴリカル分布は、ベルヌーイ分布を多カテゴリに一般化したもの 以上\nnokunoさんによるこの記事→ 多項分布の最尤推定 は、多項分布というよりカテゴリカル分布の話。本文には書いてあるけどね。あと最尤推定の結果はどちらにしろ同じなんだけどね\n導出メモ 一応最尤推定をやってみる。前回のナイーブベイズのメモの時は省略したので。入力の変数を $ Y = {y_n}_{n=1}^{N} $ とする。\nカテゴリカル分布 \\begin{align} p(l) = \\pi_{l}, \\hspace{2mm} \\sum_{l=1}^{L}\\pi_{l} = 1 \\end{align} ここで、$\\pi_{l}$がパラメータ、lはカテゴリの番号\n最尤推定 尤度関数を立てて、最大化することでパラメータを求める。各データは独立に生起すると仮定すると、尤度関数は以下のようになる。\n\\begin{align} L(Y; \\theta) = \\prod_{n=1}^{N} \\pi_{y_{n}} \\end{align} $\\theta$はパラメータの集合ということで。\nラベルlの出現回数を$N_{l} = \\sum_{n=1}^{N} \\delta (y_{n} = l)$とすると、次のように書き直せる。\n\\begin{align} L(Y; \\theta) = \\prod_{l=1}^{L}\\pi_{l}^{N_{l}} \\end{align} よって、対数尤度は以下のようになる。\n\\begin{align} \\log L(Y; \\theta) = \\sum_{l=1}^{L} N_{l}\\log \\pi_{l} \\end{align} ラグランジュの未定乗数法で解く nokunoさんの記事の通りだけど、一応手でも解いたのでメモ\n\\begin{align} G = \\sum_{l=1}^{L} N_{l}\\log \\pi_{l} + \\lambda \\Bigl[ \\sum_{l=1}^{L} \\pi_{l} -1) \\Bigr] \\end{align} として、 \\begin{align} \\frac{\\partial G}{\\partial \\pi_{l}} = \\frac{N_{l}}{\\pi_{l}} + \\lambda =0 \\end{align} よって、\n\\begin{align} \\pi_{l} = -\\frac{N_{l}}{\\lambda} \\end{align} ここで、以下の制約条件に代入すると、\n\\begin{align} \\sum_{l=1}^{L} \\pi_{l} = 1 \\end{align} $\\lambda = -N$となることがわかるので、求めたかったパラメータは以下のようになる\n\\begin{align} \\pi_{l} = \\frac{N_{l}}{N} \\end{align} カテゴリの頻度を計算するだけ、カンタン！！\n参考 Categorical distribution - Wikipedia Multinomial distribution - Wikipedia 多項分布の最尤推定 - nokunoの日記 多項分布の最尤推定とMAP推定 - 睡眠時間？ Categorical distribution - Researcher’s Eye ","date":1375228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1375228800,"objectID":"99abdf15b0db974ad8b4458b336fdc8b","permalink":"https://r9y9.github.io/blog/2013/07/31/multinomial-categorical-diff/","publishdate":"2013-07-31T00:00:00Z","relpermalink":"/blog/2013/07/31/multinomial-categorical-diff/","section":"post","summary":"[Categorical distribution - Wikipedia](http://en.wikipedia.org/wiki/Categorical_distribution), [Multinomial distribution - Wikipedia](http://en.wikipedia.org/wiki/Multinomial_distribution)","tags":["Probability"],"title":"Multinomial distributionとCategorical distributionの違い","type":"post"},{"authors":null,"categories":null,"content":"すぐ忘れるのでメモ。ナイーブベイズの学習アルゴリズムの導出とか、そもそもナイーブベイズが定番過ぎて意外とやったことなかった気もするので、復習がてらやってみた。\nちょっと修正 2013/07/30\nナイーブベイズについて整理 学習アルゴリズムの導出 Naive bayes （ナイーブベイズ） スパムフィルタで使われたことで有名な確率モデルで、シンプルだけどそこそこ実用的なのが良い所。Naive bayesという名前は、特徴ベクトル間に条件付き独立性を仮定してることにある（実際は相関あることが多いけど、まぁ簡単のためって感じ）。具体的に例を挙げて言うと、例えば文書分類タスクの場合、各単語は独立に生起するという仮定を置くことに相当する。\nまずはモデルを書き下す。入力データを$\\mathbf{x}$（D次元）、ラベルを$y$（離散値）とすると、ナイーブベイズでは以下のように同時確率をモデル化する。\n\\begin{align} p(\\mathbf{x}, y) \u0026amp;= p(y)p(\\mathbf{x}|y)\\\\ \u0026amp;= p(y)p(x_{1}, x_{2}, \\dots, x_{D}|y)\\\\ \u0026amp;= p(y)\\prod_{d=1}^{D} p(x_{d}|y) \\end{align} カンタン。基本的にdは次元に対するインデックス、nはデータに対するインデックスとして書く。\nポイントは特徴ベクトル間に条件付き独立性の仮定を置いていること（二度目）で、それによってパラメータの数が少なくて済む。\n分類 一番確率の高いラベルを選べばいい。数式で書くと以下のようになる。\n\\begin{align} \\hat{y} \u0026amp;= \\argmax_{y} [p(y|\\mathbf{x})]\\\\ \u0026amp;= \\argmax_{y} [p(\\mathbf{x}, y)]\\\\ \u0026amp;= \\argmax_{y} \\Bigl[ p(y)\\prod_{d=1}^{D} p(x_{d}|y)\\Bigr] \\end{align} argmaxを取る上では、$y$に依存しない項は無視していいので、事後確率の最大化は、同時確率の最大化に等しくなる。\n学習アルゴリズムの導出 ここからが本番。学習データを$X = {\\mathbf{x}_{n}}_{n=1}^{N}$、対応する正解ラベルを$Y = {y_n}_{n=1}^{N} $として、最尤推定により学習アルゴリズムを導出する。実際はMAP推定をすることが多いけど、今回は省略。拡張は簡単。\n尤度関数 各サンプルが独立に生起したと仮定すると、尤度関数は以下のように書ける。\n\\begin{align} L(X,Y; \\mathbf{\\theta}) \u0026amp;= \\prod_{n=1}^{N}p(y_{n})p(\\mathbf{x_{n}}|y_{n})\\\\ \u0026amp;= \\prod_{n=1}^{N} \\Bigl[ p(y_{n})\\prod_{d=1}^{D}p(x_{nd}|y_{n})\\Bigr] \\end{align} 対数を取って、\n\\begin{align} \\log L(X,Y; \\mathbf{\\theta}) = \\sum_{n=1}^{N}\\Bigl[\\log p(y_{n}) + \\sum_{d=1}^{D}\\log p(x_{nd}|y_{n})\\Bigr] \\end{align} 学習アルゴリズムは、この関数の最大化として導くことができる。\nところで 特徴ベクトルにどのような分布を仮定するかでアルゴリズムが少し変わるので、今回は以下の二つをやってみる。\nベルヌーイ分布 正規分布 前者は、binary featureを使う場合で、後者は、continuous featureを使う場合を想定してる。画像のピクセル値とか連続値を扱いたい場合は、正規分布が無難。その他、多項分布を使うこともあるけど、ベルヌーイ分布の場合とほとんど一緒なので今回は省略\nラベルに対する事前分布は、ラベルが離散値なので多項分布（間違ってた）categorical distributionとする。日本語でなんて言えばいいのか…wikipedia 参考\nBernoulli naive bayes 特徴ベクトルにベルヌーイ分布を仮定する場合。0 or 1のbinary featureを使う場合にはこれでおｋ．ベルヌーイ分布は以下\n\\begin{align} p(x;q) = q^{x}(1-q)^{1-x} \\end{align} 特徴ベクトルに対するパラメータは、ラベル数×特徴ベクトルの次元数（L×D）個ある。対数尤度関数（Gとする）は、以下のように書ける。\n\\begin{align} G \u0026amp;= \\sum_{n=1}^{N}\\Bigl[ \\log \\pi_{y_{n}} \\notag \\\\ \u0026amp;+ \\sum_{d=1}^{D} \\bigl[ x_{nd} \\log q_{y_{n}d} + (1-x_{nd}) \\log (1-q_{y_{n}d}) \\bigr] \\Bigr] \\end{align} ここで、$\\pi_{y_{n}}$ はcategorical distributionのパラメータ。\n微分方程式を解く あとは微分してゼロ。ラベルに対するインデックスをl 、学習データ中のラベルlが出現する回数を$N_{l} = \\sum_{n=1}^{N} \\delta(y_{n}= l)$、さらにその中で$x_{nd}=1 $となる回数を$N_{ld} = \\sum_{n=1}^{N} \\delta(y_{n}= l) \\cdot x_{nd} $とすると、\n\\begin{align} \\frac{\\partial G}{\\partial q_{ld}} \u0026amp;= \\frac{N_{ld}}{q_{ld}} - \\frac{N_{l} - N_{ld}}{1-q_{ld}} = 0 \\end{align} よって、\n\\begin{align} q_{ld} = \\frac{N_{ld}}{N_{l}} \\label{eq:naive1} \\end{align} できました。厳密に数式で書こうとするとめんどくさい。日本語で書くと、\n\\begin{align} パラメータ = \\frac{特徴ベクトルの出現回数}{ラベルの出現回数} \\end{align} って感じでしょうか。\ncategoricalのパラメータについては、めんどくさくなってきたのでやらないけど、もう直感的に以下。ラグランジュの未定定数法でおｋ\n\\begin{align} \\pi_{l} = \\frac{N_{l}}{N} \\label{eq:naive2} \\end{align} 学習は、式 ($\\ref{eq:naive1}$)、($\\ref{eq:naive2}$) を計算すればおｋ．やっと終わった。。。長かった。\nGaussian naive bayes 次。$x$が連続変数で、その分布に正規分布（Gaussian）を仮定する場合。まず、正規分布は以下のとおり。\n\\begin{align} p(x; \\mu, \\sigma^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\Bigl\\{-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\Bigr\\} \\end{align} 正規分布を使う場合、特徴ベクトルに対するパラメータは、ラベル数×特徴ベクトルの次元数×2個ある。×2となっているのは、平均と分散の分。対数尤度関数は、以下のようになる\n\\begin{align} G \u0026amp;= \\sum_{n=1}^{N}\\Bigl[ \\log \\pi_{y_{n}} \\notag \\\\ \u0026amp;+ \\sum_{d=1}^{D} \\bigl[ -\\frac{1}{2}\\log 2\\pi - \\log\\sigma_{y_{n}d} - \\frac{(x_{nd}-\\mu_{y_{n}d})^2}{2\\sigma_{y_{n}d}} \\bigr] \\Bigr] \\end{align} 微分方程式を解く 計算は省略するけど、偏微分してゼロと置けば、結果は以下のようになる。式が若干煩雑だけど、基本的には正規分布の最尤推定をしてるだけ。\n\\begin{align} \\mu_{ld} = \\frac{1}{N_{l}} \\sum_{n=1}^{N} x_{nd} \\cdot \\delta(y_{n} =l) = \\frac{N_{ld}}{N_{l}} \\label{eq:naive3} \\end{align} \\begin{align} \\sigma_{ld} = \\frac{1}{N_{l}} \\sum_{n=1}^{N} (x_{nd}-\\mu_{ld})^{2} \\cdot \\delta (y_{n}= l) \\label{eq:naive4} \\end{align} 学習では、式 ($\\ref{eq:naive2}$)、($\\ref{eq:naive3}$)、($\\ref{eq:naive4}$)を計算すればおｋ．式 ($\\ref{eq:naive3}$)は式 ($\\ref{eq:naive1}$)と一緒なんだけど、正規分布の場合はxが連続値なので注意。分散が特徴ベクトルの次元によらず一定とすれば、パラメータの数をぐっと減らすこともできる。\nおわりに これで終わり。予想以上に書くのに時間かかった…。今日logistic regressionを見直してて、ふとnaive bayesやったことないなーと思って、まぁ試すだけならscipy使えば一瞬なんだろうけどちょっと導出までやってみようと思った。\n実装編→Naive Bayesの復習（実装編）: MNISTを使って手書き数字認識\n参考 scikit.learn手法徹底比較！ ナイーブベイズ編Add Star - Risky Dune Gaussian Naïve Bayes, andLogistic Regression ナイーブベイズを用いたテキスト分類 - 人工知能に関する断創録 ","date":1374969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1374969600,"objectID":"cac6a00bda4c88b0a89907cc16e335f9","permalink":"https://r9y9.github.io/blog/2013/07/28/naive-bayes-formulation/","publishdate":"2013-07-28T00:00:00Z","relpermalink":"/blog/2013/07/28/naive-bayes-formulation/","section":"post","summary":"https://github.com/r9y9/naive_bayes","tags":["Machine Learning"],"title":"Naive Bayesの復習（導出編）","type":"post"},{"authors":null,"categories":null,"content":"はじめに シングルトラックにミックスダウンされた音楽から、その構成する要素（例えば、楽器とか）を分離したいと思うことがある。 音源分離と言えば、最近はNon-negative Matrix Factorization (非負値行列因子分解; NMF) が有名。 実装は非常に簡単だけど、実際にやってみるとどの程度の音源分離性能が出るのか気になったので、やってみる。\nと思ったけど、まずNMFについて整理してたら長くなったので、実装は今度にして、まずアルゴリズムを導出してみる。\n2014/10/19 追記 実装しました\nhttps://github.com/r9y9/julia-nmf-ss-toy\nNMFの問題設定 NMFとは、与えられた行列を非負という制約の元で因子分解する方法のこと。 音楽の場合、対象はスペクトログラムで、式で書くとわかりやすい。 スペクトログラムを $\\mathbf{Y}: [\\Omega \\times T] $ とすると、\n\\begin{align} \\mathbf{Y} \\simeq \\mathbf{H} \\mathbf{U} \\end{align} となる、$\\mathbf{H}: [\\Omega \\times K]、\\mathbf{U}: [K \\times T]$ を求めるのがNMFの問題。 ここで、Hが基底、Uがアクティビティ行列に相当する。 NMFは、元の行列Yと分解後の行列の距離の最小化問題として定式化できる。\n\\begin{align} \\mathbf{H}, \\mathbf{U} = \\mathop{\\rm arg~min}\\limits_{\\mathbf{H}, \\mathbf{U}} D (\\mathbf{Y}|\\mathbf{H}\\mathbf{U}), \\hspace{3mm} {\\rm subect\\ to} \\hspace{3mm} H_{\\omega,k}, U_{k, t} \u0026gt; 0 \\end{align} すごくシンプル。Dは距離関数で色んなものがある。ユークリッド距離、KLダイバージェンス、板倉斎藤距離、βダイバージェンスとか。\nユークリッド距離の最小化 ここではユークリッド距離（Frobeniusノルムともいう）として、二乗誤差最小化問題を解くことにする。 一番簡単なので。最小化すべき目的関数は次のようになる。\n\\begin{align} D (\\mathbf{Y}|\\mathbf{H}\\mathbf{U}) =\u0026amp; || \\mathbf{Y}-\\mathbf{HU}||_{F} \\\\ =\u0026amp; \\sum_{\\omega, k}|Y_{\\omega,t} - \\sum_{k}H_{\\omega, k}U_{k, t}|^{2} \\end{align} 行列同士の二乗誤差の最小化は、要素毎の二乗誤差の和の最小化ということですね。展開すると、次のようになる。\n\\begin{align} \\sum_{\\omega, k}|Y_{\\omega,t} - \\sum_{k}H_{\\omega, k}U_{k, t}|^{2} = \\sum_{\\omega, t}(|Y_{\\omega, t}|^2 -2Y_{\\omega, t} \\sum_{k}H_{\\omega, k}U_{k, t} + |\\sum_{k}H_{\\omega, k}U_{k, t}|^2) \\end{align} 微分してゼロ！としたいところだけど、3つ目の項を見ると、絶対値の中に和が入っているので、そうはいかない。 なので、補助関数法を使う。 基本的なアイデアは、目的関数の直接の最適化が難しい場合には、上界関数を立てることで間接的に最小化するということ。\n3項目に対してイェンセンの不等式を適応すると、\n\\begin{align} |\\sum_{k}H_{\\omega,k}U_{k,t}|^{2} \\le \\sum_{k} \\frac{H_{\\omega,k}^{2}U_{k, t}^{2}}{\\lambda_{k, \\omega, t}} \\end{align} これで、右辺は $ H_{\\omega,k}, U_{k, t} $ について二次関数になったので、微分できてはっぴー。 上の不等式を使えば、実際に最小化する目的関数は、次のようになる。\n\\begin{align} G := \\sum_{\\omega, t}(|Y_{\\omega, t}|^2 -2Y_{\\omega, t} \\sum_{k}H_{\\omega, k}U_{k, t} + \\sum_{k} \\frac{H_{\\omega,k}^{2}U_{k, t}^{2}}{\\lambda_{k, \\omega, t}}) \\end{align} Gを最小化すれば、間接的に元の目的関数も小さくなる。\n更新式の導出 あとは更新式を導出するだけ。 まず、目的関数を上から押さえるイメージで、イェンセンの不等式の等号条件から補助変数の更新式を求める。 この場合、kに関して和が1になることに注意して、\n\\begin{align} \\lambda_{k,\\omega,t} = \\frac{H_{\\omega, k}U_{k, t}}{\\sum_{k\u0026#39;}H_{\\omega, k\u0026#39;}U_{k\u0026#39;, t}} \\end{align} 次に、目的関数Gを $H_{\\omega,k}, U_{k,t} $で偏微分する。\n\\begin{align} \\frac{\\partial G}{\\partial H_{\\omega,k}} \u0026amp;= \\sum_{t} (-2 Y_{\\omega,t}U_{k,t} + 2 \\frac{H_{\\omega, k}U_{k, t}^2}{\\lambda_{k,\\omega,t}}) \u0026amp;= 0\\\\ \\frac{\\partial G}{\\partial U_{k, t}} \u0026amp;= \\sum_{\\omega} (-2 Y_{\\omega,t}H_{\\omega,k} + 2 \\frac{H_{\\omega, k}^2U_{k, t}}{\\lambda_{k,\\omega,t}}) \u0026amp;= 0 \\end{align} 少し変形すれば、以下の式を得る。\n\\begin{align} H_{\\omega,k} = \\frac{\\sum_{t}Y_{\\omega,t}U_{k,t}}{\\sum_{t}\\frac{U_{k, t}^2}{\\lambda_{k,\\omega,t}}}, \\hspace{3mm} U_{k,t} = \\frac{\\sum_{\\omega}Y_{\\omega,t}H_{\\omega,k}}{\\sum_{\\omega}\\frac{H_{\\omega, k}^2}{\\lambda_{k,\\omega,t}}} \\end{align} 補助変数を代入すれば、出来上がり。\n\\begin{align} H_{\\omega,k} = H_{\\omega,k} \\frac{\\sum_{t}Y_{\\omega,t}U_{k,t}}{\\sum_{t}U_{k, t}\\sum_{k\u0026#39;}H_{\\omega, k\u0026#39;}U_{k\u0026#39;, t}}, \\hspace{3mm} U_{k,t} = U_{k,t}\\frac{\\sum_{\\omega}Y_{\\omega,t}H_{\\omega,k}}{\\sum_{\\omega}H_{\\omega, k}\\sum_{k\u0026#39;}H_{\\omega, k\u0026#39;}U_{k\u0026#39;, t}} \\end{align} 行列表記で これで終わり…ではなく、もう少しスマートに書きたい。 ここで、少し実装を意識して行列表記を使って書きなおす。 行列の積は、AB（A: [m x n] 行列、B: [n x l] 行列）のようにAの列数とBの行数が等しくなることに注意して、 ほんの少し変形すれば最終的には次のように書ける。\n\\begin{align} H_{\\omega,k} \u0026amp;= H_{\\omega,k} \\frac{[\\mathbf{Y}\\mathbf{U}^{\\mathrm{T}}]_{\\omega,k}}{[\\mathbf{H}\\mathbf{U}\\mathbf{U}^{\\mathrm{T}}]_{\\omega,k}}, \\\\ U_{k,t} \u0026amp;= U_{k,t}\\frac{[\\mathbf{H}^{\\mathrm{T}}\\mathbf{Y}]_{k, t}}{[\\mathbf{H}^{\\mathrm{T}}\\mathbf{H}\\mathbf{U}]_{k,t}} \\end{align} 乗法更新式というやつですね。 元々の行列の要素が非負なら、掛けても非負のままですよってこと。 NMFのアルゴリズムは、この更新式を目的関数が収束するまで計算するだけ、簡単。Pythonなら数行で書ける。\nメモ 自分で導出していて思ったことをメモっておこうと思う。\n更新式は、行列の要素毎に独立して求められるんだなぁということ。 まぁ要素毎に偏微分して等式立ててるからそうなんだけど。更新の順番によって、収束する値、速度が変わるといったことはないんだろうか。 行列演算とスカラー演算が同じ式に同時に含まれていることがあるので注意。例えば、最終的な更新式の割り算は、要素毎のスカラー演算で、行列演算ではない。 何かいっぱいシグマがあるけど、めげない。計算ミスしやすい、つらい。 NMFという名前から行列操作を意識してしまうけど、更新式の導出の過程に行列の微分とか出てこない。更新式の導出は、行列の要素個々に対して行うイメージ。 NMFなんて簡単、と言われますが（要出典）、実際にやってみると結構めんどくさいなー、と思いました（小並感\n","date":1374883200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1374883200,"objectID":"e1642059091d4ac2c8b88168dea6b63e","permalink":"https://r9y9.github.io/blog/2013/07/27/nmf-euclid/","publishdate":"2013-07-27T00:00:00Z","relpermalink":"/blog/2013/07/27/nmf-euclid/","section":"post","summary":"https://github.com/r9y9/julia-nmf-ss-toy","tags":["Machine Learning","Matrix Factorization"],"title":"NMFアルゴリズムの導出（ユークリッド距離版）","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://r9y9.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]